{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/blocks/","title":"Block system","text":"<p><code>qadence</code> offers a block-based system to construct quantum circuits in a flexible manner.</p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock","title":"<code>AbstractBlock(tag=None, __array_priority__=1000)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for both primitive and composite blocks.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>A human-readable name attached to the block type. Notice, this is the same for all the class instances so it cannot be used for identifying different blocks</p> <p> TYPE: <code>str</code> </p> <code>qubit_support</code> <p>The qubit support of the block expressed as a tuple of integers</p> <p> TYPE: <code>tuple[int, ...]</code> </p> <code>tag</code> <p>A tag identifying a particular instance of the block which can be used for identification and pretty printing</p> <p> TYPE: <code>str | None</code> </p> <code>eigenvalues</code> <p>The eigenvalues of the matrix representing the block. This is used mainly for primitive blocks and it's needed for generalized parameter shift rule computations. Currently unused.</p> <p> TYPE: <code>list[float] | None</code> </p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.is_identity","title":"<code>is_identity</code>  <code>property</code>","text":"<p>Identity predicate for blocks.</p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.n_qubits","title":"<code>n_qubits()</code>","text":"<p>The number of qubits in the whole system.</p> <p>A block acting on qubit N would has at least n_qubits &gt;= N + 1.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef n_qubits(self) -&gt; int:\n    \"\"\"The number of qubits in the whole system.\n\n    A block acting on qubit N would has at least n_qubits &gt;= N + 1.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.n_supports","title":"<code>n_supports()</code>","text":"<p>The number of qubits the block is acting on.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef n_supports(self) -&gt; int:\n    \"\"\"The number of qubits the block is acting on.\"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.qubit_support","title":"<code>qubit_support()</code>","text":"<p>The indices of the qubit(s) the block is acting on.</p> <p>Qadence uses the ordering [0..,N-1] for qubits.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef qubit_support(self) -&gt; Tuple[int, ...]:\n    \"\"\"The indices of the qubit(s) the block is acting on.\n\n    Qadence uses the ordering [0..,N-1] for qubits.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#primitive-blocks","title":"Primitive blocks","text":""},{"location":"api/blocks/#qadence.blocks.primitive.ControlBlock","title":"<code>ControlBlock(control, target_block, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The abstract ControlBlock.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: tuple[int, ...],\n    target_block: PrimitiveBlock,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.control = control\n    self.blocks = (target_block,)\n    self.target = target_block.qubit_support\n\n    # using tuple expansion because some control operations could\n    # have multiple targets, e.g. CSWAP\n    super().__init__((*control, *self.target), noise=noise)  # target_block.qubit_support[0]))\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricBlock","title":"<code>ParametricBlock(qubit_support, noise=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>Parameterized primitive blocks.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricBlock.num_parameters","title":"<code>num_parameters()</code>  <code>abstractmethod</code>","text":"<p>The number of parameters required by the block.</p> <p>This is a class property since the number of parameters is defined automatically before instantiating the operation. Also, this could correspond to a larger number of actual user-facing parameters since any parameter expression is allowed</p> <p>Examples: - RX operation has 1 parameter - U operation has 3 parameters - HamEvo has 2 parameters (generator and time evolution)</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>@abstractmethod\ndef num_parameters(cls) -&gt; int:\n    \"\"\"The number of parameters required by the block.\n\n    This is a class property since the number of parameters is defined\n    automatically before instantiating the operation. Also, this could\n    correspond to a larger number of actual user-facing parameters\n    since any parameter expression is allowed\n\n    Examples:\n    - RX operation has 1 parameter\n    - U operation has 3 parameters\n    - HamEvo has 2 parameters (generator and time evolution)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricControlBlock","title":"<code>ParametricControlBlock(control, target_block, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The abstract parametrized ControlBlock.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: tuple[int, ...],\n    target_block: ParametricBlock,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.blocks = (target_block,)\n    self.control = control\n    self.parameters = target_block.parameters\n    super().__init__((*control, *target_block.qubit_support), noise=noise)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.PrimitiveBlock","title":"<code>PrimitiveBlock(qubit_support, noise=None)</code>","text":"<p>               Bases: <code>AbstractBlock</code></p> <p>Primitive blocks represent elementary unitary operations.</p> <p>Examples are single/multi-qubit gates or Hamiltonian evolution. See <code>qadence.operations</code> for a full list of primitive blocks.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.PrimitiveBlock.digital_decomposition","title":"<code>digital_decomposition()</code>","text":"<p>Decomposition into purely digital gates.</p> <p>This method returns a decomposition of the Block in a combination of purely digital single-qubit and two-qubit 'gates', by manual/custom knowledge of how this can be done efficiently. :return:</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def digital_decomposition(self) -&gt; AbstractBlock:\n    \"\"\"Decomposition into purely digital gates.\n\n    This method returns a decomposition of the Block in a\n    combination of purely digital single-qubit and two-qubit\n    'gates', by manual/custom knowledge of how this can be done efficiently.\n    :return:\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ProjectorBlock","title":"<code>ProjectorBlock(ket, bra, qubit_support, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The abstract ProjectorBlock.</p> <p>Arguments:</p> <pre><code>ket (str): The ket given as a bitstring.\nbra (str): The bra given as a bitstring.\nqubit_support (int | tuple[int]): The qubit_support of the block.\n</code></pre> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    ket: str,\n    bra: str,\n    qubit_support: int | tuple[int, ...],\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    \"\"\"\n    Arguments:\n\n        ket (str): The ket given as a bitstring.\n        bra (str): The bra given as a bitstring.\n        qubit_support (int | tuple[int]): The qubit_support of the block.\n    \"\"\"\n    if isinstance(qubit_support, int):\n        qubit_support = (qubit_support,)\n    if len(bra) != len(ket):\n        raise ValueError(\n            \"Bra and ket must be bitstrings of same length in the 'Projector' definition.\"\n        )\n    elif len(bra) != len(qubit_support):\n        raise ValueError(\"Bra or ket must be of same length as the 'qubit_support'\")\n    for wf in [bra, ket]:\n        if not all(int(item) == 0 or int(item) == 1 for item in wf):\n            raise ValueError(\n                \"All qubits must be either in the '0' or '1' state\"\n                \" in the 'ProjectorBlock' definition.\"\n            )\n\n    self.ket = ket\n    self.bra = bra\n    super().__init__(qubit_support, noise=noise)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ScaleBlock","title":"<code>ScaleBlock(block, parameter)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>Scale blocks are created when multiplying a block by a number or parameter.</p> <p>Example: <pre><code>from qadence import X\n\nprint(X(0) * 2)\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 X(0)\n</code></pre> </p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(self, block: AbstractBlock, parameter: Any):\n    self.block = block\n    # TODO: more meaningful name like `scale`?\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    super().__init__(block.qubit_support)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.TimeEvolutionBlock","title":"<code>TimeEvolutionBlock(qubit_support, noise=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>Simple time evolution block with time-independent Hamiltonian.</p> <p>This class is just a convenience class which is used to label blocks which contains simple time evolution with time-independent Hamiltonian operators</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#analog-blocks","title":"Analog blocks","text":"<p>To learn how to use analog blocks and how to mix digital &amp; analog blocks, check out the digital-analog section of the documentation.</p> <p>Examples on how to use digital-analog blocks can be found in the *examples folder of the qadence repo:</p> <ul> <li>Fit a simple sinus: <code>examples/digital-analog/fit-sin.py</code></li> <li>Solve a QUBO: <code>examples/digital-analog/qubo.py</code></li> </ul>"},{"location":"api/blocks/#qadence.blocks.analog.AnalogChain","title":"<code>AnalogChain(blocks)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogComposite</code></p> <p>A chain of analog blocks.</p> <p>Needed because analog blocks require stricter validation than the general <code>ChainBlock</code>.</p> <p><code>AnalogChain</code>s can only be constructed from <code>AnalogKron</code> blocks or globally supported, primitive, analog blocks (like <code>InteractionBlock</code>s and <code>ConstantAnalogRotation</code>s).</p> <p>Automatically constructed by the <code>chain</code> function if only analog blocks are given.</p> <p>Example: <pre><code>from qadence import X, chain, AnalogInteraction\n\nb = chain(AnalogInteraction(200), AnalogInteraction(200))\nprint(type(b))  # this is an `AnalogChain`\n\nb = chain(X(0), AnalogInteraction(200))\nprint(type(b))  # this is a general `ChainBlock`\n</code></pre> <pre><code>&lt;class 'qadence.blocks.analog.AnalogChain'&gt;\n&lt;class 'qadence.blocks.composite.ChainBlock'&gt;\n</code></pre> </p> Source code in <code>qadence/blocks/analog.py</code> <pre><code>def __init__(self, blocks: Tuple[AnalogBlock, ...]):\n    \"\"\"A chain of analog blocks.\n\n    Needed because analog blocks require\n    stricter validation than the general `ChainBlock`.\n\n    `AnalogChain`s can only be constructed from `AnalogKron` blocks or\n    _**globally supported**_, primitive, analog blocks (like `InteractionBlock`s and\n    `ConstantAnalogRotation`s).\n\n    Automatically constructed by the [`chain`][qadence.blocks.utils.chain]\n    function if only analog blocks are given.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, chain, AnalogInteraction\n\n    b = chain(AnalogInteraction(200), AnalogInteraction(200))\n    print(type(b))  # this is an `AnalogChain`\n\n    b = chain(X(0), AnalogInteraction(200))\n    print(type(b))  # this is a general `ChainBlock`\n    ```\n    \"\"\"\n    for b in blocks:\n        if not (isinstance(b, AnalogKron) or b.qubit_support.is_global):\n            raise ValueError(\"Only KronBlocks or global blocks can be chain'ed.\")\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.analog.AnalogKron","title":"<code>AnalogKron(blocks, interaction=Interaction.NN)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogComposite</code></p> <p>Stack analog blocks vertically (i.e. in time).</p> <p>Needed because analog require stricter validation than the general <code>KronBlock</code>.</p> <p><code>AnalogKron</code>s can only be constructed from non-global, analog blocks with the same duration.</p> Source code in <code>qadence/blocks/analog.py</code> <pre><code>def __init__(self, blocks: Tuple[AnalogBlock, ...], interaction: Interaction = Interaction.NN):\n    \"\"\"Stack analog blocks vertically (i.e. in time).\n\n    Needed because analog require\n    stricter validation than the general `KronBlock`.\n\n    `AnalogKron`s can only be constructed from _**non-global**_, analog blocks\n    with the _**same duration**_.\n    \"\"\"\n    if len(blocks) == 0:\n        raise NotImplementedError(\"Empty KronBlocks not supported\")\n\n    self.blocks = blocks\n    self.interaction = interaction\n\n    qubit_support = QubitSupport()\n    duration = blocks[0].duration\n    for b in blocks:\n        if not isinstance(b, AnalogBlock):\n            raise ValueError(\"Can only kron `AnalgoBlock`s with other `AnalgoBlock`s.\")\n\n        if b.qubit_support == QubitSupport(\"global\"):\n            raise ValueError(\"Blocks with global support cannot be kron'ed.\")\n\n        if not qubit_support.is_disjoint(b.qubit_support):\n            raise ValueError(\"Make sure blocks act on distinct qubits!\")\n\n        if not np.isclose(evaluate(duration), evaluate(b.duration)):\n            raise ValueError(\"Kron'ed blocks have to have same duration.\")\n\n        qubit_support += b.qubit_support\n\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.analog.ConstantAnalogRotation","title":"<code>ConstantAnalogRotation(tag=None, __array_priority__=1000, _eigenvalues_generator=None, parameters=ParamMap(alpha=0.0, duration=1000.0, omega=0.0, delta=0.0, phase=0.0), qubit_support=QubitSupport('global'), add_pattern=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogBlock</code></p> <p>Implements a constant analog rotation with interaction dictated by the chosen Hamiltonian.</p> <pre><code>H/h = \u2211\u1d62(\u03a9/2 cos(\u03c6)*X\u1d62 - sin(\u03c6)*Y\u1d62 - \u03b4n\u1d62) + H\u1d62\u2099\u209c.\n</code></pre> <p>To construct this block you can use of the following convenience wrappers: - The general rotation operation <code>AnalogRot</code> - Shorthands for rotatins around an axis:   <code>AnalogRX</code>,   <code>AnalogRY</code>,   <code>AnalogRZ</code></p> <p>WARNING: do not use <code>ConstantAnalogRotation</code> with <code>alpha</code> as differentiable parameter - use the convenience wrappers mentioned above.</p>"},{"location":"api/blocks/#qadence.blocks.analog.InteractionBlock","title":"<code>InteractionBlock(tag=None, __array_priority__=1000, _eigenvalues_generator=None, parameters=ParamMap(duration=1000.0), qubit_support=QubitSupport('global'), add_pattern=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogBlock</code></p> <p>Free-evolution for the Hamiltonian interaction term of a register of qubits.</p> <p>In real interacting quantum devices, it means letting the system evolve freely according to the time-dependent Schrodinger equation. With emulators, this block is translated to an appropriate interaction Hamiltonian, for example, an Ising interaction</p> <pre><code>H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c\n</code></pre> <p>or an XY-interaction</p> <pre><code>H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2083/r\u2c7c\u2c7c\u00b3 (X\u1d62X\u2c7c + Z\u1d62Z\u2c7c)\n</code></pre> <p>with <code>n\u1d62 = (1-Z\u1d62)/2</code>.</p> <p>To construct, use the <code>AnalogInteraction</code> function.</p>"},{"location":"api/blocks/#composite-blocks","title":"Composite blocks","text":""},{"location":"api/blocks/#qadence.blocks.utils.chain","title":"<code>chain(*args)</code>","text":"<p>Chain blocks sequentially.</p> <p>On digital backends this can be interpreted loosely as a matrix mutliplication of blocks. In the analog case it chains blocks in time.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to chain. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator, List[AbstractBlock]]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>ChainBlock</p> <p>Example: <pre><code>from qadence import X, Y, chain\n\nb = chain(X(0), Y(0))\n\n# or use a generator\nb = chain(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>ChainBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def chain(*args: Union[AbstractBlock, Generator, List[AbstractBlock]]) -&gt; ChainBlock:\n    \"\"\"Chain blocks sequentially.\n\n    On digital backends this can be interpreted\n    loosely as a matrix mutliplication of blocks. In the analog case it chains\n    blocks in time.\n\n    Arguments:\n        *args: Blocks to chain. Can also be a generator.\n\n    Returns:\n        ChainBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, chain\n\n    b = chain(X(0), Y(0))\n\n    # or use a generator\n    b = chain(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    # ugly hack to use `AnalogChain` if we are dealing only with analog blocks\n    if len(args) and all(\n        isinstance(a, AnalogBlock) or isinstance(a, AnalogComposite) for a in args\n    ):\n        return analog_chain(*args)  # type: ignore[return-value,arg-type]\n    return _construct(ChainBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.utils.kron","title":"<code>kron(*args)</code>","text":"<p>Stack blocks vertically.</p> <p>On digital backends this can be intepreted loosely as a kronecker product of blocks. In the analog case it executes blocks parallel in time.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to kron. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>KronBlock</p> <p>Example: <pre><code>from qadence import X, Y, kron\n\nb = kron(X(0), Y(1))\n\n# or use a generator\nb = kron(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def kron(*args: Union[AbstractBlock, Generator]) -&gt; KronBlock:\n    \"\"\"Stack blocks vertically.\n\n    On digital backends this can be intepreted\n    loosely as a kronecker product of blocks. In the analog case it executes\n    blocks parallel in time.\n\n    Arguments:\n        *args: Blocks to kron. Can also be a generator.\n\n    Returns:\n        KronBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, kron\n\n    b = kron(X(0), Y(1))\n\n    # or use a generator\n    b = kron(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    # ugly hack to use `AnalogKron` if we are dealing only with analog blocks\n    if len(args) and all(\n        isinstance(a, AnalogBlock) or isinstance(a, AnalogComposite) for a in args\n    ):\n        return analog_kron(*args)  # type: ignore[return-value,arg-type]\n    return _construct(KronBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.utils.add","title":"<code>add(*args)</code>","text":"<p>Sums blocks.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to add. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>AddBlock</code> <p>AddBlock</p> <p>Example: <pre><code>from qadence import X, Y, add\n\nb = add(X(0), Y(0))\n\n# or use a generator\nb = add(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def add(*args: Union[AbstractBlock, Generator]) -&gt; AddBlock:\n    \"\"\"Sums blocks.\n\n    Arguments:\n        *args: Blocks to add. Can also be a generator.\n\n    Returns:\n        AddBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, add\n\n    b = add(X(0), Y(0))\n\n    # or use a generator\n    b = add(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    return _construct(AddBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.AddBlock","title":"<code>AddBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Adds blocks.</p> <p>Constructed via <code>add</code>.</p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.ChainBlock","title":"<code>ChainBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Chains blocks sequentially.</p> <p>Constructed via <code>chain</code></p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.CompositeBlock","title":"<code>CompositeBlock(tag=None, __array_priority__=1000)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractBlock</code></p> <p>Block which composes multiple blocks into one larger block (which can again be composed).</p> <p>Composite blocks are constructed via <code>chain</code>, <code>kron</code>, and <code>add</code>.</p>"},{"location":"api/blocks/#qadence.blocks.composite.KronBlock","title":"<code>KronBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Stacks blocks horizontally.</p> <p>Constructed via <code>kron</code>.</p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    if len(blocks) == 0:\n        raise NotImplementedError(\"Empty KronBlocks not supported\")\n\n    qubit_support = QubitSupport()\n    for b in blocks:\n        assert (\n            QubitSupportType.GLOBAL,\n        ) != b.qubit_support, \"Blocks with global support cannot be kron'ed.\"\n        assert qubit_support.is_disjoint(\n            b.qubit_support\n        ), \"Make sure blocks act on distinct qubits!\"\n        qubit_support += b.qubit_support\n\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#converting-blocks-to-matrices","title":"Converting blocks to matrices","text":""},{"location":"api/blocks/#qadence.blocks.block_to_tensor.block_to_tensor","title":"<code>block_to_tensor(block, values={}, qubit_support=None, use_full_support=True, tensor_type=TensorType.DENSE, endianness=Endianness.BIG, device=None)</code>","text":"<p>Convert a block into a torch tensor.</p> PARAMETER DESCRIPTION <code>block</code> <p>The block to convert.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>values</code> <p>A optional dict with values for parameters.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>qubit_support</code> <p>The qubit_support of the block.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>use_full_support</code> <p>True infers the total number of qubits.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>tensor_type</code> <p>the target tensor type.</p> <p> TYPE: <code>TensorType</code> DEFAULT: <code>DENSE</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence import hea, hamiltonian_factory, Z, block_to_tensor\n\nblock = hea(2,2)\nprint(block_to_tensor(block))\n\n# In case you have a diagonal observable, you can use\nobs = hamiltonian_factory(2, detuning = Z)\nprint(block_to_tensor(obs, tensor_type=\"SparseDiagonal\"))\n</code></pre> <pre><code>tensor([[[ 0.0387+0.3724j, -0.3495-0.5767j, -0.3974-0.1981j, -0.4100+0.1995j],\n         [ 0.0401-0.4716j,  0.1132+0.3810j, -0.6205-0.2315j, -0.4046-0.1251j],\n         [-0.3800-0.3693j, -0.5508-0.1840j,  0.1208+0.3243j, -0.1543-0.4883j],\n         [-0.5209-0.2893j, -0.2202-0.0396j, -0.0544-0.4914j,  0.4233+0.4139j]]],\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\ntensor(indices=tensor([[0, 3],\n                       [0, 3]]),\n       values=tensor([ 2.+0.j, -2.+0.j]),\n       size=(4, 4), nnz=2, layout=torch.sparse_coo)\n</code></pre> </p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def block_to_tensor(\n    block: AbstractBlock,\n    values: dict[str, TNumber | torch.Tensor] = {},\n    qubit_support: tuple | None = None,\n    use_full_support: bool = True,\n    tensor_type: TensorType = TensorType.DENSE,\n    endianness: Endianness = Endianness.BIG,\n    device: torch.device = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a block into a torch tensor.\n\n    Arguments:\n        block (AbstractBlock): The block to convert.\n        values (dict): A optional dict with values for parameters.\n        qubit_support (tuple): The qubit_support of the block.\n        use_full_support (bool): True infers the total number of qubits.\n        tensor_type (TensorType): the target tensor type.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import hea, hamiltonian_factory, Z, block_to_tensor\n\n    block = hea(2,2)\n    print(block_to_tensor(block))\n\n    # In case you have a diagonal observable, you can use\n    obs = hamiltonian_factory(2, detuning = Z)\n    print(block_to_tensor(obs, tensor_type=\"SparseDiagonal\"))\n    ```\n    \"\"\"\n\n    # FIXME: default use_full_support to False. In general, it would\n    # be more efficient to do that, and make sure that computations such\n    # as observables only do the matmul of the size of the qubit support.\n\n    if tensor_type == TensorType.DENSE:\n        from qadence.blocks import embedding\n\n        (ps, embed) = embedding(block)\n        return _block_to_tensor_embedded(\n            block,\n            embed(ps, values),\n            qubit_support,\n            use_full_support,\n            endianness=endianness,\n            device=device,\n        )\n\n    elif tensor_type == TensorType.SPARSEDIAGONAL:\n        t = block_to_diagonal(block, endianness=endianness)\n        indices, values, size = torch.nonzero(t), t[t != 0], len(t)\n        indices = torch.stack((indices.flatten(), indices.flatten()))\n        return torch.sparse_coo_tensor(indices, values, (size, size))\n</code></pre>"},{"location":"api/constructors/","title":"Constructors for common quantum circuits","text":""},{"location":"api/constructors/#qadence.constructors.feature_maps.exp_fourier_feature_map","title":"<code>exp_fourier_feature_map(n_qubits, support=None, param='x', feature_range=None)</code>","text":"<p>Exponential fourier feature map.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the feature</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>qubit support</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>param</code> <p>name of feature <code>Parameter</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'x'</code> </p> <code>feature_range</code> <p>min and max value of the feature, as floats in a Tuple</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/constructors/feature_maps.py</code> <pre><code>def exp_fourier_feature_map(\n    n_qubits: int,\n    support: tuple[int, ...] = None,\n    param: str = \"x\",\n    feature_range: tuple[float, float] = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Exponential fourier feature map.\n\n    Args:\n        n_qubits: number of qubits in the feature\n        support: qubit support\n        param: name of feature `Parameter`\n        feature_range: min and max value of the feature, as floats in a Tuple\n    \"\"\"\n\n    if feature_range is None:\n        feature_range = (0.0, 2.0**n_qubits)\n\n    support = tuple(range(n_qubits)) if support is None else support\n    hlayer = kron(H(qubit) for qubit in support)\n    rlayer = feature_map(\n        n_qubits,\n        support=support,\n        param=param,\n        op=RZ,\n        fm_type=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.EXP,\n        feature_range=feature_range,\n        target_range=(0.0, 2 * PI),\n    )\n    rlayer.tag = None\n    return tag(chain(hlayer, rlayer), f\"ExpFourierFM({param})\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.feature_maps.feature_map","title":"<code>feature_map(n_qubits, support=None, param='phi', op=RX, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT, feature_range=None, target_range=None, multiplier=None, param_prefix=None)</code>","text":"<p>Construct a feature map of a given type.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Number of qubits the feature map covers. Results in <code>support=range(n_qubits)</code>.</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>Puts one feature-encoding rotation gate on every qubit in <code>support</code>. n_qubits in this case specifies the total overall qubits of the circuit, which may be wider than the support itself, but not narrower.</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>param</code> <p>Parameter of the feature map; you can pass a string or Parameter; it will be set as non-trainable (FeatureParameter) regardless.</p> <p> TYPE: <code>Parameter | str</code> DEFAULT: <code>'phi'</code> </p> <code>op</code> <p>Rotation operation of the feature map; choose from RX, RY, RZ or PHASE.</p> <p> TYPE: <code>RotationTypes</code> DEFAULT: <code>RX</code> </p> <code>fm_type</code> <p>Basis set for data encoding; choose from <code>BasisSet.FOURIER</code> for Fourier encoding, or <code>BasisSet.CHEBYSHEV</code> for Chebyshev polynomials of the first kind.</p> <p> TYPE: <code>BasisSet | Callable | str</code> DEFAULT: <code>FOURIER</code> </p> <code>reupload_scaling</code> <p>how the feature map scales the data that is re-uploaded for each qubit. choose from <code>ReuploadScaling</code> enumeration or provide your own function with a single int as input and int or float as output.</p> <p> TYPE: <code>ReuploadScaling | Callable | str</code> DEFAULT: <code>CONSTANT</code> </p> <code>feature_range</code> <p>range of data that the input data provided comes from. Used to map input data to the correct domain of the feature-encoding function.</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> <code>target_range</code> <p>range of data the data encoder assumes as the natural range. For example, in Chebyshev polynomials it is (-1, 1), while for Fourier it may be chosen as (0, 2*PI). Used to map data to the correct domain of the feature-encoding function.</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> <code>multiplier</code> <p>overall multiplier; this is useful for reuploading the feature map serially with different scalings; can be a number or parameter/expression.</p> <p> TYPE: <code>Parameter | TParameter | None</code> DEFAULT: <code>None</code> </p> <code>param_prefix</code> <p>string prefix to create trainable parameters multiplying the feature parameter inside the feature-encoding function. Note that currently this does not take into account the domain of the feature-encoding function.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <p>Example: <pre><code>from qadence import feature_map, BasisSet, ReuploadScaling\n\nfm = feature_map(3, fm_type=BasisSet.FOURIER)\nprint(f\"{fm = }\")\n\nfm = feature_map(3, fm_type=BasisSet.CHEBYSHEV)\nprint(f\"{fm = }\")\n\nfm = feature_map(3, fm_type=BasisSet.FOURIER, reupload_scaling = ReuploadScaling.TOWER)\nprint(f\"{fm = }\")\n</code></pre> <pre><code>fm = KronBlock(0,1,2) [tag: Constant Fourier FM]\n\u251c\u2500\u2500 RX(0) [params: ['phi']]\n\u251c\u2500\u2500 RX(1) [params: ['phi']]\n\u2514\u2500\u2500 RX(2) [params: ['phi']]\nfm = KronBlock(0,1,2) [tag: Constant Chebyshev FM]\n\u251c\u2500\u2500 RX(0) [params: ['acos(phi)']]\n\u251c\u2500\u2500 RX(1) [params: ['acos(phi)']]\n\u2514\u2500\u2500 RX(2) [params: ['acos(phi)']]\nfm = KronBlock(0,1,2) [tag: Tower Fourier FM]\n\u251c\u2500\u2500 RX(0) [params: ['1_0*phi']]\n\u251c\u2500\u2500 RX(1) [params: ['2_0*phi']]\n\u2514\u2500\u2500 RX(2) [params: ['3_0*phi']]\n</code></pre> </p> Source code in <code>qadence/constructors/feature_maps.py</code> <pre><code>def feature_map(\n    n_qubits: int,\n    support: tuple[int, ...] | None = None,\n    param: Parameter | str = \"phi\",\n    op: RotationTypes = RX,\n    fm_type: BasisSet | Callable | str = BasisSet.FOURIER,\n    reupload_scaling: ReuploadScaling | Callable | str = ReuploadScaling.CONSTANT,\n    feature_range: tuple[float, float] | None = None,\n    target_range: tuple[float, float] | None = None,\n    multiplier: Parameter | TParameter | None = None,\n    param_prefix: str | None = None,\n) -&gt; KronBlock:\n    \"\"\"Construct a feature map of a given type.\n\n    Arguments:\n        n_qubits: Number of qubits the feature map covers. Results in `support=range(n_qubits)`.\n        support: Puts one feature-encoding rotation gate on every qubit in `support`. n_qubits in\n            this case specifies the total overall qubits of the circuit, which may be wider than the\n            support itself, but not narrower.\n        param: Parameter of the feature map; you can pass a string or Parameter;\n            it will be set as non-trainable (FeatureParameter) regardless.\n        op: Rotation operation of the feature map; choose from RX, RY, RZ or PHASE.\n        fm_type: Basis set for data encoding; choose from `BasisSet.FOURIER` for Fourier\n            encoding, or `BasisSet.CHEBYSHEV` for Chebyshev polynomials of the first kind.\n        reupload_scaling: how the feature map scales the data that is re-uploaded for each qubit.\n            choose from `ReuploadScaling` enumeration or provide your own function with a single\n            int as input and int or float as output.\n        feature_range: range of data that the input data provided comes from. Used to map input data\n            to the correct domain of the feature-encoding function.\n        target_range: range of data the data encoder assumes as the natural range. For example,\n            in Chebyshev polynomials it is (-1, 1), while for Fourier it may be chosen as (0, 2*PI).\n            Used to map data to the correct domain of the feature-encoding function.\n        multiplier: overall multiplier; this is useful for reuploading the feature map serially with\n            different scalings; can be a number or parameter/expression.\n        param_prefix: string prefix to create trainable parameters multiplying the feature parameter\n            inside the feature-encoding function. Note that currently this does not take into\n            account the domain of the feature-encoding function.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import feature_map, BasisSet, ReuploadScaling\n\n    fm = feature_map(3, fm_type=BasisSet.FOURIER)\n    print(f\"{fm = }\")\n\n    fm = feature_map(3, fm_type=BasisSet.CHEBYSHEV)\n    print(f\"{fm = }\")\n\n    fm = feature_map(3, fm_type=BasisSet.FOURIER, reupload_scaling = ReuploadScaling.TOWER)\n    print(f\"{fm = }\")\n    ```\n    \"\"\"\n\n    # Process input\n    if support is None:\n        support = tuple(range(n_qubits))\n    elif len(support) != n_qubits:\n        raise ValueError(\"Wrong qubit support supplied\")\n\n    if op not in ROTATIONS:\n        raise ValueError(\n            f\"Operation {op} not supported. \"\n            f\"Please provide one from {[rot.__name__ for rot in ROTATIONS]}.\"\n        )\n\n    scaled_fparam = fm_parameter_scaling(\n        fm_type, param, feature_range=feature_range, target_range=target_range\n    )\n\n    transform_func = fm_parameter_func(fm_type)\n\n    basis_tag = fm_type.value if isinstance(fm_type, BasisSet) else str(fm_type)\n    rs_func, rs_tag = fm_reupload_scaling_fn(reupload_scaling)\n\n    # Set overall multiplier\n    multiplier = 1 if multiplier is None else Parameter(multiplier)\n\n    # Build feature map\n    op_list = []\n    fparam = scaled_fparam\n    for i, qubit in enumerate(support):\n        if param_prefix is not None:\n            train_param = VariationalParameter(param_prefix + f\"_{i}\")\n            fparam = train_param * scaled_fparam\n        op_list.append(op(qubit, multiplier * rs_func(i) * transform_func(fparam)))\n    fm = kron(*op_list)\n\n    fm.tag = rs_tag + \" \" + basis_tag + \" FM\"\n\n    return fm\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea.hea","title":"<code>hea(n_qubits, depth=1, param_prefix='theta', support=None, strategy=Strategy.DIGITAL, **strategy_args)</code>","text":"<p>Factory function for the Hardware Efficient Ansatz (HEA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the circuit</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the HEA is applied</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>strategy</code> <p>Strategy for the ansatz. One of the Strategy variants.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>**strategy_args</code> <p>see below</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> PARAMETER DESCRIPTION <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer. Valid for Digital and DigitalAnalog HEA.</p> <p> TYPE: <code>list</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c. Valid for only for Digital HEA.</p> <p> TYPE: <code>bool</code> </p> <code>entangler</code> <ul> <li>Digital: 2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational parameters on the rotation angles.</li> <li>SDAQC | Analog: Hamiltonian generator for the analog entangling layer. Defaults to global ZZ Hamiltonian. Time parameter is considered variational.</li> </ul> <p> TYPE: <code>AbstractBlock</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The Hardware Efficient Ansatz (HEA) circuit.</p> <p>Examples: <pre><code>from qadence import RZ, RX\nfrom qadence import hea\n\n# create the circuit\nn_qubits, depth = 2, 4\nansatz = hea(\n    n_qubits=n_qubits,\n    depth=depth,\n    strategy=\"sDAQC\",\n    operations=[RZ,RX,RZ]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def hea(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    strategy: Strategy = Strategy.DIGITAL,\n    **strategy_args: Any,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Factory function for the Hardware Efficient Ansatz (HEA).\n\n    Args:\n        n_qubits: number of qubits in the circuit\n        depth: number of layers of the HEA\n        param_prefix: the base name of the variational parameters\n        support: qubit indices where the HEA is applied\n        strategy: Strategy for the ansatz. One of the Strategy variants.\n        **strategy_args: see below\n\n    Keyword Arguments:\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer. Valid for\n            Digital and DigitalAnalog HEA.\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c. Valid for only\n            for Digital HEA.\n        entangler (AbstractBlock):\n            - Digital: 2-qubit entangling operation. Supports CNOT, CZ,\n            CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational\n            parameters on the rotation angles.\n            - SDAQC | Analog: Hamiltonian generator for the\n            analog entangling layer. Defaults to global ZZ Hamiltonian.\n            Time parameter is considered variational.\n\n    Returns:\n        The Hardware Efficient Ansatz (HEA) circuit.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import RZ, RX\n    from qadence import hea\n\n    # create the circuit\n    n_qubits, depth = 2, 4\n    ansatz = hea(\n        n_qubits=n_qubits,\n        depth=depth,\n        strategy=\"sDAQC\",\n        operations=[RZ,RX,RZ]\n    )\n    ```\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    hea_func_dict = {\n        Strategy.DIGITAL: hea_digital,\n        Strategy.SDAQC: hea_sDAQC,\n        Strategy.BDAQC: hea_bDAQC,\n        Strategy.ANALOG: hea_analog,\n    }\n\n    try:\n        hea_func = hea_func_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    hea_block: AbstractBlock = hea_func(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        **strategy_args,\n    )  # type: ignore\n\n    return hea_block\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea.hea_digital","title":"<code>hea_digital(n_qubits, depth=1, param_prefix='theta', support=None, periodic=False, operations=[RX, RY, RX], entangler=CNOT)</code>","text":"<p>Construct the Digital Hardware Efficient Ansatz (HEA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the cricuit.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY, RX]</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The digital Hardware Efficient Ansatz (HEA) circuit.</p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def hea_digital(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    periodic: bool = False,\n    operations: list[type[AbstractBlock]] = [RX, RY, RX],\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the Digital Hardware Efficient Ansatz (HEA).\n\n    Args:\n        n_qubits (int): number of qubits in the cricuit.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        support (tuple): qubit indices where the HEA is applied.\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c.\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n\n    Returns:\n        The digital Hardware Efficient Ansatz (HEA) circuit.\n    \"\"\"\n    try:\n        if entangler not in [CNOT, CZ, CRX, CRY, CRZ, CPHASE]:\n            raise ValueError(\n                \"Please provide a valid two-qubit entangler operation for digital HEA.\"\n            )\n    except TypeError:\n        raise ValueError(\"Please provide a valid two-qubit entangler operation for digital HEA.\")\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        periodic=periodic,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n    return tag(chain(*layers), \"HEA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea.hea_sDAQC","title":"<code>hea_sDAQC(n_qubits, depth=1, param_prefix='theta', support=None, operations=[RX, RY, RX], entangler=None)</code>","text":"<p>Construct the Hardware Efficient Ansatz (HEA) with analog entangling layers.</p> <p>It uses step-wise digital-analog computation.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the circuit.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY, RX]</code> </p> <code>entangler</code> <p>Hamiltonian generator for the analog entangling layer. Defaults to global ZZ Hamiltonian. Time parameter is considered variational.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The step-wise digital-analog Hardware Efficient Ansatz (sDA HEA) circuit.</p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def hea_sDAQC(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    operations: list[type[AbstractBlock]] = [RX, RY, RX],\n    entangler: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the Hardware Efficient Ansatz (HEA) with analog entangling layers.\n\n    It uses step-wise digital-analog computation.\n\n    Args:\n        n_qubits (int): number of qubits in the circuit.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        support (tuple): qubit indices where the HEA is applied.\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        entangler (AbstractBlock): Hamiltonian generator for the\n            analog entangling layer. Defaults to global ZZ Hamiltonian.\n            Time parameter is considered variational.\n\n    Returns:\n        The step-wise digital-analog Hardware Efficient Ansatz (sDA HEA) circuit.\n    \"\"\"\n\n    # TODO: Add qubit support\n    if entangler is None:\n        entangler = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n    try:\n        if not block_is_qubit_hamiltonian(entangler):\n            raise ValueError(\n                \"Please provide a valid Pauli Hamiltonian generator for digital-analog HEA.\"\n            )\n    except NotImplementedError:\n        raise ValueError(\n            \"Please provide a valid Pauli Hamiltonian generator for digital-analog HEA.\"\n        )\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_analog(\n        depth=depth,\n        param_prefix=param_prefix,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n    return tag(chain(*layers), \"HEA-sDA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.iia.identity_initialized_ansatz","title":"<code>identity_initialized_ansatz(n_qubits, depth=1, param_prefix='iia', strategy=Strategy.DIGITAL, rotations=[RX, RY], entangler=None, periodic=False)</code>","text":"<p>Identity block for barren plateau mitigation.</p> <p>The initial configuration of this block is equal to an identity unitary but can be trained in the same fashion as other ansatzes, reaching same level of expressivity.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>The base name of the variational parameter. Defaults to \"iia\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'iia'</code> </p> <code>strategy</code> <p>(Strategy) Strategy.DIGITAL for fully digital or Strategy.SDAQC for digital-analog.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>rotations</code> <p>single-qubit rotations with trainable parameters</p> <p> TYPE: <code>list of AbstractBlocks</code> DEFAULT: <code>[RX, RY]</code> </p> <code>entangler</code> <p>For Digital:     2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE.     Controlled rotations will have variational parameters on the rotation angles.     Defaults to CNOT. For Digital-analog:     Hamiltonian generator for the analog entangling layer.     Time parameter is considered variational.     Defaults to a global NN Hamiltonain.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>None</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. Valid only for digital.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The identity initialized ansatz circuit.</p> Source code in <code>qadence/constructors/iia.py</code> <pre><code>def identity_initialized_ansatz(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"iia\",\n    strategy: Strategy = Strategy.DIGITAL,\n    rotations: Any = [RX, RY],\n    entangler: Any = None,\n    periodic: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Identity block for barren plateau mitigation.\n\n    The initial configuration of this block is equal to an identity unitary\n    but can be trained in the same fashion as other ansatzes, reaching same level\n    of expressivity.\n\n    Args:\n        n_qubits: number of qubits in the block\n        depth: number of layers of the HEA\n        param_prefix (str):\n            The base name of the variational parameter. Defaults to \"iia\".\n        strategy: (Strategy)\n            Strategy.DIGITAL for fully digital or Strategy.SDAQC for digital-analog.\n        rotations (list of AbstractBlocks):\n            single-qubit rotations with trainable parameters\n        entangler (AbstractBlock):\n            For Digital:\n                2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE.\n                Controlled rotations will have variational parameters on the rotation angles.\n                Defaults to CNOT.\n            For Digital-analog:\n                Hamiltonian generator for the analog entangling layer.\n                Time parameter is considered variational.\n                Defaults to a global NN Hamiltonain.\n        periodic (bool): if the qubits should be linked periodically. Valid only for digital.\n\n    Returns:\n        The identity initialized ansatz circuit.\n    \"\"\"\n    initialized_layers = []\n    for layer in range(depth):\n        alpha = 2 * PI * torch.rand(n_qubits * len(rotations))\n        gamma = torch.zeros(n_qubits)\n        beta = -alpha\n\n        left_rotations = _rotations(\n            n_qubits=n_qubits,\n            layer=layer,\n            side=\"left\",\n            param_str=f\"{param_prefix}_\u03b1\",\n            values=alpha,\n            ops=rotations,\n        )\n\n        if strategy == Strategy.DIGITAL:\n            if entangler is None:\n                entangler = CNOT\n\n            if entangler not in [CNOT, CZ, CRZ, CRY, CRX, CPHASE]:\n                raise ValueError(\n                    \"Please provide a valid two-qubit entangler operation for digital IIA.\"\n                )\n\n            ent_param_prefix = f\"{param_prefix}_\u03b8_ent_\"\n            if not periodic:\n                left_entanglers = [\n                    chain(\n                        _entangler(\n                            control=n,\n                            target=n + 1,\n                            param_str=ent_param_prefix + f\"_{layer}{n}\",\n                            entangler=entangler,\n                        )\n                        for n in range(n_qubits - 1)\n                    )\n                ]\n            else:\n                left_entanglers = [\n                    chain(\n                        _entangler(\n                            control=n,\n                            target=(n + 1) % n_qubits,\n                            param_str=ent_param_prefix + f\"_{layer}{n}\",\n                            entangler=entangler,\n                        )\n                        for n in range(n_qubits)\n                    )\n                ]\n\n        elif strategy == Strategy.SDAQC:\n            if entangler is None:\n                entangler = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n\n            if not block_is_qubit_hamiltonian(entangler):\n                raise ValueError(\n                    \"Please provide a valid Pauli Hamiltonian generator for digital-analog IIA.\"\n                )\n\n            ent_param_prefix = f\"{param_prefix}_ent_t\"\n\n            left_entanglers = [\n                chain(\n                    _entangler_analog(\n                        param_str=f\"{ent_param_prefix}_{layer}\",\n                        generator=entangler,\n                    )\n                )\n            ]\n\n        else:\n            raise NotImplementedError\n\n        centre_rotations = [\n            kron(\n                RX(\n                    target=n,\n                    parameter=Parameter(name=f\"{param_prefix}_\u03b3\" + f\"_{layer}{n}\", value=gamma[n]),\n                )\n                for n in range(n_qubits)\n            )\n        ]\n\n        right_entanglers = reversed(*left_entanglers)\n\n        right_rotations = _rotations(\n            n_qubits=n_qubits,\n            layer=layer,\n            side=\"right\",\n            param_str=f\"{param_prefix}_\u03b2\",\n            values=beta,\n            ops=rotations,\n        )\n\n        krons = [\n            *left_rotations,\n            *left_entanglers,\n            *centre_rotations,\n            *right_entanglers,\n            *right_rotations,\n        ]\n\n        initialized_layers.append(tag(chain(*krons), tag=f\"BPMA-{layer}\"))\n\n    return chain(*initialized_layers)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ala.ala","title":"<code>ala(n_qubits, m_block_qubits, depth=1, param_prefix='theta', support=None, strategy=Strategy.DIGITAL, **strategy_args)</code>","text":"<p>Factory function for the alternating layer ansatz (ala).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the circuit</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>number of qubits in the local entangling block</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the alternating layer ansatz</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the ala is applied</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>strategy</code> <p>Strategy for the ansatz. One of the Strategy variants.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>**strategy_args</code> <p>see below</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> PARAMETER DESCRIPTION <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer. Valid for Digital .</p> <p> TYPE: <code>list</code> </p> <code>entangler</code> <ul> <li>Digital: 2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational parameters on the rotation angles.</li> <li>SDAQC | BDAQC: Hamiltonian generator for the analog entangling     layer. Must be an m-qubit operator where m is the size of the     local entangling block. Defaults to a ZZ interaction.</li> </ul> <p> TYPE: <code>AbstractBlock</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The Alternating Layer Ansatz (ALA) circuit.</p> Source code in <code>qadence/constructors/ala.py</code> <pre><code>def ala(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    strategy: Strategy = Strategy.DIGITAL,\n    **strategy_args: Any,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Factory function for the alternating layer ansatz (ala).\n\n    Args:\n        n_qubits: number of qubits in the circuit\n        m_block_qubits: number of qubits in the local entangling block\n        depth: number of layers of the alternating layer ansatz\n        param_prefix: the base name of the variational parameters\n        support: qubit indices where the ala is applied\n        strategy: Strategy for the ansatz. One of the Strategy variants.\n        **strategy_args: see below\n\n    Keyword Arguments:\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer. Valid for\n            Digital .\n        entangler (AbstractBlock):\n            - Digital: 2-qubit entangling operation. Supports CNOT, CZ,\n            CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational\n            parameters on the rotation angles.\n            - SDAQC | BDAQC: Hamiltonian generator for the analog entangling\n                layer. Must be an m-qubit operator where m is the size of the\n                local entangling block. Defaults to a ZZ interaction.\n\n    Returns:\n        The Alternating Layer Ansatz (ALA) circuit.\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    ala_func_dict = {\n        Strategy.DIGITAL: ala_digital,\n        Strategy.SDAQC: ala_sDAQC,\n        Strategy.BDAQC: ala_bDAQC,\n        Strategy.ANALOG: ala_analog,\n    }\n\n    try:\n        ala_func = ala_func_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    ala_block: AbstractBlock = ala_func(\n        n_qubits=n_qubits,\n        m_block_qubits=m_block_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        **strategy_args,\n    )  # type: ignore\n\n    return ala_block\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ala.ala_digital","title":"<code>ala_digital(n_qubits, m_block_qubits, depth=1, param_prefix='theta', support=None, operations=[RX, RY], entangler=CNOT)</code>","text":"<p>Construct the digital alternating layer ansatz (ALA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the circuit.</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>number of qubits in the local entangling block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the ALA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the ALA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY]</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The digital Alternating Layer Ansatz (ALA) circuit.</p> Source code in <code>qadence/constructors/ala.py</code> <pre><code>def ala_digital(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    operations: list[type[AbstractBlock]] = [RX, RY],\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the digital alternating layer ansatz (ALA).\n\n    Args:\n        n_qubits (int): number of qubits in the circuit.\n        m_block_qubits (int): number of qubits in the local entangling block.\n        depth (int): number of layers of the ALA.\n        param_prefix (str): the base name of the variational parameters\n        support (tuple): qubit indices where the ALA is applied.\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n\n    Returns:\n        The digital Alternating Layer Ansatz (ALA) circuit.\n    \"\"\"\n\n    try:\n        if entangler not in [CNOT, CZ, CRX, CRY, CRZ, CPHASE]:\n            raise ValueError(\n                \"Please provide a valid two-qubit entangler operation for digital ALA.\"\n            )\n    except TypeError:\n        raise ValueError(\"Please provide a valid two-qubit entangler operation for digital ALA.\")\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        support=support,\n        param_prefix=param_prefix,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_ala_block_digital(\n        n_qubits,\n        m_block_qubits,\n        param_prefix=param_prefix + \"_ent\",\n        depth=depth,\n        support=support,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n\n    return tag(chain(*layers), \"ALA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig","title":"<code>ObservableConfig(detuning, scale=1.0, shift=0.0, transformation_type=ObservableTransform.NONE, trainable_transform=None)</code>  <code>dataclass</code>","text":""},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.detuning","title":"<code>detuning</code>  <code>instance-attribute</code>","text":"<p>Single qubit detuning of the observable Hamiltonian.</p> <p>Accepts single-qubit operator N, X, Y, or Z.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.scale","title":"<code>scale = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The scale by which to multiply the output of the observable.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.shift","title":"<code>shift = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The shift to add to the output of the observable.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.trainable_transform","title":"<code>trainable_transform = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to have a trainable transformation on the output of the observable.</p> <p>If None, the scale and shift are numbers. If True, the scale and shift are VariationalParameter. If False, the scale and shift are FeatureParameter.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.transformation_type","title":"<code>transformation_type = ObservableTransform.NONE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The type of transformation.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.hamiltonian_factory","title":"<code>hamiltonian_factory(register, interaction=None, detuning=None, interaction_strength=None, detuning_strength=None, random_strength=False, use_all_node_pairs=False)</code>","text":"<p>General Hamiltonian creation function.</p> <p>Can be used to create Hamiltonians with 2-qubit interactions and single-qubit detunings, both with arbitrary strength or parameterized.</p> PARAMETER DESCRIPTION <code>register</code> <p>register of qubits with a specific graph topology, or number of qubits. When passing a number of qubits a register with all-to-all connectivity is created.</p> <p> TYPE: <code>Register | int</code> </p> <code>interaction</code> <p>Interaction.ZZ, Interaction.NN, Interaction.XY, or Interacton.XYZ.</p> <p> TYPE: <code>Interaction | Callable | None</code> DEFAULT: <code>None</code> </p> <code>detuning</code> <p>single-qubit operator N, X, Y, or Z.</p> <p> TYPE: <code>TDetuning | None</code> DEFAULT: <code>None</code> </p> <code>interaction_strength</code> <p>list of values to be used as the interaction strength for each pair of qubits. Should be ordered following the order of <code>Register(n_qubits).edges</code>. Alternatively, some string \"x\" can be passed, which will create a parameterized interactions for each pair of qubits, each labelled as <code>\"x_ij\"</code>.</p> <p> TYPE: <code>TArray | str | None</code> DEFAULT: <code>None</code> </p> <code>detuning_strength</code> <p>list of values to be used as the detuning strength for each qubit. Alternatively, some string \"x\" can be passed, which will create a parameterized detuning for each qubit, each labelled as <code>\"x_i\"</code>.</p> <p> TYPE: <code>TArray | str | None</code> DEFAULT: <code>None</code> </p> <code>random_strength</code> <p>set random interaction and detuning strengths between -1 and 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_all_node_pairs</code> <p>computes an interaction term for every pair of nodes in the graph, independent of the edge topology in the register. Useful for defining Hamiltonians where the interaction strength decays with the distance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>from qadence import hamiltonian_factory, Interaction, Register, Z\n\nn_qubits = 3\n\n# Constant total magnetization observable:\nobservable = hamiltonian_factory(n_qubits, detuning = Z)\n\n# Parameterized total magnetization observable:\nobservable = hamiltonian_factory(n_qubits, detuning = Z, detuning_strength = \"z\")\n\n# Random all-to-all XY Hamiltonian generator:\ngenerator = hamiltonian_factory(\n    n_qubits,\n    interaction = Interaction.XY,\n    random_strength = True,\n    )\n\n# Parameterized NN Hamiltonian generator with a square grid interaction topology:\nregister = Register.square(qubits_side = n_qubits)\ngenerator = hamiltonian_factory(\n    register,\n    interaction = Interaction.NN,\n    interaction_strength = \"theta\"\n    )\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def hamiltonian_factory(\n    register: Register | int,\n    interaction: Interaction | Callable | None = None,\n    detuning: TDetuning | None = None,\n    interaction_strength: TArray | str | None = None,\n    detuning_strength: TArray | str | None = None,\n    random_strength: bool = False,\n    use_all_node_pairs: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    General Hamiltonian creation function.\n\n    Can be used to create Hamiltonians with 2-qubit\n    interactions and single-qubit detunings, both with arbitrary strength or parameterized.\n\n    Arguments:\n        register: register of qubits with a specific graph topology, or number of qubits.\n            When passing a number of qubits a register with all-to-all connectivity\n            is created.\n        interaction: Interaction.ZZ, Interaction.NN, Interaction.XY, or Interacton.XYZ.\n        detuning: single-qubit operator N, X, Y, or Z.\n        interaction_strength: list of values to be used as the interaction strength for each\n            pair of qubits. Should be ordered following the order of `Register(n_qubits).edges`.\n            Alternatively, some string \"x\" can be passed, which will create a parameterized\n            interactions for each pair of qubits, each labelled as `\"x_ij\"`.\n        detuning_strength: list of values to be used as the detuning strength for each qubit.\n            Alternatively, some string \"x\" can be passed, which will create a parameterized\n            detuning for each qubit, each labelled as `\"x_i\"`.\n        random_strength: set random interaction and detuning strengths between -1 and 1.\n        use_all_node_pairs: computes an interaction term for every pair of nodes in the graph,\n            independent of the edge topology in the register. Useful for defining Hamiltonians\n            where the interaction strength decays with the distance.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import hamiltonian_factory, Interaction, Register, Z\n\n        n_qubits = 3\n\n        # Constant total magnetization observable:\n        observable = hamiltonian_factory(n_qubits, detuning = Z)\n\n        # Parameterized total magnetization observable:\n        observable = hamiltonian_factory(n_qubits, detuning = Z, detuning_strength = \"z\")\n\n        # Random all-to-all XY Hamiltonian generator:\n        generator = hamiltonian_factory(\n            n_qubits,\n            interaction = Interaction.XY,\n            random_strength = True,\n            )\n\n        # Parameterized NN Hamiltonian generator with a square grid interaction topology:\n        register = Register.square(qubits_side = n_qubits)\n        generator = hamiltonian_factory(\n            register,\n            interaction = Interaction.NN,\n            interaction_strength = \"theta\"\n            )\n        ```\n    \"\"\"\n\n    if interaction is None and detuning is None:\n        raise ValueError(\"Please provide an interaction and/or detuning for the Hamiltonian.\")\n\n    # If number of qubits is given, creates all-to-all register\n    register = Register(register) if isinstance(register, int) else register\n\n    # Get interaction function\n    if interaction is not None:\n        if callable(interaction):\n            int_fn = interaction\n            try:\n                if not block_is_qubit_hamiltonian(interaction(0, 1)):\n                    raise ValueError(\"Custom interactions must be composed of Pauli operators.\")\n            except TypeError:\n                raise TypeError(\n                    \"Please use a custom interaction function signed with two integer parameters.\"\n                )\n        else:\n            int_fn = INTERACTION_DICT.get(interaction, None)  # type: ignore [arg-type]\n            if int_fn is None:\n                raise KeyError(f\"Interaction {interaction} not supported.\")\n\n    # Check single-qubit detuning\n    if (detuning is not None) and (detuning not in DETUNINGS):\n        raise TypeError(f\"Detuning of type {type(detuning)} not supported.\")\n\n    # Pre-process detuning and interaction strengths and update register\n    detuning_strength_array = _preprocess_strengths(\n        register, detuning_strength, \"nodes\", random_strength\n    )\n\n    edge_str = \"all_node_pairs\" if use_all_node_pairs else \"edges\"\n    interaction_strength_array = _preprocess_strengths(\n        register, interaction_strength, edge_str, random_strength\n    )\n\n    # Create single-qubit detunings:\n    single_qubit_terms: List[AbstractBlock] = []\n    if detuning is not None:\n        for strength, node in zip(detuning_strength_array, register.nodes):\n            single_qubit_terms.append(strength * detuning(node))\n\n    # Create two-qubit interactions:\n    two_qubit_terms: List[AbstractBlock] = []\n    edge_data = register.all_node_pairs if use_all_node_pairs else register.edges\n    if interaction is not None and int_fn is not None:\n        for strength, edge in zip(interaction_strength_array, edge_data):\n            two_qubit_terms.append(strength * int_fn(*edge))\n\n    return add(*single_qubit_terms, *two_qubit_terms)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_nn","title":"<code>interaction_nn(i, j)</code>","text":"<p>Ising NN interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_nn(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Ising NN interaction.\"\"\"\n    return N(i) @ N(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_xy","title":"<code>interaction_xy(i, j)</code>","text":"<p>XY interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_xy(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"XY interaction.\"\"\"\n    return X(i) @ X(j) + Y(i) @ Y(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_xyz","title":"<code>interaction_xyz(i, j)</code>","text":"<p>Heisenberg XYZ interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_xyz(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Heisenberg XYZ interaction.\"\"\"\n    return X(i) @ X(j) + Y(i) @ Y(j) + Z(i) @ Z(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_zz","title":"<code>interaction_zz(i, j)</code>","text":"<p>Ising ZZ interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_zz(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Ising ZZ interaction.\"\"\"\n    return Z(i) @ Z(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft.qft","title":"<code>qft(n_qubits, support=None, inverse=False, reverse_in=False, swaps_out=False, strategy=Strategy.DIGITAL, gen_build=None)</code>","text":"<p>The Quantum Fourier Transform.</p> <p>Depending on the application, user should be careful with qubit ordering in the input and output. This can be controlled with reverse_in and swaps_out arguments.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the QFT</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>qubit support to use</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>inverse</code> <p>True performs the inverse QFT</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>reverse_in</code> <p>Reverses the input qubits to account for endianness</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>swaps_out</code> <p>Performs swaps on the output qubits to match the \"textbook\" QFT.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>strategy</code> <p>Strategy.Digital or Strategy.sDAQC</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>gen_build</code> <p>building block Ising Hamiltonian for the DAQC transform. Defaults to constant all-to-all Ising.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>from qadence import qft\n\nn_qubits = 3\n\nqft_circuit = qft(n_qubits, strategy = \"sDAQC\")\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def qft(\n    n_qubits: int,\n    support: tuple[int, ...] = None,\n    inverse: bool = False,\n    reverse_in: bool = False,\n    swaps_out: bool = False,\n    strategy: Strategy = Strategy.DIGITAL,\n    gen_build: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    The Quantum Fourier Transform.\n\n    Depending on the application, user should be careful with qubit ordering\n    in the input and output. This can be controlled with reverse_in and swaps_out\n    arguments.\n\n    Args:\n        n_qubits: number of qubits in the QFT\n        support: qubit support to use\n        inverse: True performs the inverse QFT\n        reverse_in: Reverses the input qubits to account for endianness\n        swaps_out: Performs swaps on the output qubits to match the \"textbook\" QFT.\n        strategy: Strategy.Digital or Strategy.sDAQC\n        gen_build: building block Ising Hamiltonian for the DAQC transform.\n            Defaults to constant all-to-all Ising.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import qft\n\n        n_qubits = 3\n\n        qft_circuit = qft(n_qubits, strategy = \"sDAQC\")\n        ```\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    assert len(support) &lt;= n_qubits, \"Wrong qubit support supplied\"\n\n    if reverse_in:\n        support = support[::-1]\n\n    qft_layer_dict = {\n        Strategy.DIGITAL: _qft_layer_digital,\n        Strategy.SDAQC: _qft_layer_sDAQC,\n        Strategy.BDAQC: _qft_layer_bDAQC,\n        Strategy.ANALOG: _qft_layer_analog,\n    }\n\n    try:\n        layer_func = qft_layer_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    qft_layers = reversed(range(n_qubits)) if inverse else range(n_qubits)\n\n    qft_circ = chain(\n        layer_func(\n            n_qubits=n_qubits, support=support, layer=layer, inverse=inverse, gen_build=gen_build\n        )  # type: ignore\n        for layer in qft_layers\n    )\n\n    if swaps_out:\n        swap_ops = [SWAP(support[i], support[n_qubits - i - 1]) for i in range(n_qubits // 2)]\n        qft_circ = chain(*swap_ops, qft_circ) if inverse else chain(qft_circ, *swap_ops)\n\n    return tag(qft_circ, tag=\"iQFT\") if inverse else tag(qft_circ, tag=\"QFT\")\n</code></pre>"},{"location":"api/constructors/#hardware-efficient-ansatz-for-rydberg-atom-arrays","title":"Hardware efficient ansatz for Rydberg atom arrays","text":""},{"location":"api/constructors/#qadence.constructors.rydberg_hea.rydberg_hea","title":"<code>rydberg_hea(register, n_layers=1, addressable_detuning=True, addressable_drive=False, tunable_phase=False, additional_prefix=None)</code>","text":"<p>Hardware efficient ansatz for neutral atom (Rydberg) platforms.</p> <p>This constructor implements a variational ansatz which is very close to what is implementable on 2nd generation PASQAL quantum devices. In particular, it implements evolution over a specific Hamiltonian which can be realized on the device. This Hamiltonian contains:</p> <ul> <li> <p>an interaction term given by the standard NN interaction and determined starting     from the positions in the input register: H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c</p> </li> <li> <p>a detuning term which corresponding to a n_i = (1+sigma_i^z)/2 applied to     all the qubits. If the <code>addressable_detuning</code> flag is set to True, the routine     effectively a local n_i = (1+sigma_i^z)/2 term in the     evolved Hamiltonian with a different coefficient for each atom. These     coefficients determine a local addressing pattern for the detuning on a subset     of the qubits. In this routine, the coefficients are variational parameters     and they will therefore be optimized at each optimizer step</p> </li> <li> <p>a drive term which corresponding to a sigma^x evolution operation applied to     all the qubits. If the <code>addressable_drive</code> flag is set to True, the routine     effectively a local sigma_i^x term in the evolved Hamiltonian with a different     coefficient for each atom. These coefficients determine a local addressing pattern     for the drive on a subset of the qubits. In this routine, the coefficients are     variational parameters and they will therefore be optimized at each optimizer step</p> </li> <li> <p>if the <code>tunable_phase</code> flag is set to True, the drive term is modified in the following     way: drive = cos(phi) * sigma^x - sin(phi) * sigma^y     The addressable pattern above is maintained and the phase is considered just as an     additional variational parameter which is optimized with the rest</p> </li> </ul> <p>Notice that, on real devices, the coefficients assigned to each qubit in both the detuning and drive patterns should be non-negative and they should always sum to 1. This is not the case for the implementation in this routine since the coefficients (weights) do not have any constraint. Therefore, this HEA is not completely realizable on neutral atom devices.</p> PARAMETER DESCRIPTION <code>register</code> <p>the input atomic register with Cartesian coordinates.</p> <p> TYPE: <code>Register</code> </p> <code>n_layers</code> <p>number layers in the HEA, each layer includes a drive, detuning and pure interaction pulses whose is a variational parameter</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>addressable_detuning</code> <p>whether to turn on the trainable semi-local addressing pattern on the detuning (n_i terms in the Hamiltonian)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>addressable_drive</code> <p>whether to turn on the trainable semi-local addressing pattern on the drive (sigma_i^x terms in the Hamiltonian)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tunable_phase</code> <p>whether to have a tunable phase to get both sigma^x and sigma^y rotations in the drive term. If False, only a sigma^x term will be included in the drive part of the Hamiltonian generator</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>additional_prefix</code> <p>an additional prefix to attach to the parameter names</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>The Rydberg HEA block</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def rydberg_hea(\n    register: qd.Register,\n    n_layers: int = 1,\n    addressable_detuning: bool = True,\n    addressable_drive: bool = False,\n    tunable_phase: bool = False,\n    additional_prefix: str = None,\n) -&gt; qd.blocks.ChainBlock:\n    \"\"\"Hardware efficient ansatz for neutral atom (Rydberg) platforms.\n\n    This constructor implements a variational ansatz which is very close to\n    what is implementable on 2nd generation PASQAL quantum devices. In particular,\n    it implements evolution over a specific Hamiltonian which can be realized on\n    the device. This Hamiltonian contains:\n\n    * an interaction term given by the standard NN interaction and determined starting\n        from the positions in the input register: H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c\n\n    * a detuning term which corresponding to a n_i = (1+sigma_i^z)/2 applied to\n        all the qubits. If the `addressable_detuning` flag is set to True, the routine\n        effectively a local n_i = (1+sigma_i^z)/2 term in the\n        evolved Hamiltonian with a different coefficient for each atom. These\n        coefficients determine a local addressing pattern for the detuning on a subset\n        of the qubits. In this routine, the coefficients are variational parameters\n        and they will therefore be optimized at each optimizer step\n\n    * a drive term which corresponding to a sigma^x evolution operation applied to\n        all the qubits. If the `addressable_drive` flag is set to True, the routine\n        effectively a local sigma_i^x term in the evolved Hamiltonian with a different\n        coefficient for each atom. These coefficients determine a local addressing pattern\n        for the drive on a subset of the qubits. In this routine, the coefficients are\n        variational parameters and they will therefore be optimized at each optimizer step\n\n    * if the `tunable_phase` flag is set to True, the drive term is modified in the following\n        way: drive = cos(phi) * sigma^x - sin(phi) * sigma^y\n        The addressable pattern above is maintained and the phase is considered just as an\n        additional variational parameter which is optimized with the rest\n\n    Notice that, on real devices, the coefficients assigned to each qubit in both the detuning\n    and drive patterns should be non-negative and they should always sum to 1. This is not the\n    case for the implementation in this routine since the coefficients (weights) do not have any\n    constraint. Therefore, this HEA is not completely realizable on neutral atom devices.\n\n    Args:\n        register: the input atomic register with Cartesian coordinates.\n        n_layers: number layers in the HEA, each layer includes a drive, detuning and\n            pure interaction pulses whose is a variational parameter\n        addressable_detuning: whether to turn on the trainable semi-local addressing pattern\n            on the detuning (n_i terms in the Hamiltonian)\n        addressable_drive: whether to turn on the trainable semi-local addressing pattern\n            on the drive (sigma_i^x terms in the Hamiltonian)\n        tunable_phase: whether to have a tunable phase to get both sigma^x and sigma^y rotations\n            in the drive term. If False, only a sigma^x term will be included in the drive part\n            of the Hamiltonian generator\n        additional_prefix: an additional prefix to attach to the parameter names\n\n    Returns:\n        The Rydberg HEA block\n    \"\"\"\n    n_qubits = register.n_qubits\n    prefix = \"\" if additional_prefix is None else \"_\" + additional_prefix\n\n    detunings = None\n    # add a detuning pattern locally addressing the atoms\n    if addressable_detuning:\n        detunings = [qd.VariationalParameter(f\"detmap_{j}\") for j in range(n_qubits)]\n\n    drives = None\n    # add a drive pattern locally addressing the atoms\n    if addressable_drive:\n        drives = [qd.VariationalParameter(f\"drivemap_{j}\") for j in range(n_qubits)]\n\n    phase = None\n    if tunable_phase:\n        phase = qd.VariationalParameter(\"phase\")\n\n    return chain(\n        rydberg_hea_layer(\n            register,\n            VariationalParameter(f\"At{prefix}_{layer}\"),\n            VariationalParameter(f\"Omega{prefix}_{layer}\"),\n            VariationalParameter(f\"wait{prefix}_{layer}\"),\n            detunings=detunings,\n            drives=drives,\n            phase=phase,\n        )\n        for layer in range(n_layers)\n    )\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.rydberg_hea.rydberg_hea_layer","title":"<code>rydberg_hea_layer(register, tevo_drive, tevo_det, tevo_wait, phase=None, detunings=None, drives=None, drive_scaling=1.0)</code>","text":"<p>A single layer of the Rydberg hardware efficient ansatz.</p> PARAMETER DESCRIPTION <code>register</code> <p>the input register with atomic coordinates needed to build the interaction.</p> <p> TYPE: <code>Register</code> </p> <code>tevo_drive</code> <p>a variational parameter for the duration of the drive term of the Hamiltonian generator, including optional semi-local addressing</p> <p> TYPE: <code>Parameter | float</code> </p> <code>tevo_det</code> <p>a variational parameter for the duration of the detuning term of the Hamiltonian generator, including optional semi-local addressing</p> <p> TYPE: <code>Parameter | float</code> </p> <code>tevo_wait</code> <p>a variational parameter for the duration of the waiting time with interaction only</p> <p> TYPE: <code>Parameter | float</code> </p> <code>phase</code> <p>a variational parameter representing the global phase. If None, the global phase is set to 0 which results in a drive term in sigma^x only. Otherwise both sigma^x and sigma^y terms will be present</p> <p> TYPE: <code>Parameter | float | None</code> DEFAULT: <code>None</code> </p> <code>detunings</code> <p>a list of parameters with the weights of the locally addressed detuning terms. These are variational parameters which are tuned by the optimizer</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> <code>drives</code> <p>a list of parameters with the weights of the locally addressed drive terms. These are variational parameters which are tuned by the optimizer</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> <code>drive_scaling</code> <p>a scaling term to be added to the drive Hamiltonian generator</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>A block with a single layer of Rydberg HEA</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def rydberg_hea_layer(\n    register: qd.Register,\n    tevo_drive: Parameter | float,\n    tevo_det: Parameter | float,\n    tevo_wait: Parameter | float,\n    phase: Parameter | float | None = None,\n    detunings: list[Parameter] | list[float] | None = None,\n    drives: list[Parameter] | list[float] | None = None,\n    drive_scaling: float = 1.0,\n) -&gt; ChainBlock:\n    \"\"\"A single layer of the Rydberg hardware efficient ansatz.\n\n    Args:\n        register: the input register with atomic coordinates needed to build the interaction.\n        tevo_drive: a variational parameter for the duration of the drive term of\n            the Hamiltonian generator, including optional semi-local addressing\n        tevo_det: a variational parameter for the duration of the detuning term of the\n            Hamiltonian generator, including optional semi-local addressing\n        tevo_wait: a variational parameter for the duration of the waiting\n            time with interaction only\n        phase: a variational parameter representing the global phase. If None, the\n            global phase is set to 0 which results in a drive term in sigma^x only. Otherwise\n            both sigma^x and sigma^y terms will be present\n        detunings: a list of parameters with the weights of the locally addressed\n            detuning terms. These are variational parameters which are tuned by the optimizer\n        drives: a list of parameters with the weights of the locally addressed\n            drive terms. These are variational parameters which are tuned by the optimizer\n        drive_scaling: a scaling term to be added to the drive Hamiltonian generator\n\n    Returns:\n        A block with a single layer of Rydberg HEA\n    \"\"\"\n    n_qubits = register.n_qubits\n\n    drive_x = _amplitude_map(n_qubits, qd.X, weights=drives)\n    drive_y = _amplitude_map(n_qubits, qd.Y, weights=drives)\n    detuning = _amplitude_map(n_qubits, qd.N, weights=detunings)\n    interaction = hamiltonian_factory(register, qd.Interaction.NN)\n\n    # drive and interaction are not commuting thus they need to be\n    # added directly into the final Hamiltonian generator\n    if phase is not None:\n        generator = (\n            drive_scaling * sympy.cos(phase) * drive_x\n            - drive_scaling * sympy.sin(phase) * drive_y\n            + interaction\n        )\n    else:\n        generator = drive_scaling * drive_x + interaction\n\n    return chain(\n        qd.HamEvo(generator, tevo_drive),\n        # detuning and interaction are commuting, so they\n        # can be ordered arbitrarily and treated separately\n        qd.HamEvo(interaction, tevo_wait),\n        qd.HamEvo(detuning, tevo_det),\n    )\n</code></pre>"},{"location":"api/constructors/#the-daqc-transform","title":"The DAQC Transform","text":""},{"location":"api/constructors/#qadence.constructors.daqc.daqc.daqc_transform","title":"<code>daqc_transform(n_qubits, gen_target, t_f, gen_build=None, zero_tol=1e-08, strategy=Strategy.SDAQC, ignore_global_phases=False)</code>","text":"<p>Implements the DAQC transform for representing an arbitrary 2-body Hamiltonian.</p> <p>The result is another fixed 2-body Hamiltonian.</p> <p>Reference for universality of 2-body Hamiltonians:</p> <p>-- https://arxiv.org/abs/quant-ph/0106064</p> <p>Based on the transformation for Ising (ZZ) interactions, as described in the paper</p> <p>-- https://arxiv.org/abs/1812.03637</p> <p>The transform translates a target weighted generator of the type:</p> <pre><code>`gen_target = add(g_jk * kron(op(j), op(k)) for j &lt; k)`\n</code></pre> <p>To a circuit using analog evolutions with a fixed building block generator:</p> <pre><code>`gen_build = add(f_jk * kron(op(j), op(k)) for j &lt; k)`\n</code></pre> <p>where <code>op = Z</code> or <code>op = N</code>.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>total number of qubits to use.</p> <p> TYPE: <code>int</code> </p> <code>gen_target</code> <p>target generator built with the structure above. The type of the generator will be automatically evaluated when parsing.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>t_f</code> <p>total time for the gen_target evolution.</p> <p> TYPE: <code>float</code> </p> <code>gen_build</code> <p>fixed generator to act as a building block. Defaults to constant NN: add(1.0 * kron(N(j), N(k)) for j &lt; k). The type of the generator will be automatically evaluated when parsing.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <code>zero_tol</code> <p>default \"zero\" for a missing interaction. Included for numerical reasons, see notes below.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>strategy</code> <p>sDAQC or bDAQC, following definitions in the reference paper.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>SDAQC</code> </p> <code>ignore_global_phases</code> <p>if <code>True</code> the transform does not correct the global phases coming from the mapping between ZZ and NN interactions.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Notes:</p> <pre><code>The paper follows an index convention of running from 1 to N. A few functions\nhere also use that convention to be consistent with the paper. However, for qadence\nrelated things the indices are converted to [0, N-1].\n\nThe case for `n_qubits = 4` is an edge case where the sign matrix is not invertible.\nThere is a workaround for this described in the paper, but it is currently not implemented.\n\nThe current implementation may result in evolution times that are both positive or\nnegative. In practice, both can be represented by simply changing the signs of the\ninteractions. However, for a real implementation where the interactions should remain\nfixed, the paper discusses a workaround that is not currently implemented.\n\nThe transformation works by representing each interaction in the target hamiltonian by\na set of evolutions using the build hamiltonian. As a consequence, some care must be\ntaken when choosing the build hamiltonian. Some cases:\n\n- The target hamiltonian can have any interaction, as long as it is sufficiently\nrepresented in the build hamiltonian. E.g., if the interaction `g_01 * kron(Z(0), Z(1))`\nis in the target hamiltonian, the corresponding interaction `f_01 * kron(Z(0), Z(1))`\nneeds to be in the build hamiltonian. This is checked when the generators are parsed.\n\n- The build hamiltonian can have any interaction, irrespectively of it being needed\nfor the target hamiltonian. This is especially useful for designing local operations\nthrough the repeated evolution of a \"global\" hamiltonian.\n\n- The parameter `zero_tol` controls what it means for an interaction to be \"missing\".\nAny interaction strength smaller than `zero_tol` in the build hamiltonian will not be\nconsidered, and thus that interaction is missing.\n\n- The various ratios `g_jk / f_jk` will influence the time parameter for the various\nevolution slices, meaning that if there is a big discrepancy in the interaction strength\nfor a given qubit pair (j, k), the output circuit may require the usage of hamiltonian\nevolutions with very large times.\n\n- A warning will be issued for evolution times larger than `1/sqrt(zero_tol)`. Evolution\ntimes smaller than `zero_tol` will not be represented.\n</code></pre> <p>Examples:</p> <pre><code>from qadence import Z, N, daqc_transform\n\nn_qubits = 3\n\ngen_build = 0.5 * (N(0)@N(1)) + 0.7 * (N(1)@N(2)) + 0.2 * (N(0)@N(2))\n\ngen_target = 0.1 * (Z(1)@Z(2))\n\nt_f = 2.0\n\ntransformed_circuit = daqc_transform(\n    n_qubits = n_qubits,\n    gen_target = gen_target,\n    t_f = t_f,\n    gen_build = gen_build,\n)\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/daqc/daqc.py</code> <pre><code>def daqc_transform(\n    n_qubits: int,\n    gen_target: AbstractBlock,\n    t_f: float,\n    gen_build: AbstractBlock | None = None,\n    zero_tol: float = 1e-08,\n    strategy: Strategy = Strategy.SDAQC,\n    ignore_global_phases: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Implements the DAQC transform for representing an arbitrary 2-body Hamiltonian.\n\n    The result is another fixed 2-body Hamiltonian.\n\n    Reference for universality of 2-body Hamiltonians:\n\n    -- https://arxiv.org/abs/quant-ph/0106064\n\n    Based on the transformation for Ising (ZZ) interactions, as described in the paper\n\n    -- https://arxiv.org/abs/1812.03637\n\n    The transform translates a target weighted generator of the type:\n\n        `gen_target = add(g_jk * kron(op(j), op(k)) for j &lt; k)`\n\n    To a circuit using analog evolutions with a fixed building block generator:\n\n        `gen_build = add(f_jk * kron(op(j), op(k)) for j &lt; k)`\n\n    where `op = Z` or `op = N`.\n\n    Args:\n        n_qubits: total number of qubits to use.\n        gen_target: target generator built with the structure above. The type\n            of the generator will be automatically evaluated when parsing.\n        t_f (float): total time for the gen_target evolution.\n        gen_build: fixed generator to act as a building block. Defaults to\n            constant NN: add(1.0 * kron(N(j), N(k)) for j &lt; k). The type\n            of the generator will be automatically evaluated when parsing.\n        zero_tol: default \"zero\" for a missing interaction. Included for\n            numerical reasons, see notes below.\n        strategy: sDAQC or bDAQC, following definitions in the reference paper.\n        ignore_global_phases: if `True` the transform does not correct the global\n            phases coming from the mapping between ZZ and NN interactions.\n\n    Notes:\n\n        The paper follows an index convention of running from 1 to N. A few functions\n        here also use that convention to be consistent with the paper. However, for qadence\n        related things the indices are converted to [0, N-1].\n\n        The case for `n_qubits = 4` is an edge case where the sign matrix is not invertible.\n        There is a workaround for this described in the paper, but it is currently not implemented.\n\n        The current implementation may result in evolution times that are both positive or\n        negative. In practice, both can be represented by simply changing the signs of the\n        interactions. However, for a real implementation where the interactions should remain\n        fixed, the paper discusses a workaround that is not currently implemented.\n\n        The transformation works by representing each interaction in the target hamiltonian by\n        a set of evolutions using the build hamiltonian. As a consequence, some care must be\n        taken when choosing the build hamiltonian. Some cases:\n\n        - The target hamiltonian can have any interaction, as long as it is sufficiently\n        represented in the build hamiltonian. E.g., if the interaction `g_01 * kron(Z(0), Z(1))`\n        is in the target hamiltonian, the corresponding interaction `f_01 * kron(Z(0), Z(1))`\n        needs to be in the build hamiltonian. This is checked when the generators are parsed.\n\n        - The build hamiltonian can have any interaction, irrespectively of it being needed\n        for the target hamiltonian. This is especially useful for designing local operations\n        through the repeated evolution of a \"global\" hamiltonian.\n\n        - The parameter `zero_tol` controls what it means for an interaction to be \"missing\".\n        Any interaction strength smaller than `zero_tol` in the build hamiltonian will not be\n        considered, and thus that interaction is missing.\n\n        - The various ratios `g_jk / f_jk` will influence the time parameter for the various\n        evolution slices, meaning that if there is a big discrepancy in the interaction strength\n        for a given qubit pair (j, k), the output circuit may require the usage of hamiltonian\n        evolutions with very large times.\n\n        - A warning will be issued for evolution times larger than `1/sqrt(zero_tol)`. Evolution\n        times smaller than `zero_tol` will not be represented.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import Z, N, daqc_transform\n\n        n_qubits = 3\n\n        gen_build = 0.5 * (N(0)@N(1)) + 0.7 * (N(1)@N(2)) + 0.2 * (N(0)@N(2))\n\n        gen_target = 0.1 * (Z(1)@Z(2))\n\n        t_f = 2.0\n\n        transformed_circuit = daqc_transform(\n            n_qubits = n_qubits,\n            gen_target = gen_target,\n            t_f = t_f,\n            gen_build = gen_build,\n        )\n        ```\n    \"\"\"\n\n    ##################\n    # Input controls #\n    ##################\n\n    if strategy != Strategy.SDAQC:\n        raise NotImplementedError(\"Currently only the sDAQC transform is implemented.\")\n\n    if n_qubits == 4:\n        raise NotImplementedError(\"DAQC transform 4-qubit edge case not implemented.\")\n\n    if gen_build is None:\n        gen_build = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n\n    try:\n        if (not block_is_qubit_hamiltonian(gen_target)) or (\n            not block_is_qubit_hamiltonian(gen_build)\n        ):\n            raise ValueError(\n                \"Generator block is not a qubit Hamiltonian. Only ZZ or NN interactions allowed.\"\n            )\n    except NotImplementedError:\n        # Happens when block_is_qubit_hamiltonian is called on something that is not a block.\n        raise TypeError(\n            \"Generator block is not a qubit Hamiltonian. Only ZZ or NN interactions allowed.\"\n        )\n\n    #####################\n    # Generator parsing #\n    #####################\n\n    g_jk_target, mat_jk_target, target_type = _parse_generator(n_qubits, gen_target, 0.0)\n    g_jk_build, mat_jk_build, build_type = _parse_generator(n_qubits, gen_build, zero_tol)\n\n    # Get the global phase hamiltonian and single-qubit detuning hamiltonian\n    if build_type == GenDAQC.NN:\n        h_phase_build, h_sq_build = _nn_phase_and_detunings(n_qubits, mat_jk_build)\n\n    if target_type == GenDAQC.NN:\n        h_phase_target, h_sq_target = _nn_phase_and_detunings(n_qubits, mat_jk_target)\n\n    # Time re-scalings\n    if build_type == GenDAQC.ZZ and target_type == GenDAQC.NN:\n        t_star = t_f / 4.0\n    elif build_type == GenDAQC.NN and target_type == GenDAQC.ZZ:\n        t_star = 4.0 * t_f\n    else:\n        t_star = t_f\n\n    # Check if target Hamiltonian can be mapped with the build Hamiltonian\n    assert _check_compatibility(g_jk_target, g_jk_build, zero_tol)\n\n    ##################\n    # DAQC Transform #\n    ##################\n\n    # Section III A of https://arxiv.org/abs/1812.03637:\n\n    # Matrix M for the linear system, exemplified in Table I:\n    matrix_M = _build_matrix_M(n_qubits)\n\n    # Linear system mapping interaction ratios -&gt; evolution times.\n    t_slices = torch.linalg.solve(matrix_M, g_jk_target / g_jk_build) * t_star\n\n    # ZZ-DAQC with ZZ or NN build Hamiltonian\n    daqc_slices = []\n    for m in range(2, n_qubits + 1):\n        for n in range(1, m):\n            alpha = _ix_map(n_qubits, n, m)\n            t = t_slices[alpha - 1]\n            if abs(t) &gt; zero_tol:\n                if abs(t) &gt; (1 / (zero_tol**0.5)):\n                    logger.warning(\n                        \"\"\"\nTransformed circuit with very long evolution time.\nMake sure your target interactions are sufficiently\nrepresented in the build Hamiltonian.\"\"\"\n                    )\n                x_gates = kron(X(n - 1), X(m - 1))\n                analog_evo = HamEvo(gen_build, t)\n                # TODO: Fix repeated X-gates\n                if build_type == GenDAQC.NN:\n                    # Local detuning at each DAQC layer for NN build Hamiltonian\n                    sq_detuning_build = HamEvo(h_sq_build, t)\n                    daqc_slices.append(chain(x_gates, sq_detuning_build, analog_evo, x_gates))\n                elif build_type == GenDAQC.ZZ:\n                    daqc_slices.append(chain(x_gates, analog_evo, x_gates))\n\n    daqc_circuit = chain(*daqc_slices)\n\n    ########################\n    # Phases and Detunings #\n    ########################\n\n    if target_type == GenDAQC.NN:\n        # Local detuning given a NN target Hamiltonian\n        sq_detuning_target = HamEvo(h_sq_target, t_f).dagger()\n        daqc_circuit = chain(sq_detuning_target, daqc_circuit)\n\n    if not ignore_global_phases:\n        if build_type == GenDAQC.NN:\n            # Constant global phase given a NN build Hamiltonian\n            global_phase_build = HamEvo(h_phase_build, t_slices.sum())\n            daqc_circuit = chain(global_phase_build, daqc_circuit)\n\n        if target_type == GenDAQC.NN:\n            # Constant global phase and given a NN target Hamiltonian\n            global_phase_target = HamEvo(h_phase_target, t_f).dagger()\n            daqc_circuit = chain(global_phase_target, daqc_circuit)\n\n    return daqc_circuit\n</code></pre>"},{"location":"api/constructors/#some-utility-functions","title":"Some utility functions","text":""},{"location":"api/constructors/#qadence.constructors.utils.build_idx_fms","title":"<code>build_idx_fms(basis, fm_pauli, multivariate_strategy, n_features, n_qubits, reupload_scaling)</code>","text":"<p>Builds the index feature maps based on the given parameters.</p> PARAMETER DESCRIPTION <code>basis</code> <p>Type of basis chosen for the feature map.</p> <p> TYPE: <code>BasisSet</code> </p> <code>fm_pauli</code> <p>The chosen Pauli rotation type.</p> <p> TYPE: <code>PrimitiveBlock type</code> </p> <code>multivariate_strategy</code> <p>The strategy used for encoding the multivariate feature map.</p> <p> TYPE: <code>MultivariateStrategy</code> </p> <code>n_features</code> <p>The number of features.</p> <p> TYPE: <code>int</code> </p> <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>reupload_scaling</code> <p>The chosen scaling for the reupload.</p> <p> TYPE: <code>ReuploadScaling</code> </p> RETURNS DESCRIPTION <code>list[KronBlock]</code> <p>List[KronBlock]: The list of index feature maps.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def build_idx_fms(\n    basis: BasisSet,\n    fm_pauli: Type[RY],\n    multivariate_strategy: MultivariateStrategy,\n    n_features: int,\n    n_qubits: int,\n    reupload_scaling: ReuploadScaling,\n) -&gt; list[KronBlock]:\n    \"\"\"Builds the index feature maps based on the given parameters.\n\n    Args:\n        basis (BasisSet): Type of basis chosen for the feature map.\n        fm_pauli (PrimitiveBlock type): The chosen Pauli rotation type.\n        multivariate_strategy (MultivariateStrategy): The strategy used for encoding\n            the multivariate feature map.\n        n_features (int): The number of features.\n        n_qubits (int): The number of qubits.\n        reupload_scaling (ReuploadScaling): The chosen scaling for the reupload.\n\n    Returns:\n        List[KronBlock]: The list of index feature maps.\n    \"\"\"\n    idx_fms = []\n    for i in range(n_features):\n        target_qubits = get_fm_qubits(multivariate_strategy, i, n_qubits, n_features)\n        param = FeatureParameter(f\"x{i}\")\n        block = kron(\n            *[\n                fm_pauli(qubit, generator_prefactor(reupload_scaling, j) * basis_func(basis, param))\n                for j, qubit in enumerate(target_qubits)\n            ]\n        )\n        idx_fm = block\n        idx_fms.append(idx_fm)\n    return idx_fms\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.utils.generator_prefactor","title":"<code>generator_prefactor(reupload_scaling, qubit_index)</code>","text":"<p>Converts a spectrum string, e.g. tower or exponential.</p> <p>The result is the correct generator prefactor.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def generator_prefactor(reupload_scaling: ReuploadScaling, qubit_index: int) -&gt; float | int:\n    \"\"\"Converts a spectrum string, e.g. tower or exponential.\n\n    The result is the correct generator prefactor.\n    \"\"\"\n    conversion_dict: dict[str, float | int] = {\n        ReuploadScaling.CONSTANT: 1,\n        ReuploadScaling.TOWER: qubit_index + 1,\n        ReuploadScaling.EXP: 2 * PI / (2 ** (qubit_index + 1)),\n    }\n    return conversion_dict[reupload_scaling]\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.utils.get_fm_qubits","title":"<code>get_fm_qubits(multivariate_strategy, i, n_qubits, n_features)</code>","text":"<p>Returns the list of target qubits for the given feature map strategy and feature index.</p> PARAMETER DESCRIPTION <code>multivariate_strategy</code> <p>The strategy used for encoding the multivariate feature map.</p> <p> TYPE: <code>MultivariateStrategy</code> </p> <code>i</code> <p>The feature index.</p> <p> TYPE: <code>int</code> </p> <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>The number of features.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Iterable</code> <p>List[int]: The list of target qubits.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the feature map strategy is not implemented.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def get_fm_qubits(\n    multivariate_strategy: MultivariateStrategy, i: int, n_qubits: int, n_features: int\n) -&gt; Iterable:\n    \"\"\"Returns the list of target qubits for the given feature map strategy and feature index.\n\n    Args:\n        multivariate_strategy (MultivariateStrategy): The strategy used for encoding\n            the multivariate feature map.\n        i (int): The feature index.\n        n_qubits (int): The number of qubits.\n        n_features (int): The number of features.\n\n    Returns:\n        List[int]: The list of target qubits.\n\n    Raises:\n        ValueError: If the feature map strategy is not implemented.\n    \"\"\"\n    if multivariate_strategy == MultivariateStrategy.PARALLEL:\n        n_qubits_per_feature = int(n_qubits / n_features)\n        target_qubits = range(i * n_qubits_per_feature, (i + 1) * n_qubits_per_feature)\n    elif multivariate_strategy == MultivariateStrategy.SERIES:\n        target_qubits = range(0, n_qubits)\n    else:\n        raise ValueError(f\"Multivariate strategy {multivariate_strategy} not implemented.\")\n    return target_qubits\n</code></pre>"},{"location":"api/draw/","title":"Drawing","text":""},{"location":"api/draw/#drawing","title":"Drawing","text":""},{"location":"api/draw/#qadence.draw.display","title":"<code>display(x, qcd=None, layout='LR', theme='light', fill=True, **kwargs)</code>","text":"<p>Display a block, circuit, or quantum model.</p> <p>The <code>kwargs</code> are forwarded to the underlying <code>nx.Graph</code>, so you can e.g. specify the size of the resulting plot via <code>size=\"2,2\"</code> (see examples)</p> PARAMETER DESCRIPTION <code>x</code> <p><code>AbstractBlock</code>, <code>QuantumCircuit</code>, or <code>QuantumModel</code>.</p> <p> TYPE: <code>Any</code> </p> <code>qcd</code> <p>Circuit diagram to plot the block into.</p> <p> TYPE: <code>QuantumCircuitDiagram | Cluster | None</code> DEFAULT: <code>None</code> </p> <code>layout</code> <p>Can be either \"LR\" (left-right), or \"TB\" (top-bottom).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'LR'</code> </p> <code>theme</code> <p>Available themes are: [\"light\", \"dark\", \"black\", \"white\"].</p> <p> TYPE: <code>str</code> DEFAULT: <code>'light'</code> </p> <code>fill</code> <p>Whether to fill the passed <code>x</code> with identities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>Passed on to <code>nx.Graph</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Examples: <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\ndisplay(b, size=\"1,1\", theme=\"dark\")\n</code></pre> </p> Source code in <code>qadence/draw/__init__.py</code> <pre><code>def display(\n    x: Any,\n    qcd: QuantumCircuitDiagram | Cluster | None = None,\n    layout: str = \"LR\",\n    theme: str = \"light\",\n    fill: bool = True,\n    **kwargs: Any,\n) -&gt; Graph:\n    \"\"\"Display a block, circuit, or quantum model.\n\n    The `kwargs` are forwarded to\n    the underlying `nx.Graph`, so you can e.g. specify the size of the resulting plot via\n    `size=\"2,2\"` (see examples)\n\n    Arguments:\n        x: `AbstractBlock`, `QuantumCircuit`, or `QuantumModel`.\n        qcd: Circuit diagram to plot the block into.\n        layout: Can be either \"LR\" (left-right), or \"TB\" (top-bottom).\n        theme: Available themes are: [\"light\", \"dark\", \"black\", \"white\"].\n        fill: Whether to fill the passed `x` with identities.\n        kwargs: Passed on to `nx.Graph`\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" html=\"1\"\n    from qadence import X, Y, kron\n    from qadence.draw import display\n\n    b = kron(X(0), Y(1))\n    def display(*args, **kwargs): return args # markdown-exec: hide\n    display(b, size=\"1,1\", theme=\"dark\")\n    ```\n    \"\"\"\n    return make_diagram(x, **kwargs).show()\n</code></pre>"},{"location":"api/draw/#qadence.draw.savefig","title":"<code>savefig(x, filename, *args, **kwargs)</code>","text":"<p>Save a block, circuit, or quantum model to file. Accepts the same args/kwargs as <code>display</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>AbstractBlock</code>, <code>QuantumCircuit</code>, or <code>QuantumModel</code>.</p> <p> TYPE: <code>Any</code> </p> <code>filename</code> <p>Should end in svg/png.</p> <p> TYPE: <code>str</code> </p> <code>args</code> <p>Same as in <code>display</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>Same as in <code>display</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Examples: <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\nsavefig(b, \"test.svg\", size=\"1,1\", theme=\"dark\")\n</code></pre> </p> Source code in <code>qadence/draw/__init__.py</code> <pre><code>def savefig(x: Any, filename: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Save a block, circuit, or quantum model to file. Accepts the same args/kwargs as `display`.\n\n    Arguments:\n        x: `AbstractBlock`, `QuantumCircuit`, or `QuantumModel`.\n        filename: Should end in svg/png.\n        args: Same as in `display`.\n        kwargs: Same as in `display`.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" html=\"1\"\n    from qadence import X, Y, kron\n    from qadence.draw import display\n\n    b = kron(X(0), Y(1))\n    def savefig(*args, **kwargs): return args # markdown-exec: hide\n    savefig(b, \"test.svg\", size=\"1,1\", theme=\"dark\")\n    ```\n    \"\"\"\n    make_diagram(x, *args, **kwargs).savefig(filename)\n</code></pre>"},{"location":"api/execution/","title":"Execution","text":""},{"location":"api/execution/#qadence.execution.expectation","title":"<code>expectation(x, observable, values=None, state=None, backend=BackendName.PYQTORCH, diff_mode=None, noise=None, endianness=Endianness.BIG, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.expectation</code> method.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>observable</code> <p>Observable(s) w.r.t. which the expectation is computed.</p> <p> TYPE: <code>Union[list[AbstractBlock], AbstractBlock]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>Which differentiation mode to use.</p> <p> TYPE: <code>Union[DiffMode, str, None]</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A wavefunction</p> <pre><code>from qadence import RX, Z, Register, QuantumCircuit, expectation\n\nreg = Register(1)\nblock = RX(0, 0.5)\nobservable = Z(0)\ncirc = QuantumCircuit(reg, block)\n\n# You can compute the expectation for a\n# QuantumCircuit with a given observable.\nexpectation(circ, observable)\n\n# You can also use only a block.\n# In this case the register is constructed automatically to\n# Register.line(block.n_qubits)\nexpectation(block, observable)\n\n# Or a register and block\nexpectation(reg, block, observable)\n</code></pre> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef expectation(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    observable: Union[list[AbstractBlock], AbstractBlock],\n    values: Union[dict, None] = None,\n    state: Tensor = None,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: Union[DiffMode, str, None] = None,\n    noise: Union[NoiseHandler, None] = None,\n    endianness: Endianness = Endianness.BIG,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; Tensor:\n    \"\"\"Convenience wrapper for the `QuantumModel.expectation` method.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        observable: Observable(s) w.r.t. which the expectation is computed.\n        values: User-facing parameter dict.\n        state: Initial state.\n        backend: Name of the backend to run on.\n        diff_mode: Which differentiation mode to use.\n        endianness: The target device endianness.\n        configuration: The backend configuration.\n\n    Returns:\n        A wavefunction\n\n    ```python exec=\"on\" source=\"material-block\"\n    from qadence import RX, Z, Register, QuantumCircuit, expectation\n\n    reg = Register(1)\n    block = RX(0, 0.5)\n    observable = Z(0)\n    circ = QuantumCircuit(reg, block)\n\n    # You can compute the expectation for a\n    # QuantumCircuit with a given observable.\n    expectation(circ, observable)\n\n    # You can also use only a block.\n    # In this case the register is constructed automatically to\n    # Register.line(block.n_qubits)\n    expectation(block, observable)\n\n    # Or a register and block\n    expectation(reg, block, observable)\n    ```\n    \"\"\"\n\n    raise ValueError(f\"Cannot execute {type(x)}\")\n</code></pre>"},{"location":"api/execution/#qadence.execution.run","title":"<code>run(x, *args, values=None, state=None, backend=BackendName.PYQTORCH, endianness=Endianness.BIG, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.run</code> method.</p> <p>This is a <code>functools.singledispatch</code>ed function so it can be called with a number of different arguments. See the examples of the <code>expectation</code> function. This function works exactly the same.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A wavefunction</p> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef run(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    *args: Any,\n    values: Union[dict, None] = None,\n    state: Tensor = None,\n    backend: BackendName = BackendName.PYQTORCH,\n    endianness: Endianness = Endianness.BIG,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; Tensor:\n    \"\"\"Convenience wrapper for the `QuantumModel.run` method.\n\n     This is a\n    `functools.singledispatch`ed function so it can be called with a number of different arguments.\n    See the examples of the [`expectation`][qadence.execution.expectation] function. This function\n    works exactly the same.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        values: User-facing parameter dict.\n        state: Initial state.\n        backend: Name of the backend to run on.\n        endianness: The target device endianness.\n        configuration: The backend configuration.\n\n    Returns:\n        A wavefunction\n    \"\"\"\n    raise ValueError(f\"Cannot run {type(x)}\")\n</code></pre>"},{"location":"api/execution/#qadence.execution.sample","title":"<code>sample(x, *args, values=None, state=None, n_shots=100, backend=BackendName.PYQTORCH, endianness=Endianness.BIG, noise=None, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.sample</code> method.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Union[Tensor, None]</code> DEFAULT: <code>None</code> </p> <code>n_shots</code> <p>Number of shots per element in the batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>noise</code> <p>The noise model to use if any.</p> <p> TYPE: <code>Union[NoiseHandler, None]</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Counter]</code> <p>A list of Counter instances with the sample results</p> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef sample(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    *args: Any,\n    values: Union[dict, None] = None,\n    state: Union[Tensor, None] = None,\n    n_shots: int = 100,\n    backend: BackendName = BackendName.PYQTORCH,\n    endianness: Endianness = Endianness.BIG,\n    noise: Union[NoiseHandler, None] = None,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; list[Counter]:\n    \"\"\"Convenience wrapper for the `QuantumModel.sample` method.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        values: User-facing parameter dict.\n        state: Initial state.\n        n_shots: Number of shots per element in the batch.\n        backend: Name of the backend to run on.\n        endianness: The target device endianness.\n        noise: The noise model to use if any.\n        configuration: The backend configuration.\n\n    Returns:\n        A list of Counter instances with the sample results\n    \"\"\"\n    raise ValueError(f\"Cannot sample from {type(x)}\")\n</code></pre>"},{"location":"api/ml_tools/","title":"QML tools","text":""},{"location":"api/ml_tools/#ml-tools","title":"ML Tools","text":"<p>This module implements a <code>Trainer</code> class for torch <code>Modules</code> and <code>QuantumModel</code>. It also implements the <code>QNN</code> class and callbacks that can be used with the trainer module.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer","title":"<code>Trainer(model, optimizer, config, loss_fn='mse', train_dataloader=None, val_dataloader=None, test_dataloader=None, optimize_step=optimize_step, device=None, dtype=None, max_batches=None)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class to manage and execute training, validation, and testing loops for a model (eg.</p> <p>QNN).</p> <p>This class handles the overall training process, including: - Managing epochs and steps - Handling data loading and batching - Computing and updating gradients - Logging and monitoring training metrics</p> ATTRIBUTE DESCRIPTION <code>current_epoch</code> <p>The current epoch number.</p> <p> TYPE: <code>int</code> </p> <code>global_step</code> <p>The global step across all epochs.</p> <p> TYPE: <code>int</code> </p> <code>log_device</code> <p>Device for logging, default is \"cpu\".</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device used for computation.</p> <p> TYPE: <code>device</code> </p> <code>dtype</code> <p>Data type used for computation.</p> <p> TYPE: <code>dtype | None</code> </p> <code>data_dtype</code> <p>Data type for data. Depends on the model's data type.</p> <p> TYPE: <code>dtype | None</code> </p> Inherited Attributes <p>use_grad (bool): Indicates if gradients are used for optimization. Default is True.</p> <p>model (nn.Module): The neural network model. optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training. config (TrainConfig): The configuration settings for training. train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data. val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data. test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.</p> <p>optimize_step (Callable): Function for performing an optimization step. loss_fn (Callable): loss function to use.</p> <p>num_training_batches (int): Number of training batches. num_validation_batches (int): Number of validation batches. num_test_batches (int): Number of test batches.</p> <p>state (str): Current state in the training process</p> <p>Default training routine <pre><code>for epoch in max_iter + 1:\n    # Training\n    for batch in train_batches:\n        train model\n    # Validation\n    if val_every % epoch == 0:\n        for batch in val_batches:\n            train model\n</code></pre></p> Notes <ul> <li>In case of InfiniteTensorDataset, number of batches = 1.</li> <li>In case of TensorDataset, number of batches are default.</li> <li>Training is run for max_iter + 1 epochs. Epoch 0 logs untrained model.</li> <li>Please look at the CallbackManager initialize_callbacks method to review the default     logging behavior.</li> </ul> <p>Examples:</p> <pre><code>import torch\nfrom torch.optim import SGD\nfrom qadence import (\n    feature_map,\n    hamiltonian_factory,\n    hea,\n    QNN,\n    QuantumCircuit,\n    TrainConfig,\n    Z,\n)\nfrom qadence.ml_tools.trainer import Trainer\nfrom qadence.ml_tools.optimize_step import optimize_step\nfrom qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.data import to_dataloader\n\n# Initialize the model\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=2)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\n\n# Set up the optimizer\noptimizer = SGD(model.parameters(), lr=0.001)\n\n# Use TrainConfig for configuring the training process\nconfig = TrainConfig(\n    max_iter=100,\n    print_every=10,\n    write_every=10,\n    checkpoint_every=10,\n    val_every=10\n)\n\n# Create the Trainer instance with TrainConfig\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\",\n    optimize_step=optimize_step\n)\n\nbatch_size = 25\nx = torch.linspace(0, 1, 32).reshape(-1, 1)\ny = torch.sin(x)\ntrain_loader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_loader = to_dataloader(x, y, batch_size=batch_size, infinite=False)\n\n# Train the model\nmodel, optimizer = trainer.fit(train_loader, val_loader)\n</code></pre> <p>This also supports both gradient based and gradient free optimization. The default support is for gradient based optimization.</p> <p>Notes:</p> <ul> <li>set_use_grad() (class level):This method is used to set the global <code>use_grad</code> flag,     controlling whether the trainer uses gradient-based optimization. <pre><code># gradient based\nTrainer.set_use_grad(True)\n\n# gradient free\nTrainer.set_use_grad(False)\n</code></pre></li> <li>Context Managers (instance level):  <code>enable_grad_opt()</code> and <code>disable_grad_opt()</code> are     context managers that temporarily switch the optimization mode for specific code blocks.     This is useful when you want to mix gradient-based and gradient-free optimization     in the same training process. <pre><code># gradient based\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit()\n\n# gradient free\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit()\n</code></pre></li> </ul> <p>Examples</p> <p>Gradient based optimization example Usage: <pre><code>from torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nTrainer.set_use_grad(True)\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>trainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Gradient free optimization example Usage: <pre><code>import nevergrad as ng\nfrom qadence.ml_tools.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n                budget=config.max_iter, parametrization= num_parameters(model)\n                )\n\nTrainer.set_use_grad(False)\ntrainer = Trainer(\n    model=model,\n    optimizer=ng_optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>import nevergrad as ng\nfrom qadence.ml_tools.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n        budget=config.max_iter, parametrization= num_parameters(model)\n        )\n\ntrainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Initializes the Trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>Training configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>Loss function used for training. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>optimize_step</code> <p>Function to execute an optimization step.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>optimize_step</code> </p> <code>device</code> <p>Device to use for computation.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for computation.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    optimize_step: Callable = optimize_step,\n    device: torch_device | None = None,\n    dtype: torch_dtype | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the Trainer class.\n\n    Args:\n        model (nn.Module): The PyTorch model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training.\n        config (TrainConfig): Training configuration object.\n        loss_fn (str | Callable ): Loss function used for training.\n            If not specified, default mse loss will be used.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for test data.\n        optimize_step (Callable): Function to execute an optimization step.\n        device (torch_device): Device to use for computation.\n        dtype (torch_dtype): Data type for computation.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    super().__init__(\n        model=model,\n        optimizer=optimizer,\n        config=config,\n        loss_fn=loss_fn,\n        optimize_step=optimize_step,\n        train_dataloader=train_dataloader,\n        val_dataloader=val_dataloader,\n        test_dataloader=test_dataloader,\n        max_batches=max_batches,\n    )\n    self.current_epoch: int = 0\n    self.global_step: int = 0\n    self.log_device: str = \"cpu\" if device is None else device\n    self.device: torch_device | None = device\n    self.dtype: torch_dtype | None = dtype\n    self.data_dtype: torch_dtype | None = None\n    self.stop_training: bool = False\n    if self.dtype:\n        self.data_dtype = float64 if (self.dtype == complex128) else float32\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.build_optimize_result","title":"<code>build_optimize_result(result)</code>","text":"<p>Builds and stores the optimization result by calculating the average loss and metrics.</p> <p>Result (or loss_metrics) can have multiple formats: - <code>None</code> Indicates no loss or metrics data is provided. - <code>tuple[torch.Tensor, dict[str, Any]]</code> A single tuple containing the loss tensor     and metrics dictionary - at the end of batch. - <code>list[tuple[torch.Tensor, dict[str, Any]]]</code> A list of tuples for     multiple batches. - <code>list[list[tuple[torch.Tensor, dict[str, Any]]]]</code> A list of lists of tuples, where each inner list represents metrics across multiple batches within an epoch.</p> PARAMETER DESCRIPTION <code>result</code> <p>(None |     tuple[torch.Tensor, dict[Any, Any]] |     list[tuple[torch.Tensor, dict[Any, Any]]] |     list[list[tuple[torch.Tensor, dict[Any, Any]]]])         The loss and metrics data, which can have multiple formats</p> <p> TYPE: <code>None | tuple[Tensor, dict[Any, Any]] | list[tuple[Tensor, dict[Any, Any]]] | list[list[tuple[Tensor, dict[Any, Any]]]]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>This method does not return anything. It sets <code>self.opt_result</code> with</p> <p> TYPE: <code>None</code> </p> <code>None</code> <p>the computed average loss and metrics.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def build_optimize_result(\n    self,\n    result: (\n        None\n        | tuple[torch.Tensor, dict[Any, Any]]\n        | list[tuple[torch.Tensor, dict[Any, Any]]]\n        | list[list[tuple[torch.Tensor, dict[Any, Any]]]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Builds and stores the optimization result by calculating the average loss and metrics.\n\n    Result (or loss_metrics) can have multiple formats:\n    - `None` Indicates no loss or metrics data is provided.\n    - `tuple[torch.Tensor, dict[str, Any]]` A single tuple containing the loss tensor\n        and metrics dictionary - at the end of batch.\n    - `list[tuple[torch.Tensor, dict[str, Any]]]` A list of tuples for\n        multiple batches.\n    - `list[list[tuple[torch.Tensor, dict[str, Any]]]]` A list of lists of tuples,\n    where each inner list represents metrics across multiple batches within an epoch.\n\n    Args:\n        result: (None |\n                tuple[torch.Tensor, dict[Any, Any]] |\n                list[tuple[torch.Tensor, dict[Any, Any]]] |\n                list[list[tuple[torch.Tensor, dict[Any, Any]]]])\n                    The loss and metrics data, which can have multiple formats\n\n    Returns:\n        None: This method does not return anything. It sets `self.opt_result` with\n        the computed average loss and metrics.\n    \"\"\"\n    loss_metrics = result\n    if loss_metrics is None:\n        loss = None\n        metrics: dict[Any, Any] = {}\n    elif isinstance(loss_metrics, tuple):\n        # Single tuple case\n        loss, metrics = loss_metrics\n    else:\n        last_epoch: list[tuple[torch.Tensor, dict[Any, Any]]] = []\n        if isinstance(loss_metrics, list):\n            # Check if it's a list of tuples\n            if all(isinstance(item, tuple) for item in loss_metrics):\n                last_epoch = cast(list[tuple[torch.Tensor, dict[Any, Any]]], loss_metrics)\n            # Check if it's a list of lists of tuples\n            elif all(isinstance(item, list) for item in loss_metrics):\n                last_epoch = cast(\n                    list[tuple[torch.Tensor, dict[Any, Any]]],\n                    loss_metrics[-1] if loss_metrics else [],\n                )\n            else:\n                raise ValueError(\n                    \"Invalid format for result: Expected None, tuple, list of tuples,\"\n                    \" or list of lists of tuples.\"\n                )\n\n        if not last_epoch:\n            loss, metrics = None, {}\n        else:\n            # Compute the average loss over the batches\n            loss_tensor = torch.stack([loss_batch for loss_batch, _ in last_epoch])\n            avg_loss = loss_tensor.mean()\n\n            # Collect and average metrics for all batches\n            metric_keys = last_epoch[0][1].keys()\n            metrics_stacked: dict = {key: [] for key in metric_keys}\n\n            for _, metrics_batch in last_epoch:\n                for key in metric_keys:\n                    value = metrics_batch[key]\n                    metrics_stacked[key].append(value)\n\n            avg_metrics = {key: torch.stack(metrics_stacked[key]).mean() for key in metric_keys}\n\n            loss, metrics = avg_loss, avg_metrics\n\n    # Store the optimization result\n    self.opt_result = OptimizeResult(\n        self.current_epoch, self.model_old, self.optimizer_old, loss, metrics\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.fit","title":"<code>fit(train_dataloader=None, val_dataloader=None)</code>","text":"<p>Fits the model using the specified training configuration.</p> <p>The dataloaders can be provided to train on new datasets, or the default dataloaders provided in the trainer will be used.</p> PARAMETER DESCRIPTION <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Module, Optimizer]</code> <p>tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def fit(\n    self,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; tuple[nn.Module, optim.Optimizer]:\n    \"\"\"\n    Fits the model using the specified training configuration.\n\n    The dataloaders can be provided to train on new datasets, or the default dataloaders\n    provided in the trainer will be used.\n\n    Args:\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n\n    Returns:\n        tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.\n    \"\"\"\n    if train_dataloader is not None:\n        self.train_dataloader = train_dataloader\n    if val_dataloader is not None:\n        self.val_dataloader = val_dataloader\n\n    self._fit_setup()\n    self._train()\n    self._fit_end()\n    self.training_stage = TrainingStage(\"idle\")\n    return self.model, self.optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_test_batch","title":"<code>run_test_batch(batch)</code>","text":"<p>Runs a single test batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"test_batch\")\ndef run_test_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single test batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_train_batch","title":"<code>run_train_batch(batch)</code>","text":"<p>Runs a single training batch, performing optimization.</p> <p>We use the step function to optimize the model based on use_grad.     use_grad = True entails gradient based optimization, for which we use     optimize_step function.     use_grad = False entails gradient free optimization, for which we use     update_ng_parameters function.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch. tuple of (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_batch\")\ndef run_train_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single training batch, performing optimization.\n\n    We use the step function to optimize the model based on use_grad.\n        use_grad = True entails gradient based optimization, for which we use\n        optimize_step function.\n        use_grad = False entails gradient free optimization, for which we use\n        update_ng_parameters function.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n            tuple of (loss, metrics)\n    \"\"\"\n\n    if self.use_grad:\n        # Perform gradient-based optimization\n        loss_metrics = self.optimize_step(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            xs=batch,\n            device=self.device,\n            dtype=self.data_dtype,\n        )\n    else:\n        # Perform optimization using Nevergrad\n        loss, metrics, ng_params = update_ng_parameters(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            data=batch,\n            ng_params=self.ng_params,  # type: ignore[arg-type]\n        )\n        self.ng_params = ng_params\n        loss_metrics = loss, metrics\n\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_training","title":"<code>run_training(dataloader)</code>","text":"<p>Runs the training for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_epoch\")\ndef run_training(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the training for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for training data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.train()\n    train_epoch_loss_metrics = []\n    # Quick Fix for iteration 0\n    self._reset_model_and_opt()\n\n    for batch in self._batch_iter(dataloader, self.num_training_batches):\n        self.on_train_batch_start(batch)\n        train_batch_loss_metrics = self.run_train_batch(batch)\n        train_epoch_loss_metrics.append(train_batch_loss_metrics)\n        self.on_train_batch_end(train_batch_loss_metrics)\n\n    return train_epoch_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_val_batch","title":"<code>run_val_batch(batch)</code>","text":"<p>Runs a single validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_batch\")\ndef run_val_batch(self, batch: tuple[torch.Tensor, ...]) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single validation batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_validation","title":"<code>run_validation(dataloader)</code>","text":"<p>Runs the validation loop for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_epoch\")\ndef run_validation(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the validation loop for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for validation data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.eval()\n    val_epoch_loss_metrics = []\n\n    for batch in self._batch_iter(dataloader, self.num_validation_batches):\n        self.on_val_batch_start(batch)\n        val_batch_loss_metrics = self.run_val_batch(batch)\n        val_epoch_loss_metrics.append(val_batch_loss_metrics)\n        self.on_val_batch_end(val_batch_loss_metrics)\n\n    return val_epoch_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.test","title":"<code>test(test_dataloader=None)</code>","text":"<p>Runs the testing loop if a test DataLoader is provided.</p> <p>if the test_dataloader is not provided, default test_dataloader defined in the Trainer class is used.</p> PARAMETER DESCRIPTION <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                    -&gt; tuples Test Batches            -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def test(self, test_dataloader: DataLoader = None) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the testing loop if a test DataLoader is provided.\n\n    if the test_dataloader is not provided, default test_dataloader defined\n    in the Trainer class is used.\n\n    Args:\n        test_dataloader (DataLoader): DataLoader for test data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                    -&gt; tuples\n            Test Batches            -&gt; (loss, metrics)\n    \"\"\"\n    if test_dataloader is not None:\n        self.test_dataloader = test_dataloader\n\n    self.model.eval()\n    test_loss_metrics = []\n\n    for batch in self._batch_iter(test_dataloader, self.num_training_batches):\n        self.on_test_batch_start(batch)\n        loss_metrics = self.run_test_batch(batch)\n        test_loss_metrics.append(loss_metrics)\n        self.on_test_batch_end(loss_metrics)\n\n    return test_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig","title":"<code>AnsatzConfig(depth=1, ansatz_type=AnsatzType.HEA, ansatz_strategy=Strategy.DIGITAL, strategy_args=dict(), param_prefix='theta', tag=None)</code>  <code>dataclass</code>","text":""},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.ansatz_strategy","title":"<code>ansatz_strategy = Strategy.DIGITAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Ansatz strategy.</p> <p><code>Strategy.DIGITAL</code> for fully digital ansatz. Required if <code>ansatz_type</code> is <code>AnsatzType.IIA</code>. <code>Strategy.SDAQC</code> for analog entangling block. <code>Strategy.RYDBERG</code> for fully rydberg hea ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.ansatz_type","title":"<code>ansatz_type = AnsatzType.HEA</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>What type of ansatz.</p> <p><code>AnsatzType.HEA</code> for Hardware Efficient Ansatz. <code>AnsatzType.IIA</code> for Identity intialized Ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.depth","title":"<code>depth = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of layers of the ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.param_prefix","title":"<code>param_prefix = 'theta'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The base bame of the variational parameter.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.strategy_args","title":"<code>strategy_args = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary containing keyword arguments to the function creating the ansatz.</p> <p>Details about each below.</p> <p>For <code>Strategy.DIGITAL</code> strategy, accepts the following:     periodic (bool): if the qubits should be linked periodically.         periodic=False is not supported in emu-c.     operations (list): list of operations to cycle through in the         digital single-qubit rotations of each layer.         Defaults to  [RX, RY, RX] for hea and [RX, RY] for iia.     entangler (AbstractBlock): 2-qubit entangling operation.         Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlld rotations         will have variational parameters on the rotation angles.         Defaults to CNOT</p> <p>For <code>Strategy.SDAQC</code> strategy, accepts the following:     operations (list): list of operations to cycle through in the         digital single-qubit rotations of each layer.         Defaults to  [RX, RY, RX] for hea and [RX, RY] for iia.     entangler (AbstractBlock): Hamiltonian generator for the         analog entangling layer. Time parameter is considered variational.         Defaults to NN interaction.</p> <p>For <code>Strategy.RYDBERG</code> strategy, accepts the following:     addressable_detuning: whether to turn on the trainable semi-local addressing pattern         on the detuning (n_i terms in the Hamiltonian).         Defaults to True.     addressable_drive: whether to turn on the trainable semi-local addressing pattern         on the drive (sigma_i^x terms in the Hamiltonian).         Defaults to False.     tunable_phase: whether to have a tunable phase to get both sigma^x and sigma^y rotations         in the drive term. If False, only a sigma^x term will be included in the drive part         of the Hamiltonian generator.         Defaults to False.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.tag","title":"<code>tag = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to indicate the name tag of the ansatz.</p> <p>Defaults to None, in which case no tag will be applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig","title":"<code>FeatureMapConfig(num_features=0, basis_set=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT, feature_range=None, target_range=None, multivariate_strategy=MultivariateStrategy.PARALLEL, feature_map_strategy=Strategy.DIGITAL, param_prefix=None, num_repeats=0, operation=None, inputs=None, tag=None)</code>  <code>dataclass</code>","text":""},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.basis_set","title":"<code>basis_set = BasisSet.FOURIER</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basis set for feature encoding.</p> <p>Takes qadence.BasisSet. Give a single BasisSet to use the same for all features. Give a dict of (str, BasisSet) where the key is the name of the variable and the value is the BasisSet to use for encoding that feature. BasisSet.FOURIER for Fourier encoding. BasisSet.CHEBYSHEV for Chebyshev encoding.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.feature_map_strategy","title":"<code>feature_map_strategy = Strategy.DIGITAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Strategy for feature map.</p> <p>Accepts DIGITAL, ANALOG or RYDBERG. Defaults to DIGITAL. If the strategy is incompatible with the <code>operation</code> chosen, then <code>operation</code> gets preference and the given strategy is ignored.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.feature_range","title":"<code>feature_range = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Range of data that the input data is assumed to come from.</p> <p>Give a single tuple to use the same range for all features. Give a dict of (str, tuple) where the key is the name of the variable and the value is the feature range to use for that feature.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.inputs","title":"<code>inputs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List that indicates the order of variables of the tensors that are passed.</p> <p>Optional if a single feature is being encoded, required otherwise. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.multivariate_strategy","title":"<code>multivariate_strategy = MultivariateStrategy.PARALLEL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The encoding strategy in case of multi-variate function.</p> <p>Takes qadence.MultivariateStrategy. If PARALLEL, the features are encoded in one block of rotation gates with the register being split in sub-registers for each feature. If SERIES, the features are encoded sequentially using the full register for each feature, with an ansatz block between them. PARALLEL is allowed only for DIGITAL <code>feature_map_strategy</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.num_features","title":"<code>num_features = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of feature parameters to be encoded.</p> <p>Defaults to 0. Thus, no feature parameters are encoded.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.num_repeats","title":"<code>num_repeats = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of feature map layers repeated in the data reuploading step.</p> <p>If all features are to be repeated the same number of times, then can give a single <code>int</code>. For different number of repetitions for each feature, provide a dict of (str, int) where the key is the name of the variable and the value is the number of repetitions for that feature. This amounts to the number of additional reuploads. So if <code>num_repeats</code> is N, the data gets uploaded N+1 times. Defaults to no repetition.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.operation","title":"<code>operation = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of operation.</p> <p>Choose among the analog or digital rotations or a custom callable function returning an AnalogBlock instance. If the type of operation is incompatible with the <code>strategy</code> chosen, then <code>operation</code> gets preference and the given strategy is ignored.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.param_prefix","title":"<code>param_prefix = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String prefix to create trainable parameters in Feature Map.</p> <p>A string prefix to create trainable parameters multiplying the feature parameter inside the feature-encoding function. Note that currently this does not take into account the domain of the feature-encoding function. Defaults to <code>None</code> and thus, the feature map is not trainable. Note that this is separate from the name of the parameter. The user can provide a single prefix for all features, and it will be appended by appropriate feature name automatically.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.reupload_scaling","title":"<code>reupload_scaling = ReuploadScaling.CONSTANT</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Scaling for encoding the same feature on different qubits.</p> <p>Scaling used to encode the same feature on different qubits in the same layer of the feature maps. Takes qadence.ReuploadScaling. Give a single ReuploadScaling to use the same for all features. Give a dict of (str, ReuploadScaling) where the key is the name of the variable and the value is the ReuploadScaling to use for encoding that feature. ReuploadScaling.CONSTANT for constant scaling. ReuploadScaling.TOWER for linearly increasing scaling. ReuploadScaling.EXP for exponentially increasing scaling.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.tag","title":"<code>tag = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to indicate the name tag of the feature map.</p> <p>Defaults to None, in which case no tag will be applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.target_range","title":"<code>target_range = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Range of data the data encoder assumes as natural range.</p> <p>Give a single tuple to use the same range for all features. Give a dict of (str, tuple) where the key is the name of the variable and the value is the target range to use for that feature.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig","title":"<code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=lambda: list()(), log_model=False, root_folder=Path('./qml_logs'), create_subfolder_per_run=False, log_folder=Path('./'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=ExperimentTrackingTool.TENSORBOARD, hyperparams=dict(), plotting_functions=tuple(), _subfolders=list())</code>  <code>dataclass</code>","text":"<p>Default configuration for the training process.</p> <p>This class provides default settings for various aspects of the training loop, such as logging, checkpointing, and validation. The default values for these fields can be customized when an instance of <code>TrainConfig</code> is created.</p> <p>Example: <pre><code>from qadence.ml_tools import TrainConfig\nc = TrainConfig(root_folder=\"/tmp/train\")\n</code></pre> <pre><code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=[], log_model=False, root_folder='/tmp/train', create_subfolder_per_run=False, log_folder=PosixPath('.'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=&lt;ExperimentTrackingTool.TENSORBOARD: 'tensorboard'&gt;, hyperparams={}, plotting_functions=(), _subfolders=[])\n</code></pre> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.batch_size","title":"<code>batch_size = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The batch size to use when processing a list or tuple of torch.Tensors.</p> <p>This specifies how many samples are processed in each training iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.callbacks","title":"<code>callbacks = field(default_factory=lambda: list())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of callbacks to execute during training.</p> <p>Callbacks can be used for custom behaviors, such as early stopping, custom logging, or other actions triggered at specific events.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.checkpoint_best_only","title":"<code>checkpoint_best_only = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>True</code>, checkpoints are only saved if there is an improvement in the.</p> <p>validation metric. This conserves storage by only keeping the best models.</p> <p>validation_criterion is required when this is set to True.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.checkpoint_every","title":"<code>checkpoint_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for saving model and optimizer checkpoints during training.</p> <p>Set to 0 to disable checkpointing. This helps in resuming training or recovering models. Note that setting checkpoint_best_only = True will disable this and only best checkpoints will be saved.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.create_subfolder_per_run","title":"<code>create_subfolder_per_run = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to create a subfolder for each run, named <code>&lt;id&gt;_&lt;timestamp&gt;_&lt;PID&gt;</code>.</p> <p>This ensures logs and checkpoints from different runs do not overwrite each other, which is helpful for rapid prototyping. If <code>False</code>, training will resume from the latest checkpoint if one exists in the specified log folder.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.hyperparams","title":"<code>hyperparams = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of hyperparameters to be tracked.</p> <p>This can include learning rates, regularization parameters, or any other training-related configurations.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.log_folder","title":"<code>log_folder = Path('./')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The log folder for saving checkpoints and tensorboard logs.</p> <p>This stores the path where all logs and checkpoints are being saved for this training session. <code>log_folder</code> takes precedence over <code>root_folder</code> and <code>create_subfolder_per_run</code> arguments. If the user specifies a log_folder, all checkpoints will be saved in this folder and <code>root_folder</code> argument will not be used.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.log_model","title":"<code>log_model = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to log a serialized version of the model.</p> <p>When set to <code>True</code>, the model's state will be logged, useful for model versioning and reproducibility.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.max_iter","title":"<code>max_iter = 10000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of training iterations (epochs) to perform.</p> <p>This defines the total number of times the model will be updated.</p> <p>In case of InfiniteTensorDataset, each epoch will have 1 batch. In case of TensorDataset, each epoch will have len(dataloader) batches.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.plot_every","title":"<code>plot_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for generating and saving figures during training.</p> <p>Set to 0 to disable plotting.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.plotting_functions","title":"<code>plotting_functions = field(default_factory=tuple)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Functions used for in-training plotting.</p> <p>These are called to generate plots that are logged or saved at specified intervals.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.print_every","title":"<code>print_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for printing loss and metrics to the console during training.</p> <p>Set to 0 to disable this output, meaning that metrics and loss will not be printed during training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.root_folder","title":"<code>root_folder = Path('./qml_logs')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The root folder for saving checkpoints and tensorboard logs.</p> <p>The default path is \"./qml_logs\"</p> <p>This can be set to a specific directory where training artifacts are to be stored. Checkpoints will be saved inside a subfolder in this directory. Subfolders will be created based on <code>create_subfolder_per_run</code> argument.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.tracking_tool","title":"<code>tracking_tool = ExperimentTrackingTool.TENSORBOARD</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The tool used for tracking training progress and logging metrics.</p> <p>Options include tools like TensorBoard, which help visualize and monitor model training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.trainstop_criterion","title":"<code>trainstop_criterion = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to determine if the training process should stop based on a.</p> <p>specific stopping metric. If <code>None</code>, training continues until <code>max_iter</code> is reached.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.val_epsilon","title":"<code>val_epsilon = 1e-05</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A small safety margin used to compare the current validation loss with the.</p> <p>best previous validation loss. This is used to determine improvements in metrics.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.val_every","title":"<code>val_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for performing validation.</p> <p>If set to 0, validation is not performed. Note that metrics from validation are always written, regardless of the <code>write_every</code> setting. Note that initial validation happens at the start of training (when val_every &gt; 0)     For initial validation  - initial metrics are written.                             - checkpoint is saved (when checkpoint_best_only = False)</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.validation_criterion","title":"<code>validation_criterion = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to evaluate whether a given validation metric meets a desired condition.</p> <p>The validation_criterion has the following format: def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:     # process</p> <p>If <code>None</code>, no custom validation criterion is applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.verbose","title":"<code>verbose = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to print metrics and status messages during training.</p> <p>If <code>True</code>, detailed metrics and status updates will be displayed in the console.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.write_every","title":"<code>write_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for writing loss and metrics using the tracking tool during training.</p> <p>Set to 0 to disable this logging, which prevents metrics from being logged to the tracking tool. Note that the metrics will always be written at the end of training regardless of this setting.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.get_parameters","title":"<code>get_parameters(model)</code>","text":"<p>Retrieve all trainable model parameters in a single vector.</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a 1-dimensional tensor with the parameters</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def get_parameters(model: Module) -&gt; Tensor:\n    \"\"\"Retrieve all trainable model parameters in a single vector.\n\n    Args:\n        model (Module): the input PyTorch model\n\n    Returns:\n        Tensor: a 1-dimensional tensor with the parameters\n    \"\"\"\n    ps = [p.reshape(-1) for p in model.parameters() if p.requires_grad]\n    return torch.concat(ps)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.num_parameters","title":"<code>num_parameters(model)</code>","text":"<p>Return the total number of parameters of the given model.</p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def num_parameters(model: Module) -&gt; int:\n    \"\"\"Return the total number of parameters of the given model.\"\"\"\n    return len(get_parameters(model))\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.set_parameters","title":"<code>set_parameters(model, theta)</code>","text":"<p>Set all trainable parameters of a model from a single vector.</p> <p>Notice that this function assumes prior knowledge of right number of parameters in the model</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> <code>theta</code> <p>the parameters to assign</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def set_parameters(model: Module, theta: Tensor) -&gt; None:\n    \"\"\"Set all trainable parameters of a model from a single vector.\n\n    Notice that this function assumes prior knowledge of right number\n    of parameters in the model\n\n    Args:\n        model (Module): the input PyTorch model\n        theta (Tensor): the parameters to assign\n    \"\"\"\n\n    with torch.no_grad():\n        idx = 0\n        for ps in model.parameters():\n            if ps.requires_grad:\n                n = torch.numel(ps)\n                if ps.ndim == 0:\n                    ps[()] = theta[idx : idx + n]\n                else:\n                    ps[:] = theta[idx : idx + n].reshape(ps.size())\n                idx += n\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.optimize_step.optimize_step","title":"<code>optimize_step(model, optimizer, loss_fn, xs, device=None, dtype=None)</code>","text":"<p>Default Torch optimize step with closure.</p> <p>This is the default optimization step.</p> PARAMETER DESCRIPTION <code>model</code> <p>The input model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The chosen Torch optimizer.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable</code> </p> <code>xs</code> <p>The input data. If None, it means the given model does not require any input data.</p> <p> TYPE: <code>dict | list | Tensor | None</code> </p> <code>device</code> <p>A target device to run computations on.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for <code>xs</code> conversion.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor | float, dict | None]</code> <p>tuple[Tensor | float, dict | None]: A tuple containing the computed loss value and a dictionary with collected metrics.</p> Source code in <code>qadence/ml_tools/optimize_step.py</code> <pre><code>def optimize_step(\n    model: Module,\n    optimizer: Optimizer,\n    loss_fn: Callable,\n    xs: dict | list | torch.Tensor | None,\n    device: torch.device = None,\n    dtype: torch.dtype = None,\n) -&gt; tuple[torch.Tensor | float, dict | None]:\n    \"\"\"Default Torch optimize step with closure.\n\n    This is the default optimization step.\n\n    Args:\n        model (Module): The input model to be optimized.\n        optimizer (Optimizer): The chosen Torch optimizer.\n        loss_fn (Callable): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        xs (dict | list | Tensor | None): The input data. If None, it means\n            the given model does not require any input data.\n        device (torch.device): A target device to run computations on.\n        dtype (torch.dtype): Data type for `xs` conversion.\n\n    Returns:\n        tuple[Tensor | float, dict | None]: A tuple containing the computed loss value\n            and a dictionary with collected metrics.\n    \"\"\"\n\n    loss, metrics = None, {}\n    xs_to_device = data_to_device(xs, device=device, dtype=dtype)\n\n    def closure() -&gt; Any:\n        # NOTE: We need the nonlocal as we can't return a metric dict and\n        # because e.g. LBFGS calls this closure multiple times but for some\n        # reason the returned loss is always the first one...\n        nonlocal metrics, loss\n        optimizer.zero_grad()\n        loss, metrics = loss_fn(model, xs_to_device)\n        loss.backward(retain_graph=True)\n        return loss.item()\n\n    optimizer.step(closure)\n    # return the loss/metrics that are being mutated inside the closure...\n    return loss, metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.optimize_step.update_ng_parameters","title":"<code>update_ng_parameters(model, optimizer, loss_fn, data, ng_params)</code>","text":"<p>Update the model parameters using Nevergrad.</p> <p>This function integrates Nevergrad for derivative-free optimization.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>A Nevergrad optimizer instance.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable[[Module, Tensor | None], tuple[float, dict]]</code> </p> <code>data</code> <p>Input data for the model. If None, it means the model does not require input data.</p> <p> TYPE: <code>Tensor | None</code> </p> <code>ng_params</code> <p>The current set of parameters managed by Nevergrad.</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>tuple[float, dict, Array]</code> <p>tuple[float, dict, ng.p.Array]: A tuple containing the computed loss value, a dictionary of metrics, and the updated Nevergrad parameters.</p> Source code in <code>qadence/ml_tools/optimize_step.py</code> <pre><code>def update_ng_parameters(\n    model: Module,\n    optimizer: ng.optimizers.Optimizer,\n    loss_fn: Callable[[Module, torch.Tensor | None], tuple[float, dict]],\n    data: torch.Tensor | None,\n    ng_params: ng.p.Array,\n) -&gt; tuple[float, dict, ng.p.Array]:\n    \"\"\"Update the model parameters using Nevergrad.\n\n    This function integrates Nevergrad for derivative-free optimization.\n\n    Args:\n        model (Module): The PyTorch model to be optimized.\n        optimizer (ng.optimizers.Optimizer): A Nevergrad optimizer instance.\n        loss_fn (Callable[[Module, Tensor | None], tuple[float, dict]]): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        data (Tensor | None): Input data for the model. If None, it means the model does\n            not require input data.\n        ng_params (ng.p.Array): The current set of parameters managed by Nevergrad.\n\n    Returns:\n        tuple[float, dict, ng.p.Array]: A tuple containing the computed loss value,\n            a dictionary of metrics, and the updated Nevergrad parameters.\n    \"\"\"\n    loss, metrics = loss_fn(model, data)  # type: ignore[misc]\n    optimizer.tell(ng_params, float(loss))\n    ng_params = optimizer.ask()  # type: ignore[assignment]\n    params = promote_to_tensor(ng_params.value, requires_grad=False)\n    set_parameters(model, params)\n    return loss, metrics, ng_params\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.DictDataLoader","title":"<code>DictDataLoader(dataloaders)</code>  <code>dataclass</code>","text":"<p>This class only holds a dictionary of <code>DataLoader</code>s and samples from them.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.InfiniteTensorDataset","title":"<code>InfiniteTensorDataset(*tensors)</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Randomly sample points from the first dimension of the given tensors.</p> <p>Behaves like a normal torch <code>Dataset</code> just that we can sample from it as many times as we want.</p> <p>Examples: <pre><code>import torch\nfrom qadence.ml_tools.data import InfiniteTensorDataset\n\nx_data, y_data = torch.rand(5,2), torch.ones(5,1)\n# The dataset accepts any number of tensors with the same batch dimension\nds = InfiniteTensorDataset(x_data, y_data)\n\n# call `next` to get one sample from each tensor:\nxs = next(iter(ds))\n</code></pre> <pre><code>(tensor([0.3343, 0.7113]), tensor([1.]))\n</code></pre></p> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>def __init__(self, *tensors: Tensor):\n    \"\"\"Randomly sample points from the first dimension of the given tensors.\n\n    Behaves like a normal torch `Dataset` just that we can sample from it as\n    many times as we want.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from qadence.ml_tools.data import InfiniteTensorDataset\n\n    x_data, y_data = torch.rand(5,2), torch.ones(5,1)\n    # The dataset accepts any number of tensors with the same batch dimension\n    ds = InfiniteTensorDataset(x_data, y_data)\n\n    # call `next` to get one sample from each tensor:\n    xs = next(iter(ds))\n    print(str(xs)) # markdown-exec: hide\n    ```\n    \"\"\"\n    self.tensors = tensors\n    self.indices = list(range(self.tensors[0].size(0)))\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult","title":"<code>OptimizeResult(iteration, model, optimizer, loss=None, metrics=lambda: dict()(), extra=lambda: dict()())</code>  <code>dataclass</code>","text":"<p>OptimizeResult stores many optimization intermediate values.</p> <p>We store at a current iteration, the model, optimizer, loss values, metrics. An extra dict can be used for saving other information to be used for callbacks.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.extra","title":"<code>extra = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Extra dict for saving anything else to be used in callbacks.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.iteration","title":"<code>iteration</code>  <code>instance-attribute</code>","text":"<p>Current iteration number.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.loss","title":"<code>loss = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Loss value.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.metrics","title":"<code>metrics = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that can be saved during training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model at iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.optimizer","title":"<code>optimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer at iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.data_to_device","title":"<code>data_to_device(xs, *args, **kwargs)</code>","text":"<p>Utility method to move arbitrary data to 'device'.</p> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>@singledispatch\ndef data_to_device(xs: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Utility method to move arbitrary data to 'device'.\"\"\"\n    raise ValueError(f\"Unable to move {type(xs)} with input args: {args} and kwargs: {kwargs}.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.to_dataloader","title":"<code>to_dataloader(*tensors, batch_size=1, infinite=False)</code>","text":"<p>Convert torch tensors an (infinite) Dataloader.</p> PARAMETER DESCRIPTION <code>*tensors</code> <p>Torch tensors to use in the dataloader.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>()</code> </p> <code>batch_size</code> <p>batch size of sampled tensors</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>infinite</code> <p>if <code>True</code>, the dataloader will keep sampling indefinitely even after the whole dataset was sampled once</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>import torch\nfrom qadence.ml_tools import to_dataloader\n\n(x, y, z) = [torch.rand(10) for _ in range(3)]\nloader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\nprint(next(loader))\nprint(next(loader))\nprint(next(loader))\n</code></pre> <pre><code>[tensor([0.0151, 0.9583, 0.7978, 0.8943, 0.3428]), tensor([0.5832, 0.1299, 0.5613, 0.7914, 0.6855]), tensor([0.4468, 0.4279, 0.6553, 0.2204, 0.8142])]\n[tensor([0.5019, 0.9958, 0.5648, 0.1390, 0.4362]), tensor([0.4130, 0.3590, 0.4108, 0.9739, 0.0743]), tensor([0.3099, 0.8700, 0.2857, 0.0713, 0.5670])]\n[tensor([0.0151, 0.9583, 0.7978, 0.8943, 0.3428]), tensor([0.5832, 0.1299, 0.5613, 0.7914, 0.6855]), tensor([0.4468, 0.4279, 0.6553, 0.2204, 0.8142])]\n</code></pre> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>def to_dataloader(*tensors: Tensor, batch_size: int = 1, infinite: bool = False) -&gt; DataLoader:\n    \"\"\"Convert torch tensors an (infinite) Dataloader.\n\n    Arguments:\n        *tensors: Torch tensors to use in the dataloader.\n        batch_size: batch size of sampled tensors\n        infinite: if `True`, the dataloader will keep sampling indefinitely even after the whole\n            dataset was sampled once\n\n    Examples:\n\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from qadence.ml_tools import to_dataloader\n\n    (x, y, z) = [torch.rand(10) for _ in range(3)]\n    loader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\n    print(next(loader))\n    print(next(loader))\n    print(next(loader))\n    ```\n    \"\"\"\n    ds = InfiniteTensorDataset(*tensors) if infinite else TensorDataset(*tensors)\n    return DataLoader(ds, batch_size=batch_size)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN","title":"<code>QNN(circuit, observable, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, inputs=None, input_diff_mode=InputDiffMode.AD)</code>","text":"<p>               Bases: <code>QuantumModel</code></p> <p>Quantum neural network model for n-dimensional inputs.</p> <p>Examples: <pre><code>import torch\nfrom qadence import QuantumCircuit, QNN, Z\nfrom qadence import hea, feature_map, hamiltonian_factory, kron\n\n# create the circuit\nn_qubits, depth = 2, 4\nfm = kron(\n    feature_map(1, support=(0,), param=\"x\"),\n    feature_map(1, support=(1,), param=\"y\")\n)\nansatz = hea(n_qubits=n_qubits, depth=depth)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nobs_base = hamiltonian_factory(n_qubits, detuning=Z)\n\n# the QNN will yield two outputs\nobs = [2.0 * obs_base, 4.0 * obs_base]\n\n# initialize and use the model\nqnn = QNN(circuit, obs, inputs=[\"x\", \"y\"])\ny = qnn(torch.rand(3, 2))\n</code></pre> <pre><code>tensor([[1.6556, 3.3113],\n        [1.2871, 2.5742],\n        [1.0609, 2.1219]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> <p>Initialize the QNN.</p> <p>The number of inputs is determined by the feature parameters in the input quantum circuit while the number of outputs is determined by how many observables are provided as input</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The quantum circuit to use for the QNN.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>The observable.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>optional configuration for the backend</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>inputs</code> <p>List that indicates the order of variables of the tensors that are passed to the model. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p> <p> TYPE: <code>list[Basic | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode | str</code> DEFAULT: <code>AD</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    inputs: list[sympy.Basic | str] | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n):\n    \"\"\"Initialize the QNN.\n\n    The number of inputs is determined by the feature parameters in the input\n    quantum circuit while the number of outputs is determined by how many\n    observables are provided as input\n\n    Args:\n        circuit: The quantum circuit to use for the QNN.\n        observable: The observable.\n        backend: The chosen quantum backend.\n        diff_mode: The differentiation engine to use. Choices 'gpsr' or 'ad'.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        configuration: optional configuration for the backend\n        inputs: List that indicates the order of variables of the tensors that are passed\n            to the model. Given input tensors `xs = torch.rand(batch_size, input_size:=2)` a QNN\n            with `inputs=[\"t\", \"x\"]` will assign `t, x = xs[:,0], xs[:,1]`.\n        input_diff_mode: The differentiation mode for the input tensor.\n    \"\"\"\n    super().__init__(\n        circuit,\n        observable=observable,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        configuration=configuration,\n        noise=noise,\n    )\n    if self._observable is None:\n        raise ValueError(\"You need to provide at least one observable in the QNN constructor\")\n    if (inputs is not None) and (len(self.inputs) == len(inputs)):\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in inputs]  # type: ignore[union-attr]\n    elif (inputs is None) and len(self.inputs) &lt;= 1:\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in self.inputs]  # type: ignore[union-attr]\n    else:\n        raise ValueError(\n            \"\"\"\n            Your QNN has more than one input. Please provide a list of inputs in the order of\n            your tensor domain. For example, if you want to pass\n            `xs = torch.rand(batch_size, input_size:=3)` to you QNN, where\n            ```\n            t = x[:,0]\n            x = x[:,1]\n            y = x[:,2]\n            ```\n            you have to specify\n            ```\n            QNN(circuit, observable, inputs=[\"t\", \"x\", \"y\"])\n            ```\n            You can also pass a list of sympy symbols.\n        \"\"\"\n        )\n    self.format_to_dict = format_to_dict_fn(self.inputs)  # type: ignore[arg-type]\n    self.input_diff_mode = InputDiffMode(input_diff_mode)\n    if self.input_diff_mode == InputDiffMode.FD:\n        from qadence.backends.utils import finitediff\n\n        self.__derivative = finitediff\n    elif self.input_diff_mode == InputDiffMode.AD:\n        self.__derivative = _torch_derivative  # type: ignore[assignment]\n    else:\n        raise ValueError(f\"Unkown forward diff mode: {self.input_diff_mode}\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN.forward","title":"<code>forward(values=None, state=None, measurement=None, noise=None, endianness=Endianness.BIG)</code>","text":"<p>Forward pass of the model.</p> <p>This returns the (differentiable) expectation value of the given observable operator defined in the constructor. Differently from the base QuantumModel class, the QNN accepts also a tensor as input for the forward pass. The tensor is expected to have shape: <code>n_batches x in_features</code> where <code>n_batches</code> is the number of data points and <code>in_features</code> is the dimensionality of the problem</p> <p>The output of the forward pass is the expectation value of the input observable(s). If a single observable is given, the output shape is <code>n_batches</code> while if multiple observables are given the output shape is instead <code>n_batches x n_observables</code></p> PARAMETER DESCRIPTION <code>values</code> <p>the values of the feature parameters</p> <p> TYPE: <code>dict[str, Tensor] | Tensor</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a tensor with the expectation value of the observables passed in the constructor of the model</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def forward(\n    self,\n    values: dict[str, Tensor] | Tensor = None,\n    state: Tensor | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the model.\n\n    This returns the (differentiable) expectation value of the given observable\n    operator defined in the constructor. Differently from the base QuantumModel\n    class, the QNN accepts also a tensor as input for the forward pass. The\n    tensor is expected to have shape: `n_batches x in_features` where `n_batches`\n    is the number of data points and `in_features` is the dimensionality of the problem\n\n    The output of the forward pass is the expectation value of the input\n    observable(s). If a single observable is given, the output shape is\n    `n_batches` while if multiple observables are given the output shape\n    is instead `n_batches x n_observables`\n\n    Args:\n        values: the values of the feature parameters\n        state: Initial state.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n\n    Returns:\n        Tensor: a tensor with the expectation value of the observables passed\n            in the constructor of the model\n    \"\"\"\n    return self.expectation(\n        values, state=state, measurement=measurement, noise=noise, endianness=endianness\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN.from_configs","title":"<code>from_configs(register, obs_config, fm_config=FeatureMapConfig(), ansatz_config=AnsatzConfig(), backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, input_diff_mode=InputDiffMode.AD)</code>  <code>classmethod</code>","text":"<p>Create a QNN from a set of configurations.</p> PARAMETER DESCRIPTION <code>register</code> <p>The number of qubits or a register object.</p> <p> TYPE: <code>int | Register</code> </p> <code>obs_config</code> <p>The configuration(s) for the observable(s).</p> <p> TYPE: <code>list[ObservableConfig] | ObservableConfig</code> </p> <code>fm_config</code> <p>The configuration for the feature map. Defaults to no feature encoding block.</p> <p> TYPE: <code>FeatureMapConfig</code> DEFAULT: <code>FeatureMapConfig()</code> </p> <code>ansatz_config</code> <p>The configuration for the ansatz. Defaults to a single layer of hardware efficient ansatz.</p> <p> TYPE: <code>AnsatzConfig</code> DEFAULT: <code>AnsatzConfig()</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices are 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>Noise</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Optional backend configuration.</p> <p> TYPE: <code>BackendConfiguration | dict</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode</code> DEFAULT: <code>AD</code> </p> RETURNS DESCRIPTION <code>QNN</code> <p>A QNN object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the observable configuration is not provided.</p> <p>Example: <pre><code>import torch\nfrom qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\nfrom qadence.ml_tools import QNN\nfrom qadence.constructors import ObservableConfig\nfrom qadence.operations import Z\nfrom qadence.types import (\n    AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n)\n\nregister = 4\nobs_config = ObservableConfig(\n    detuning=Z,\n    scale=5.0,\n    shift=0.0,\n    transformation_type=ObservableTransform.SCALE,\n    trainable_transform=None,\n)\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs=[\"x\", \"y\"],\n    basis_set=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.CONSTANT,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nqnn = QNN.from_configs(\n    register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n)\n\nx = torch.rand(2, 2)\ny = qnn(x)\n</code></pre> <pre><code>tensor([[ 1.1293],\n        [-0.3442]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>@classmethod\ndef from_configs(\n    cls,\n    register: int | Register,\n    obs_config: Any,\n    fm_config: Any = FeatureMapConfig(),\n    ansatz_config: Any = AnsatzConfig(),\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n) -&gt; QNN:\n    \"\"\"Create a QNN from a set of configurations.\n\n    Args:\n        register (int | Register): The number of qubits or a register object.\n        obs_config (list[ObservableConfig] | ObservableConfig): The configuration(s)\n            for the observable(s).\n        fm_config (FeatureMapConfig): The configuration for the feature map.\n            Defaults to no feature encoding block.\n        ansatz_config (AnsatzConfig): The configuration for the ansatz.\n            Defaults to a single layer of hardware efficient ansatz.\n        backend (BackendName): The chosen quantum backend.\n        diff_mode (DiffMode): The differentiation engine to use. Choices are\n            'gpsr' or 'ad'.\n        measurement (Measurements): Optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator.\n        noise (Noise): A noise model to use.\n        configuration (BackendConfiguration | dict): Optional backend configuration.\n        input_diff_mode (InputDiffMode): The differentiation mode for the input tensor.\n\n    Returns:\n        A QNN object.\n\n    Raises:\n        ValueError: If the observable configuration is not provided.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\n    from qadence.ml_tools import QNN\n    from qadence.constructors import ObservableConfig\n    from qadence.operations import Z\n    from qadence.types import (\n        AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n    )\n\n    register = 4\n    obs_config = ObservableConfig(\n        detuning=Z,\n        scale=5.0,\n        shift=0.0,\n        transformation_type=ObservableTransform.SCALE,\n        trainable_transform=None,\n    )\n    fm_config = FeatureMapConfig(\n        num_features=2,\n        inputs=[\"x\", \"y\"],\n        basis_set=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.CONSTANT,\n        feature_range={\n            \"x\": (-1.0, 1.0),\n            \"y\": (0.0, 1.0),\n        },\n    )\n    ansatz_config = AnsatzConfig(\n        depth=2,\n        ansatz_type=AnsatzType.HEA,\n        ansatz_strategy=Strategy.DIGITAL,\n    )\n\n    qnn = QNN.from_configs(\n        register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n    )\n\n    x = torch.rand(2, 2)\n    y = qnn(x)\n    print(str(y)) # markdown-exec: hide\n    ```\n    \"\"\"\n    from .constructors import build_qnn_from_configs\n\n    return build_qnn_from_configs(\n        register=register,\n        observable_config=obs_config,\n        fm_config=fm_config,\n        ansatz_config=ansatz_config,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        noise=noise,\n        configuration=configuration,\n        input_diff_mode=input_diff_mode,\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.derivative","title":"<code>derivative(ufa, x, derivative_indices)</code>","text":"<p>Compute derivatives w.r.t.</p> <p>inputs of a UFA with a single output. The <code>derivative_indices</code> specify which derivative(s) are computed.  E.g. <code>derivative_indices=(1,2)</code> would compute the a second order derivative w.r.t to the indices <code>1</code> and <code>2</code> of the input tensor.</p> PARAMETER DESCRIPTION <code>ufa</code> <p>The model for which we want to compute the derivative.</p> <p> TYPE: <code>Module</code> </p> <code>x</code> <p>(batch_size, input_size) input tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>derivative_indices</code> <p>Define which derivatives to compute.</p> <p> TYPE: <code>tuple</code> </p> <p>Examples: If we create a UFA with three inputs and denote the first, second, and third input with <code>x</code>, <code>y</code>, and <code>z</code> we can compute the following derivatives w.r.t to those inputs: <pre><code>import torch\nfrom qadence.ml_tools.models import derivative, QNN\nfrom qadence.ml_tools.config import FeatureMapConfig, AnsatzConfig\nfrom qadence.constructors.hamiltonians import ObservableConfig\nfrom qadence.operations import Z\n\nfm_config = FeatureMapConfig(num_features=3, inputs=[\"x\", \"y\", \"z\"])\nansatz_config = AnsatzConfig()\nobs_config = ObservableConfig(detuning=Z)\n\nf = QNN.from_configs(\n    register=3, obs_config=obs_config, fm_config=fm_config, ansatz_config=ansatz_config,\n)\ninputs = torch.rand(5,3,requires_grad=True)\n\n# df_dx\nderivative(f, inputs, (0,))\n\n# d2f_dydz\nderivative(f, inputs, (1,2))\n\n# d3fdy2dx\nderivative(f, inputs, (1,1,0))\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def derivative(ufa: torch.nn.Module, x: Tensor, derivative_indices: tuple[int, ...]) -&gt; Tensor:\n    \"\"\"Compute derivatives w.r.t.\n\n    inputs of a UFA with a single output. The\n    `derivative_indices` specify which derivative(s) are computed.  E.g.\n    `derivative_indices=(1,2)` would compute the a second order derivative w.r.t\n    to the indices `1` and `2` of the input tensor.\n\n    Arguments:\n        ufa: The model for which we want to compute the derivative.\n        x (Tensor): (batch_size, input_size) input tensor.\n        derivative_indices (tuple): Define which derivatives to compute.\n\n    Examples:\n    If we create a UFA with three inputs and denote the first, second, and third\n    input with `x`, `y`, and `z` we can compute the following derivatives w.r.t\n    to those inputs:\n    ```py exec=\"on\" source=\"material-block\"\n    import torch\n    from qadence.ml_tools.models import derivative, QNN\n    from qadence.ml_tools.config import FeatureMapConfig, AnsatzConfig\n    from qadence.constructors.hamiltonians import ObservableConfig\n    from qadence.operations import Z\n\n    fm_config = FeatureMapConfig(num_features=3, inputs=[\"x\", \"y\", \"z\"])\n    ansatz_config = AnsatzConfig()\n    obs_config = ObservableConfig(detuning=Z)\n\n    f = QNN.from_configs(\n        register=3, obs_config=obs_config, fm_config=fm_config, ansatz_config=ansatz_config,\n    )\n    inputs = torch.rand(5,3,requires_grad=True)\n\n    # df_dx\n    derivative(f, inputs, (0,))\n\n    # d2f_dydz\n    derivative(f, inputs, (1,2))\n\n    # d3fdy2dx\n    derivative(f, inputs, (1,1,0))\n    ```\n    \"\"\"\n    assert ufa.out_features == 1, \"Can only call `derivative` on models with 1D output.\"\n    return ufa._derivative(x, derivative_indices)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.format_to_dict_fn","title":"<code>format_to_dict_fn(inputs=[])</code>","text":"<p>Format an input tensor into the format required by the forward pass.</p> <p>The tensor is assumed to have dimensions: n_batches x in_features where in_features corresponds to the number of input features of the QNN</p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def format_to_dict_fn(\n    inputs: list[sympy.Symbol | str] = [],\n) -&gt; Callable[[Tensor | ParamDictType], ParamDictType]:\n    \"\"\"Format an input tensor into the format required by the forward pass.\n\n    The tensor is assumed to have dimensions: n_batches x in_features where in_features\n    corresponds to the number of input features of the QNN\n    \"\"\"\n    in_features = len(inputs)\n\n    def tensor_to_dict(values: Tensor | ParamDictType) -&gt; ParamDictType:\n        if isinstance(values, Tensor):\n            values = values.reshape(-1, 1) if len(values.size()) == 1 else values\n            if not values.shape[1] == in_features:\n                raise ValueError(\n                    f\"Model expects in_features={in_features} but got {values.shape[1]}.\"\n                )\n            values = {fparam.name: values[:, inputs.index(fparam)] for fparam in inputs}  # type: ignore[union-attr]\n        return values\n\n    return tensor_to_dict\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback","title":"<code>Callback(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>Base class for defining various training callbacks.</p> ATTRIBUTE DESCRIPTION <code>on</code> <p>The event on which to trigger the callback. Must be a valid on value from: [\"train_start\", \"train_end\",     \"train_epoch_start\", \"train_epoch_end\", \"train_batch_start\",     \"train_batch_end\",\"val_epoch_start\", \"val_epoch_end\",     \"val_batch_start\", \"val_batch_end\", \"test_batch_start\",     \"test_batch_end\"]</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>callback</code> <p>The function to call if the condition is met.</p> <p> TYPE: <code>CallbackFunction | None</code> </p> <code>callback_condition</code> <p>Condition to check before calling.</p> <p> TYPE: <code>CallbackConditionFunction | None</code> </p> <code>modify_optimize_result</code> <p>Function to modify <code>OptimizeResult</code>.</p> <p> TYPE: <code>CallbackFunction | dict[str, Any] | None</code> </p> <p>A callback can be defined in two ways:</p> <ol> <li>By providing a callback function directly in the base class:    This is useful for simple callbacks that don't require subclassing.</li> </ol> <p>Example:    <pre><code>from qadence.ml_tools.callbacks import Callback\n\ndef custom_callback_function(trainer, config, writer):\n    print(\"Custom callback executed.\")\n\ncustom_callback = Callback(\n    on=\"train_end\",\n    called_every=5,\n    callback=custom_callback_function\n)\n</code></pre> <pre><code>\n</code></pre> </p> <ol> <li>By inheriting and implementing the <code>run_callback</code> method:    This is suitable for more complex callbacks that require customization.</li> </ol> <p>Example:    <pre><code>from qadence.ml_tools.callbacks import Callback\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in the inherited run_callback method.\")\n\ncustom_callback = CustomCallback(on=\"train_end\", called_every=10)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.on","title":"<code>on</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the TrainingStage.</p> RETURNS DESCRIPTION <code>TrainingStage</code> <p>TrainingStage for the callback</p> <p> TYPE: <code>TrainingStage | str</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.__call__","title":"<code>__call__(when, trainer, config, writer)</code>","text":"<p>Executes the callback if conditions are met.</p> PARAMETER DESCRIPTION <code>when</code> <p>The event when the callback is triggered.</p> <p> TYPE: <code>str</code> </p> <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback function if executed.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __call__(\n    self, when: TrainingStage, trainer: Any, config: TrainConfig, writer: BaseWriter\n) -&gt; Any:\n    \"\"\"Executes the callback if conditions are met.\n\n    Args:\n        when (str): The event when the callback is triggered.\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback function if executed.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if self.on == when:\n        if opt_result:\n            opt_result = self.modify_optimize_result(opt_result)\n        if self._should_call(when, opt_result):\n            return self.run_callback(trainer, config, writer)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Executes the defined callback.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback execution.</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If not implemented in subclasses.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Executes the defined callback.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback execution.\n\n    Raises:\n        NotImplementedError: If not implemented in subclasses.\n    \"\"\"\n    if self.callback is not None:\n        return self.callback(trainer, config, writer)\n    raise NotImplementedError(\"Subclasses should override the run_callback method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.EarlyStopping","title":"<code>EarlyStopping(on, called_every, monitor, patience=5, mode='min')</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Stops training when a monitored metric has not improved for a specified number of epochs.</p> <p>This callback monitors a specified metric (e.g., validation loss or accuracy). If the metric does not improve for a given patience period, training is stopped.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>EarlyStopping</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import EarlyStopping\n\n# Create an instance of the EarlyStopping callback\nearly_stopping = EarlyStopping(on=\"val_epoch_end\",\n                               called_every=1,\n                               monitor=\"val_loss\",\n                               patience=5,\n                               mode=\"min\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    print_every=1000,\n    callbacks=[early_stopping]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the EarlyStopping callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback (e.g., \"val_epoch_end\").</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>monitor</code> <p>The metric to monitor (e.g., \"val_loss\" or \"train_loss\"). All metrics returned by optimize step are available to monitor. Please add \"val_\" and \"train_\" strings at the start of the metric name.</p> <p> TYPE: <code>str</code> </p> <code>patience</code> <p>Number of iterations to wait for improvement. Default is 5.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>mode</code> <p>Whether to minimize (\"min\") or maximize (\"max\") the metric. Default is \"min\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'min'</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self, on: str, called_every: int, monitor: str, patience: int = 5, mode: str = \"min\"\n):\n    \"\"\"Initializes the EarlyStopping callback.\n\n    Args:\n        on (str): The event to trigger the callback (e.g., \"val_epoch_end\").\n        called_every (int): Frequency of callback calls in terms of iterations.\n        monitor (str): The metric to monitor (e.g., \"val_loss\" or \"train_loss\").\n            All metrics returned by optimize step are available to monitor.\n            Please add \"val_\" and \"train_\" strings at the start of the metric name.\n        patience (int, optional): Number of iterations to wait for improvement. Default is 5.\n        mode (str, optional): Whether to minimize (\"min\") or maximize (\"max\") the metric.\n            Default is \"min\".\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.monitor = monitor\n    self.patience = patience\n    self.mode = mode\n    self.best_value = float(\"inf\") if mode == \"min\" else -float(\"inf\")\n    self.counter = 0\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.EarlyStopping.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Monitors the metric and stops training if no improvement is observed.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Monitors the metric and stops training if no improvement is observed.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    current_value = trainer.opt_result.metrics.get(self.monitor)\n    if current_value is None:\n        raise ValueError(f\"Metric '{self.monitor}' is not available in the trainer's metrics.\")\n\n    if (self.mode == \"min\" and current_value &lt; self.best_value) or (\n        self.mode == \"max\" and current_value &gt; self.best_value\n    ):\n        self.best_value = current_value\n        self.counter = 0\n    else:\n        self.counter += 1\n\n    if self.counter &gt;= self.patience:\n        logger.info(\n            f\"EarlyStopping: No improvement in '{self.monitor}' for {self.patience} epochs. \"\n            \"Stopping training.\"\n        )\n        trainer.stop_training = True\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.GradientMonitoring","title":"<code>GradientMonitoring(on, called_every=1)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Logs gradient statistics (e.g., mean, standard deviation, max) during training.</p> <p>This callback monitors and logs statistics about the gradients of the model parameters to help debug or optimize the training process.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>GradientMonitoring</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import GradientMonitoring\n\n# Create an instance of the GradientMonitoring callback\ngradient_monitoring = GradientMonitoring(on=\"train_batch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    print_every=1000,\n    callbacks=[gradient_monitoring]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the GradientMonitoring callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback (e.g., \"train_batch_end\").</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int = 1):\n    \"\"\"Initializes the GradientMonitoring callback.\n\n    Args:\n        on (str): The event to trigger the callback (e.g., \"train_batch_end\").\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.GradientMonitoring.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs gradient statistics.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Logs gradient statistics.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    gradient_stats = {}\n    for name, param in trainer.model.named_parameters():\n        if param.grad is not None:\n            grad = param.grad\n            gradient_stats.update(\n                {\n                    name + \"_mean\": grad.mean().item(),\n                    name + \"_std\": grad.std().item(),\n                    name + \"_max\": grad.max().item(),\n                    name + \"_min\": grad.min().item(),\n                }\n            )\n\n    writer.write(trainer.opt_result.iteration, gradient_stats)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCosineAnnealing","title":"<code>LRSchedulerCosineAnnealing(on, called_every, t_max, min_lr=0.0)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Applies cosine annealing to the learning rate during training.</p> <p>This callback decreases the learning rate following a cosine curve, starting from the initial learning rate and annealing to a minimum (min_lr).</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerCosineAnnealing</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCosineAnnealing\n\n# Create an instance of the LRSchedulerCosineAnnealing callback\nlr_cosine = LRSchedulerCosineAnnealing(on=\"train_batch_end\",\n                                       called_every=1,\n                                       t_max=5000,\n                                       min_lr=1e-6)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_cosine]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerCosineAnnealing callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>t_max</code> <p>The total number of iterations for one annealing cycle.</p> <p> TYPE: <code>int</code> </p> <code>min_lr</code> <p>The minimum learning rate. Default is 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, t_max: int, min_lr: float = 0.0):\n    \"\"\"Initializes the LRSchedulerCosineAnnealing callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        t_max (int): The total number of iterations for one annealing cycle.\n        min_lr (float, optional): The minimum learning rate. Default is 0.0.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.t_max = t_max\n    self.min_lr = min_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCosineAnnealing.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Adjusts the learning rate using cosine annealing.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Adjusts the learning rate using cosine annealing.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    for param_group in trainer.optimizer.param_groups:\n        max_lr = param_group[\"lr\"]\n        new_lr = (\n            self.min_lr\n            + (max_lr - self.min_lr)\n            * (1 + math.cos(math.pi * trainer.opt_result.iteration / self.t_max))\n            / 2\n        )\n        param_group[\"lr\"] = new_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCyclic","title":"<code>LRSchedulerCyclic(on, called_every, base_lr, max_lr, step_size)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Applies a cyclic learning rate schedule during training.</p> <p>This callback oscillates the learning rate between a minimum (base_lr) and a maximum (max_lr) over a defined cycle length (step_size). The learning rate follows a triangular wave pattern.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerCyclic</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCyclic\n\n# Create an instance of the LRSchedulerCyclic callback\nlr_cyclic = LRSchedulerCyclic(on=\"train_batch_end\",\n                              called_every=1,\n                              base_lr=0.001,\n                              max_lr=0.01,\n                              step_size=2000)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_cyclic]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerCyclic callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>base_lr</code> <p>The minimum learning rate.</p> <p> TYPE: <code>float</code> </p> <code>max_lr</code> <p>The maximum learning rate.</p> <p> TYPE: <code>float</code> </p> <code>step_size</code> <p>Number of iterations for half a cycle.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, base_lr: float, max_lr: float, step_size: int):\n    \"\"\"Initializes the LRSchedulerCyclic callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        base_lr (float): The minimum learning rate.\n        max_lr (float): The maximum learning rate.\n        step_size (int): Number of iterations for half a cycle.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.base_lr = base_lr\n    self.max_lr = max_lr\n    self.step_size = step_size\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCyclic.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Adjusts the learning rate cyclically.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Adjusts the learning rate cyclically.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    cycle = trainer.opt_result.iteration // (2 * self.step_size)\n    x = abs(trainer.opt_result.iteration / self.step_size - 2 * cycle - 1)\n    scale = max(0, (1 - x))\n    new_lr = self.base_lr + (self.max_lr - self.base_lr) * scale\n    for param_group in trainer.optimizer.param_groups:\n        param_group[\"lr\"] = new_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerStepDecay","title":"<code>LRSchedulerStepDecay(on, called_every, gamma=0.5)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Reduces the learning rate by a factor at regular intervals.</p> <p>This callback adjusts the learning rate by multiplying it with a decay factor after a specified number of iterations. The learning rate is updated as:     lr = lr * gamma</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerStepDecay</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerStepDecay\n\n# Create an instance of the LRSchedulerStepDecay callback\nlr_step_decay = LRSchedulerStepDecay(on=\"train_epoch_end\",\n                                     called_every=100,\n                                     gamma=0.5)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_step_decay]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerStepDecay callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>gamma</code> <p>The decay factor applied to the learning rate. A value &lt; 1 reduces the learning rate over time. Default is 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, gamma: float = 0.5):\n    \"\"\"Initializes the LRSchedulerStepDecay callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        gamma (float, optional): The decay factor applied to the learning rate.\n            A value &lt; 1 reduces the learning rate over time. Default is 0.5.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.gamma = gamma\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerStepDecay.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Runs the callback to apply step decay to the learning rate.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Runs the callback to apply step decay to the learning rate.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    for param_group in trainer.optimizer.param_groups:\n        param_group[\"lr\"] *= self.gamma\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LoadCheckpoint","title":"<code>LoadCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to load a model checkpoint.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LoadCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Loads a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The result of loading the checkpoint.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Loads a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: The result of loading the checkpoint.\n    \"\"\"\n    folder = config.log_folder\n    model = trainer.model\n    optimizer = trainer.optimizer\n    device = trainer.log_device\n    return load_checkpoint(folder, model, optimizer, device=device)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogHyperparameters","title":"<code>LogHyperparameters(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log hyperparameters using the writer.</p> <p>The <code>LogHyperparameters</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LogHyperparameters</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogHyperparameters\n\n# Create an instance of the LogHyperparameters callback\nlog_hyper_callback = LogHyperparameters(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[log_hyper_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogHyperparameters.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs hyperparameters using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs hyperparameters using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    hyperparams = config.hyperparams\n    writer.log_hyperparams(hyperparams)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogModelTracker","title":"<code>LogModelTracker(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log the model using the writer.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogModelTracker.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs the model using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs the model using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    model = trainer.model\n    writer.log_model(\n        model, trainer.train_dataloader, trainer.val_dataloader, trainer.test_dataloader\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PlotMetrics","title":"<code>PlotMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to plot metrics using the writer.</p> <p>The <code>PlotMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PlotMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PlotMetrics\n\n# Create an instance of the PlotMetrics callback\nplot_metrics_callback = PlotMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[plot_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PlotMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Plots metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Plots metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    plotting_functions = config.plotting_functions\n    writer.plot(trainer.model, opt_result.iteration, plotting_functions)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PrintMetrics","title":"<code>PrintMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to print metrics using the writer.</p> <p>The <code>PrintMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PrintMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\n# Create an instance of the PrintMetrics callback\nprint_metrics_callback = PrintMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[print_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PrintMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Prints metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Prints metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.print_metrics(opt_result)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveBestCheckpoint","title":"<code>SaveBestCheckpoint(on, called_every)</code>","text":"<p>               Bases: <code>SaveCheckpoint</code></p> <p>Callback to save the best model checkpoint based on a validation criterion.</p> <p>Initializes the SaveBestCheckpoint callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int):\n    \"\"\"Initializes the SaveBestCheckpoint callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.best_loss = float(\"inf\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveBestCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves the checkpoint if the current loss is better than the best loss.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves the checkpoint if the current loss is better than the best loss.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if config.validation_criterion and config.validation_criterion(\n        opt_result.loss, self.best_loss, config.val_epsilon\n    ):\n        self.best_loss = opt_result.loss\n\n        folder = config.log_folder\n        model = trainer.model\n        optimizer = trainer.optimizer\n        opt_result = trainer.opt_result\n        write_checkpoint(folder, model, optimizer, \"best\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveCheckpoint","title":"<code>SaveCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to save a model checkpoint.</p> <p>The <code>SaveCheckpoint</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>SaveCheckpoint</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint\n\n# Create an instance of the SaveCheckpoint callback\nsave_checkpoint_callback = SaveCheckpoint(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    folder = config.log_folder\n    model = trainer.model\n    optimizer = trainer.optimizer\n    opt_result = trainer.opt_result\n    write_checkpoint(folder, model, optimizer, opt_result.iteration)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.WriteMetrics","title":"<code>WriteMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to write metrics using the writer.</p> <p>The <code>WriteMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>WriteMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import WriteMetrics\n\n# Create an instance of the WriteMetrics callback\nwrite_metrics_callback = WriteMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[write_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.WriteMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Writes metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Writes metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.write(opt_result.iteration, opt_result.metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer","title":"<code>BaseTrainer(model, optimizer, config, loss_fn='mse', optimize_step=optimize_step, train_dataloader=None, val_dataloader=None, test_dataloader=None, max_batches=None)</code>","text":"<p>Base class for training machine learning models using a given optimizer.</p> <p>The base class implements contextmanager for gradient based/free optimization, properties, property setters, input validations, callback decorator generator, and empty hooks for different training steps.</p> This class provides <ul> <li>Context managers for enabling/disabling gradient-based optimization</li> <li>Properties for managing models, optimizers, and dataloaders</li> <li>Input validations and a callback decorator generator</li> <li>Config and callback managers using the provided <code>TrainConfig</code></li> </ul> ATTRIBUTE DESCRIPTION <code>use_grad</code> <p>Indicates if gradients are used for optimization. Default is True.</p> <p> TYPE: <code>bool</code> </p> <code>model</code> <p>The neural network model.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The configuration settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>optimize_step</code> <p>Function for performing an optimization step.</p> <p> TYPE: <code>Callable</code> </p> <code>loss_fn</code> <p>loss function to use. Default loss function used is 'mse'</p> <p> TYPE: <code>Callable | str ]</code> </p> <code>num_training_batches</code> <p>Number of training batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_validation_batches</code> <p>Number of validation batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_test_batches</code> <p>Number of test batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>state</code> <p>Current state in the training process</p> <p> TYPE: <code>str</code> </p> <p>Initializes the BaseTrainer.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The TrainConfig settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>The loss function to use. str input to be specified to use a default loss function. currently supported loss functions: 'mse', 'cross_entropy'. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data. If the model does not need data to evaluate loss, no dataset should be provided.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    optimize_step: Callable = optimize_step,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the BaseTrainer.\n\n    Args:\n        model (nn.Module): The model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer\n            for training.\n        config (TrainConfig): The TrainConfig settings for training.\n        loss_fn (str | Callable): The loss function to use.\n            str input to be specified to use a default loss function.\n            currently supported loss functions: 'mse', 'cross_entropy'.\n            If not specified, default mse loss will be used.\n        train_dataloader (Dataloader | DictDataLoader | None): DataLoader for training data.\n            If the model does not need data to evaluate loss, no dataset\n            should be provided.\n        val_dataloader (Dataloader | DictDataLoader | None): DataLoader for validation data.\n        test_dataloader (Dataloader | DictDataLoader | None): DataLoader for testing data.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    self._model: nn.Module\n    self._optimizer: optim.Optimizer | NGOptimizer | None\n    self._config: TrainConfig\n    self._train_dataloader: DataLoader | DictDataLoader | None = None\n    self._val_dataloader: DataLoader | DictDataLoader | None = None\n    self._test_dataloader: DataLoader | DictDataLoader | None = None\n\n    self.config = config\n    self.model = model\n    self.optimizer = optimizer\n    self.max_batches = max_batches\n\n    self.num_training_batches: int\n    self.num_validation_batches: int\n    self.num_test_batches: int\n\n    self.train_dataloader = train_dataloader\n    self.val_dataloader = val_dataloader\n    self.test_dataloader = test_dataloader\n\n    self.loss_fn: Callable = get_loss_fn(loss_fn)\n    self.optimize_step: Callable = optimize_step\n    self.ng_params: ng.p.Array\n    self.training_stage: TrainingStage = TrainingStage(\"idle\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.config","title":"<code>config</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training configuration.</p> RETURNS DESCRIPTION <code>TrainConfig</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.model","title":"<code>model</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the model if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Module</code> <p>nn.Module: The model.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.optimizer","title":"<code>optimizer</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimizer if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Optimizer | Optimizer | None</code> <p>optim.Optimizer | NGOptimizer | None: The optimizer.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.test_dataloader","title":"<code>test_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the test DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for testing data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.train_dataloader","title":"<code>train_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.use_grad","title":"<code>use_grad</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimization framework for the trainer.</p> <p>use_grad = True : Gradient based optimization use_grad = False : Gradient free optimization</p> RETURNS DESCRIPTION <code>bool</code> <p>Bool value for using gradient.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.val_dataloader","title":"<code>val_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the validation DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.callback","title":"<code>callback(phase)</code>  <code>staticmethod</code>","text":"<p>Decorator for executing callbacks before and after a phase.</p> <p>Phase are different hooks during the training. list of valid phases is defined in Callbacks. We also update the current state of the training process in the callback decorator.</p> PARAMETER DESCRIPTION <code>phase</code> <p>The phase for which the callback is executed (e.g., \"train\", \"train_epoch\", \"train_batch\").</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>The decorated function.</p> <p> TYPE: <code>Callable</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@staticmethod\ndef callback(phase: str) -&gt; Callable:\n    \"\"\"\n    Decorator for executing callbacks before and after a phase.\n\n    Phase are different hooks during the training. list of valid\n    phases is defined in Callbacks.\n    We also update the current state of the training process in\n    the callback decorator.\n\n    Args:\n        phase (str): The phase for which the callback is executed (e.g., \"train\",\n            \"train_epoch\", \"train_batch\").\n\n    Returns:\n        Callable: The decorated function.\n    \"\"\"\n\n    def decorator(method: Callable) -&gt; Callable:\n        def wrapper(self: Any, *args: Any, **kwargs: Any) -&gt; Any:\n            start_event = f\"{phase}_start\"\n            end_event = f\"{phase}_end\"\n\n            self.training_stage = TrainingStage(start_event)\n            self.callback_manager.run_callbacks(trainer=self)\n            result = method(self, *args, **kwargs)\n\n            self.training_stage = TrainingStage(end_event)\n            # build_optimize_result method is defined in the trainer.\n            self.build_optimize_result(result)\n            self.callback_manager.run_callbacks(trainer=self)\n\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.disable_grad_opt","title":"<code>disable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily disable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The Nevergrad optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef disable_grad_opt(self, optimizer: NGOptimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily disable gradient-based optimization.\n\n    Args:\n        optimizer (NGOptimizer): The Nevergrad optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = False\n        self.callback_manager.use_grad = False\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.enable_grad_opt","title":"<code>enable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily enable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The PyTorch optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef enable_grad_opt(self, optimizer: optim.Optimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily enable gradient-based optimization.\n\n    Args:\n        optimizer (optim.Optimizer): The PyTorch optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = True\n        self.callback_manager.use_grad = True\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_test_batch_end","title":"<code>on_test_batch_end(test_batch_loss_metrics)</code>","text":"<p>Called at the end of each testing batch.</p> PARAMETER DESCRIPTION <code>test_batch_loss_metrics</code> <p>Metrics for the testing batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_end(self, test_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each testing batch.\n\n    Args:\n        test_batch_loss_metrics: Metrics for the testing batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_test_batch_start","title":"<code>on_test_batch_start(batch)</code>","text":"<p>Called at the start of each testing batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each testing batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_batch_end","title":"<code>on_train_batch_end(train_batch_loss_metrics)</code>","text":"<p>Called at the end of each training batch.</p> PARAMETER DESCRIPTION <code>train_batch_loss_metrics</code> <p>Metrics for the training batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_end(self, train_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each training batch.\n\n    Args:\n        train_batch_loss_metrics: Metrics for the training batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_batch_start","title":"<code>on_train_batch_start(batch)</code>","text":"<p>Called at the start of each training batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each training batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_end","title":"<code>on_train_end(train_losses, val_losses=None)</code>","text":"<p>Called at the end of training.</p> PARAMETER DESCRIPTION <code>train_losses</code> <p>Metrics for the training losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]]</code> </p> <code>val_losses</code> <p>Metrics for the validation losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_end(\n    self,\n    train_losses: list[list[tuple[torch.Tensor, Any]]],\n    val_losses: list[list[tuple[torch.Tensor, Any]]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Called at the end of training.\n\n    Args:\n        train_losses (list[list[tuple[torch.Tensor, Any]]]):\n            Metrics for the training losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Training Batches      -&gt; (loss, metrics)\n        val_losses (list[list[tuple[torch.Tensor, Any]]] | None):\n            Metrics for the validation losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_epoch_end","title":"<code>on_train_epoch_end(train_epoch_loss_metrics)</code>","text":"<p>Called at the end of each training epoch.</p> PARAMETER DESCRIPTION <code>train_epoch_loss_metrics</code> <p>Metrics for the training epoch losses. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_end(self, train_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each training epoch.\n\n    Args:\n        train_epoch_loss_metrics: Metrics for the training epoch losses.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>Called at the start of each training epoch.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each training epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_start","title":"<code>on_train_start()</code>","text":"<p>Called at the start of training.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_start(self) -&gt; None:\n    \"\"\"Called at the start of training.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_batch_end","title":"<code>on_val_batch_end(val_batch_loss_metrics)</code>","text":"<p>Called at the end of each validation batch.</p> PARAMETER DESCRIPTION <code>val_batch_loss_metrics</code> <p>Metrics for the validation batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_end(self, val_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation batch.\n\n    Args:\n        val_batch_loss_metrics: Metrics for the validation batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_batch_start","title":"<code>on_val_batch_start(batch)</code>","text":"<p>Called at the start of each validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each validation batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_epoch_end","title":"<code>on_val_epoch_end(val_epoch_loss_metrics)</code>","text":"<p>Called at the end of each validation epoch.</p> PARAMETER DESCRIPTION <code>val_epoch_loss_metrics</code> <p>Metrics for the validation epoch loss. list                    -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_end(self, val_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation epoch.\n\n    Args:\n        val_epoch_loss_metrics: Metrics for the validation epoch loss.\n            list                    -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_epoch_start","title":"<code>on_val_epoch_start()</code>","text":"<p>Called at the start of each validation epoch.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each validation epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.set_use_grad","title":"<code>set_use_grad(value)</code>  <code>classmethod</code>","text":"<p>Sets the global use_grad flag.</p> PARAMETER DESCRIPTION <code>value</code> <p>Whether to use gradient-based optimization.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@classmethod\ndef set_use_grad(cls, value: bool) -&gt; None:\n    \"\"\"\n    Sets the global use_grad flag.\n\n    Args:\n        value (bool): Whether to use gradient-based optimization.\n    \"\"\"\n    if not isinstance(value, bool):\n        raise TypeError(\"use_grad must be a boolean value.\")\n    cls._use_grad = value\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for experiment tracking writers.</p> METHOD DESCRIPTION <code>open</code> <p>Opens the writer and sets up the logging environment.</p> <code>close</code> <p>Closes the writer and finalizes any ongoing logging processes.</p> <code>print_metrics</code> <p>Prints metrics and loss in a formatted manner.</p> <code>write</code> <p>Writes the optimization results to the tracking tool.</p> <code>log_hyperparams</code> <p>Logs the hyperparameters to the tracking tool.</p> <code>plot</code> <p>Logs model plots using provided plotting functions.</p> <code>log_model</code> <p>Logs the model and any relevant information.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Closes the writer and finalizes logging.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Closes the writer and finalizes logging.\"\"\"\n    raise NotImplementedError(\"Writers must implement a close method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Logs hyperparameters.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_hyperparams method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>  <code>abstractmethod</code>","text":"<p>Logs the model and associated data.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and associated data.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_model method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.open","title":"<code>open(config, iteration=None)</code>  <code>abstractmethod</code>","text":"<p>Opens the writer and prepares it for logging.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef open(self, config: TrainConfig, iteration: int | None = None) -&gt; Any:\n    \"\"\"\n    Opens the writer and prepares it for logging.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement an open method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>  <code>abstractmethod</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used to\n            generate plots.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a plot method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.print_metrics","title":"<code>print_metrics(result)</code>","text":"<p>Prints the metrics and loss in a readable format.</p> PARAMETER DESCRIPTION <code>result</code> <p>The optimization results to display.</p> <p> TYPE: <code>OptimizeResult</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def print_metrics(self, result: OptimizeResult) -&gt; None:\n    \"\"\"Prints the metrics and loss in a readable format.\n\n    Args:\n        result (OptimizeResult): The optimization results to display.\n    \"\"\"\n\n    # Find the key in result.metrics that contains \"loss\" (case-insensitive)\n    loss_key = next((k for k in result.metrics if \"loss\" in k.lower()), None)\n    if loss_key:\n        loss_value = result.metrics[loss_key]\n        msg = f\"Iteration {result.iteration: &gt;7} | {loss_key.title()}: {loss_value:.7f} -\"\n    else:\n        msg = f\"Iteration {result.iteration: &gt;7} | Loss: None -\"\n    msg += \" \".join([f\"{k}: {v:.7f}\" for k, v in result.metrics.items() if k != loss_key])\n    print(msg)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.write","title":"<code>write(iteration, metrics)</code>  <code>abstractmethod</code>","text":"<p>Logs the results of the current iteration.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a write method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter","title":"<code>MLFlowWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to MLflow.</p> ATTRIBUTE DESCRIPTION <code>run</code> <p>The active MLflow run.</p> <p> TYPE: <code>Run</code> </p> <code>mlflow</code> <p>The MLflow module.</p> <p> TYPE: <code>ModuleType</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    try:\n        from mlflow.entities import Run\n    except ImportError:\n        raise ImportError(\n            \"mlflow is not installed. Please install qadence with the mlflow feature: \"\n            \"`pip install qadence[mlflow]`.\"\n        )\n\n    self.run: Run\n    self.mlflow: ModuleType\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.close","title":"<code>close()</code>","text":"<p>Closes the MLflow run.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the MLflow run.\"\"\"\n    if self.run:\n        self.mlflow.end_run()\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.get_signature_from_dataloader","title":"<code>get_signature_from_dataloader(model, dataloader)</code>","text":"<p>Infers the signature of the model based on the input data from the dataloader.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to use for inference.</p> <p> TYPE: <code>Module</code> </p> <code>dataloader</code> <p>DataLoader for model inputs.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Optional[Any]: The inferred signature, if available.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def get_signature_from_dataloader(\n    self, model: Module, dataloader: DataLoader | DictDataLoader | None\n) -&gt; Any:\n    \"\"\"\n    Infers the signature of the model based on the input data from the dataloader.\n\n    Args:\n        model (Module): The model to use for inference.\n        dataloader (DataLoader | DictDataLoader |  None): DataLoader for model inputs.\n\n    Returns:\n        Optional[Any]: The inferred signature, if available.\n    \"\"\"\n    from mlflow.models import infer_signature\n\n    if dataloader is None:\n        return None\n\n    xs: InputData\n    xs, *_ = next(iter(dataloader))\n    preds = model(xs)\n\n    if isinstance(xs, Tensor):\n        xs = xs.detach().cpu().numpy()\n        preds = preds.detach().cpu().numpy()\n        return infer_signature(xs, preds)\n\n    return None\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to MLflow.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to MLflow.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_params(hyperparams)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model and its signature to MLflow using the provided data loaders.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and its signature to MLflow using the provided data loaders.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    if not self.mlflow:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n\n    signatures = self.get_signature_from_dataloader(model, train_dataloader)\n    self.mlflow.pytorch.log_model(model, artifact_path=\"model\", signature=signatures)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the MLflow writer and initializes an MLflow run.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>mlflow</code> <p>The MLflow module instance.</p> <p> TYPE: <code>ModuleType | None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; ModuleType | None:\n    \"\"\"\n    Opens the MLflow writer and initializes an MLflow run.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        mlflow: The MLflow module instance.\n    \"\"\"\n    import mlflow\n\n    self.mlflow = mlflow\n    tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"\")\n    experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", str(uuid4()))\n    run_name = os.getenv(\"MLFLOW_RUN_NAME\", str(uuid4()))\n\n    if self.mlflow:\n        self.mlflow.set_tracking_uri(tracking_uri)\n\n        # Create or get the experiment\n        exp_filter_string = f\"name = '{experiment_name}'\"\n        experiments = self.mlflow.search_experiments(filter_string=exp_filter_string)\n        if not experiments:\n            self.mlflow.create_experiment(name=experiment_name)\n\n        self.mlflow.set_experiment(experiment_name)\n        self.run = self.mlflow.start_run(run_name=run_name, nested=False)\n\n    return self.mlflow\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.mlflow:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.mlflow.log_figure(fig, descr)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.write","title":"<code>write(iteration, metrics)</code>","text":"<p>Logs the results of the current iteration to MLflow.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to MLflow.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_metrics(metrics, step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing.\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter","title":"<code>TensorBoardWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to TensorBoard.</p> ATTRIBUTE DESCRIPTION <code>writer</code> <p>The TensorBoard SummaryWriter instance.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.writer = None\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.close","title":"<code>close()</code>","text":"<p>Closes the TensorBoard writer.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the TensorBoard writer.\"\"\"\n    if self.writer:\n        self.writer.close()\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to TensorBoard.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to TensorBoard.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.writer:\n        self.writer.add_hparams(hyperparams, {})\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model.</p> <p>Currently not supported by TensorBoard.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model.\n\n    Currently not supported by TensorBoard.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    logger.warning(\"Model logging is not supported by tensorboard. No model will be logged.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the TensorBoard writer.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SummaryWriter</code> <p>The initialized TensorBoard writer.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; SummaryWriter:\n    \"\"\"\n    Opens the TensorBoard writer.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        SummaryWriter: The initialized TensorBoard writer.\n    \"\"\"\n    log_dir = str(config.log_folder)\n    purge_step = iteration if isinstance(iteration, int) else None\n    self.writer = SummaryWriter(log_dir=log_dir, purge_step=purge_step)\n    return self.writer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.writer:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.writer.add_figure(descr, fig, global_step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.write","title":"<code>write(iteration, metrics)</code>","text":"<p>Logs the results of the current iteration to TensorBoard.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to TensorBoard.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    if self.writer:\n        for key, value in metrics.items():\n            self.writer.add_scalar(key, value, iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing.\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.get_writer","title":"<code>get_writer(tracking_tool)</code>","text":"<p>Factory method to get the appropriate writer based on the tracking tool.</p> PARAMETER DESCRIPTION <code>tracking_tool</code> <p>The experiment tracking tool to use.</p> <p> TYPE: <code>ExperimentTrackingTool</code> </p> RETURNS DESCRIPTION <code>BaseWriter</code> <p>An instance of the appropriate writer.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def get_writer(tracking_tool: ExperimentTrackingTool) -&gt; BaseWriter:\n    \"\"\"Factory method to get the appropriate writer based on the tracking tool.\n\n    Args:\n        tracking_tool (ExperimentTrackingTool): The experiment tracking tool to use.\n\n    Returns:\n        BaseWriter: An instance of the appropriate writer.\n    \"\"\"\n    writer_class = WRITER_REGISTRY.get(tracking_tool)\n    if writer_class:\n        return writer_class()\n    else:\n        raise ValueError(f\"Unsupported tracking tool: {tracking_tool}\")\n</code></pre>"},{"location":"api/models/","title":"Quantum models","text":""},{"location":"api/models/#qadence.model.QuantumModel","title":"<code>QuantumModel(circuit, observable=None, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, mitigation=None, configuration=None)</code>","text":"<p>               Bases: <code>Module</code></p> <p>The central class of qadence that executes <code>QuantumCircuit</code>s and make them differentiable.</p> <p>This class should be used as base class for any new quantum model supported in the qadence framework for information on the implementation of custom models see here.</p> <p>Example: <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit, RX, RY, Z, PI, chain, kron\nfrom qadence import FeatureParameter, VariationalParameter\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, theta), RY(1, theta)),\n    kron(RX(0, phi), RY(1, phi)),\n)\n\ncircuit = QuantumCircuit(2, block)\n\nobservable = Z(0) + Z(1)\n\nmodel = QuantumModel(circuit, observable)\nvalues = {\"phi\": torch.tensor([PI, PI/2]), \"theta\": torch.tensor([PI, PI/2])}\n\nwf = model.run(values)\nxs = model.sample(values, n_shots=100)\nex = model.expectation(values)\nprint(wf)\nprint(xs)\nprint(ex)\n</code></pre> <pre><code>tensor([[ 1.0000e+00+0.0000e+00j, -1.2246e-16+0.0000e+00j,\n          0.0000e+00+1.2246e-16j,  0.0000e+00-1.4998e-32j],\n        [ 4.9304e-32+0.0000e+00j,  2.2204e-16+0.0000e+00j,\n          0.0000e+00-2.2204e-16j,  0.0000e+00-1.0000e+00j]])\n[OrderedCounter({'00': 100}), OrderedCounter({'11': 100})]\ntensor([[ 2.],\n        [-2.]], requires_grad=True)\n</code></pre>  ```</p> <p>Initialize a generic QuantumModel instance.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The circuit that is executed.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>Optional observable(s) that are used only in the <code>expectation</code> method. You can also provide observables on the fly to the expectation call directly.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>A backend for circuit execution.</p> <p> TYPE: <code>BackendName | str</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>A differentiability mode. Parameter shift based modes work on all backends. AD based modes only on PyTorch based backends.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Configuration for the backend.</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the <code>diff_mode</code> argument is set to None</p> Source code in <code>qadence/model.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock | None = None,\n    backend: BackendName | str = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n):\n    \"\"\"Initialize a generic QuantumModel instance.\n\n    Arguments:\n        circuit: The circuit that is executed.\n        observable: Optional observable(s) that are used only in the `expectation` method. You\n            can also provide observables on the fly to the expectation call directly.\n        backend: A backend for circuit execution.\n        diff_mode: A differentiability mode. Parameter shift based modes work on all backends.\n            AD based modes only on PyTorch based backends.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        configuration: Configuration for the backend.\n        noise: A noise model to use.\n\n    Raises:\n        ValueError: if the `diff_mode` argument is set to None\n    \"\"\"\n    super().__init__()\n\n    if not isinstance(circuit, QuantumCircuit):\n        TypeError(\n            f\"The circuit should be of type '&lt;class QuantumCircuit&gt;'. Got {type(circuit)}.\"\n        )\n\n    if diff_mode is None:\n        raise ValueError(\"`diff_mode` cannot be `None` in a `QuantumModel`.\")\n\n    self.backend = backend_factory(\n        backend=backend, diff_mode=diff_mode, configuration=configuration\n    )\n\n    if isinstance(observable, list) or observable is None:\n        observable = observable\n    else:\n        observable = [observable]\n\n    def _is_feature_param(p: Parameter) -&gt; bool:\n        return not p.trainable and not p.is_number\n\n    if observable is None:\n        self.inputs = list(filter(_is_feature_param, circuit.unique_parameters))\n    else:\n        uparams = unique_parameters(chain(circuit.block, *observable))\n        self.inputs = list(filter(_is_feature_param, uparams))\n\n    conv = self.backend.convert(circuit, observable)\n    self.embedding_fn = conv.embedding_fn\n    self._circuit = conv.circuit\n    self._observable = conv.observable\n    self._backend_name = backend\n    self._diff_mode = diff_mode\n    self._measurement = measurement\n    self._noise = noise\n    self._mitigation = mitigation\n    self._params = nn.ParameterDict(\n        {\n            str(key): nn.Parameter(val, requires_grad=val.requires_grad)\n            for key, val in conv.params.items()\n        }\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.device","title":"<code>device</code>  <code>property</code>","text":"<p>Get device.</p> RETURNS DESCRIPTION <code>device</code> <p>torch.device</p>"},{"location":"api/models/#qadence.model.QuantumModel.in_features","title":"<code>in_features</code>  <code>property</code>","text":"<p>Number of inputs.</p>"},{"location":"api/models/#qadence.model.QuantumModel.num_vparams","title":"<code>num_vparams</code>  <code>property</code>","text":"<p>The number of variational parameters.</p>"},{"location":"api/models/#qadence.model.QuantumModel.out_features","title":"<code>out_features</code>  <code>property</code>","text":"<p>Number of outputs.</p>"},{"location":"api/models/#qadence.model.QuantumModel.vals_vparams","title":"<code>vals_vparams</code>  <code>property</code>","text":"<p>Dictionary with parameters which are actually updated during optimization.</p>"},{"location":"api/models/#qadence.model.QuantumModel.vparams","title":"<code>vparams</code>  <code>property</code>","text":"<p>Variational parameters.</p>"},{"location":"api/models/#qadence.model.QuantumModel.assign_parameters","title":"<code>assign_parameters(values)</code>","text":"<p>Return the final, assigned circuit that is used in e.g. <code>backend.run</code>.</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Final, assigned circuit that is used in e.g. <code>backend.run</code></p> Source code in <code>qadence/model.py</code> <pre><code>def assign_parameters(self, values: dict[str, Tensor]) -&gt; Any:\n    \"\"\"Return the final, assigned circuit that is used in e.g. `backend.run`.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n\n    Returns:\n        Final, assigned circuit that is used in e.g. `backend.run`\n    \"\"\"\n    params = self.embedding_fn(self._params, values)\n    return self.backend.assign_parameters(self._circuit, params)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.circuit","title":"<code>circuit(circuit)</code>","text":"<p>Get backend-converted circuit.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>QuantumCircuit instance.</p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>Backend circuit.</p> Source code in <code>qadence/model.py</code> <pre><code>def circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Get backend-converted circuit.\n\n    Args:\n        circuit: QuantumCircuit instance.\n\n    Returns:\n        Backend circuit.\n    \"\"\"\n    return self.backend.circuit(circuit)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.expectation","title":"<code>expectation(values={}, observable=None, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute expectation using the given backend.</p> <p>Given an input state \\(|\\psi_0 \\rangle\\), a set of variational parameters \\(\\vec{\\theta}\\) and the unitary representation of the model \\(U(\\vec{\\theta})\\) we return \\(\\langle \\psi_0 | U(\\vec{\\theta}) | \\psi_0 \\rangle\\).</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>observable</code> <p>Observable part of the expectation.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable | None</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Optional input state.</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>A mitigation protocol to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>when no observable is set.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor of shape n_batches x n_obs</p> Source code in <code>qadence/model.py</code> <pre><code>def expectation(\n    self,\n    values: dict[str, Tensor] = {},\n    observable: list[ConvertedObservable] | ConvertedObservable | None = None,\n    state: Optional[Tensor] = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    r\"\"\"Compute expectation using the given backend.\n\n\n\n    Given an input state $|\\psi_0 \\rangle$,\n    a set of variational parameters $\\vec{\\theta}$\n    and the unitary representation of the model $U(\\vec{\\theta})$\n    we return $\\langle \\psi_0 | U(\\vec{\\theta}) | \\psi_0 \\rangle$.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        observable: Observable part of the expectation.\n        state: Optional input state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: A mitigation protocol to use.\n        endianness: Storage convention for binary information.\n\n    Raises:\n        ValueError: when no observable is set.\n\n    Returns:\n        A torch.Tensor of shape n_batches x n_obs\n    \"\"\"\n    if observable is None:\n        if self._observable is None:\n            raise ValueError(\n                \"Provide an AbstractBlock as the observable to compute expectation.\"\n                \"Either pass a 'native_observable' directly to 'QuantumModel.expectation'\"\n                \"or pass a (non-native) '&lt;class AbstractBlock&gt;' to the 'QuantumModel.__init__'.\"\n            )\n        observable = self._observable\n\n    params = self.embedding_fn(self._params, values)\n    if measurement is None:\n        measurement = self._measurement\n    if noise is None:\n        noise = self._noise\n    else:\n        self._noise = noise\n    if mitigation is None:\n        mitigation = self._mitigation\n    return self.backend.expectation(\n        circuit=self._circuit,\n        observable=observable,\n        param_values=params,\n        state=state,\n        measurement=measurement,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Calls run method with arguments.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor representing output.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/model.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Calls run method with arguments.\n\n    Returns:\n        Tensor: A torch.Tensor representing output.\n    \"\"\"\n    return self.run(*args, **kwargs)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.load","title":"<code>load(file_path, as_torch=False, map_location='cpu')</code>  <code>classmethod</code>","text":"<p>Load QuantumModel.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>File path to load model from.</p> <p> TYPE: <code>str | Path</code> </p> <code>as_torch</code> <p>Load parameters as torch tensor. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>map_location</code> <p>Location for loading. Defaults to \"cpu\".</p> <p> TYPE: <code>str | device</code> DEFAULT: <code>'cpu'</code> </p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel from file_path.</p> Source code in <code>qadence/model.py</code> <pre><code>@classmethod\ndef load(\n    cls, file_path: str | Path, as_torch: bool = False, map_location: str | torch.device = \"cpu\"\n) -&gt; QuantumModel:\n    \"\"\"Load QuantumModel.\n\n    Arguments:\n        file_path: File path to load model from.\n        as_torch: Load parameters as torch tensor. Defaults to False.\n        map_location (str | torch.device, optional): Location for loading. Defaults to \"cpu\".\n\n    Returns:\n        QuantumModel from file_path.\n    \"\"\"\n    qm_pt = {}\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if os.path.isdir(file_path):\n        from qadence.ml_tools.callbacks.saveload import get_latest_checkpoint_name\n\n        file_path = file_path / get_latest_checkpoint_name(file_path, \"model\")\n\n    try:\n        qm_pt = torch.load(file_path, map_location=map_location)\n    except Exception as e:\n        logger.error(f\"Unable to load QuantumModel due to {e}\")\n    return cls._from_dict(qm_pt, as_torch)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.load_params_from_dict","title":"<code>load_params_from_dict(d, strict=True)</code>","text":"<p>Copy parameters from dictionary into this QuantumModel.</p> <p>Unlike :meth:<code>~qadence.QuantumModel.from_dict</code>, this method does not create a new QuantumModel instance, but rather loads the parameters into the same QuantumModel. The behaviour of this method is similar to :meth:<code>~torch.nn.Module.load_state_dict</code>.</p> <p>The dictionary is assumed to have the format as saved via :meth:<code>~qadence.QuantumModel.to_dict</code></p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary</p> <p> TYPE: <code>dict</code> </p> <code>strict</code> <p>Whether to strictly enforce that the parameter keys in the dictionary and in the model match exactly. Default: <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>qadence/model.py</code> <pre><code>def load_params_from_dict(self, d: dict, strict: bool = True) -&gt; None:\n    \"\"\"Copy parameters from dictionary into this QuantumModel.\n\n    Unlike :meth:`~qadence.QuantumModel.from_dict`, this method does not create a new\n    QuantumModel instance, but rather loads the parameters into the same QuantumModel.\n    The behaviour of this method is similar to :meth:`~torch.nn.Module.load_state_dict`.\n\n    The dictionary is assumed to have the format as saved via\n    :meth:`~qadence.QuantumModel.to_dict`\n\n    Args:\n        d (dict): The dictionary\n        strict (bool, optional):\n            Whether to strictly enforce that the parameter keys in the dictionary and\n            in the model match exactly. Default: ``True``.\n    \"\"\"\n    param_dict = d[\"param_dict\"]\n    missing_keys = set(self._params.keys()) - set(param_dict.keys())\n    unexpected_keys = set(param_dict.keys()) - set(self._params.keys())\n\n    if strict:\n        error_msgs = []\n        if len(unexpected_keys) &gt; 0:\n            error_msgs.append(f\"Unexpected key(s) in dictionary: {unexpected_keys}\")\n        if len(missing_keys) &gt; 0:\n            error_msgs.append(f\"Missing key(s) in dictionary: {missing_keys}\")\n        if len(error_msgs) &gt; 0:\n            errors_string = \"\\n\\t\".join(error_msgs)\n            raise RuntimeError(\n                f\"Error(s) loading the parameter dictionary due to: \\n\\t{errors_string}\\n\"\n                \"This error was thrown because the `strict` argument is set `True`.\"\n                \"If you don't need the parameter keys of the dictionary to exactly match \"\n                \"the model parameters, set `strict=False`.\"\n            )\n\n    for n, param in param_dict.items():\n        try:\n            with torch.no_grad():\n                self._params[n].copy_(\n                    torch.nn.Parameter(param, requires_grad=param.requires_grad)\n                )\n        except Exception as e:\n            logger.warning(f\"Unable to load parameter {n} from dictionary due to {e}.\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.observable","title":"<code>observable(observable, n_qubits)</code>","text":"<p>Get backend observable.</p> PARAMETER DESCRIPTION <code>observable</code> <p>Observable block.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Backend observable.</p> Source code in <code>qadence/model.py</code> <pre><code>def observable(self, observable: AbstractBlock, n_qubits: int) -&gt; Any:\n    \"\"\"Get backend observable.\n\n    Args:\n        observable: Observable block.\n        n_qubits: Number of qubits\n\n    Returns:\n        Backend observable.\n    \"\"\"\n    return self.backend.observable(observable, n_qubits)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.overlap","title":"<code>overlap()</code>","text":"<p>Overlap of model.</p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>The overlap method is not implemented for this model.</p> Source code in <code>qadence/model.py</code> <pre><code>def overlap(self) -&gt; Tensor:\n    \"\"\"Overlap of model.\n\n    Raises:\n        NotImplementedError: The overlap method is not implemented for this model.\n    \"\"\"\n    raise NotImplementedError(\"The overlap method is not implemented for this model.\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.reset_vparams","title":"<code>reset_vparams(values)</code>","text":"<p>Reset all the variational parameters with a given list of values.</p> Source code in <code>qadence/model.py</code> <pre><code>def reset_vparams(self, values: Sequence) -&gt; None:\n    \"\"\"Reset all the variational parameters with a given list of values.\"\"\"\n    current_vparams = OrderedDict({k: v for k, v in self._params.items() if v.requires_grad})\n\n    assert (\n        len(values) == self.num_vparams\n    ), \"Pass an iterable with the values of all variational parameters\"\n    for i, k in enumerate(current_vparams.keys()):\n        current_vparams[k].data = torch.tensor([values[i]])\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.run","title":"<code>run(values=None, state=None, endianness=Endianness.BIG)</code>","text":"<p>Run model.</p> <p>Given an input state \\(| \\psi_0 \\rangle\\), a set of variational parameters \\(\\vec{\\theta}\\) and the unitary representation of the model \\(U(\\vec{\\theta})\\) we return \\(U(\\vec{\\theta}) | \\psi_0 \\rangle\\).</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Optional input state to apply model on.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor representing output.</p> Source code in <code>qadence/model.py</code> <pre><code>def run(\n    self,\n    values: dict[str, Tensor] = None,\n    state: Tensor | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    r\"\"\"Run model.\n\n    Given an input state $| \\psi_0 \\rangle$,\n    a set of variational parameters $\\vec{\\theta}$\n    and the unitary representation of the model $U(\\vec{\\theta})$\n    we return $U(\\vec{\\theta}) | \\psi_0 \\rangle$.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        state: Optional input state to apply model on.\n        endianness: Storage convention for binary information.\n\n    Returns:\n        A torch.Tensor representing output.\n    \"\"\"\n    if values is None:\n        values = {}\n\n    params = self.embedding_fn(self._params, values)\n\n    return self.backend.run(self._circuit, params, state=state, endianness=endianness)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.sample","title":"<code>sample(values={}, n_shots=1000, state=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Obtain samples from model.</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>n_shots</code> <p>Observable part of the expectation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>state</code> <p>Optional input state to apply model on.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>A mitigation protocol to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>list[Counter]</code> <p>A list of Counter instances with the sample results.</p> Source code in <code>qadence/model.py</code> <pre><code>def sample(\n    self,\n    values: dict[str, torch.Tensor] = {},\n    n_shots: int = 1000,\n    state: torch.Tensor | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; list[Counter]:\n    \"\"\"Obtain samples from model.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        n_shots: Observable part of the expectation.\n        state: Optional input state to apply model on.\n        noise: A noise model to use.\n        mitigation: A mitigation protocol to use.\n        endianness: Storage convention for binary information.\n\n    Returns:\n        A list of Counter instances with the sample results.\n    \"\"\"\n    params = self.embedding_fn(self._params, values)\n    if noise is None:\n        noise = self._noise\n    if mitigation is None:\n        mitigation = self._mitigation\n    return self.backend.sample(\n        self._circuit,\n        params,\n        n_shots=n_shots,\n        state=state,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.save","title":"<code>save(folder, file_name='quantum_model.pt', save_params=True)</code>","text":"<p>Save model.</p> PARAMETER DESCRIPTION <code>folder</code> <p>Folder where model is saved.</p> <p> TYPE: <code>str | Path</code> </p> <code>file_name</code> <p>File name for saving model. Defaults to \"quantum_model.pt\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'quantum_model.pt'</code> </p> <code>save_params</code> <p>Save parameters if True. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If folder is not a directory.</p> Source code in <code>qadence/model.py</code> <pre><code>def save(\n    self, folder: str | Path, file_name: str = \"quantum_model.pt\", save_params: bool = True\n) -&gt; None:\n    \"\"\"Save model.\n\n    Arguments:\n        folder: Folder where model is saved.\n        file_name: File name for saving model. Defaults to \"quantum_model.pt\".\n        save_params: Save parameters if True. Defaults to True.\n\n    Raises:\n        FileNotFoundError: If folder is not a directory.\n    \"\"\"\n    if not os.path.isdir(folder):\n        raise FileNotFoundError\n    try:\n        torch.save(self._to_dict(save_params), folder / Path(file_name))\n    except Exception as e:\n        logger.error(f\"Unable to write QuantumModel to disk due to {e}\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.to","title":"<code>to(*args, **kwargs)</code>","text":"<p>Conversion method for device or types.</p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel with conversions.</p> Source code in <code>qadence/model.py</code> <pre><code>def to(self, *args: Any, **kwargs: Any) -&gt; QuantumModel:\n    \"\"\"Conversion method for device or types.\n\n    Returns:\n        QuantumModel with conversions.\n    \"\"\"\n    from pyqtorch import QuantumCircuit as PyQCircuit\n\n    try:\n        if isinstance(self._circuit.native, PyQCircuit):\n            self._circuit.native = self._circuit.native.to(*args, **kwargs)\n            if self._observable is not None:\n                if isinstance(self._observable, ConvertedObservable):\n                    self._observable.native = self._observable.native.to(*args, **kwargs)\n                elif isinstance(self._observable, list):\n                    for obs in self._observable:\n                        obs.native = obs.native.to(*args, **kwargs)\n            self._params = self._params.to(\n                device=self._circuit.native.device,\n                dtype=(\n                    torch.float64\n                    if self._circuit.native.dtype == torch.cdouble\n                    else torch.float32\n                ),\n            )\n            logger.debug(f\"Moved {self} to {args}, {kwargs}.\")\n        else:\n            logger.debug(\"QuantumModel.to only supports pyqtorch.QuantumCircuits.\")\n    except Exception as e:\n        logger.warning(f\"Unable to move {self} to {args}, {kwargs} due to {e}.\")\n    return self\n</code></pre>"},{"location":"api/noise/","title":"Noise","text":""},{"location":"api/noise/#noise-for-simulations","title":"Noise for simulations","text":""},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler","title":"<code>NoiseHandler(protocol, options=dict())</code>","text":"<p>A container for multiple sources of noise.</p> <p>Note <code>NoiseProtocol.ANALOG</code> and <code>NoiseProtocol.DIGITAL</code> sources cannot be both present. Also <code>NoiseProtocol.READOUT</code> can only be present once as the last noise sources, and only exclusively with <code>NoiseProtocol.DIGITAL</code> sources.</p> PARAMETER DESCRIPTION <code>protocol</code> <p>The protocol(s) applied. To be defined from <code>NoiseProtocol</code>.</p> <p> TYPE: <code>NoiseEnum | list[NoiseEnum]</code> </p> <code>options</code> <p>A list of options defining the protocol. For <code>NoiseProtocol.ANALOG</code>, options should contain a field <code>noise_probs</code>. For <code>NoiseProtocol.DIGITAL</code>, options should contain a field <code>error_probability</code>.</p> <p> TYPE: <code>dict | list[dict]</code> DEFAULT: <code>dict()</code> </p> <p>Examples:</p> <pre><code>    from qadence import NoiseProtocol, NoiseHandler\n\n    analog_options = {\"noise_probs\": 0.1}\n    digital_options = {\"error_probability\": 0.1}\n    readout_options = {\"error_probability\": 0.1, \"seed\": 0}\n\n    # single noise sources\n    analog_noise = NoiseHandler(NoiseProtocol.ANALOG.DEPOLARIZING, analog_options)\n    digital_depo_noise = NoiseHandler(NoiseProtocol.DIGITAL.DEPOLARIZING, digital_options)\n    readout_noise = NoiseHandler(NoiseProtocol.READOUT, readout_options)\n\n    # init from multiple sources\n    protocols: list = [NoiseProtocol.DIGITAL.DEPOLARIZING, NoiseProtocol.READOUT]\n    options: list = [digital_options, readout_noise]\n    noise_combination = NoiseHandler(protocols, options)\n\n    # Appending noise sources\n    noise_combination = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, digital_options)\n    noise_combination.append([digital_depo_noise, readout_noise])\n</code></pre> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def __init__(\n    self,\n    protocol: NoiseEnum | list[NoiseEnum],\n    options: dict | list[dict] = dict(),\n) -&gt; None:\n    self.protocol = protocol if isinstance(protocol, list) else [protocol]\n    self.options = options if isinstance(options, list) else [options] * len(self.protocol)\n    self.verify_all_protocols()\n</code></pre>"},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler.append","title":"<code>append(other)</code>","text":"<p>Append noises.</p> PARAMETER DESCRIPTION <code>other</code> <p>The noises to add.</p> <p> TYPE: <code>NoiseHandler | list[NoiseHandler]</code> </p> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def append(self, other: NoiseHandler | list[NoiseHandler]) -&gt; None:\n    \"\"\"Append noises.\n\n    Args:\n        other (NoiseHandler | list[NoiseHandler]): The noises to add.\n    \"\"\"\n    # To avoid overwriting the noise_sources list if an error is raised, make a copy\n    other_list = other if isinstance(other, list) else [other]\n    protocols = self.protocol[:]\n    options = self.options[:]\n\n    for noise in other_list:\n        protocols += noise.protocol\n        options += noise.options\n\n    # init may raise an error\n    temp_handler = NoiseHandler(protocols, options)\n    # if verify passes, replace protocols and options\n    self.protocol = temp_handler.protocol\n    self.options = temp_handler.options\n</code></pre>"},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler.verify_all_protocols","title":"<code>verify_all_protocols()</code>","text":"<p>Make sure all protocols are correct in terms and their combination too.</p> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def verify_all_protocols(self) -&gt; None:\n    \"\"\"Make sure all protocols are correct in terms and their combination too.\"\"\"\n\n    if len(self.protocol) == 0:\n        raise ValueError(\"NoiseHandler should be specified with one valid configuration.\")\n\n    if len(self.protocol) != len(self.options):\n        raise ValueError(\"Specify lists of same length when defining noises.\")\n\n    for protocol, option in zip(self.protocol, self.options):\n        self._verify_single_protocol(protocol, option)\n\n    types = [type(p) for p in self.protocol]\n    unique_types = set(types)\n    if NoiseProtocol.DIGITAL in unique_types and NoiseProtocol.ANALOG in unique_types:\n        raise ValueError(\"Cannot define a config with both Digital and Analog noises.\")\n\n    if NoiseProtocol.ANALOG in unique_types:\n        if NoiseProtocol.READOUT in unique_types:\n            raise ValueError(\"Cannot define a config with both READOUT and Analog noises.\")\n        if types.count(NoiseProtocol.ANALOG) &gt; 1:\n            raise ValueError(\"Multiple Analog Noises are not supported yet.\")\n\n    if NoiseProtocol.READOUT in unique_types:\n        if (\n            not isinstance(self.protocol[-1], NoiseProtocol.READOUT)\n            or types.count(NoiseProtocol.READOUT) &gt; 1\n        ):\n            raise ValueError(\"Only define a NoiseHandler with one READOUT as the last Noise.\")\n</code></pre>"},{"location":"api/operations/","title":"Operations","text":"<p>Operations are common <code>PrimitiveBlocks</code>, these are often called gates elsewhere.</p>"},{"location":"api/operations/#constant-blocks","title":"Constant blocks","text":"<p>CY gate not implemented</p>"},{"location":"api/operations/#qadence.operations.X","title":"<code>X(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The X gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.Y","title":"<code>Y(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Y gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.Z","title":"<code>Z(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Z gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.I","title":"<code>I(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The identity gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.H","title":"<code>H(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hadamard or H gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = (1 / np.sqrt(2)) * (X(target) + Z(target) - np.sqrt(2) * I(target))\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.S","title":"<code>S(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The S / Phase gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.SDagger","title":"<code>SDagger(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hermitian adjoint/conjugate transpose of the S / Phase gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.SWAP","title":"<code>SWAP(control, target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The SWAP gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    a11 = 0.5 * (Z(control) - I(control))\n    a22 = -0.5 * (Z(target) + I(target))\n    a12 = 0.5 * (chain(X(control), Z(control)) + X(control))\n    a21 = 0.5 * (chain(Z(target), X(target)) + X(target))\n    self.generator = (\n        kron(-1.0 * a22, a11) + kron(-1.0 * a11, a22) + kron(a12, a21) + kron(a21, a12)\n    )\n    super().__init__((control, target), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.T","title":"<code>T(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The T gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.TDagger","title":"<code>TDagger(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hermitian adjoint/conjugate transpose of the T gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CNOT","title":"<code>CNOT(control, target, noise=None)</code>","text":"<p>               Bases: <code>ControlBlock</code></p> <p>The CNot, or CX, gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(self, control: int, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    self.generator = kron(N(control), X(target) - I(target))\n    super().__init__((control,), X(target), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CZ","title":"<code>CZ(control, target, noise=None)</code>","text":"<p>               Bases: <code>MCZ</code></p> <p>The CZ gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(self, control: int, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((control,), target, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CPHASE","title":"<code>CPHASE(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCPHASE</code></p> <p>The CPHASE gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#parametrized-blocks","title":"Parametrized blocks","text":""},{"location":"api/operations/#qadence.operations.RX","title":"<code>RX(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Rx gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    # TODO: should we give them more meaningful names? like 'angle'?\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = X(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.RY","title":"<code>RY(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Ry gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = Y(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.RZ","title":"<code>RZ(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Rz gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRX","title":"<code>CRX(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRX</code></p> <p>The CRX gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRY","title":"<code>CRY(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRY</code></p> <p>The CRY gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self, control: int, target: int, parameter: TParameter, noise: NoiseHandler | None = None\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRZ","title":"<code>CRZ(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRZ</code></p> <p>The CRZ gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.PHASE","title":"<code>PHASE(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Parametric Phase / S gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = ParamMap(parameter=parameter)\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#hamiltonian-evolution","title":"Hamiltonian Evolution","text":"<p>AnalogSWAP should be turned into a proper analog block</p>"},{"location":"api/operations/#qadence.operations.HamEvo","title":"<code>HamEvo(generator, parameter, qubit_support=None, duration=None, noise_operators=list())</code>","text":"<p>               Bases: <code>TimeEvolutionBlock</code></p> <p>The Hamiltonian evolution operator U(t).</p> <p>For time-independent Hamiltonians the solution is exact:</p> <pre><code>U(t) = exp(-iGt)\n</code></pre> <p>where G represents an Hermitian generator, or Hamiltonian and t represents the time parameter. For time-dependent Hamiltonians, the solution is obtained by numerical integration of the Schrodinger equation.</p> PARAMETER DESCRIPTION <code>generator</code> <p>Hamiltonian generator, either symbolic as an AbstractBlock, or as a torch.Tensor or numpy.ndarray.</p> <p> TYPE: <code>Union[TGenerator, AbstractBlock]</code> </p> <code>parameter</code> <p>The time parameter for evolution operator. For the time-independent case, it represents the actual value for which the evolution will be evaluated. For the time-dependent case, it should be an instance of TimeParameter to signal the solver the variable that will be integrated over.</p> <p> TYPE: <code>TParameter</code> </p> <code>qubit_support</code> <p>The qubits on which the evolution will be performed on. Only required for generators that are not a composition of blocks.</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>(optional) duration of the evolution in case of time-dependent generator. By default, a FeatureParameter with tag \"duration\" will be initialized, and the value will then be required in the values dict.</p> <p> TYPE: <code>TParameter | None</code> DEFAULT: <code>None</code> </p> <code>noise_operators</code> <p>(optional) the list of jump operators to use when using a shrodinger solver, allowing to perform noisy simulations.</p> <p> TYPE: <code>list[AbstractBlock]</code> DEFAULT: <code>list()</code> </p> <p>Examples:</p> <pre><code>from qadence import X, HamEvo, PI, add, run\nfrom qadence import FeatureParameter, TimeParameter\nimport torch\n\nn_qubits = 3\n\n# Hamiltonian as a block composition\nhamiltonian = add(X(i) for i in range(n_qubits))\nhevo = HamEvo(hamiltonian, parameter=torch.rand(2))\nstate = run(hevo)\n\n# Hamiltonian as a random matrix\nhamiltonian = torch.rand(2, 2, dtype=torch.complex128)\nhevo = HamEvo(hamiltonian, parameter=torch.rand(2), qubit_support=(0,))\nstate = run(hevo)\n\n# Time-dependent Hamiltonian\nt = TimeParameter(\"t\")\nhamiltonian = t * add(X(i) for i in range(n_qubits))\nhevo = HamEvo(hamiltonian, parameter=t)\nstate = run(hevo, values = {\"duration\": torch.tensor(1.0)})\n\n# Adding noise operators\nnoise_ops = [X(0)]\nhevo = HamEvo(hamiltonian, parameter=t, noise_operators=noise_ops)\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/operations/ham_evo.py</code> <pre><code>def __init__(\n    self,\n    generator: Union[TGenerator, AbstractBlock],\n    parameter: TParameter,\n    qubit_support: tuple[int, ...] = None,\n    duration: TParameter | None = None,\n    noise_operators: list[AbstractBlock] = list(),\n):\n    params = {}\n    if qubit_support is None and not isinstance(generator, AbstractBlock):\n        raise ValueError(\"You have to supply a qubit support for non-block generators.\")\n    super().__init__(qubit_support if qubit_support else generator.qubit_support)\n    if isinstance(generator, AbstractBlock):\n        qubit_support = generator.qubit_support\n        if generator.is_parametric:\n            params = {str(e): e for e in expressions(generator)}\n        if generator.is_time_dependent:\n            if isinstance(duration, str):\n                duration = Parameter(duration, trainable=False)\n            elif duration is None:\n                duration = Parameter(\"duration\", trainable=False)\n        if not generator.is_time_dependent and duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n    elif isinstance(generator, torch.Tensor):\n        if duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n        msg = \"Please provide a square generator.\"\n        if len(generator.shape) == 2:\n            assert generator.shape[0] == generator.shape[1], msg\n        elif len(generator.shape) == 3:\n            assert generator.shape[1] == generator.shape[2], msg\n            assert generator.shape[0] == 1, \"Qadence doesnt support batched generators.\"\n        else:\n            raise TypeError(\n                \"Only 2D or 3D generators are supported.\\\n                            In case of a 3D generator, the batch dim\\\n                            is expected to be at dim 0.\"\n            )\n        params = {str(generator.__hash__()): generator}\n    elif isinstance(generator, (sympy.Basic, sympy.Array)):\n        if duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n        params = {str(generator): generator}\n    else:\n        raise TypeError(\n            f\"Generator of type {type(generator)} not supported.\\\n                        If you're using a numpy.ndarray, please cast it to a torch tensor.\"\n        )\n    if duration is not None:\n        params = {\"duration\": Parameter(duration), **params}\n    params = {\"parameter\": Parameter(parameter), **params}\n    self.parameters = ParamMap(**params)\n    self.time_param = parameter\n    self.generator = generator\n    self.duration = duration\n\n    if len(noise_operators) &gt; 0:\n        if not all(\n            [\n                len(set(op.qubit_support + self.qubit_support) - set(self.qubit_support)) == 0\n                for op in noise_operators\n            ]\n        ):\n            raise ValueError(\n                \"Noise operators should be defined\"\n                \" over the same or a subset of the qubit support\"\n            )\n        if True in [op.is_parametric for op in noise_operators]:\n            raise ValueError(\"Parametric operators are not supported\")\n    self.noise_operators = noise_operators\n</code></pre>"},{"location":"api/operations/#qadence.operations.HamEvo.digital_decomposition","title":"<code>digital_decomposition(approximation=LTSOrder.ST4)</code>","text":"<p>Decompose the Hamiltonian evolution into digital gates.</p> PARAMETER DESCRIPTION <code>approximation</code> <p>Choose the type of decomposition. Defaults to \"st4\". Available types are: * 'basic' = apply first-order Trotter formula and decompose each term of     the exponential into digital gates. It is exact only if applied to an     operator whose terms are mutually commuting. * 'st2' = Trotter-Suzuki 2nd order formula for approximating non-commuting     Hamiltonians. * 'st4' = Trotter-Suzuki 4th order formula for approximating non-commuting     Hamiltonians.</p> <p> TYPE: <code>str</code> DEFAULT: <code>ST4</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>a block with the digital decomposition</p> <p> TYPE: <code>AbstractBlock</code> </p> Source code in <code>qadence/operations/ham_evo.py</code> <pre><code>def digital_decomposition(self, approximation: LTSOrder = LTSOrder.ST4) -&gt; AbstractBlock:\n    \"\"\"Decompose the Hamiltonian evolution into digital gates.\n\n    Args:\n        approximation (str, optional): Choose the type of decomposition. Defaults to \"st4\".\n            Available types are:\n            * 'basic' = apply first-order Trotter formula and decompose each term of\n                the exponential into digital gates. It is exact only if applied to an\n                operator whose terms are mutually commuting.\n            * 'st2' = Trotter-Suzuki 2nd order formula for approximating non-commuting\n                Hamiltonians.\n            * 'st4' = Trotter-Suzuki 4th order formula for approximating non-commuting\n                Hamiltonians.\n\n    Returns:\n        AbstractBlock: a block with the digital decomposition\n    \"\"\"\n\n    # psi(t) = exp(-i * H * t * psi0)\n    # psi(t) = exp(-i * lambda * t * psi0)\n    # H = sum(Paulin) + sum(Pauli1*Pauli2)\n    logger.info(\"Quantum simulation of the time-independent Schr\u00f6dinger equation.\")\n\n    blocks = []\n\n    # how to change the type/dict to enum effectively\n\n    # when there is a term including non-commuting matrices use st2 or st4\n\n    # 1) should check that the given generator respects the constraints\n    # single-qubit gates\n\n    assert isinstance(\n        self.generator, AbstractBlock\n    ), \"Only a generator represented as a block can be decomposed\"\n\n    if block_is_qubit_hamiltonian(self.generator):\n        try:\n            block_is_commuting_hamiltonian(self.generator)\n            approximation = LTSOrder.BASIC  # use the simpler approach if the H is commuting\n        except TypeError:\n            logger.warning(\n                \"\"\"Non-commuting terms in the Pauli operator.\n                The Suzuki-Trotter approximation is applied.\"\"\"\n            )\n\n        blocks.extend(\n            lie_trotter_suzuki(\n                block=self.generator,\n                parameter=self.parameters.parameter,\n                order=LTSOrder[approximation],\n            )\n        )\n\n        # 2) return an AbstractBlock instance with the set of gates\n        # resulting from the decomposition\n\n        return chain(*blocks)\n    else:\n        raise NotImplementedError(\n            \"The current digital decomposition can be applied only to Pauli Hamiltonians.\"\n        )\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogSWAP","title":"<code>AnalogSWAP(control, target, parameter=3 * PI / 4)</code>","text":"<p>               Bases: <code>HamEvo</code></p> <p>Single time-independent Hamiltonian evolution over a Rydberg Ising.</p> <p>hamiltonian yielding a SWAP (up to global phase).</p> <p>Derived from Bapat et al. where it is applied to XX-type Hamiltonian</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def __init__(self, control: int, target: int, parameter: TParameter = 3 * PI / 4):\n    rydberg_ising_hamiltonian_generator = (\n        4.0 * kron((I(control) - Z(control)) / 2.0, (I(target) - Z(target)) / 2.0)\n        + (2.0 / 3.0) * np.sqrt(2.0) * X(control)\n        + (2.0 / 3.0) * np.sqrt(2.0) * X(target)\n        + (1.0 + np.sqrt(5.0) / 3) * Z(control)\n        + (1.0 + np.sqrt(5.0) / 3) * Z(target)\n    )\n    super().__init__(rydberg_ising_hamiltonian_generator, parameter, (control, target))\n</code></pre>"},{"location":"api/operations/#analog-blocks","title":"Analog blocks","text":""},{"location":"api/operations/#qadence.operations.AnalogRX","title":"<code>AnalogRX(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog X rotation.</p> <p>Shorthand for <code>AnalogRot</code>:</p> <pre><code>\u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\nAnalogRot(duration=t, omega=\u03a9)\n</code></pre> PARAMETER DESCRIPTION <code>angle</code> <p>Rotation angle [rad]</p> <p> TYPE: <code>float | str | Parameter</code> </p> <code>qubit_support</code> <p>Defines the (local/global) qubit support</p> <p> TYPE: <code>str | QubitSupport | Tuple</code> DEFAULT: <code>'global'</code> </p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRX(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog X rotation.\n\n    Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n\n    ```python\n    \u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\n    AnalogRot(duration=t, omega=\u03a9)\n    ```\n\n    Arguments:\n        angle: Rotation angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n    return _analog_rot(angle, qubit_support, phase=0, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRY","title":"<code>AnalogRY(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog Y rotation.</p> <p>Shorthand for <code>AnalogRot</code>:</p> <p><pre><code>\u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\nAnalogRot(duration=t, omega=\u03a9, phase=-\u03c0/2)\n</code></pre> Arguments:     angle: Rotation angle [rad]     qubit_support: Defines the (local/global) qubit support</p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRY(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog Y rotation.\n\n    Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n\n    ```python\n    \u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\n    AnalogRot(duration=t, omega=\u03a9, phase=-\u03c0/2)\n    ```\n    Arguments:\n        angle: Rotation angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n    return _analog_rot(angle, qubit_support, phase=-PI / 2, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRZ","title":"<code>AnalogRZ(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog Z rotation. Shorthand for <code>AnalogRot</code>: <pre><code>\u03c6=2.4; \u03b4=\u03c0; t = \u03c6/\u03b4 * 100)\nAnalogRot(duration=t, delta=\u03b4, phase=\u03c0/2)\n</code></pre></p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRZ(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog Z rotation. Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n    ```\n    \u03c6=2.4; \u03b4=\u03c0; t = \u03c6/\u03b4 * 100)\n    AnalogRot(duration=t, delta=\u03b4, phase=\u03c0/2)\n    ```\n    \"\"\"\n    q = _cast(QubitSupport, qubit_support)\n    alpha = _cast(Parameter, angle)\n    delta = PI\n    omega = 0\n    duration = alpha / delta * 1000\n    h_norm = sympy.sqrt(omega**2 + delta**2)\n    ps = ParamMap(\n        alpha=alpha, duration=duration, omega=omega, delta=delta, phase=0.0, h_norm=h_norm\n    )\n    return ConstantAnalogRotation(qubit_support=q, parameters=ps, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRot","title":"<code>AnalogRot(duration, omega=0, delta=0, phase=0, qubit_support='global', add_pattern=True)</code>","text":"<p>General analog rotation operation.</p> PARAMETER DESCRIPTION <code>duration</code> <p>Duration of the rotation [ns].</p> <p> TYPE: <code>float | str | Parameter</code> </p> <code>omega</code> <p>Rotation frequency [rad/\u03bcs]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>delta</code> <p>Rotation frequency [rad/\u03bcs]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>phase</code> <p>Phase angle [rad]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>qubit_support</code> <p>Defines the (local/global) qubit support</p> <p> TYPE: <code>str | QubitSupport | Tuple</code> DEFAULT: <code>'global'</code> </p> <code>add_pattern</code> <p>False disables the semi-local addressing pattern for the execution of this specific block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRot(\n    duration: float | str | Parameter,\n    omega: float | str | Parameter = 0,\n    delta: float | str | Parameter = 0,\n    phase: float | str | Parameter = 0,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"General analog rotation operation.\n\n    Arguments:\n        duration: Duration of the rotation [ns].\n        omega: Rotation frequency [rad/\u03bcs]\n        delta: Rotation frequency [rad/\u03bcs]\n        phase: Phase angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n        add_pattern: False disables the semi-local addressing pattern\n            for the execution of this specific block.\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n\n    if omega == 0 and delta == 0:\n        raise ValueError(\"Parameters omega and delta cannot both be 0.\")\n\n    q = _cast(QubitSupport, qubit_support)\n    duration = Parameter(duration)\n    omega = Parameter(omega)\n    delta = Parameter(delta)\n    phase = Parameter(phase)\n    h_norm = sympy.sqrt(omega**2 + delta**2)\n    alpha = duration * h_norm / 1000\n    ps = ParamMap(\n        alpha=alpha, duration=duration, omega=omega, delta=delta, phase=phase, h_norm=h_norm\n    )\n    return ConstantAnalogRotation(parameters=ps, qubit_support=q, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogInteraction","title":"<code>AnalogInteraction(duration, qubit_support='global', add_pattern=True)</code>","text":"<p>Evolution of the interaction term for a register of qubits.</p> <p>Constructs a <code>InteractionBlock</code>.</p> PARAMETER DESCRIPTION <code>duration</code> <p>Time to evolve the interaction for in nanoseconds.</p> <p> TYPE: <code>TNumber | Basic</code> </p> <code>qubit_support</code> <p>Qubits the <code>InteractionBlock</code> is applied to. Can be either <code>\"global\"</code> to evolve the interaction block to all qubits or a tuple of integers.</p> <p> TYPE: <code>str | QubitSupport | tuple</code> DEFAULT: <code>'global'</code> </p> <code>add_pattern</code> <p>False disables the semi-local addressing pattern for the execution of this specific block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>InteractionBlock</code> <p>a <code>InteractionBlock</code></p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogInteraction(\n    duration: TNumber | sympy.Basic,\n    qubit_support: str | QubitSupport | tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; InteractionBlock:\n    \"\"\"Evolution of the interaction term for a register of qubits.\n\n    Constructs a [`InteractionBlock`][qadence.blocks.analog.InteractionBlock].\n\n    Arguments:\n        duration: Time to evolve the interaction for in nanoseconds.\n        qubit_support: Qubits the `InteractionBlock` is applied to. Can be either\n            `\"global\"` to evolve the interaction block to all qubits or a tuple of integers.\n        add_pattern: False disables the semi-local addressing pattern\n            for the execution of this specific block.\n\n    Returns:\n        a `InteractionBlock`\n    \"\"\"\n    q = _cast(QubitSupport, qubit_support)\n    ps = ParamMap(duration=duration)\n    return InteractionBlock(parameters=ps, qubit_support=q, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/parameters/","title":"Parameters","text":""},{"location":"api/parameters/#parameters","title":"Parameters","text":""},{"location":"api/parameters/#qadence.parameters.ParamMap","title":"<code>ParamMap(**kwargs)</code>","text":"<p>Connects UUIDs of parameters to their expressions and names.</p> <p>This class is not user-facing and only needed for more complex block definitions. It provides convenient access to expressions/UUIDs/names needed in different backends.</p> PARAMETER DESCRIPTION <code>kwargs</code> <p>Parameters.</p> <p> TYPE: <code>str | TNumber | Tensor | Basic | Parameter</code> DEFAULT: <code>{}</code> </p> <p>Example: <pre><code>import sympy\nfrom qadence.parameters import ParamMap\n\n(x,y) = sympy.symbols(\"x y\")\nps = ParamMap(omega=2.0, duration=x+y)\n\nprint(f\"{ps.names() = }\")\nprint(f\"{ps.expressions() = }\")\nprint(f\"{ps.uuids() = }\")\n</code></pre> <pre><code>ps.names() = dict_keys(['omega', 'duration'])\nps.expressions() = dict_values([2.00000000000000, x + y])\nps.uuids() = dict_keys(['a0450897-c8f8-42e2-b062-7394969c9dbc', '4c22881f-d081-4a43-9a14-e8fa049b9861'])\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def __init__(self, **kwargs: str | TNumber | Tensor | Basic | Parameter):\n    self._name_dict: dict[str, tuple[str, Basic]] = {}\n    self._uuid_dict: dict[str, str] = {}\n    for name, v in kwargs.items():\n        param = v if isinstance(v, sympy.Basic) else Parameter(v)\n        uuid = str(uuid4())\n        self._name_dict[name] = (uuid, param)\n        self._uuid_dict[uuid] = param\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.Parameter","title":"<code>Parameter</code>","text":"<p>               Bases: <code>Symbol</code></p> <p>A wrapper on top of <code>sympy.Symbol</code>.</p> <p>Includes two additional keywords: <code>trainable</code> and <code>value</code>. This class is to define both feature parameter and variational parameters.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.trainable","title":"<code>trainable</code>  <code>instance-attribute</code>","text":"<p>Trainable parameters are variational parameters.</p> <p>Non-trainable parameters are feature parameters.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.value","title":"<code>value</code>  <code>instance-attribute</code>","text":"<p>(Initial) value of the parameter.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.__new__","title":"<code>__new__(name, **assumptions)</code>","text":"<p>Arguments:</p> <pre><code>name: When given a string only, the class\n    constructs a trainable Parameter with a a randomly initialized value.\n**assumptions: are passed on to the parent class `sympy.Symbol`. Two new assumption\n    kwargs are supported by this constructor: `trainable: bool`, and `value: TNumber`.\n</code></pre> <p>Example: <pre><code>from qadence.parameters import Parameter, VariationalParameter\n\ntheta = Parameter(\"theta\")\nprint(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\nassert not theta.is_number\n\n# you can specify both trainable/value in the constructor\ntheta = Parameter(\"theta\", trainable=True, value=2.0)\nprint(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n\n# VariationalParameter/FeatureParameter are constructing\n# trainable/untrainable Parameters\ntheta = VariationalParameter(\"theta\", value=2.0)\nassert theta == Parameter(\"theta\", trainable=True, value=2.0)\n\n# When provided with a numeric type, Parameter constructs a sympy numeric type\":\nconstant_zero = Parameter(0)\nassert constant_zero.is_number\n\n# When passed a Parameter or a sympy expression, it just returns it.\nexpr = Parameter(\"x\") * Parameter(\"y\")\nprint(f\"{expr=} : {expr.free_symbols}\")\n</code></pre> <pre><code>theta: trainable=True value=0.6940674329330621\ntheta: trainable=True value=2.0\nexpr=x*y : {x, y}\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def __new__(\n    cls, name: str | TNumber | Tensor | Basic | Parameter, **assumptions: Any\n) -&gt; Parameter | Basic | Expr | Array:\n    \"\"\"\n    Arguments:\n\n        name: When given a string only, the class\n            constructs a trainable Parameter with a a randomly initialized value.\n        **assumptions: are passed on to the parent class `sympy.Symbol`. Two new assumption\n            kwargs are supported by this constructor: `trainable: bool`, and `value: TNumber`.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.parameters import Parameter, VariationalParameter\n\n    theta = Parameter(\"theta\")\n    print(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n    assert not theta.is_number\n\n    # you can specify both trainable/value in the constructor\n    theta = Parameter(\"theta\", trainable=True, value=2.0)\n    print(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n\n    # VariationalParameter/FeatureParameter are constructing\n    # trainable/untrainable Parameters\n    theta = VariationalParameter(\"theta\", value=2.0)\n    assert theta == Parameter(\"theta\", trainable=True, value=2.0)\n\n    # When provided with a numeric type, Parameter constructs a sympy numeric type\":\n    constant_zero = Parameter(0)\n    assert constant_zero.is_number\n\n    # When passed a Parameter or a sympy expression, it just returns it.\n    expr = Parameter(\"x\") * Parameter(\"y\")\n    print(f\"{expr=} : {expr.free_symbols}\")\n    ```\n    \"\"\"\n    p: Parameter\n    if isinstance(name, get_args(TNumber)):\n        return sympify(name)\n    elif isinstance(name, Tensor):\n        if name.numel() == 1:\n            return sympify(name)\n        else:\n            return Array(name.detach().numpy())\n    elif isinstance(name, Parameter):\n        p = super().__new__(cls, name.name, **assumptions)\n        p.name = name.name\n        p.trainable = name.trainable\n        p.value = name.value\n        p.is_time = name.is_time\n        return p\n    elif isinstance(name, (Basic, Expr)):\n        if name.is_number:\n            return sympify(evaluate(name))\n        return name\n    elif isinstance(name, str):\n        p = super().__new__(cls, name, **assumptions)\n        p.trainable = assumptions.get(\"trainable\", True)\n        p.value = assumptions.get(\"value\", None)\n        p.is_time = assumptions.get(\"is_time\", False)\n        if p.value is None:\n            p.value = rand(1).item()\n        return p\n    else:\n        raise TypeError(f\"Parameter does not support type {type(name)}\")\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.FeatureParameter","title":"<code>FeatureParameter(name, **kwargs)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=False)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def FeatureParameter(name: str, **kwargs: Any) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=False)`.\"\"\"\n    return Parameter(name, trainable=False, **kwargs)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.TimeParameter","title":"<code>TimeParameter(name)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=False, is_time=True)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def TimeParameter(name: str) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=False, is_time=True)`.\"\"\"\n    return Parameter(name, trainable=False, is_time=True)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.VariationalParameter","title":"<code>VariationalParameter(name, **kwargs)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=True)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def VariationalParameter(name: str, **kwargs: Any) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=True)`.\"\"\"\n    return Parameter(name, trainable=True, **kwargs)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.evaluate","title":"<code>evaluate(expr, values=None, as_torch=False)</code>","text":"<p>Arguments:</p> <pre><code>expr: An expression consisting of Parameters.\nvalues: values dict which contains values for the Parameters,\n    if empty, Parameter.value will be used.\nas_torch: Whether to retrieve a torch-differentiable expression result.\n</code></pre> <p>Example: <pre><code>from qadence.parameters import Parameter, evaluate\n\nexpr = Parameter(\"x\") * Parameter(\"y\")\n\n# Unless specified, Parameter initialized random values\n# Lets evaluate this expression and see what the result is\nres = evaluate(expr)\nprint(res)\n\n# We can also evaluate the expr using a custom dict\nd = {\"x\": 1, \"y\":2}\nres = evaluate(expr, d)\nprint(res)\n\n# Lastly, if we want a differentiable result, lets put the as_torch flag\nres = evaluate(expr, d, as_torch=True)\nprint(res)\n</code></pre> <pre><code>0.3683408693491492\n2.0\ntensor([2])\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def evaluate(expr: Expr, values: dict | None = None, as_torch: bool = False) -&gt; TNumber | Tensor:\n    \"\"\"\n    Arguments:\n\n        expr: An expression consisting of Parameters.\n        values: values dict which contains values for the Parameters,\n            if empty, Parameter.value will be used.\n        as_torch: Whether to retrieve a torch-differentiable expression result.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.parameters import Parameter, evaluate\n\n    expr = Parameter(\"x\") * Parameter(\"y\")\n\n    # Unless specified, Parameter initialized random values\n    # Lets evaluate this expression and see what the result is\n    res = evaluate(expr)\n    print(res)\n\n    # We can also evaluate the expr using a custom dict\n    d = {\"x\": 1, \"y\":2}\n    res = evaluate(expr, d)\n    print(res)\n\n    # Lastly, if we want a differentiable result, lets put the as_torch flag\n    res = evaluate(expr, d, as_torch=True)\n    print(res)\n    ```\n    \"\"\"\n    res: Basic\n    res_value: TNumber | Tensor\n    query: dict[Parameter, TNumber | Tensor] = dict()\n    values = values or dict()\n    if isinstance(expr, Array):\n        return Tensor(expr.tolist())\n    else:\n        if not expr.is_number:\n            for s in expr.free_symbols:\n                if s.name in values.keys():\n                    query[s] = values[s.name]\n                elif hasattr(s, \"value\"):\n                    query[s] = s.value\n                else:\n                    raise ValueError(f\"No value provided for symbol {s.name}\")\n        if as_torch:\n            res_value = make_differentiable(expr)(**{s.name: tensor(v) for s, v in query.items()})\n        else:\n            res = expr.subs(query)\n            res_value = sympy_to_numeric(res)\n        return res_value\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.extract_original_param_entry","title":"<code>extract_original_param_entry(param)</code>","text":"<p>Given an Expression, what was the original \"param\" given by the user? It is either.</p> <p>going to be a numeric value, or a sympy Expression (in case a string was given, it was converted via Parameter(\"string\").</p> Source code in <code>qadence/parameters.py</code> <pre><code>def extract_original_param_entry(\n    param: Expr,\n) -&gt; TNumber | Tensor | Expr:\n    \"\"\"\n    Given an Expression, what was the original \"param\" given by the user? It is either.\n\n    going to be a numeric value, or a sympy Expression (in case a string was given,\n    it was converted via Parameter(\"string\").\n    \"\"\"\n    return param if not param.is_number else evaluate(param)\n</code></pre>"},{"location":"api/parameters/#parameter-embedding","title":"Parameter embedding","text":""},{"location":"api/parameters/#qadence.blocks.embedding.embedding","title":"<code>embedding(block, to_gate_params=False, engine=Engine.TORCH)</code>","text":"<p>Construct embedding function which maps user-facing parameters to either expression-level.</p> <p>parameters or gate-level parameters. The constructed embedding function has the signature:</p> <pre><code> embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n</code></pre> <p>which means that it maps the variational parameter dict <code>params</code> and the feature parameter dict <code>inputs</code> to one new parameter dict <code>embedded_dict</code> which holds all parameters that are needed to execute a circuit on a given backend. There are two different modes for this mapping:</p> <ul> <li>Expression-level parameters: For AD-based optimization. For every unique expression we end   up with one entry in the embedded dict:   <code>len(embedded_dict) == len(unique_parameter_expressions)</code>.</li> <li>Gate-level parameters: For PSR-based optimization or real devices. One parameter for each   gate parameter, regardless if they are based on the same expression. <code>len(embedded_dict) ==   len(parametric_gates)</code>. This is needed because PSR requires to shift the angles of every   gate where the same parameter appears.</li> </ul> PARAMETER DESCRIPTION <code>block</code> <p>parametrized block into which we want to embed parameters.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>to_gate_params</code> <p>A boolean flag whether to generate gate-level parameters or expression-level parameters.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[ParamDictType, Callable[[ParamDictType, ParamDictType], ParamDictType]]</code> <p>A tuple with variational parameter dict and the embedding function.</p> Source code in <code>qadence/blocks/embedding.py</code> <pre><code>def embedding(\n    block: AbstractBlock, to_gate_params: bool = False, engine: Engine = Engine.TORCH\n) -&gt; tuple[\n    ParamDictType,\n    Callable[[ParamDictType, ParamDictType], ParamDictType],\n]:\n    \"\"\"Construct embedding function which maps user-facing parameters to either *expression-level*.\n\n    parameters or *gate-level* parameters. The constructed embedding function has the signature:\n\n         embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n\n    which means that it maps the *variational* parameter dict `params` and the *feature* parameter\n    dict `inputs` to one new parameter dict `embedded_dict` which holds all parameters that are\n    needed to execute a circuit on a given backend. There are two different *modes* for this\n    mapping:\n\n    - *Expression-level* parameters: For AD-based optimization. For every unique expression we end\n      up with one entry in the embedded dict:\n      `len(embedded_dict) == len(unique_parameter_expressions)`.\n    - *Gate-level* parameters: For PSR-based optimization or real devices. One parameter for each\n      gate parameter, regardless if they are based on the same expression. `len(embedded_dict) ==\n      len(parametric_gates)`. This is needed because PSR requires to shift the angles of **every**\n      gate where the same parameter appears.\n\n    Arguments:\n        block: parametrized block into which we want to embed parameters.\n        to_gate_params: A boolean flag whether to generate gate-level parameters or\n            expression-level parameters.\n\n    Returns:\n        A tuple with variational parameter dict and the embedding function.\n    \"\"\"\n    concretize_parameter = _concretize_parameter(engine)\n    if engine == Engine.TORCH:\n        cast_dtype = tensor\n    else:\n        from jax.numpy import array\n\n        cast_dtype = array\n\n    unique_expressions = unique(expressions(block))\n    unique_symbols = [p for p in unique(parameters(block)) if not isinstance(p, sympy.Array)]\n    unique_const_matrices = [e for e in unique_expressions if isinstance(e, sympy.Array)]\n    unique_expressions = [e for e in unique_expressions if not isinstance(e, sympy.Array)]\n\n    # NOTE\n    # there are 3 kinds of parameters in qadence\n    # - non-trainable which are considered as inputs for classical data\n    # - trainable which are the variational parameters to be optimized\n    # - fixed: which are non-trainable parameters with fixed value (e.g. pi/2)\n    #\n    # both non-trainable and trainable parameters can have the same element applied\n    # to different operations in the quantum circuit, e.g. assigning the same parameter\n    # to multiple gates.\n    non_numeric_symbols = [p for p in unique_symbols if not p.is_number]\n    trainable_symbols = [p for p in non_numeric_symbols if p.trainable]\n    constant_expressions = [expr for expr in unique_expressions if expr.is_number]\n    # we dont need to care about constant symbols if they are contained in an symbolic expression\n    # we only care about gate params which are ONLY a constant\n\n    embeddings: dict[sympy.Expr, DifferentiableExpression] = {\n        expr: make_differentiable(expr=expr, engine=engine)\n        for expr in unique_expressions\n        if not expr.is_number\n    }\n\n    uuid_to_expr = uuid_to_expression(block)\n\n    def embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n        embedded_params: dict[sympy.Expr, ArrayLike] = {}\n        for expr, fn in embeddings.items():\n            angle: ArrayLike\n            values = {}\n            for symbol in expr.free_symbols:\n                if symbol.name in inputs:\n                    value = inputs[symbol.name]\n                elif symbol.name in params:\n                    value = params[symbol.name]\n                else:\n                    if symbol.is_time:\n                        value = tensor(1.0)\n                    else:\n                        msg_trainable = \"Trainable\" if symbol.trainable else \"Non-trainable\"\n                        raise KeyError(\n                            f\"{msg_trainable} parameter '{symbol.name}' not found in the \"\n                            f\"inputs list: {list(inputs.keys())} nor the \"\n                            f\"params list: {list(params.keys())}.\"\n                        )\n                values[symbol.name] = value\n            angle = fn(**values)\n            # do not reshape parameters which are multi-dimensional\n            # tensors, such as for example generator matrices\n            if not len(angle.squeeze().shape) &gt; 1:\n                angle = angle.reshape(-1)\n            embedded_params[expr] = angle\n\n        for e in constant_expressions + unique_const_matrices:\n            embedded_params[e] = params[stringify(e)]\n\n        if to_gate_params:\n            gate_lvl_params: ParamDictType = {}\n            for uuid, e in uuid_to_expr.items():\n                gate_lvl_params[uuid] = embedded_params[e]\n            return gate_lvl_params\n        else:\n            embedded_params.update(inputs)\n            for k, v in params.items():\n                if k not in embedded_params:\n                    embedded_params[k] = v\n            out = {\n                stringify(k) if not isinstance(k, str) else k: (\n                    as_tensor(v)[None] if as_tensor(v).ndim == 0 else v\n                )\n                for k, v in embedded_params.items()\n            }\n            return out\n\n    params: ParamDictType\n    params = {\n        p.name: concretize_parameter(value=p.value, trainable=True) for p in trainable_symbols\n    }\n    params.update(\n        {\n            stringify(expr): concretize_parameter(value=evaluate(expr), trainable=False)\n            for expr in constant_expressions\n        }\n    )\n    params.update(\n        {\n            stringify(expr): cast_dtype(nparray(expr.tolist(), dtype=npcdouble))\n            for expr in unique_const_matrices\n        }\n    )\n    return params, embedding_fn\n</code></pre>"},{"location":"api/pasqal_cloud_connection/","title":"Pasqal Cloud Connection","text":""},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.WorkloadNotDoneError","title":"<code>WorkloadNotDoneError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Is raised if a workload is not yet finished running on remote.</p>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.WorkloadSpec","title":"<code>WorkloadSpec(circuit, backend, result_types, parameter_values=None, observable=None)</code>  <code>dataclass</code>","text":"<p>Specification of a workload to be executed on Pasqal Cloud.</p> <p>This data class defines a single workload specification that is to be executed on Pasqal's cloud platform.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The quantum circuit to be executed.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>backend</code> <p>The backend to execute the workload on. Not all backends are available on the cloud platform. Currently the supported backend is <code>BackendName.PYQTORCH</code>.</p> <p> TYPE: <code>BackendName | str</code> </p> <code>result_types</code> <p>The types of result to compute for this workload. The circuit will be run for all result types specified here one by one.</p> <p> TYPE: <code>list[ResultType]</code> </p> <code>parameter_values</code> <p>If the quantum circuit has feature parameters, values for those need to be provided. In the case there are only variational parameters, this field is optional. In the case there are no parameters, this field needs to be <code>None</code>. The parameter values can be either a tensor of dimension 0 or 1, which can differ per parameter. For parameters that are an array, i.e. dimension 1, all array lengths should be equal.</p> <p> TYPE: <code>dict[str, Tensor] | None</code> DEFAULT: <code>None</code> </p> <code>observable</code> <p>Observable that is used when <code>result_types</code> contains <code>ResultType.EXPECTATION</code>. The observable field is mandatory in this case. If not, the value of this field will be ignored. Only a single observable can be passed for cloud submission; providing a list of observables is not supported.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.WorkloadStoppedError","title":"<code>WorkloadStoppedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Is raised when a workload has stopped running on remote for some reason.</p>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.check_status","title":"<code>check_status(connection, workload_id)</code>","text":"<p>Checks if the workload is successfully finished on remote connection.</p> PARAMETER DESCRIPTION <code>connection</code> <p>A <code>pasqal_cloud.SDK</code> instance which is used to connect to the cloud.</p> <p> TYPE: <code>SDK</code> </p> <code>workload_id</code> <p>the id <code>str</code> that is associated with the workload.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>WorkloadNotDoneError</code> <p>Is raised when the workload status is \"PENDING\", \"RUNNING\" or \"PAUSED\".</p> <code>WorkloadStoppedError</code> <p>Is raise when the workload status is \"CANCELED\", \"TIMED_OUT\" or \"ERROR\".</p> <code>ValueError</code> <p>Is raised when the workload status has an unsupported value.</p> RETURNS DESCRIPTION <code>Workload</code> <p>The workload result if its status is \"DONE\" as a <code>pasqal_cloud.Workload</code> object.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def check_status(connection: SDK, workload_id: str) -&gt; WorkloadResult:\n    \"\"\"Checks if the workload is successfully finished on remote connection.\n\n    Args:\n        connection: A `pasqal_cloud.SDK` instance which is used to connect to the cloud.\n        workload_id: the id `str` that is associated with the workload.\n\n    Raises:\n        WorkloadNotDoneError: Is raised when the workload status is \"PENDING\", \"RUNNING\" or\n            \"PAUSED\".\n        WorkloadStoppedError: Is raise when the workload status is \"CANCELED\", \"TIMED_OUT\" or\n            \"ERROR\".\n        ValueError: Is raised when the workload status has an unsupported value.\n\n    Returns:\n        The workload result if its status is \"DONE\" as a `pasqal_cloud.Workload` object.\n    \"\"\"\n    # TODO Make the function return a \"nice\" result object\n    result = connection.get_workload(workload_id)\n    if result.status == \"DONE\":\n        return result\n    if result.status in (\"PENDING\", \"RUNNING\", \"PAUSED\"):\n        raise WorkloadNotDoneError(\n            f\"Workload with id {workload_id} is not yet finished, the status is {result.status}\"\n        )\n    if result.status in (\"CANCELED\", \"TIMED_OUT\", \"ERROR\"):\n        message = f\"Workload with id {workload_id} couldn't finish, the status is {result.status}.\"\n        if result.status == \"ERROR\":\n            message += f\"The following error(s) occurred {result.errors}\"\n        raise WorkloadStoppedError(message)\n    raise ValueError(\n        f\"Undefined workload status ({result.status}) was returned for workload ({result.id})\"\n    )\n</code></pre>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.get_result","title":"<code>get_result(connection, workload_id, timeout=60.0, refresh_time=1.0)</code>","text":"<p>Repeatedly checks if a workload has finished and returns the result.</p> PARAMETER DESCRIPTION <code>connection</code> <p>A <code>pasqal_cloud.SDK</code> instance which is used to connect to the cloud.</p> <p> TYPE: <code>SDK</code> </p> <code>workload_id</code> <p>the id <code>str</code> that is associated with the workload.</p> <p> TYPE: <code>str</code> </p> <code>timeout</code> <p>Time in seconds after which the function times out. Defaults to 60.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>60.0</code> </p> <code>refresh_time</code> <p>Time in seconds after which the remote is requested to update the status again, when the workload is not finished yet. Defaults to 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RAISES DESCRIPTION <code>TimeoutError</code> <p>description</p> RETURNS DESCRIPTION <code>Workload</code> <p>The workload result if its status is \"DONE\" as a <code>pasqal_cloud.Workload</code> object.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def get_result(\n    connection: SDK, workload_id: str, timeout: float = 60.0, refresh_time: float = 1.0\n) -&gt; WorkloadResult:\n    \"\"\"Repeatedly checks if a workload has finished and returns the result.\n\n    Args:\n        connection: A `pasqal_cloud.SDK` instance which is used to connect to the cloud.\n        workload_id: the id `str` that is associated with the workload.\n        timeout: Time in seconds after which the function times out. Defaults to 60.0.\n        refresh_time: Time in seconds after which the remote is requested to update the status\n            again, when the workload is not finished yet. Defaults to 1.0.\n\n    Raises:\n        TimeoutError: _description_\n\n    Returns:\n        The workload result if its status is \"DONE\" as a `pasqal_cloud.Workload` object.\n    \"\"\"\n    max_refresh_count = int(timeout // refresh_time)\n    for _ in range(max_refresh_count):\n        try:\n            result = check_status(connection, workload_id)\n        except WorkloadNotDoneError:\n            time.sleep(refresh_time)\n            continue\n        return result\n    raise TimeoutError(\"Request timed out because it wasn't finished in the specified time. \")\n</code></pre>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.get_workload_spec","title":"<code>get_workload_spec(model, result_types, parameter_values=None, observable=None)</code>","text":"<p>Creates a <code>WorkloadSpec</code> from a quantum model.</p> <p>This function creates a <code>WorkloadSpec</code> from a <code>QuantumModel</code> and the other arguments provided. The circuit, that is extracted from the model, is the original circuit that was used to initialize the model, not the backend converted circuit in <code>model.circuit</code>. The backend set in the model will be used in the workload specification.</p> <p>It is important to note that in case there is an observable defined in the model, it is ignored in the workload specification. To provide an observable to the workload specification, it is only possible to set it in the observable argument of this function.</p> PARAMETER DESCRIPTION <code>model</code> <p>The quantum model that defines the circuit and backend for the workload spec.</p> <p> TYPE: <code>QuantumModel</code> </p> <code>result_types</code> <p>A list of result types that is requested in this workload.</p> <p> TYPE: <code>list[ResultType]</code> </p> <code>parameter_values</code> <p>The parameter values that should be used during execution of the workload.</p> <p> TYPE: <code>dict[str, Tensor] | None</code> DEFAULT: <code>None</code> </p> <code>observable</code> <p>Observable that is used when <code>result_types</code> contains <code>ResultType.EXPECTATION</code>. The observable field is mandatory in this case. If not, the value of this field will be ignored. Only a single observable can be passed for cloud submission; providing a list of observables is not supported.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>WorkloadSpec</code> <p>A <code>WorkloadSpec</code> instance based on the quantum model passed to this function.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def get_workload_spec(\n    model: QuantumModel,\n    result_types: list[ResultType],\n    parameter_values: dict[str, Tensor] | None = None,\n    observable: AbstractBlock | None = None,\n) -&gt; WorkloadSpec:\n    \"\"\"Creates a `WorkloadSpec` from a quantum model.\n\n    This function creates a `WorkloadSpec` from a `QuantumModel` and the other arguments provided.\n    The circuit, that is extracted from the model, is the original circuit that was used to\n    initialize the model, not the backend converted circuit in `model.circuit`. The backend set in\n    the model will be used in the workload specification.\n\n    It is important to note that in case there is an observable defined in the model, it is ignored\n    in the workload specification. To provide an observable to the workload specification, it is\n    only possible to set it in the observable argument of this function.\n\n    Args:\n        model: The quantum model that defines the circuit and backend for the workload spec.\n        result_types: A list of result types that is requested in this workload.\n        parameter_values: The parameter values that should be used during execution of the\n            workload.\n        observable: Observable that is used when `result_types` contains `ResultType.EXPECTATION`.\n            The observable field is mandatory in this case. If not, the value of this field will\n            be ignored. Only a single observable can be passed for cloud submission; providing a\n            list of observables is not supported.\n\n    Returns:\n        A `WorkloadSpec` instance based on the quantum model passed to this function.\n    \"\"\"\n    circuit = model._circuit.original\n    backend = model._backend_name\n    return WorkloadSpec(circuit, backend, result_types, parameter_values, observable)\n</code></pre>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.submit_workload","title":"<code>submit_workload(connection, workload)</code>","text":"<p>Uploads a workload to Pasqal's Cloud and returns the created workload ID.</p> PARAMETER DESCRIPTION <code>connection</code> <p>A <code>pasqal_cloud.SDK</code> instance which is used to connect to the cloud.</p> <p> TYPE: <code>SDK</code> </p> <code>workload</code> <p>A <code>WorkloadSpec</code> object, defining the specification of the workload that needs to be uploaded.</p> <p> TYPE: <code>WorkloadSpec</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A workload id as a <code>str</code>.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def submit_workload(connection: SDK, workload: WorkloadSpec) -&gt; str:\n    \"\"\"Uploads a workload to Pasqal's Cloud and returns the created workload ID.\n\n    Args:\n        connection: A `pasqal_cloud.SDK` instance which is used to connect to the cloud.\n        workload: A `WorkloadSpec` object, defining the specification of the workload that needs to\n            be uploaded.\n\n    Returns:\n        A workload id as a `str`.\n    \"\"\"\n    workload_json = _workload_spec_to_json(workload)\n    remote_workload = connection.create_workload(\n        workload_json.workload_type, workload_json.backend_type, workload_json.config\n    )\n    workload_id: str = remote_workload.id\n    return workload_id\n</code></pre>"},{"location":"api/quantumcircuit/","title":"QuantumCircuit","text":""},{"location":"api/quantumcircuit/#quantumcircuit","title":"QuantumCircuit","text":"<p>The abstract <code>QuantumCircuit</code> is the key object in Qadence, as it is what can be executed.</p>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit","title":"<code>QuantumCircuit(support, *blocks)</code>  <code>dataclass</code>","text":"<p>Am abstract QuantumCircuit instance.</p> <p>It needs to be passed to a quantum backend for execution.</p> <p>Arguments:</p> <pre><code>support: `Register` or number of qubits. If an integer is provided, a register is\n    constructed with `Register.all_to_all(x)`\n*blocks: (Possibly multiple) blocks to construct the circuit from.\n</code></pre> Source code in <code>qadence/circuit.py</code> <pre><code>def __init__(self, support: int | Register, *blocks: AbstractBlock):\n    \"\"\"\n    Arguments:\n\n        support: `Register` or number of qubits. If an integer is provided, a register is\n            constructed with `Register.all_to_all(x)`\n        *blocks: (Possibly multiple) blocks to construct the circuit from.\n    \"\"\"\n    self.block = chain(*blocks) if len(blocks) != 1 else blocks[0]\n    self.register = Register(support) if isinstance(support, int) else support\n\n    global_block = isinstance(self.block, AnalogBlock) and self.block.qubit_support.is_global\n    if not global_block and len(self.block) and self.block.n_qubits &gt; self.register.n_qubits:\n        raise ValueError(\n            f\"Register with {self.register.n_qubits} qubits is too small for the \"\n            f\"given block with {self.block.n_qubits} qubits\"\n        )\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.unique_parameters","title":"<code>unique_parameters</code>  <code>property</code>","text":"<p>Return the unique parameters in the circuit.</p> <p>These parameters are the actual user-facing parameters which can be assigned by the user. Multiple gates can contain the same unique parameter</p> RETURNS DESCRIPTION <code>list[Parameter]</code> <p>list[Parameter]: List of unique parameters in the circuit</p>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.dagger","title":"<code>dagger()</code>","text":"<p>Reverse the QuantumCircuit by calling dagger on the block.</p> Source code in <code>qadence/circuit.py</code> <pre><code>def dagger(self) -&gt; QuantumCircuit:\n    \"\"\"Reverse the QuantumCircuit by calling dagger on the block.\"\"\"\n    return QuantumCircuit(self.n_qubits, self.block.dagger())\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.get_blocks_by_tag","title":"<code>get_blocks_by_tag(tag)</code>","text":"<p>Extract one or more blocks using the human-readable tag.</p> <p>This function recursively explores all composite blocks to find all the occurrences of a certain tag in the blocks.</p> PARAMETER DESCRIPTION <code>tag</code> <p>the tag to look for</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>list[AbstractBlock]: The block(s) corresponding to the given tag</p> Source code in <code>qadence/circuit.py</code> <pre><code>def get_blocks_by_tag(self, tag: str) -&gt; list[AbstractBlock]:\n    \"\"\"Extract one or more blocks using the human-readable tag.\n\n    This function recursively explores all composite blocks to find\n    all the occurrences of a certain tag in the blocks.\n\n    Args:\n        tag (str): the tag to look for\n\n    Returns:\n        list[AbstractBlock]: The block(s) corresponding to the given tag\n    \"\"\"\n\n    def _get_block(block: AbstractBlock) -&gt; list[AbstractBlock]:\n        blocks = []\n        if block.tag == tag:\n            blocks += [block]\n        if isinstance(block, CompositeBlock):\n            blocks += flatten(*[_get_block(b) for b in block.blocks])\n        return blocks\n\n    return _get_block(self.block)\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.parameters","title":"<code>parameters()</code>","text":"<p>Extract all parameters for primitive blocks in the circuit.</p> <p>Notice that this function returns all the unique Parameters used in the quantum circuit. These can correspond to constants too.</p> RETURNS DESCRIPTION <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>List[tuple[Parameter]]: A list of tuples containing the Parameter</p> <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>instance of each of the primitive blocks in the circuit or, if the <code>flatten</code></p> <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>flag is set to True, a flattened list of all circuit parameters</p> Source code in <code>qadence/circuit.py</code> <pre><code>def parameters(self) -&gt; list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]:\n    \"\"\"Extract all parameters for primitive blocks in the circuit.\n\n    Notice that this function returns all the unique Parameters used\n    in the quantum circuit. These can correspond to constants too.\n\n    Returns:\n        List[tuple[Parameter]]: A list of tuples containing the Parameter\n        instance of each of the primitive blocks in the circuit or, if the `flatten`\n        flag is set to True, a flattened list of all circuit parameters\n    \"\"\"\n    return parameters(self.block)\n</code></pre>"},{"location":"api/register/","title":"Register","text":""},{"location":"api/register/#quantum-registers","title":"Quantum Registers","text":""},{"location":"api/register/#qadence.register.Register","title":"<code>Register(support, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>","text":"<p>A register of qubits including 2D coordinates.</p> <p>Instantiating the Register class directly is only recommended for building custom registers. For most uses where a predefined lattice is desired it is recommended to use the various class methods available, e.g. <code>Register.triangular_lattice</code>.</p> PARAMETER DESCRIPTION <code>support</code> <p>A NetworkX graph or number of qubits. Nodes can include a <code>\"pos\"</code> attribute such that e.g.: <code>graph.nodes = {0: {\"pos\": (2,3)}, 1: {\"pos\": (0,0)}, ...}</code> which will be used in backends that need qubit coordinates. Passing a number of qubits calls <code>Register.all_to_all(n_qubits)</code>.</p> <p> TYPE: <code>Graph | int</code> </p> <code>spacing</code> <p>Value set as the distance between the two closest qubits. The spacing argument is also available for all the class method constructors.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>1.0</code> </p> <p>Examples: <pre><code>from qadence import Register\n\nreg_all = Register.all_to_all(n_qubits = 4)\nreg_line = Register.line(n_qubits = 4)\nreg_circle = Register.circle(n_qubits = 4)\nreg_squre = Register.square(qubits_side = 2)\nreg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\nreg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\nreg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n</code></pre> </p> Source code in <code>qadence/register.py</code> <pre><code>def __init__(\n    self,\n    support: nx.Graph | int,\n    spacing: float | None = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n):\n    \"\"\"\n    A register of qubits including 2D coordinates.\n\n    Instantiating the Register class directly is only recommended for building custom registers.\n    For most uses where a predefined lattice is desired it is recommended to use the various\n    class methods available, e.g. `Register.triangular_lattice`.\n\n    Arguments:\n        support: A NetworkX graph or number of qubits. Nodes can include a `\"pos\"` attribute\n            such that e.g.: `graph.nodes = {0: {\"pos\": (2,3)}, 1: {\"pos\": (0,0)}, ...}` which\n            will be used in backends that need qubit coordinates. Passing a number of qubits\n            calls `Register.all_to_all(n_qubits)`.\n        spacing: Value set as the distance between the two closest qubits. The spacing\n            argument is also available for all the class method constructors.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\"\n    from qadence import Register\n\n    reg_all = Register.all_to_all(n_qubits = 4)\n    reg_line = Register.line(n_qubits = 4)\n    reg_circle = Register.circle(n_qubits = 4)\n    reg_squre = Register.square(qubits_side = 2)\n    reg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\n    reg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\n    reg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n    ```\n    \"\"\"\n    if device_specs is not None and not isinstance(device_specs, RydbergDevice):\n        raise ValueError(\"Device specs are not valid. Please pass a `RydbergDevice` instance.\")\n\n    self.device_specs = device_specs\n\n    self.graph = support if isinstance(support, nx.Graph) else alltoall_graph(support)\n\n    if spacing is not None and self.min_distance != 0.0:\n        _scale_node_positions(self.graph, self.min_distance, spacing)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.all_node_pairs","title":"<code>all_node_pairs</code>  <code>property</code>","text":"<p>Return a list of all possible qubit pairs in the register.</p>"},{"location":"api/register/#qadence.register.Register.coords","title":"<code>coords</code>  <code>property</code>","text":"<p>Return the dictionary of qubit coordinates.</p>"},{"location":"api/register/#qadence.register.Register.distances","title":"<code>distances</code>  <code>property</code>","text":"<p>Return a dictionary of distances for all qubit pairs in the register.</p>"},{"location":"api/register/#qadence.register.Register.edge_distances","title":"<code>edge_distances</code>  <code>property</code>","text":"<p>Return a dictionary of distances for the qubit pairs that are.</p> <p>connected by an edge in the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.edges","title":"<code>edges</code>  <code>property</code>","text":"<p>Return the EdgeView of the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.min_distance","title":"<code>min_distance</code>  <code>property</code>","text":"<p>Return the minimum distance between two qubts in the register.</p>"},{"location":"api/register/#qadence.register.Register.n_qubits","title":"<code>n_qubits</code>  <code>property</code>","text":"<p>Total number of qubits in the register.</p>"},{"location":"api/register/#qadence.register.Register.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Return the NodeView of the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.support","title":"<code>support</code>  <code>property</code>","text":"<p>Return the set of qubits in the register.</p>"},{"location":"api/register/#qadence.register.Register.all_to_all","title":"<code>all_to_all(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a register with an all-to-all connectivity graph.</p> <p>The graph is projected onto a 2D space and the qubit coordinates are set using a spring layout algorithm.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef all_to_all(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a register with an all-to-all connectivity graph.\n\n    The graph is projected\n    onto a 2D space and the qubit coordinates are set using a spring layout algorithm.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    return cls(alltoall_graph(n_qubits), spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.circle","title":"<code>circle(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a circle register.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef circle(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a circle register.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    graph = nx.grid_2d_graph(n_qubits, 1, periodic=True)\n    graph = nx.relabel_nodes(graph, {(i, 0): i for i in range(n_qubits)})\n    coords = nx.circular_layout(graph)\n    values = {i: {\"pos\": pos} for i, pos in coords.items()}\n    nx.set_node_attributes(graph, values)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.draw","title":"<code>draw(show=True)</code>","text":"<p>Draw the underlying NetworkX graph representing the register.</p> Source code in <code>qadence/register.py</code> <pre><code>def draw(self, show: bool = True) -&gt; None:\n    \"\"\"Draw the underlying NetworkX graph representing the register.\"\"\"\n    coords = {i: n[\"pos\"] for i, n in self.graph.nodes.items()}\n    nx.draw(self.graph, with_labels=True, pos=coords)\n    if show:\n        plt.gcf().show()\n</code></pre>"},{"location":"api/register/#qadence.register.Register.from_coordinates","title":"<code>from_coordinates(coords, lattice=LatticeTopology.ARBITRARY, spacing=None, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a register from a list of qubit coordinates.</p> <p>Each node is added to the underlying graph with the respective coordinates, but the edges are left empty.</p> PARAMETER DESCRIPTION <code>coords</code> <p>List of qubit coordinate tuples.</p> <p> TYPE: <code>list[tuple]</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef from_coordinates(\n    cls,\n    coords: list[tuple],\n    lattice: LatticeTopology | str = LatticeTopology.ARBITRARY,\n    spacing: float | None = None,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a register from a list of qubit coordinates.\n\n    Each node is added to the underlying\n    graph with the respective coordinates, but the edges are left empty.\n\n    Arguments:\n        coords: List of qubit coordinate tuples.\n    \"\"\"\n    graph = nx.Graph()\n    for i, pos in enumerate(coords):\n        graph.add_node(i, pos=pos)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.honeycomb_lattice","title":"<code>honeycomb_lattice(n_cells_row, n_cells_col, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a honeycomb lattice register.</p> <p>Each cell is an hexagon made up of six qubits.</p> PARAMETER DESCRIPTION <code>n_cells_row</code> <p>Number of cells in each row.</p> <p> TYPE: <code>int</code> </p> <code>n_cells_col</code> <p>Number of cells in each column.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef honeycomb_lattice(\n    cls,\n    n_cells_row: int,\n    n_cells_col: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a honeycomb lattice register.\n\n    Each cell is an hexagon made up of six qubits.\n\n    Arguments:\n        n_cells_row: Number of cells in each row.\n        n_cells_col: Number of cells in each column.\n    \"\"\"\n    graph = nx.hexagonal_lattice_graph(n_cells_row, n_cells_col)\n    graph = nx.relabel_nodes(graph, {(i, j): k for k, (i, j) in enumerate(graph.nodes)})\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.line","title":"<code>line(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a line register.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef line(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a line register.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    return cls(line_graph(n_qubits), spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.rescale_coords","title":"<code>rescale_coords(scaling)</code>","text":"<p>Rescale the coordinates of all qubits in the register.</p> PARAMETER DESCRIPTION <code>scaling</code> <p>Scaling value.</p> <p> TYPE: <code>float</code> </p> Source code in <code>qadence/register.py</code> <pre><code>def rescale_coords(self, scaling: float) -&gt; Register:\n    \"\"\"\n    Rescale the coordinates of all qubits in the register.\n\n    Arguments:\n        scaling: Scaling value.\n    \"\"\"\n    g = deepcopy(self.graph)\n    _scale_node_positions(g, min_distance=1.0, spacing=scaling)\n    return Register(g, spacing=None, device_specs=self.device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.square","title":"<code>square(qubits_side, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a square register.</p> PARAMETER DESCRIPTION <code>qubits_side</code> <p>Number of qubits on one side of the square.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef square(\n    cls,\n    qubits_side: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a square register.\n\n    Arguments:\n        qubits_side: Number of qubits on one side of the square.\n    \"\"\"\n    n_points = 4 * (qubits_side - 1)\n\n    def gen_points() -&gt; np.ndarray:\n        rotate_left = np.array([[0.0, -1.0], [1.0, 0.0]])\n        increment = np.array([0.0, 1.0])\n\n        points = [np.array([0.0, 0.0])]\n        counter = 1\n        while len(points) &lt; n_points:\n            points.append(points[-1] + increment)\n\n            counter = (counter + 1) % qubits_side\n            if counter == 0:\n                increment = rotate_left.dot(increment)\n                counter = 1\n        points = np.array(points)  # type: ignore[assignment]\n        points -= np.mean(points, axis=0)\n\n        return points  # type: ignore[return-value]\n\n    graph = nx.grid_2d_graph(n_points, 1, periodic=True)\n    graph = nx.relabel_nodes(graph, {(i, 0): i for i in range(n_points)})\n    values = {i: {\"pos\": point} for i, point in zip(graph.nodes, gen_points())}\n    nx.set_node_attributes(graph, values)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.triangular_lattice","title":"<code>triangular_lattice(n_cells_row, n_cells_col, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a triangular lattice register.</p> <p>Each cell is a triangle made up of three qubits.</p> PARAMETER DESCRIPTION <code>n_cells_row</code> <p>Number of cells in each row.</p> <p> TYPE: <code>int</code> </p> <code>n_cells_col</code> <p>Number of cells in each column.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef triangular_lattice(\n    cls,\n    n_cells_row: int,\n    n_cells_col: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a triangular lattice register.\n\n    Each cell is a triangle made up of three qubits.\n\n    Arguments:\n        n_cells_row: Number of cells in each row.\n        n_cells_col: Number of cells in each column.\n    \"\"\"\n    return cls(triangular_lattice_graph(n_cells_row, n_cells_col), spacing, device_specs)\n</code></pre>"},{"location":"api/serialization/","title":"Serialization","text":""},{"location":"api/serialization/#serialization","title":"Serialization","text":""},{"location":"api/serialization/#qadence.serialization.SerializationModel","title":"<code>SerializationModel(d=dict())</code>  <code>dataclass</code>","text":"<p>A serialization model class to serialize data from <code>QuantumModel</code>s,.</p> <p><code>torch.nn.Module</code> and similar structures. The data included in the serialization logic includes: the <code>AbstractBlock</code> and its children classes, <code>QuantumCircuit</code>, <code>Register</code>, and <code>sympy</code> expressions (including <code>Parameter</code> class from <code>qadence.parameters</code>).</p> <p>A children class must define the <code>value</code> attribute type and how to handle it, since it is the main property for the class to be used by the serialization process. For instance:</p> <pre><code>@dataclass\nclass QuantumCircuitSerialization(SerializationModel):\n    value: QuantumCircuit = dataclass_field(init=False)\n\n    def __post_init__(self) -&gt; None:\n        self.value = (\n            QuantumCircuit._from_dict(self.d)\n            if isinstance(self.d, dict)\n            else self.d\n        )\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.deserialize","title":"<code>deserialize(d, as_torch=False)</code>","text":"<p>Supported Types:</p> <p>AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module Deserializes a dict to one of the supported types.</p> PARAMETER DESCRIPTION <code>d</code> <p>A dict containing a serialized object.</p> <p> TYPE: <code>dict</code> </p> <code>as_torch</code> <p>Whether to transform to torch for the deserialized object.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     AbstractBlock, QuantumCircuit, QuantumModel, Register, torch.nn.Module.</p> <p>Examples: <pre><code>import torch\nfrom qadence import serialize, deserialize, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nblock_dict = serialize(myblock)\nprint(block_dict)\n\n## Lets use myblock in a QuantumCircuit and serialize it.\n\nqc = QuantumCircuit(n_qubits, myblock)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n## Finally, let's wrap it in a QuantumModel\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n# Lets check if the loaded QuantumModel returns the same expectation\nassert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n</code></pre> <pre><code>{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': 'HEA', 'blocks': [{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('d090e4cf-7f43-4a9d-87f7-97d7b52de4f7', {'name': 'theta_0', 'expression': \"Parameter('theta_0')\", 'symbols': {'theta_0': {'name': 'theta_0', 'trainable': 'True', 'value': '0.9929729755855087'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('7911f946-d2d1-413c-b190-03f5e670704f', {'name': 'theta_1', 'expression': \"Parameter('theta_1')\", 'symbols': {'theta_1': {'name': 'theta_1', 'trainable': 'True', 'value': '0.8160586131201085'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RY', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('cb123ab0-6b7f-4c90-9036-b8b29b9c0ed5', {'name': 'theta_2', 'expression': \"Parameter('theta_2')\", 'symbols': {'theta_2': {'name': 'theta_2', 'trainable': 'True', 'value': '0.4935169156790826'}}})}}, 'noise': None}, {'type': 'RY', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('b0f37c16-d527-449b-850e-69356f941852', {'name': 'theta_3', 'expression': \"Parameter('theta_3')\", 'symbols': {'theta_3': {'name': 'theta_3', 'trainable': 'True', 'value': '0.9174378586714305'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('21d6ae98-aa35-45cd-a950-5338529dc56f', {'name': 'theta_4', 'expression': \"Parameter('theta_4')\", 'symbols': {'theta_4': {'name': 'theta_4', 'trainable': 'True', 'value': '0.2895278778467999'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('92c270e1-9996-4c7e-a8d7-a925acd87efd', {'name': 'theta_5', 'expression': \"Parameter('theta_5')\", 'symbols': {'theta_5': {'name': 'theta_5', 'trainable': 'True', 'value': '0.9596052720203944'}}})}}, 'noise': None}]}]}, {'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'CNOT', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'X', 'qubit_support': (1,), 'tag': None, 'noise': None}], 'noise': None}]}]}]}\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def deserialize(d: dict, as_torch: bool = False) -&gt; SUPPORTED_TYPES:\n    \"\"\"\n    Supported Types:\n\n    AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module\n    Deserializes a dict to one of the supported types.\n\n    Arguments:\n        d (dict): A dict containing a serialized object.\n        as_torch (bool): Whether to transform to torch for the deserialized object.\n    Returns:\n        AbstractBlock, QuantumCircuit, QuantumModel, Register, torch.nn.Module.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence import serialize, deserialize, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    block_dict = serialize(myblock)\n    print(block_dict)\n\n    ## Lets use myblock in a QuantumCircuit and serialize it.\n\n    qc = QuantumCircuit(n_qubits, myblock)\n    qc_dict = serialize(qc)\n    qc_deserialized = deserialize(qc_dict)\n    assert qc == qc_deserialized\n\n    ## Finally, let's wrap it in a QuantumModel\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\n    qm_dict = serialize(qm)\n    qm_deserialized = deserialize(qm_dict)\n    # Lets check if the loaded QuantumModel returns the same expectation\n    assert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n    ```\n    \"\"\"\n    obj: SerializationModel\n    if d.get(\"expression\"):\n        obj = ExpressionSerialization(d)\n    elif d.get(\"block\") and d.get(\"register\"):\n        obj = QuantumCircuitSerialization(d)\n    elif d.get(\"graph\"):\n        obj = RegisterSerialization(d)\n    elif d.get(\"type\"):\n        obj = BlockTypeSerialization(d)\n    else:\n        obj = ModelSerialization(d, as_torch=as_torch)\n    return obj.value\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.load","title":"<code>load(file_path, map_location='cpu')</code>","text":"<p>Same as serialize/deserialize but for storing/loading files.</p> <p>Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register Loads a .json or .pt file to one of the supported types.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>The name of the file.</p> <p> TYPE: <code>str</code> </p> <code>map_location</code> <p>In case of a .pt file, on which device to load the object (cpu,cuda).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cpu'</code> </p> <p>Returns:     A object of type AbstractBlock, QuantumCircuit, QuantumModel, Register.</p> <p>Examples: <pre><code>import torch\nfrom pathlib import Path\nimport os\n\nfrom qadence import save, load, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nqc = QuantumCircuit(n_qubits, myblock)\n# Lets store the circuit in a json file\nsave(qc, '.', 'circ')\nloaded_qc = load(Path('circ.json'))\nqc == loaded_qc\nos.remove('circ.json')\n## Let's wrap it in a QuantumModel and store that\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\nsave(qm, folder= '.',file_name= 'quantum_model')\nqm_loaded = load('quantum_model.json')\nos.remove('quantum_model.json')\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def load(file_path: str | Path, map_location: str = \"cpu\") -&gt; SUPPORTED_TYPES:\n    \"\"\"\n    Same as serialize/deserialize but for storing/loading files.\n\n    Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register\n    Loads a .json or .pt file to one of the supported types.\n\n    Arguments:\n        file_path (str): The name of the file.\n        map_location (str): In case of a .pt file, on which device to load the object (cpu,cuda).\n    Returns:\n        A object of type AbstractBlock, QuantumCircuit, QuantumModel, Register.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from pathlib import Path\n    import os\n\n    from qadence import save, load, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    qc = QuantumCircuit(n_qubits, myblock)\n    # Lets store the circuit in a json file\n    save(qc, '.', 'circ')\n    loaded_qc = load(Path('circ.json'))\n    qc == loaded_qc\n    os.remove('circ.json')\n    ## Let's wrap it in a QuantumModel and store that\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n    save(qm, folder= '.',file_name= 'quantum_model')\n    qm_loaded = load('quantum_model.json')\n    os.remove('quantum_model.json')\n    ```\n    \"\"\"\n    d = {}\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if not os.path.exists(file_path):\n        logger.error(f\"File {file_path} not found.\")\n        raise FileNotFoundError\n    FORMAT = file_extension(file_path)\n    _, _, load_fn, _ = FORMAT_DICT[FORMAT]  # type: ignore[index]\n    try:\n        d = load_fn(file_path, map_location)\n        logger.debug(f\"Successfully loaded {d} from {file_path}.\")\n    except Exception as e:\n        logger.error(f\"Unable to load Object from {file_path} due to {e}\")\n    return deserialize(d)\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.parse_expr_fn","title":"<code>parse_expr_fn(code)</code>","text":"<p>A parsing expressions function that checks whether a given code is valid on.</p> <p>the parsing grammar. The grammar is defined to be compatible with <code>sympy</code> expressions, such as <code>Float('-0.33261030434342942', precision=53)</code>, while avoiding code injection such as <code>2*3</code> or <code>__import__('os').system('ls -la')</code>.</p> PARAMETER DESCRIPTION <code>code</code> <p>code to be parsed and checked.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Boolean indicating whether the code matches the defined grammar or not.</p> Source code in <code>qadence/serialization.py</code> <pre><code>def parse_expr_fn(code: str) -&gt; bool:\n    \"\"\"\n    A parsing expressions function that checks whether a given code is valid on.\n\n    the parsing grammar. The grammar is defined to be compatible with `sympy`\n    expressions, such as `Float('-0.33261030434342942', precision=53)`, while\n    avoiding code injection such as `2*3` or `__import__('os').system('ls -la')`.\n\n    Args:\n        code (str): code to be parsed and checked.\n\n    Returns:\n        Boolean indicating whether the code matches the defined grammar or not.\n    \"\"\"\n\n    parser = _parsing_serialize_expr\n    try:\n        parser.parse(code)\n    except NoMatch:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.save","title":"<code>save(obj, folder, file_name='', format=SerializationFormat.JSON)</code>","text":"<p>Same as serialize/deserialize but for storing/loading files.</p> <p>Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module Saves a qadence object to a json/.pt.</p> PARAMETER DESCRIPTION <code>obj</code> <pre><code>Either AbstractBlock, QuantumCircuit, QuantumModel, Register.\n</code></pre> <p> TYPE: <code>AbstractBlock | QuantumCircuit | QuantumModel | Register</code> </p> <code>file_name</code> <p>The name of the file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>format</code> <p>The type of file to save.</p> <p> TYPE: <code>str</code> DEFAULT: <code>JSON</code> </p> <p>Returns:     None.</p> <p>Examples: <pre><code>import torch\nfrom pathlib import Path\nimport os\n\nfrom qadence import save, load, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nqc = QuantumCircuit(n_qubits, myblock)\n# Lets store the circuit in a json file\nsave(qc, '.', 'circ')\nloaded_qc = load(Path('circ.json'))\nqc == loaded_qc\nos.remove('circ.json')\n## Let's wrap it in a QuantumModel and store that\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\nsave(qm, folder= '.',file_name= 'quantum_model')\nqm_loaded = load('quantum_model.json')\nos.remove('quantum_model.json')\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def save(\n    obj: SUPPORTED_TYPES,\n    folder: str | Path,\n    file_name: str = \"\",\n    format: SerializationFormat = SerializationFormat.JSON,\n) -&gt; None:\n    \"\"\"\n    Same as serialize/deserialize but for storing/loading files.\n\n    Supported types:\n    AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module\n    Saves a qadence object to a json/.pt.\n\n    Arguments:\n        obj (AbstractBlock | QuantumCircuit | QuantumModel | Register):\n                Either AbstractBlock, QuantumCircuit, QuantumModel, Register.\n        file_name (str): The name of the file.\n        format (str): The type of file to save.\n    Returns:\n        None.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from pathlib import Path\n    import os\n\n    from qadence import save, load, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    qc = QuantumCircuit(n_qubits, myblock)\n    # Lets store the circuit in a json file\n    save(qc, '.', 'circ')\n    loaded_qc = load(Path('circ.json'))\n    qc == loaded_qc\n    os.remove('circ.json')\n    ## Let's wrap it in a QuantumModel and store that\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n    save(qm, folder= '.',file_name= 'quantum_model')\n    qm_loaded = load('quantum_model.json')\n    os.remove('quantum_model.json')\n    ```\n    \"\"\"\n    if not isinstance(obj, get_args(SUPPORTED_TYPES)):\n        logger.error(f\"Serialization of object type {type(obj)} not supported.\")\n    folder = Path(folder)\n    if not folder.is_dir():\n        logger.error(NotADirectoryError)\n    if file_name == \"\":\n        file_name = type(obj).__name__\n    try:\n        suffix, save_fn, _, save_params = FORMAT_DICT[format]\n        d = serialize(obj, save_params)\n        file_path = folder / Path(file_name + suffix)\n        save_fn(d, file_path)\n        logger.debug(f\"Successfully saved {obj} from to {folder}.\")\n    except Exception as e:\n        logger.error(f\"Unable to write {type(obj)} to disk due to {e}\")\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.serialize","title":"<code>serialize(obj, save_params=False)</code>","text":"<p>Supported Types:</p> <p>AbstractBlock | QuantumCircuit | QuantumModel | torch.nn.Module | Register | Module Serializes a qadence object to a dictionary.</p> PARAMETER DESCRIPTION <code>obj</code> <p> TYPE: <code>AbstractBlock | QuantumCircuit | QuantumModel | Register | Module</code> </p> <p>Returns:     A dict.</p> <p>Examples: <pre><code>import torch\nfrom qadence import serialize, deserialize, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nblock_dict = serialize(myblock)\nprint(block_dict)\n\n## Lets use myblock in a QuantumCircuit and serialize it.\n\nqc = QuantumCircuit(n_qubits, myblock)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n## Finally, let's wrap it in a QuantumModel\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n# Lets check if the loaded QuantumModel returns the same expectation\nassert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n</code></pre> <pre><code>{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': 'HEA', 'blocks': [{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('a9cf441f-e157-4349-9224-af2e1bd83363', {'name': 'theta_0', 'expression': \"Parameter('theta_0')\", 'symbols': {'theta_0': {'name': 'theta_0', 'trainable': 'True', 'value': '0.22527052473067855'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('f8ca6d4c-3bd4-430b-a0d0-f14d698f997a', {'name': 'theta_1', 'expression': \"Parameter('theta_1')\", 'symbols': {'theta_1': {'name': 'theta_1', 'trainable': 'True', 'value': '0.37085104504525057'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RY', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('2df7a12e-c5c7-4924-8ae5-88d162606d63', {'name': 'theta_2', 'expression': \"Parameter('theta_2')\", 'symbols': {'theta_2': {'name': 'theta_2', 'trainable': 'True', 'value': '0.21461084106384765'}}})}}, 'noise': None}, {'type': 'RY', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('bb2db5cf-9461-478b-a4e2-dab1c799414d', {'name': 'theta_3', 'expression': \"Parameter('theta_3')\", 'symbols': {'theta_3': {'name': 'theta_3', 'trainable': 'True', 'value': '0.7699690154746708'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('5981179a-7f9e-48da-936e-e052cec54c09', {'name': 'theta_4', 'expression': \"Parameter('theta_4')\", 'symbols': {'theta_4': {'name': 'theta_4', 'trainable': 'True', 'value': '0.49858658958075297'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('f7f807dd-a48e-441c-b0b6-d05341c4888d', {'name': 'theta_5', 'expression': \"Parameter('theta_5')\", 'symbols': {'theta_5': {'name': 'theta_5', 'trainable': 'True', 'value': '0.4524561524099917'}}})}}, 'noise': None}]}]}, {'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'CNOT', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'X', 'qubit_support': (1,), 'tag': None, 'noise': None}], 'noise': None}]}]}]}\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def serialize(obj: SUPPORTED_TYPES, save_params: bool = False) -&gt; dict:\n    \"\"\"\n    Supported Types:\n\n    AbstractBlock | QuantumCircuit | QuantumModel | torch.nn.Module | Register | Module\n    Serializes a qadence object to a dictionary.\n\n    Arguments:\n        obj (AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module):\n    Returns:\n        A dict.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence import serialize, deserialize, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    block_dict = serialize(myblock)\n    print(block_dict)\n\n    ## Lets use myblock in a QuantumCircuit and serialize it.\n\n    qc = QuantumCircuit(n_qubits, myblock)\n    qc_dict = serialize(qc)\n    qc_deserialized = deserialize(qc_dict)\n    assert qc == qc_deserialized\n\n    ## Finally, let's wrap it in a QuantumModel\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\n    qm_dict = serialize(qm)\n    qm_deserialized = deserialize(qm_dict)\n    # Lets check if the loaded QuantumModel returns the same expectation\n    assert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n    ```\n    \"\"\"\n    if not isinstance(obj, get_args(SUPPORTED_TYPES)):\n        logger.error(TypeError(f\"Serialization of object type {type(obj)} not supported.\"))\n\n    d: dict = dict()\n    try:\n        if isinstance(obj, core.Expr):\n            symb_dict = dict()\n            expr_dict = {\"name\": str(obj), \"expression\": srepr(obj)}\n            symbs: set[Parameter | core.Basic] = obj.free_symbols\n            if symbs:\n                symb_dict = {\"symbols\": {str(s): s._to_dict() for s in symbs}}\n            d = {**expr_dict, **symb_dict}\n        else:\n            if hasattr(obj, \"_to_dict\"):\n                model_to_dict: Callable = obj._to_dict\n                d = (\n                    model_to_dict(save_params)\n                    if isinstance(obj, torch.nn.Module)\n                    else model_to_dict()\n                )\n            elif hasattr(obj, \"state_dict\"):\n                d = {type(obj).__name__: obj.state_dict()}\n            else:\n                raise ValueError(f\"Cannot serialize object {obj}.\")\n    except Exception as e:\n        logger.error(f\"Serialization of object {obj} failed due to {e}\")\n    return d\n</code></pre>"},{"location":"api/states/","title":"State preparation","text":""},{"location":"api/states/#state-preparation-routines","title":"State Preparation Routines","text":""},{"location":"api/states/#qadence.states.density_mat","title":"<code>density_mat(state)</code>","text":"<p>Computes the density matrix from a pure state vector.</p> PARAMETER DESCRIPTION <code>state</code> <p>The pure state vector :math:<code>|\\psi\\rangle</code>.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The density matrix :math:<code>\\rho = |\\psi \\rangle \\langle\\psi|</code>.</p> <p> TYPE: <code>DensityMatrix</code> </p> Source code in <code>qadence/states.py</code> <pre><code>def density_mat(state: Tensor) -&gt; DensityMatrix:\n    \"\"\"\n    Computes the density matrix from a pure state vector.\n\n    Arguments:\n        state: The pure state vector :math:`|\\\\psi\\\\rangle`.\n\n    Returns:\n        Tensor: The density matrix :math:`\\\\rho = |\\psi \\\\rangle \\\\langle\\\\psi|`.\n    \"\"\"\n    if isinstance(state, DensityMatrix):\n        return state\n    return DensityMatrix(torch.einsum(\"bi,bj-&gt;bij\", (state, state.conj())))\n</code></pre>"},{"location":"api/states/#qadence.states.ghz_block","title":"<code>ghz_block(n_qubits)</code>","text":"<p>Generates the abstract ghz state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>A ChainBlock representing the GHZ state.</p> <p>Examples: <pre><code>from qadence.states import ghz_block\n\nblock = ghz_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 CNOT(0, 1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def ghz_block(n_qubits: int) -&gt; ChainBlock:\n    \"\"\"\n    Generates the abstract ghz state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A ChainBlock representing the GHZ state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import ghz_block\n\n    block = ghz_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    cnots = chain(CNOT(i - 1, i) for i in range(1, n_qubits))\n    return chain(H(0), cnots)\n</code></pre>"},{"location":"api/states/#qadence.states.ghz_state","title":"<code>ghz_state(n_qubits, batch_size=1)</code>","text":"<p>Creates a GHZ state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>How many bitstrings to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import ghz_state\n\nprint(ghz_state(n_qubits=2, batch_size=2))\n</code></pre> <pre><code>tensor([[0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j],\n        [0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def ghz_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Creates a GHZ state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): How many bitstrings to use.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import ghz_state\n\n    print(ghz_state(n_qubits=2, batch_size=2))\n    ```\n    \"\"\"\n    norm = 1 / torch.sqrt(torch.tensor(2))\n    return norm * (zero_state(n_qubits, batch_size) + one_state(n_qubits, batch_size))\n</code></pre>"},{"location":"api/states/#qadence.states.is_normalized","title":"<code>is_normalized(wf, atol=NORMALIZATION_ATOL)</code>","text":"<p>Checks if a wave function is normalized.</p> PARAMETER DESCRIPTION <code>wf</code> <p>The wave function as a torch tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>atol</code> <p>The tolerance.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>NORMALIZATION_ATOL</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, is_normalized\n\nprint(is_normalized(uniform_state(2)))\n</code></pre> <pre><code>True\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def is_normalized(wf: Tensor, atol: float = NORMALIZATION_ATOL) -&gt; bool:\n    \"\"\"\n    Checks if a wave function is normalized.\n\n    Arguments:\n        wf (torch.Tensor): The wave function as a torch tensor.\n        atol (float) : The tolerance.\n\n    Returns:\n        A bool.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, is_normalized\n\n    print(is_normalized(uniform_state(2)))\n    ```\n    \"\"\"\n    if wf.dim() == 1:\n        wf = wf.unsqueeze(0)\n    sum_probs: Tensor = (wf.abs() ** 2).sum(dim=1)\n    ones = torch.ones_like(sum_probs)\n    return torch.allclose(sum_probs, ones, rtol=0.0, atol=atol)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/states/#qadence.states.normalize","title":"<code>normalize(wf)</code>","text":"<p>Normalizes a wavefunction or batch of wave functions.</p> PARAMETER DESCRIPTION <code>wf</code> <p>Normalized wavefunctions.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, normalize\n\nprint(normalize(uniform_state(2, 2)))\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j],\n        [0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def normalize(wf: Tensor) -&gt; Tensor:\n    \"\"\"\n    Normalizes a wavefunction or batch of wave functions.\n\n    Arguments:\n        wf (torch.Tensor): Normalized wavefunctions.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, normalize\n\n    print(normalize(uniform_state(2, 2)))\n    ```\n    \"\"\"\n    if wf.dim() == 1:\n        return wf / torch.sqrt((wf.abs() ** 2).sum())\n    else:\n        return wf / torch.sqrt((wf.abs() ** 2).sum(1)).unsqueeze(1)\n</code></pre>"},{"location":"api/states/#qadence.states.one_block","title":"<code>one_block(n_qubits)</code>","text":"<p>Generates the abstract one state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the one state.</p> <p>Examples: <pre><code>from qadence.states import one_block\n\nblock = one_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def one_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract one state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the one state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import one_block\n\n    block = one_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(X, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.one_state","title":"<code>one_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the one state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import one_state\n\nstate = one_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def one_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the one state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): The batch size.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import one_state\n\n    state = one_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    bitstring = \"1\" * n_qubits\n    return _state_from_bitstring(bitstring, batch_size)\n</code></pre>"},{"location":"api/states/#qadence.states.pmf","title":"<code>pmf(wf)</code>","text":"<p>Converts a wave function into a torch Distribution.</p> PARAMETER DESCRIPTION <code>wf</code> <p>The wave function as a torch tensor.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Distribution</code> <p>A torch.distributions.Distribution.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, pmf\n\nprint(pmf(uniform_state(2)).probs)\n</code></pre> <pre><code>tensor([[0.2500, 0.2500, 0.2500, 0.2500]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def pmf(wf: Tensor) -&gt; Distribution:\n    \"\"\"\n    Converts a wave function into a torch Distribution.\n\n    Arguments:\n        wf (torch.Tensor): The wave function as a torch tensor.\n\n    Returns:\n        A torch.distributions.Distribution.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, pmf\n\n    print(pmf(uniform_state(2)).probs)\n    ```\n    \"\"\"\n    return Categorical(torch.abs(torch.pow(wf, 2)))\n</code></pre>"},{"location":"api/states/#qadence.states.product_block","title":"<code>product_block(bitstring)</code>","text":"<p>Creates an abstract product state from a bitstring.</p> PARAMETER DESCRIPTION <code>bitstring</code> <p>A bitstring.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the product state.</p> <p>Examples: <pre><code>from qadence.states import product_block\n\nprint(product_block(\"1100\"))\n</code></pre> <pre><code>KronBlock(0,1,2,3)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u251c\u2500\u2500 I(2)\n\u2514\u2500\u2500 I(3)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def product_block(bitstring: str) -&gt; KronBlock:\n    \"\"\"\n    Creates an abstract product state from a bitstring.\n\n    Arguments:\n        bitstring (str): A bitstring.\n\n    Returns:\n        A KronBlock representing the product state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import product_block\n\n    print(product_block(\"1100\"))\n    ```\n    \"\"\"\n    return _block_from_bitstring(bitstring)\n</code></pre>"},{"location":"api/states/#qadence.states.product_state","title":"<code>product_state(bitstring, batch_size=1, endianness=Endianness.BIG, backend=BackendName.PYQTORCH)</code>","text":"<p>Creates a product state from a bitstring.</p> PARAMETER DESCRIPTION <code>bitstring</code> <p>A bitstring.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1</code> </p> <code>backend</code> <p>The backend to use. Default is \"pyqtorch\".</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import product_state\n\nprint(product_state(\"1100\", backend=\"pyqtorch\"))\nprint(product_state(\"1100\", backend=\"horqrux\"))\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n         1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j\n 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>@singledispatch\ndef product_state(\n    bitstring: str,\n    batch_size: int = 1,\n    endianness: Endianness = Endianness.BIG,\n    backend: BackendName = BackendName.PYQTORCH,\n) -&gt; ArrayLike:\n    \"\"\"\n    Creates a product state from a bitstring.\n\n    Arguments:\n        bitstring (str): A bitstring.\n        batch_size (int) : Batch size.\n        backend (BackendName): The backend to use. Default is \"pyqtorch\".\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import product_state\n\n    print(product_state(\"1100\", backend=\"pyqtorch\"))\n    print(product_state(\"1100\", backend=\"horqrux\"))\n    ```\n    \"\"\"\n    if batch_size:\n        logger.debug(\n            \"The input `batch_size` is going to be deprecated. \"\n            \"For now, default batch_size is set to 1.\"\n        )\n    return run(product_block(bitstring), backend=backend, endianness=endianness)\n</code></pre>"},{"location":"api/states/#qadence.states.rand_bitstring","title":"<code>rand_bitstring(N)</code>","text":"<p>Creates a random bistring.</p> PARAMETER DESCRIPTION <code>N</code> <p>The length of the bitstring.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A string.</p> <p>Examples: <pre><code>from qadence.states import rand_bitstring\n\nprint(rand_bitstring(N=8))\n</code></pre> <pre><code>11000100\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_bitstring(N: int) -&gt; str:\n    \"\"\"\n    Creates a random bistring.\n\n    Arguments:\n        N (int): The length of the bitstring.\n\n    Returns:\n        A string.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_bitstring\n\n    print(rand_bitstring(N=8))\n    ```\n    \"\"\"\n    return \"\".join(str(random.randint(0, 1)) for _ in range(N))\n</code></pre>"},{"location":"api/states/#qadence.states.rand_product_block","title":"<code>rand_product_block(n_qubits)</code>","text":"<p>Creates a block representing a random abstract product state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the product state.</p> <p>Examples: <pre><code>from qadence.states import rand_product_block\n\nprint(rand_product_block(n_qubits=2))\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 I(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_product_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Creates a block representing a random abstract product state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the product state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_product_block\n\n    print(rand_product_block(n_qubits=2))\n    ```\n    \"\"\"\n    return product_block(rand_bitstring(n_qubits))\n</code></pre>"},{"location":"api/states/#qadence.states.rand_product_state","title":"<code>rand_product_state(n_qubits, batch_size=1)</code>","text":"<p>Creates a random product state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>How many bitstrings to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import rand_product_state\n\nprint(rand_product_state(n_qubits=2, batch_size=2))\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_product_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Creates a random product state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): How many bitstrings to use.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_product_state\n\n    print(rand_product_state(n_qubits=2, batch_size=2))\n    ```\n    \"\"\"\n    wf_batch = torch.zeros(batch_size, 2**n_qubits, dtype=DTYPE)\n    rand_pos = torch.randint(0, 2**n_qubits, (batch_size,))\n    wf_batch[torch.arange(batch_size), rand_pos] = torch.tensor(1.0 + 0j, dtype=DTYPE)\n    return wf_batch\n</code></pre>"},{"location":"api/states/#qadence.states.random_state","title":"<code>random_state(n_qubits, batch_size=1, backend=BackendName.PYQTORCH, type=StateGeneratorType.HAAR_MEASURE_FAST)</code>","text":"<p>Generates a random state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>backend</code> <p>The backend to use.</p> <p> TYPE: <code>str</code> DEFAULT: <code>PYQTORCH</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>type</code> <p>StateGeneratorType.</p> <p> DEFAULT: <code>HAAR_MEASURE_FAST</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import random_state, StateGeneratorType\nfrom qadence.states import random_state, is_normalized, pmf\nfrom qadence.types import BackendName\nfrom torch.distributions import Distribution\n\n### We have the following options:\nprint([g.value for g in StateGeneratorType])\n\nn_qubits = 2\n# The default is StateGeneratorType.HAAR_MEASURE_FAST\nstate = random_state(n_qubits=n_qubits)\nprint(state)\n\n### Lets initialize a state using random rotations, i.e., StateGeneratorType.RANDOM_ROTATIONS.\nrandom = random_state(n_qubits=n_qubits, type=StateGeneratorType.RANDOM_ROTATIONS)\nprint(random)\n</code></pre> <pre><code>['RandomRotations', 'HaarMeasureFast', 'HaarMeasureSlow']\ntensor([[ 0.3292+0.3049j, -0.1989+0.5249j, -0.4751+0.0861j, -0.1067+0.4890j]])\ntensor([[ 0.3815+0.0000j,  0.0000-0.5181j, -0.4539+0.0000j,  0.0000+0.6164j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def random_state(\n    n_qubits: int,\n    batch_size: int = 1,\n    backend: str = BackendName.PYQTORCH,\n    type: StateGeneratorType = StateGeneratorType.HAAR_MEASURE_FAST,\n) -&gt; Tensor:\n    \"\"\"\n    Generates a random state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        backend (str): The backend to use.\n        batch_size (int): The batch size.\n        type : StateGeneratorType.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import random_state, StateGeneratorType\n    from qadence.states import random_state, is_normalized, pmf\n    from qadence.types import BackendName\n    from torch.distributions import Distribution\n\n    ### We have the following options:\n    print([g.value for g in StateGeneratorType])\n\n    n_qubits = 2\n    # The default is StateGeneratorType.HAAR_MEASURE_FAST\n    state = random_state(n_qubits=n_qubits)\n    print(state)\n\n    ### Lets initialize a state using random rotations, i.e., StateGeneratorType.RANDOM_ROTATIONS.\n    random = random_state(n_qubits=n_qubits, type=StateGeneratorType.RANDOM_ROTATIONS)\n    print(random)\n    ```\n    \"\"\"\n\n    if type == StateGeneratorType.HAAR_MEASURE_FAST:\n        state = concat(tuple(_rand_haar_fast(n_qubits) for _ in range(batch_size)), dim=0)\n    elif type == StateGeneratorType.HAAR_MEASURE_SLOW:\n        state = concat(tuple(_rand_haar_slow(n_qubits) for _ in range(batch_size)), dim=0)\n    elif type == StateGeneratorType.RANDOM_ROTATIONS:\n        state = run(_abstract_random_state(n_qubits, batch_size))  # type: ignore\n    assert all(list(map(is_normalized, state)))\n    return state\n</code></pre>"},{"location":"api/states/#qadence.states.uniform_block","title":"<code>uniform_block(n_qubits)</code>","text":"<p>Generates the abstract uniform state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the uniform state.</p> <p>Examples: <pre><code>from qadence.states import uniform_block\n\nblock = uniform_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 H(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def uniform_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract uniform state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the uniform state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_block\n\n    block = uniform_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(H, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.uniform_state","title":"<code>uniform_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the uniform state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import uniform_state\n\nstate = uniform_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def uniform_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the uniform state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): The batch size.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state\n\n    state = uniform_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    norm = 1 / torch.sqrt(torch.tensor(2**n_qubits))\n    return norm * torch.ones(batch_size, 2**n_qubits, dtype=DTYPE)\n</code></pre>"},{"location":"api/states/#qadence.states.zero_block","title":"<code>zero_block(n_qubits)</code>","text":"<p>Generates the abstract zero state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the zero state.</p> <p>Examples: <pre><code>from qadence.states import zero_block\n\nblock = zero_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 I(0)\n\u2514\u2500\u2500 I(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def zero_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract zero state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the zero state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import zero_block\n\n    block = zero_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(I, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.zero_state","title":"<code>zero_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the zero state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits for which the zero state is to be generated.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size for the zero state.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import zero_state\n\nstate = zero_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def zero_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the zero state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits for which the zero state is to be generated.\n        batch_size (int): The batch size for the zero state.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import zero_state\n\n    state = zero_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    bitstring = \"0\" * n_qubits\n    return _state_from_bitstring(bitstring, batch_size)\n</code></pre>"},{"location":"api/transpile/","title":"Transpilation","text":"<p>Contains functions that operate on blocks and circuits to <code>transpile</code> them to new blocks/circuits.</p>"},{"location":"api/transpile/#qadence.transpile.transpile.transpile","title":"<code>transpile(*fs)</code>","text":"<pre><code>transpile(*fs: Callable[[AbstractBlock], AbstractBlock]) -&gt; Callable[[AbstractBlock], AbstractBlock]\n</code></pre><pre><code>transpile(*fs: Callable[[QuantumCircuit], QuantumCircuit]) -&gt; Callable[[QuantumCircuit], QuantumCircuit]\n</code></pre> <p><code>AbstractBlock</code> or <code>QuantumCircuit</code> transpilation.</p> <p>Compose functions that accept a circuit/block and returns a circuit/block.</p> PARAMETER DESCRIPTION <code>*fs</code> <p>composable functions that either map blocks to blocks (<code>Callable[[AbstractBlock], AbstractBlock]</code>) or circuits to circuits (<code>Callable[[QuantumCircuit], QuantumCircuit]</code>).</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>Composed function.</p> <p>Examples:</p> <p>Flatten a block of nested chains and krons: <pre><code>from qadence import *\nfrom qadence.transpile import transpile, flatten, scale_primitive_blocks_only\n\nb = chain(2 * chain(chain(X(0), Y(0))), kron(kron(X(0), X(1))))\nprint(b)\n\n# both flatten and scale_primitive_blocks_only are functions that accept and\n# return a block\nt = transpile(flatten, scale_primitive_blocks_only)(b)\nprint(t)\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 [mul: 2] \n\u2502   \u2514\u2500\u2500 ChainBlock(0)\n\u2502       \u2514\u2500\u2500 ChainBlock(0)\n\u2502           \u251c\u2500\u2500 X(0)\n\u2502           \u2514\u2500\u2500 Y(0)\n\u2514\u2500\u2500 KronBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u251c\u2500\u2500 X(0)\n        \u2514\u2500\u2500 X(1)\n\nChainBlock(0,1)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u251c\u2500\u2500 Y(0)\n\u2514\u2500\u2500 KronBlock(0,1)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 X(1)\n</code></pre> </p> <p>We also proved a decorator to easily turn a function <code>Callable[[AbstractBlock], AbstractBlock]</code> into a <code>Callable[[QuantumCircuit], QuantumCircuit]</code> to be used in circuit transpilation. <pre><code>from qadence import *\nfrom qadence.transpile import transpile, blockfn_to_circfn, flatten\n\n# We want to pass this circuit to `transpile` instead of a block,\n# so we need functions that map from a circuit to a circuit.\ncirc = QuantumCircuit(2, chain(chain(X(0), chain(X(1)))))\n\n@blockfn_to_circfn\ndef fn(block):\n    # un-decorated function accepts a block and returns a block\n    return block * block\n\ntransp = transpile(\n    # the decorated function accepts a circuit and returns a circuit\n    fn,\n    # already existing functions can also be decorated\n    blockfn_to_circfn(flatten)\n)\nprint(transp(circ))\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 X(0)\n\u2502   \u2514\u2500\u2500 X(1)\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/transpile/transpile.py</code> <pre><code>def transpile(*fs: Callable) -&gt; Callable:\n    \"\"\"`AbstractBlock` or `QuantumCircuit` transpilation.\n\n    Compose functions that\n    accept a circuit/block and returns a circuit/block.\n\n    Arguments:\n        *fs: composable functions that either map blocks to blocks\n            (`Callable[[AbstractBlock], AbstractBlock]`)\n            or circuits to circuits (`Callable[[QuantumCircuit], QuantumCircuit]`).\n\n    Returns:\n        Composed function.\n\n    Examples:\n\n    Flatten a block of nested chains and krons:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import *\n    from qadence.transpile import transpile, flatten, scale_primitive_blocks_only\n\n    b = chain(2 * chain(chain(X(0), Y(0))), kron(kron(X(0), X(1))))\n    print(b)\n    print() # markdown-exec: hide\n\n    # both flatten and scale_primitive_blocks_only are functions that accept and\n    # return a block\n    t = transpile(flatten, scale_primitive_blocks_only)(b)\n    print(t)\n    ```\n\n    We also proved a decorator to easily turn a function `Callable[[AbstractBlock], AbstractBlock]`\n    into a `Callable[[QuantumCircuit], QuantumCircuit]` to be used in circuit transpilation.\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import *\n    from qadence.transpile import transpile, blockfn_to_circfn, flatten\n\n    # We want to pass this circuit to `transpile` instead of a block,\n    # so we need functions that map from a circuit to a circuit.\n    circ = QuantumCircuit(2, chain(chain(X(0), chain(X(1)))))\n\n    @blockfn_to_circfn\n    def fn(block):\n        # un-decorated function accepts a block and returns a block\n        return block * block\n\n    transp = transpile(\n        # the decorated function accepts a circuit and returns a circuit\n        fn,\n        # already existing functions can also be decorated\n        blockfn_to_circfn(flatten)\n    )\n    print(transp(circ))\n    ```\n    \"\"\"\n    return lambda x: reduce(lambda acc, f: f(acc), reversed(fs), x)\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.chain_single_qubit_ops","title":"<code>chain_single_qubit_ops(block)</code>","text":"<p>Transpile a chain of krons into a kron of chains of single qubit operations.</p> <p>Examples: <pre><code>from qadence import hea\nfrom qadence.transpile.block import chain_single_qubit_ops\n\n# Consider a single HEA layer\nblock = hea(2,1)\nprint(block)\n\n# After applying chain_single_qubit_ops, we get:\nprint(chain_single_qubit_ops(block))\n</code></pre> <pre><code>ChainBlock(0,1) [tag: HEA]\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n\u2502   \u2502   \u2514\u2500\u2500 RX(1) [params: ['theta_1']]\n\u2502   \u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_2']]\n\u2502   \u2502   \u2514\u2500\u2500 RY(1) [params: ['theta_3']]\n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 RX(0) [params: ['theta_4']]\n\u2502       \u2514\u2500\u2500 RX(1) [params: ['theta_5']]\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u2514\u2500\u2500 CNOT(0, 1)\nChainBlock(0,1)\n\u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u251c\u2500\u2500 ChainBlock(0)\n\u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n\u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_2']]\n\u2502   \u2502   \u2514\u2500\u2500 RX(0) [params: ['theta_4']]\n\u2502   \u2514\u2500\u2500 ChainBlock(1)\n\u2502       \u251c\u2500\u2500 RX(1) [params: ['theta_1']]\n\u2502       \u251c\u2500\u2500 RY(1) [params: ['theta_3']]\n\u2502       \u2514\u2500\u2500 RX(1) [params: ['theta_5']]\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u2514\u2500\u2500 CNOT(0, 1)\n</code></pre></p> Source code in <code>qadence/transpile/block.py</code> <pre><code>def chain_single_qubit_ops(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Transpile a chain of krons into a kron of chains of single qubit operations.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import hea\n    from qadence.transpile.block import chain_single_qubit_ops\n\n    # Consider a single HEA layer\n    block = hea(2,1)\n    print(block)\n\n    # After applying chain_single_qubit_ops, we get:\n    print(chain_single_qubit_ops(block))\n    ```\n    \"\"\"\n    if is_chain_of_primitivekrons(block):\n        try:\n            return kron(*map(lambda bs: chain(*bs), zip(*block)))  # type: ignore[misc]\n        except Exception as e:\n            logger.debug(\n                f\"Unable to transpile {block} using chain_single_qubit_ops\\\n                         due to {e}. Returning original circuit.\"\n            )\n            return block\n\n    elif isinstance(block, CompositeBlock):\n        return _construct(type(block), tuple(chain_single_qubit_ops(b) for b in block.blocks))\n    else:\n        return block\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.scale_primitive_blocks_only","title":"<code>scale_primitive_blocks_only(block, scale=None)</code>","text":"<p>Push the scale all the way down into the leaves of the block tree.</p> <p>When given a scaled CompositeBlock consisting of several PrimitiveBlocks.</p> PARAMETER DESCRIPTION <code>block</code> <p>The block to be transpiled.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>scale</code> <p>An optional scale parameter. Only to be used for recursive calls internally.</p> <p> TYPE: <code>Basic</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>A block of the same type where the scales have been moved into the subblocks.</p> <p> TYPE: <code>AbstractBlock</code> </p> <p>Examples:</p> <p>There are two different cases: <code>ChainBlock</code>s/<code>KronBlock</code>s: Only the first subblock needs to be scaled because chains/krons represent multiplications. <pre><code>from qadence import chain, X, RX\nfrom qadence.transpile import scale_primitive_blocks_only\nb = 2 * chain(X(0), RX(0, \"theta\"))\nprint(b)\n# After applying scale_primitive_blocks_only\nprint(scale_primitive_blocks_only(b))\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 ChainBlock(0)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\nChainBlock(0)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 RX(0) [params: ['theta']]\n</code></pre></p> <p><code>AddBlock</code>s: Consider 2 * add(X(0), RX(0, \"theta\")).  The scale needs to be added to all subblocks.  We get add(2 * X(0), 2 * RX(0, \"theta\")). <pre><code>from qadence import add, X, RX\nfrom qadence.transpile import scale_primitive_blocks_only\nb = 2 * add(X(0), RX(0, \"theta\"))\nprint(b)\n# After applying scale_primitive_blocks_only\nprint(scale_primitive_blocks_only(b))\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 AddBlock(0)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\nAddBlock(0)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 [mul: 2.000] \n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\n</code></pre></p> Source code in <code>qadence/transpile/block.py</code> <pre><code>@singledispatch\ndef scale_primitive_blocks_only(block: AbstractBlock, scale: sympy.Basic = None) -&gt; AbstractBlock:\n    \"\"\"Push the scale all the way down into the leaves of the block tree.\n\n    When given a scaled CompositeBlock consisting of several PrimitiveBlocks.\n\n    Arguments:\n        block: The block to be transpiled.\n        scale: An optional scale parameter. Only to be used for recursive calls internally.\n\n    Returns:\n        AbstractBlock: A block of the same type where the scales have been moved into the subblocks.\n\n    Examples:\n\n    There are two different cases:\n    `ChainBlock`s/`KronBlock`s: Only the first subblock needs to be scaled because chains/krons\n    represent multiplications.\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import chain, X, RX\n    from qadence.transpile import scale_primitive_blocks_only\n    b = 2 * chain(X(0), RX(0, \"theta\"))\n    print(b)\n    # After applying scale_primitive_blocks_only\n    print(scale_primitive_blocks_only(b))\n    ```\n\n    `AddBlock`s: Consider 2 * add(X(0), RX(0, \"theta\")).  The scale needs to be added to all\n    subblocks.  We get add(2 * X(0), 2 * RX(0, \"theta\")).\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import add, X, RX\n    from qadence.transpile import scale_primitive_blocks_only\n    b = 2 * add(X(0), RX(0, \"theta\"))\n    print(b)\n    # After applying scale_primitive_blocks_only\n    print(scale_primitive_blocks_only(b))\n    ```\n    \"\"\"\n    raise NotImplementedError(f\"scale_primitive_blocks_only is not implemented for {type(block)}\")\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.set_trainable","title":"<code>set_trainable(blocks, value=True, inplace=True)</code>","text":"<p>Set the trainability of all parameters in a block to a given value.</p> PARAMETER DESCRIPTION <code>blocks</code> <p>Block or list of blocks for which to set the trainable attribute</p> <p> TYPE: <code>AbstractBlock | list[AbstractBlock]</code> </p> <code>value</code> <p>The value of the trainable attribute to assign to the input blocks</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <p>Whether to modify the block(s) in place or not. Currently, only</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>if the <code>inplace</code> argument is set to False, the function will raise  this exception</p> RETURNS DESCRIPTION <code>AbstractBlock | list[AbstractBlock]</code> <p>AbstractBlock | list[AbstractBlock]: the input block or list of blocks with the trainable attribute set to the given value</p> Source code in <code>qadence/transpile/block.py</code> <pre><code>def set_trainable(\n    blocks: AbstractBlock | list[AbstractBlock], value: bool = True, inplace: bool = True\n) -&gt; AbstractBlock | list[AbstractBlock]:\n    \"\"\"Set the trainability of all parameters in a block to a given value.\n\n    Args:\n        blocks (AbstractBlock | list[AbstractBlock]): Block or list of blocks for which\n            to set the trainable attribute\n        value (bool, optional): The value of the trainable attribute to assign to the input blocks\n        inplace (bool, optional): Whether to modify the block(s) in place or not. Currently, only\n\n    Raises:\n        NotImplementedError: if the `inplace` argument is set to False, the function will\n            raise  this exception\n\n    Returns:\n        AbstractBlock | list[AbstractBlock]: the input block or list of blocks with the trainable\n            attribute set to the given value\n    \"\"\"\n\n    if isinstance(blocks, AbstractBlock):\n        blocks = [blocks]\n\n    if inplace:\n        for block in blocks:\n            params: list[sympy.Basic] = parameters(block)\n            for p in params:\n                if not p.is_number:\n                    p.trainable = value\n    else:\n        raise NotImplementedError(\"Not inplace set_trainable is not yet available\")\n\n    return blocks if len(blocks) &gt; 1 else blocks[0]\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.validate","title":"<code>validate(block)</code>","text":"<p>Moves a block from global to local qubit numbers by adding PutBlocks.</p> <p>Reassigns qubit locations appropriately.</p>"},{"location":"api/transpile/#qadence.transpile.block.validate--example","title":"Example","text":"<pre><code>from qadence.blocks import chain\nfrom qadence.operations import X\nfrom qadence.transpile import validate\n\nx = chain(chain(X(0)), chain(X(1)))\nprint(x)\nprint(validate(x))\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 ChainBlock(0)\n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 ChainBlock(1)\n    \u2514\u2500\u2500 X(1)\nChainBlock(0,1)\n\u251c\u2500\u2500 put on (0)\n\u2502   \u2514\u2500\u2500 ChainBlock(0)\n\u2502       \u2514\u2500\u2500 put on (0)\n\u2502           \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 put on (1)\n    \u2514\u2500\u2500 ChainBlock(0)\n        \u2514\u2500\u2500 put on (0)\n            \u2514\u2500\u2500 X(0)\n</code></pre> Source code in <code>qadence/transpile/block.py</code> <pre><code>def validate(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Moves a block from global to local qubit numbers by adding PutBlocks.\n\n    Reassigns qubit locations appropriately.\n\n    # Example\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence.blocks import chain\n    from qadence.operations import X\n    from qadence.transpile import validate\n\n    x = chain(chain(X(0)), chain(X(1)))\n    print(x)\n    print(validate(x))\n    ```\n    \"\"\"\n    vblock: AbstractBlock\n    from qadence.transpile import reassign\n\n    if isinstance(block, ControlBlock):\n        vblock = deepcopy(block)\n        b: AbstractBlock\n        (b,) = block.blocks\n        b = reassign(b, {i: i - min(b.qubit_support) for i in b.qubit_support})\n        b = validate(b)\n        vblock.blocks = (b,)  # type: ignore[assignment]\n\n    elif isinstance(block, CompositeBlock):\n        blocks = []\n        for b in block.blocks:\n            mi, ma = min(b.qubit_support), max(b.qubit_support)\n            nb = reassign(b, {i: i - min(b.qubit_support) for i in b.qubit_support})\n            nb = validate(nb)\n            nb = PutBlock(nb, tuple(range(mi, ma + 1)))\n            blocks.append(nb)\n        try:\n            vblock = _construct(type(block), tuple(blocks))\n        except AssertionError as e:\n            if str(e) == \"Make sure blocks act on distinct qubits!\":\n                vblock = chain(*blocks)\n            else:\n                raise e\n\n    elif isinstance(block, PrimitiveBlock):\n        vblock = deepcopy(block)\n\n    else:\n        raise NotImplementedError\n\n    vblock.tag = block.tag\n    return vblock\n</code></pre>"},{"location":"api/types/","title":"Types","text":""},{"location":"api/types/#qadence-types","title":"Qadence Types","text":""},{"location":"api/types/#qadence.types.TArray","title":"<code>TArray = Union[Iterable, Tensor, np.ndarray]</code>  <code>module-attribute</code>","text":"<p>Union of common array types.</p>"},{"location":"api/types/#qadence.types.TGenerator","title":"<code>TGenerator = Union[Tensor, sympy.Array, sympy.Basic]</code>  <code>module-attribute</code>","text":"<p>Union of torch tensors and numpy arrays.</p>"},{"location":"api/types/#qadence.types.TNumber","title":"<code>TNumber = Union[int, float, complex, np.int64, np.float64]</code>  <code>module-attribute</code>","text":"<p>Union of python and numpy numeric types.</p>"},{"location":"api/types/#qadence.types.TParameter","title":"<code>TParameter = Union[TNumber, Tensor, sympy.Basic, str]</code>  <code>module-attribute</code>","text":"<p>Union of numbers, tensors, and parameter types.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo","title":"<code>AlgoHEvo</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Hamiltonian Evolution algorithms that can be used by the backend.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.EIG","title":"<code>EIG = 'EIG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using Hamiltonian diagonalization.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.EXP","title":"<code>EXP = 'EXP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using torch.matrix_exp on the generator matrix.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.RK4","title":"<code>RK4 = 'RK4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>4th order Runge-Kutta approximation.</p>"},{"location":"api/types/#qadence.types.AnalogNoise","title":"<code>AnalogNoise</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of noise protocol.</p>"},{"location":"api/types/#qadence.types.AnsatzType","title":"<code>AnsatzType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Ansatz types for variational circuits.</p>"},{"location":"api/types/#qadence.types.AnsatzType.HEA","title":"<code>HEA = 'hea'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hardware-efficient ansatz.</p>"},{"location":"api/types/#qadence.types.AnsatzType.IIA","title":"<code>IIA = 'iia'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Identity-Initialised Ansatz.</p>"},{"location":"api/types/#qadence.types.BasisSet","title":"<code>BasisSet</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Basis set for feature maps.</p>"},{"location":"api/types/#qadence.types.BasisSet.CHEBYSHEV","title":"<code>CHEBYSHEV = 'Chebyshev'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Chebyshev polynomials of the first kind.</p>"},{"location":"api/types/#qadence.types.BasisSet.FOURIER","title":"<code>FOURIER = 'Fourier'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fourier basis set.</p>"},{"location":"api/types/#qadence.types.DeviceType","title":"<code>DeviceType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported types of devices for Pulser backend.</p>"},{"location":"api/types/#qadence.types.DeviceType.IDEALIZED","title":"<code>IDEALIZED = 'IdealDevice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Idealized device, least realistic.</p>"},{"location":"api/types/#qadence.types.DeviceType.REALISTIC","title":"<code>REALISTIC = 'RealisticDevice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Device with realistic specs.</p>"},{"location":"api/types/#qadence.types.Endianness","title":"<code>Endianness</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The endianness convention to use.</p>"},{"location":"api/types/#qadence.types.Endianness.BIG","title":"<code>BIG = 'Big'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use Big endianness.</p>"},{"location":"api/types/#qadence.types.Endianness.LITTLE","title":"<code>LITTLE = 'Little'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use little endianness.</p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool","title":"<code>ExperimentTrackingTool</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool.MLFLOW","title":"<code>MLFLOW = 'mlflow'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the ml-flow experiment tracker.</p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool.TENSORBOARD","title":"<code>TENSORBOARD = 'tensorboard'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the tensorboard experiment tracker.</p>"},{"location":"api/types/#qadence.types.FigFormat","title":"<code>FigFormat</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available output formats for exporting visualized circuits to a file.</p>"},{"location":"api/types/#qadence.types.FigFormat.PDF","title":"<code>PDF = 'PDF'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PDF format.</p>"},{"location":"api/types/#qadence.types.FigFormat.PNG","title":"<code>PNG = 'PNG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PNG format.</p>"},{"location":"api/types/#qadence.types.FigFormat.SVG","title":"<code>SVG = 'SVG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>SVG format.</p>"},{"location":"api/types/#qadence.types.GenDAQC","title":"<code>GenDAQC</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The type of interaction for the DAQC transform.</p>"},{"location":"api/types/#qadence.types.GenDAQC.NN","title":"<code>NN = 'NN'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NN</p>"},{"location":"api/types/#qadence.types.GenDAQC.ZZ","title":"<code>ZZ = 'ZZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ZZ</p>"},{"location":"api/types/#qadence.types.InputDiffMode","title":"<code>InputDiffMode</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Derivative modes w.r.t inputs of UFAs.</p>"},{"location":"api/types/#qadence.types.InputDiffMode.AD","title":"<code>AD = 'ad'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Reverse automatic differentiation.</p>"},{"location":"api/types/#qadence.types.InputDiffMode.FD","title":"<code>FD = 'fd'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Central finite differencing.</p>"},{"location":"api/types/#qadence.types.Interaction","title":"<code>Interaction</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Interaction types used in.</p> <ul> <li><code>RydbergDevice</code>.</li> <li><code>hamiltonian_factory</code>.</li> </ul>"},{"location":"api/types/#qadence.types.Interaction.NN","title":"<code>NN = 'NN'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NN-Ising Interaction, N=(I-Z)/2.</p>"},{"location":"api/types/#qadence.types.Interaction.XY","title":"<code>XY = 'XY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>XY Interaction.</p>"},{"location":"api/types/#qadence.types.Interaction.XYZ","title":"<code>XYZ = 'XYZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>XYZ Interaction.</p>"},{"location":"api/types/#qadence.types.Interaction.ZZ","title":"<code>ZZ = 'ZZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ZZ-Ising Interaction.</p>"},{"location":"api/types/#qadence.types.LTSOrder","title":"<code>LTSOrder</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Lie-Trotter-Suzuki approximation order.</p>"},{"location":"api/types/#qadence.types.LTSOrder.BASIC","title":"<code>BASIC = 'BASIC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basic.</p>"},{"location":"api/types/#qadence.types.LTSOrder.ST2","title":"<code>ST2 = 'ST2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ST2.</p>"},{"location":"api/types/#qadence.types.LTSOrder.ST4","title":"<code>ST4 = 'ST4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ST4.</p>"},{"location":"api/types/#qadence.types.LatticeTopology","title":"<code>LatticeTopology</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Lattice topologies to choose from for the register.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.ALL_TO_ALL","title":"<code>ALL_TO_ALL = 'all_to_all'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>All to all- connected lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.ARBITRARY","title":"<code>ARBITRARY = 'arbitrary'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Arbitrarily-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.CIRCLE","title":"<code>CIRCLE = 'circle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Circular lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.HONEYCOMB_LATTICE","title":"<code>HONEYCOMB_LATTICE = 'honeycomb_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Honeycomb-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.LINE","title":"<code>LINE = 'line'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Line-format lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.RECTANGULAR_LATTICE","title":"<code>RECTANGULAR_LATTICE = 'rectangular_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Rectangular-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.SQUARE","title":"<code>SQUARE = 'square'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Square lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.TRIANGULAR_LATTICE","title":"<code>TRIANGULAR_LATTICE = 'triangular_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Triangular-shaped shape.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy","title":"<code>MultivariateStrategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Multivariate strategy for feature maps.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy.PARALLEL","title":"<code>PARALLEL = 'Parallel'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parallel strategy.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy.SERIES","title":"<code>SERIES = 'Series'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Serial strategy.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol","title":"<code>NoiseProtocol()</code>  <code>dataclass</code>","text":"<p>Type of noise protocol.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.ANALOG","title":"<code>ANALOG = AnalogNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied in analog blocks.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.DIGITAL","title":"<code>DIGITAL = DigitalNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied to digital blocks.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.READOUT","title":"<code>READOUT = ReadoutNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied on outputs of quantum programs.</p>"},{"location":"api/types/#qadence.types.ObservableTransform","title":"<code>ObservableTransform</code>","text":"<p>Observable transformation type.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.NONE","title":"<code>NONE = 'none'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>No transformation.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.RANGE","title":"<code>RANGE = 'range'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the given values as min and max.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.SCALE","title":"<code>SCALE = 'scale'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the given values as scale and shift.</p>"},{"location":"api/types/#qadence.types.OpName","title":"<code>OpName</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>A list of all available of digital-analog operations.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGENTANG","title":"<code>ANALOGENTANG = 'AnalogEntanglement'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog entanglement operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGINTERACTION","title":"<code>ANALOGINTERACTION = 'AnalogInteraction'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog interaction operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRX","title":"<code>ANALOGRX = 'AnalogRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RX operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRY","title":"<code>ANALOGRY = 'AnalogRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RY operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRZ","title":"<code>ANALOGRZ = 'AnalogRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RZ operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGSWAP","title":"<code>ANALOGSWAP = 'AnalogSWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog SWAP operation.</p>"},{"location":"api/types/#qadence.types.OpName.CNOT","title":"<code>CNOT = 'CNOT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The CNOT gate.</p>"},{"location":"api/types/#qadence.types.OpName.CPHASE","title":"<code>CPHASE = 'CPHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The controlled PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRX","title":"<code>CRX = 'CRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRY","title":"<code>CRY = 'CRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Controlled RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRZ","title":"<code>CRZ = 'CRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.CSWAP","title":"<code>CSWAP = 'CSWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control SWAP gate.</p>"},{"location":"api/types/#qadence.types.OpName.CZ","title":"<code>CZ = 'CZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The CZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.ENTANGLE","title":"<code>ENTANGLE = 'entangle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The entanglement operation.</p>"},{"location":"api/types/#qadence.types.OpName.H","title":"<code>H = 'H'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Hadamard gate.</p>"},{"location":"api/types/#qadence.types.OpName.HAMEVO","title":"<code>HAMEVO = 'HamEvo'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Hamiltonian Evolution operation.</p>"},{"location":"api/types/#qadence.types.OpName.I","title":"<code>I = 'I'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Identity gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCPHASE","title":"<code>MCPHASE = 'MCPHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRX","title":"<code>MCRX = 'MCRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRY","title":"<code>MCRY = 'MCRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRZ","title":"<code>MCRZ = 'MCRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCZ","title":"<code>MCZ = 'MCZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol CZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.N","title":"<code>N = 'N'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The N = (1/2)(I-Z) operator.</p>"},{"location":"api/types/#qadence.types.OpName.PHASE","title":"<code>PHASE = 'PHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.PROJ","title":"<code>PROJ = 'Projector'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The projector operation.</p>"},{"location":"api/types/#qadence.types.OpName.RX","title":"<code>RX = 'RX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.RY","title":"<code>RY = 'RY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.RZ","title":"<code>RZ = 'RZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.S","title":"<code>S = 'S'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The S gate.</p>"},{"location":"api/types/#qadence.types.OpName.SDAGGER","title":"<code>SDAGGER = 'SDagger'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The S dagger gate.</p>"},{"location":"api/types/#qadence.types.OpName.SWAP","title":"<code>SWAP = 'SWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The SWAP gate.</p>"},{"location":"api/types/#qadence.types.OpName.T","title":"<code>T = 'T'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The T gate.</p>"},{"location":"api/types/#qadence.types.OpName.TDAGGER","title":"<code>TDAGGER = 'TDagger'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The T dagger gate.</p>"},{"location":"api/types/#qadence.types.OpName.TOFFOLI","title":"<code>TOFFOLI = 'Toffoli'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Toffoli gate.</p>"},{"location":"api/types/#qadence.types.OpName.U","title":"<code>U = 'U'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The U gate.</p>"},{"location":"api/types/#qadence.types.OpName.X","title":"<code>X = 'X'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The X gate.</p>"},{"location":"api/types/#qadence.types.OpName.Y","title":"<code>Y = 'Y'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Y gate.</p>"},{"location":"api/types/#qadence.types.OpName.Z","title":"<code>Z = 'Z'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Z gate.</p>"},{"location":"api/types/#qadence.types.OpName.ZERO","title":"<code>ZERO = 'Zero'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The zero gate.</p>"},{"location":"api/types/#qadence.types.OverlapMethod","title":"<code>OverlapMethod</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Overlap Methods to choose from.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.COMPUTE_UNCOMPUTE","title":"<code>COMPUTE_UNCOMPUTE = 'compute_uncompute'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Compute-uncompute.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.EXACT","title":"<code>EXACT = 'exact'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exact.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.HADAMARD_TEST","title":"<code>HADAMARD_TEST = 'hadamard_test'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hadamard-test.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.JENSEN_SHANNON","title":"<code>JENSEN_SHANNON = 'jensen_shannon'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Jensen-shannon.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.SWAP_TEST","title":"<code>SWAP_TEST = 'swap_test'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Swap-test.</p>"},{"location":"api/types/#qadence.types.ParameterType","title":"<code>ParameterType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Parameter types available in qadence.</p>"},{"location":"api/types/#qadence.types.ParameterType.FEATURE","title":"<code>FEATURE = 'Feature'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>FeatureParameters act as input and are not trainable.</p>"},{"location":"api/types/#qadence.types.ParameterType.FIXED","title":"<code>FIXED = 'Fixed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fixed/ constant parameters are neither trainable nor act as input.</p>"},{"location":"api/types/#qadence.types.ParameterType.VARIATIONAL","title":"<code>VARIATIONAL = 'Variational'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>VariationalParameters are trainable.</p>"},{"location":"api/types/#qadence.types.QubitSupportType","title":"<code>QubitSupportType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Qubit support types.</p>"},{"location":"api/types/#qadence.types.QubitSupportType.GLOBAL","title":"<code>GLOBAL = 'global'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use global qubit support.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise","title":"<code>ReadoutNoise</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of readout protocol.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise.CORRELATED","title":"<code>CORRELATED = 'Correlated Readout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using a confusion matrix (2n, 2n) for corrupting bitstrings values.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise.INDEPENDENT","title":"<code>INDEPENDENT = 'Independent Readout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Simple readout protocols where each qubit is corrupted independently.</p>"},{"location":"api/types/#qadence.types.ResultType","title":"<code>ResultType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available data types for generating certain results.</p>"},{"location":"api/types/#qadence.types.ResultType.NUMPY","title":"<code>NUMPY = 'Numpy'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Numpy Array Type.</p>"},{"location":"api/types/#qadence.types.ResultType.STRING","title":"<code>STRING = 'String'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String Type.</p>"},{"location":"api/types/#qadence.types.ResultType.TORCH","title":"<code>TORCH = 'Torch'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Torch Tensor Type.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling","title":"<code>ReuploadScaling</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Scaling for data reuploads in feature maps.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.CONSTANT","title":"<code>CONSTANT = 'Constant'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Constant scaling.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.EXP","title":"<code>EXP = 'Exponential'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exponentially increasing scaling.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.TOWER","title":"<code>TOWER = 'Tower'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Linearly increasing scaling.</p>"},{"location":"api/types/#qadence.types.SerializationFormat","title":"<code>SerializationFormat</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available serialization formats for circuits.</p>"},{"location":"api/types/#qadence.types.SerializationFormat.JSON","title":"<code>JSON = 'JSON'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Json format.</p>"},{"location":"api/types/#qadence.types.SerializationFormat.PT","title":"<code>PT = 'PT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PT format used by Torch.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType","title":"<code>StateGeneratorType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Methods to generate random states.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.HAAR_MEASURE_FAST","title":"<code>HAAR_MEASURE_FAST = 'HaarMeasureFast'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>HaarMeasure.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.HAAR_MEASURE_SLOW","title":"<code>HAAR_MEASURE_SLOW = 'HaarMeasureSlow'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>HaarMeasure non-optimized version.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.RANDOM_ROTATIONS","title":"<code>RANDOM_ROTATIONS = 'RandomRotations'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random Rotations.</p>"},{"location":"api/types/#qadence.types.StrEnum","title":"<code>StrEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"api/types/#qadence.types.StrEnum.__str__","title":"<code>__str__()</code>","text":"<p>Used when dumping enum fields in a schema.</p> Source code in <code>qadence/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Used when dumping enum fields in a schema.\"\"\"\n    ret: str = self.value\n    return ret\n</code></pre>"},{"location":"api/types/#qadence.types.Strategy","title":"<code>Strategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Computing paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.ANALOG","title":"<code>ANALOG = 'Analog'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the analog paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.BDAQC","title":"<code>BDAQC = 'bDAQC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the banged digital-analog QC paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.DIGITAL","title":"<code>DIGITAL = 'Digital'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the digital paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.RYDBERG","title":"<code>RYDBERG = 'Rydberg'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the Rydberg QC paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.SDAQC","title":"<code>SDAQC = 'sDAQC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the step-wise digital-analog QC paradigm.</p>"},{"location":"api/types/#qadence.types.TensorType","title":"<code>TensorType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Tensor Types for converting blocks to tensors.</p>"},{"location":"api/types/#qadence.types.TensorType.DENSE","title":"<code>DENSE = 'Dense'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a block to a dense tensor.</p>"},{"location":"api/types/#qadence.types.TensorType.SPARSE","title":"<code>SPARSE = 'Sparse'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a observable block to a sparse tensor.</p>"},{"location":"api/types/#qadence.types.TensorType.SPARSEDIAGONAL","title":"<code>SPARSEDIAGONAL = 'SparseDiagonal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a diagonal observable block to a sparse diagonal if possible.</p>"},{"location":"api/backends/backend/","title":"Abstract backend","text":""},{"location":"api/backends/backend/#qadence.backend.Backend","title":"<code>Backend(name, supports_ad, support_bp, supports_adjoint, is_remote, with_measurements, native_endianness, engine, with_noise, config)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract class that defines the interface for the backends.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>backend unique string identifier</p> <p> TYPE: <code>BackendName</code> </p> <code>supports_ad</code> <p>whether or not the backend has a native autograd</p> <p> TYPE: <code>bool</code> </p> <code>supports_bp</code> <p>whether or not the backend has a native backprop</p> <p> TYPE: <code>bool</code> </p> <code>supports_adjoint</code> <p>Does the backend support native adjoint differentation.</p> <p> TYPE: <code>bool</code> </p> <code>is_remote</code> <p>whether computations are executed locally or remotely on this backend, useful when using cloud platforms where credentials are needed for example.</p> <p> TYPE: <code>bool</code> </p> <code>with_measurements</code> <p>whether it supports counts or not</p> <p> TYPE: <code>bool</code> </p> <code>with_noise</code> <p>whether to add realistic noise or not</p> <p> TYPE: <code>bool</code> </p> <code>native_endianness</code> <p>The native endianness of the backend</p> <p> TYPE: <code>Endianness</code> </p> <code>engine</code> <p>The underlying (native) automatic differentiation engine of the backend.</p> <p> TYPE: <code>Engine</code> </p>"},{"location":"api/backends/backend/#qadence.backend.Backend.circuit","title":"<code>circuit(circuit)</code>  <code>abstractmethod</code>","text":"<p>Converts an abstract <code>QuantumCircuit</code> to the native backend representation.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A circuit, for example: <code>QuantumCircuit(2, X(0))</code></p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>A converted circuit <code>c</code>. You can access the original, arbstract circuit via <code>c.abstract</code></p> <code>ConvertedCircuit</code> <p>and the converted (or backend native) circuit via <code>c.native</code>.</p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Converts an abstract `QuantumCircuit` to the native backend representation.\n\n    Arguments:\n        circuit: A circuit, for example: `QuantumCircuit(2, X(0))`\n\n    Returns:\n        A converted circuit `c`. You can access the original, arbstract circuit via `c.abstract`\n        and the converted (or backend *native*) circuit via `c.native`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.convert","title":"<code>convert(circuit, observable=None)</code>","text":"<p>Convert an abstract circuit and an optional observable to their native representation.</p> <p>Additionally, this function constructs an embedding function which maps from user-facing parameters to device parameters (read more on parameter embedding here).</p> Source code in <code>qadence/backend.py</code> <pre><code>def convert(\n    self, circuit: QuantumCircuit, observable: list[AbstractBlock] | AbstractBlock | None = None\n) -&gt; Converted:\n    \"\"\"Convert an abstract circuit and an optional observable to their native representation.\n\n    Additionally, this function constructs an embedding function which maps from\n    user-facing parameters to device parameters (read more on parameter embedding\n    [here][qadence.blocks.embedding.embedding]).\n    \"\"\"\n\n    def check_observable(obs_obj: Any) -&gt; AbstractBlock:\n        if isinstance(obs_obj, QubitOperator):\n            from qadence.blocks.manipulate import from_openfermion\n\n            assert len(obs_obj.terms) &gt; 0, \"Make sure to give a non-empty qubit hamiltonian\"\n\n            return from_openfermion(obs_obj)\n\n        elif isinstance(obs_obj, (CompositeBlock, PrimitiveBlock, ScaleBlock)):\n            from qadence.blocks.utils import block_is_qubit_hamiltonian\n\n            assert block_is_qubit_hamiltonian(\n                obs_obj\n            ), \"Make sure the QubitHamiltonian consists only of Pauli operators X, Y, Z, I\"\n            return obs_obj\n        raise TypeError(\n            \"qubit_hamiltonian should be a Pauli-like AbstractBlock or a QubitOperator\"\n        )\n\n    conv_circ = self.circuit(circuit)\n    circ_params, circ_embedding_fn = embedding(\n        conv_circ.abstract.block, self.config._use_gate_params, self.engine\n    )\n    params = circ_params\n    if observable is not None:\n        observable = observable if isinstance(observable, list) else [observable]\n        conv_obs = []\n        obs_embedding_fn_list = []\n\n        for obs in observable:\n            obs = check_observable(obs)\n            c_obs = self.observable(obs, max(circuit.n_qubits, obs.n_qubits))\n            obs_params, obs_embedding_fn = embedding(\n                c_obs.abstract, self.config._use_gate_params, self.engine\n            )\n            params.update(obs_params)\n            obs_embedding_fn_list.append(obs_embedding_fn)\n            conv_obs.append(c_obs)\n\n        def embedding_fn_dict(a: dict, b: dict) -&gt; dict:\n            embedding_dict = circ_embedding_fn(a, b)\n            for o in obs_embedding_fn_list:\n                embedding_dict.update(o(a, b))\n            return embedding_dict\n\n        return Converted(conv_circ, conv_obs, embedding_fn_dict, params)\n\n    def embedding_fn(a: dict, b: dict) -&gt; dict:\n        return circ_embedding_fn(a, b)\n\n    return Converted(conv_circ, None, embedding_fn, params)\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>  <code>abstractmethod</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.observable","title":"<code>observable(observable, n_qubits)</code>  <code>abstractmethod</code>","text":"<p>Converts an abstract observable (which is just an <code>AbstractBlock</code>) to the native backend.</p> <p>representation.</p> PARAMETER DESCRIPTION <code>observable</code> <p>An observable.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits the observable covers. This is typically <code>circuit.n_qubits</code>.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ConvertedObservable</code> <p>A converted observable <code>o</code>. You can access the original, arbstract observable via</p> <code>ConvertedObservable</code> <p><code>o.abstract</code> and the converted (or backend native) observable via <code>o.native</code>.</p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef observable(self, observable: AbstractBlock, n_qubits: int) -&gt; ConvertedObservable:\n    \"\"\"Converts an abstract observable (which is just an `AbstractBlock`) to the native backend.\n\n    representation.\n\n    Arguments:\n        observable: An observable.\n        n_qubits: Number of qubits the observable covers. This is typically `circuit.n_qubits`.\n\n    Returns:\n        A converted observable `o`. You can access the original, arbstract observable via\n        `o.abstract` and the converted (or backend *native*) observable via `o.native`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.run","title":"<code>run(circuit, param_values={}, state=None, endianness=Endianness.BIG, *args, **kwargs)</code>","text":"<p>Run a circuit and return the resulting wave function.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>dict[str, ArrayLike]</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting wavefunction.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>A list of Counter objects where each key represents a bitstring</p> <code>ArrayLike</code> <p>and its value the number of times it has been sampled from the given wave function.</p> Source code in <code>qadence/backend.py</code> <pre><code>def run(\n    self,\n    circuit: ConvertedCircuit,\n    param_values: dict[str, ArrayLike] = {},\n    state: Tensor | None = None,\n    endianness: Endianness = Endianness.BIG,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ArrayLike:\n    \"\"\"Run a circuit and return the resulting wave function.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        endianness: Endianness of the resulting wavefunction.\n\n    Returns:\n        A list of Counter objects where each key represents a bitstring\n        and its value the number of times it has been sampled from the given wave function.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.sample","title":"<code>sample(circuit, param_values={}, n_shots=1000, state=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>  <code>abstractmethod</code>","text":"<p>Sample bit strings.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>n_shots</code> <p>Number of shots to sample.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>An error mitigation protocol to apply.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    circuit: ConvertedCircuit,\n    param_values: dict[str, Tensor] = {},\n    n_shots: int = 1000,\n    state: ArrayLike | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; list[Counter]:\n    \"\"\"Sample bit strings.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        n_shots: Number of shots to sample.\n        state: Initial state.\n        noise: A noise model to use.\n        mitigation: An error mitigation protocol to apply.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration","title":"<code>BackendConfiguration(_use_gate_params=True, use_sparse_observable=False, use_gradient_checkpointing=False, use_single_qubit_composition=False, transpilation_passes=None)</code>  <code>dataclass</code>","text":""},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration.available_options","title":"<code>available_options()</code>","text":"<p>Return as a string the available fields with types of the configuration.</p> RETURNS DESCRIPTION <code>str</code> <p>a string with all the available fields, one per line</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>def available_options(self) -&gt; str:\n    \"\"\"Return as a string the available fields with types of the configuration.\n\n    Returns:\n        str: a string with all the available fields, one per line\n    \"\"\"\n    conf_msg = \"\"\n    for _field in fields(self):\n        if not _field.name.startswith(\"_\"):\n            conf_msg += (\n                f\"Name: {_field.name} - Type: {_field.type} - Default value: {_field.default}\\n\"\n            )\n    return conf_msg\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration.get_param_name","title":"<code>get_param_name(blk)</code>","text":"<p>Return parameter names for the current backend.</p> <p>Depending on which backend is in use this function returns either UUIDs or expressions of parameters.</p> Source code in <code>qadence/backend.py</code> <pre><code>def get_param_name(self, blk: AbstractBlock) -&gt; Tuple[str, ...]:\n    \"\"\"Return parameter names for the current backend.\n\n    Depending on which backend is in use this\n    function returns either UUIDs or expressions of parameters.\n    \"\"\"\n    param_ids: Tuple\n    # FIXME: better type hiearchy?\n    types = (TimeEvolutionBlock, ParametricBlock, ConstantAnalogRotation, InteractionBlock)\n    if not isinstance(blk, types):\n        raise TypeError(f\"Can not infer param name from {type(blk)}\")\n    else:\n        if self._use_gate_params:\n            param_ids = tuple(blk.parameters.uuids())\n        else:\n            param_ids = tuple(map(stringify, blk.parameters.expressions()))\n    return param_ids\n</code></pre>"},{"location":"api/backends/differentiable/","title":"DifferentiableBackend","text":""},{"location":"api/backends/differentiable/#qadence.engines.torch.differentiable_backend.DifferentiableBackend","title":"<code>DifferentiableBackend(backend, diff_mode=DiffMode.AD, **psr_args)</code>","text":"<p>               Bases: <code>DifferentiableBackend</code></p> <p>A class which wraps a QuantumBackend with the automatic differentation engine TORCH.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of the QuantumBackend type perform execution.</p> <p> TYPE: <code>Backend</code> </p> <code>diff_mode</code> <p>A differentiable mode supported by the differentiation engine.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>**psr_args</code> <p>Arguments that will be passed on to <code>DifferentiableExpectation</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>{}</code> </p> Source code in <code>qadence/engines/torch/differentiable_backend.py</code> <pre><code>def __init__(\n    self,\n    backend: QuantumBackend,\n    diff_mode: DiffMode = DiffMode.AD,\n    **psr_args: int | float | None,\n) -&gt; None:\n    super().__init__(backend=backend, engine=Engine.TORCH, diff_mode=diff_mode)\n    self.psr_args = psr_args\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.torch.differentiable_backend.DifferentiableBackend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>observable</code> <p>A converted observable as returned by <code>backend.observable</code>.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>The error mitigation to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/engines/torch/differentiable_backend.py</code> <pre><code>def expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        observable: A converted observable as returned by `backend.observable`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: The error mitigation to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    observable = observable if isinstance(observable, list) else [observable]\n    differentiable_expectation = DifferentiableExpectation(\n        backend=self.backend,\n        circuit=circuit,\n        observable=observable,\n        param_values=param_values,\n        state=state,\n        measurement=measurement,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n\n    if self.diff_mode == DiffMode.AD:\n        expectation = differentiable_expectation.ad\n    elif self.diff_mode == DiffMode.ADJOINT:\n        expectation = differentiable_expectation.adjoint\n    else:\n        try:\n            fns = get_gpsr_fns()\n            psr_fn = fns[self.diff_mode]\n        except KeyError:\n            raise ValueError(f\"{self.diff_mode} differentiation mode is not supported\")\n        expectation = partial(differentiable_expectation.psr, psr_fn=psr_fn, **self.psr_args)\n    return expectation()\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.jax.differentiable_backend.DifferentiableBackend","title":"<code>DifferentiableBackend(backend, diff_mode=DiffMode.AD, **psr_args)</code>","text":"<p>               Bases: <code>DifferentiableBackend</code></p> <p>A class which wraps a QuantumBackend with the automatic differentation engine JAX.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of the QuantumBackend type perform execution.</p> <p> TYPE: <code>Backend</code> </p> <code>diff_mode</code> <p>A differentiable mode supported by the differentiation engine.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>**psr_args</code> <p>Arguments that will be passed on to <code>DifferentiableExpectation</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>{}</code> </p> Source code in <code>qadence/engines/jax/differentiable_backend.py</code> <pre><code>def __init__(\n    self,\n    backend: Backend,\n    diff_mode: DiffMode = DiffMode.AD,\n    **psr_args: int | float | None,\n) -&gt; None:\n    super().__init__(backend=backend, engine=Engine.JAX, diff_mode=diff_mode)\n    self.psr_args = psr_args\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.jax.differentiable_backend.DifferentiableBackend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>observable</code> <p>A converted observable as returned by <code>backend.observable</code>.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>The error mitigation to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/engines/jax/differentiable_backend.py</code> <pre><code>def expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        observable: A converted observable as returned by `backend.observable`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: The error mitigation to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    observable = observable if isinstance(observable, list) else [observable]\n\n    if self.diff_mode == DiffMode.AD:\n        expectation = self.backend.expectation(circuit, observable, param_values, state)\n    else:\n        expectation = DifferentiableExpectation(\n            backend=self.backend,\n            circuit=circuit,\n            observable=observable,\n            param_values=param_values,\n            state=state,\n            measurement=measurement,\n            noise=noise,\n            mitigation=mitigation,\n            endianness=endianness,\n        ).psr()\n    return expectation\n</code></pre>"},{"location":"api/backends/pulser/","title":"Pulser","text":"<p>The Pulser backend features a basic integration with the pulse-level programming interface Pulser. This backend offers for now few simple operations which are translated into a valid, non time-dependent pulse sequence. In particular, one has access to:</p> <ul> <li>analog rotations: <code>AnalogRx</code> and <code>AnalogRy</code> blocks</li> <li>free evolution blocks (basically no pulse, just interaction): <code>AnalogWait</code> block</li> <li>a block for creating entangled states: <code>AnalogEntanglement</code></li> <li>digital rotation <code>Rx</code> and <code>Ry</code></li> </ul>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend.Backend","title":"<code>Backend(name=BackendName.PULSER, supports_ad=False, support_bp=False, supports_adjoint=False, is_remote=False, with_measurements=True, native_endianness=Endianness.BIG, engine=Engine.TORCH, with_noise=False, config=Configuration())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Backend</code></p> <p>The Pulser backend.</p>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend.create_register","title":"<code>create_register(register)</code>","text":"<p>Convert Qadence Register to Pulser Register.</p> Source code in <code>qadence/backends/pulser/backend.py</code> <pre><code>def create_register(register: Register) -&gt; PulserRegister:\n    \"\"\"Convert Qadence Register to Pulser Register.\"\"\"\n    coords = np.array(list(register.coords.values()))\n    return PulserRegister.from_coordinates(coords)\n</code></pre>"},{"location":"api/backends/pyqtorch/","title":"PyQTorch","text":"<p>Fast differentiable statevector emulator based on PyTorch. The code is open source, hosted on Github and maintained by Pasqal.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend","title":"<code>Backend(name=BackendName.PYQTORCH, supports_ad=True, support_bp=True, supports_adjoint=True, is_remote=False, with_measurements=True, native_endianness=Endianness.BIG, engine=Engine.TORCH, with_noise=False, config=Configuration())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Backend</code></p> <p>PyQTorch backend.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend.circuit","title":"<code>circuit(circuit)</code>","text":"<p>Return the converted circuit.</p> <p>Note that to get a representation with noise, noise should be passed within the config.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Original circuit</p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>ConvertedCircuit instance for backend.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Return the converted circuit.\n\n    Note that to get a representation with noise, noise\n    should be passed within the config.\n\n    Args:\n        circuit (QuantumCircuit): Original circuit\n\n    Returns:\n        ConvertedCircuit: ConvertedCircuit instance for backend.\n    \"\"\"\n    passes = self.config.transpilation_passes\n    if passes is None:\n        passes = default_passes(self.config)\n\n    original_circ = circuit\n    if len(passes) &gt; 0:\n        circuit = transpile(*passes)(circuit)\n    # Setting noise in the circuit.\n    if self.config.noise:\n        set_noise(circuit, self.config.noise)\n\n    ops = convert_block(circuit.block, n_qubits=circuit.n_qubits, config=self.config)\n    readout_noise = (\n        convert_readout_noise(circuit.n_qubits, self.config.noise)\n        if self.config.noise\n        else None\n    )\n    if self.config.dropout_probability == 0:\n        native = pyq.QuantumCircuit(\n            circuit.n_qubits,\n            ops,\n            readout_noise,\n        )\n    else:\n        native = pyq.DropoutQuantumCircuit(\n            circuit.n_qubits,\n            ops,\n            readout_noise,\n            dropout_prob=self.config.dropout_probability,\n            dropout_mode=self.config.dropout_mode,\n        )\n    return ConvertedCircuit(native=native, abstract=circuit, original=original_circ)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend.convert","title":"<code>convert(circuit, observable=None)</code>","text":"<p>Convert an abstract circuit and an optional observable to their native representation.</p> <p>Additionally, this function constructs an embedding function which maps from user-facing parameters to device parameters (read more on parameter embedding here).</p> Source code in <code>qadence/backend.py</code> <pre><code>def convert(\n    self, circuit: QuantumCircuit, observable: list[AbstractBlock] | AbstractBlock | None = None\n) -&gt; Converted:\n    \"\"\"Convert an abstract circuit and an optional observable to their native representation.\n\n    Additionally, this function constructs an embedding function which maps from\n    user-facing parameters to device parameters (read more on parameter embedding\n    [here][qadence.blocks.embedding.embedding]).\n    \"\"\"\n\n    def check_observable(obs_obj: Any) -&gt; AbstractBlock:\n        if isinstance(obs_obj, QubitOperator):\n            from qadence.blocks.manipulate import from_openfermion\n\n            assert len(obs_obj.terms) &gt; 0, \"Make sure to give a non-empty qubit hamiltonian\"\n\n            return from_openfermion(obs_obj)\n\n        elif isinstance(obs_obj, (CompositeBlock, PrimitiveBlock, ScaleBlock)):\n            from qadence.blocks.utils import block_is_qubit_hamiltonian\n\n            assert block_is_qubit_hamiltonian(\n                obs_obj\n            ), \"Make sure the QubitHamiltonian consists only of Pauli operators X, Y, Z, I\"\n            return obs_obj\n        raise TypeError(\n            \"qubit_hamiltonian should be a Pauli-like AbstractBlock or a QubitOperator\"\n        )\n\n    conv_circ = self.circuit(circuit)\n    circ_params, circ_embedding_fn = embedding(\n        conv_circ.abstract.block, self.config._use_gate_params, self.engine\n    )\n    params = circ_params\n    if observable is not None:\n        observable = observable if isinstance(observable, list) else [observable]\n        conv_obs = []\n        obs_embedding_fn_list = []\n\n        for obs in observable:\n            obs = check_observable(obs)\n            c_obs = self.observable(obs, max(circuit.n_qubits, obs.n_qubits))\n            obs_params, obs_embedding_fn = embedding(\n                c_obs.abstract, self.config._use_gate_params, self.engine\n            )\n            params.update(obs_params)\n            obs_embedding_fn_list.append(obs_embedding_fn)\n            conv_obs.append(c_obs)\n\n        def embedding_fn_dict(a: dict, b: dict) -&gt; dict:\n            embedding_dict = circ_embedding_fn(a, b)\n            for o in obs_embedding_fn_list:\n                embedding_dict.update(o(a, b))\n            return embedding_dict\n\n        return Converted(conv_circ, conv_obs, embedding_fn_dict, params)\n\n    def embedding_fn(a: dict, b: dict) -&gt; dict:\n        return circ_embedding_fn(a, b)\n\n    return Converted(conv_circ, None, embedding_fn, params)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_block_and_readout_noises","title":"<code>set_block_and_readout_noises(circuit, noise, config)</code>","text":"<p>Add noise on blocks and readout on circuit.</p> <p>We first start by adding noise to the abstract blocks. Then we do a conversion to their native representation. Finally, we add readout.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>noise</code> <p>Noise to add.</p> <p> TYPE: <code>NoiseHandler | None</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_block_and_readout_noises(\n    circuit: ConvertedCircuit, noise: NoiseHandler | None, config: Configuration\n) -&gt; None:\n    \"\"\"Add noise on blocks and readout on circuit.\n\n    We first start by adding noise to the abstract blocks. Then we do a conversion to their\n    native representation. Finally, we add readout.\n\n    Args:\n        circuit (ConvertedCircuit): Input circuit.\n        noise (NoiseHandler | None): Noise to add.\n    \"\"\"\n    if noise:\n        set_noise(circuit, noise)\n        set_noise_abstract_to_native(circuit, config)\n        set_readout_noise(circuit, noise)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_noise_abstract_to_native","title":"<code>set_noise_abstract_to_native(circuit, config)</code>","text":"<p>Set noise in native blocks from the abstract ones with noise.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input converted circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_noise_abstract_to_native(circuit: ConvertedCircuit, config: Configuration) -&gt; None:\n    \"\"\"Set noise in native blocks from the abstract ones with noise.\n\n    Args:\n        circuit (ConvertedCircuit): Input converted circuit.\n    \"\"\"\n    ops = convert_block(circuit.abstract.block, n_qubits=circuit.native.n_qubits, config=config)\n    circuit.native = pyq.QuantumCircuit(circuit.native.n_qubits, ops, circuit.native.readout_noise)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_readout_noise","title":"<code>set_readout_noise(circuit, noise)</code>","text":"<p>Set readout noise in place in native.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input converted circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>noise</code> <p>Noise.</p> <p> TYPE: <code>NoiseHandler | None</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_readout_noise(circuit: ConvertedCircuit, noise: NoiseHandler) -&gt; None:\n    \"\"\"Set readout noise in place in native.\n\n    Args:\n        circuit (ConvertedCircuit):  Input converted circuit.\n        noise (NoiseHandler | None): Noise.\n    \"\"\"\n    readout = convert_readout_noise(circuit.abstract.n_qubits, noise)\n    if readout:\n        circuit.native.readout_noise = readout\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration","title":"<code>Configuration(_use_gate_params=True, use_sparse_observable=False, use_gradient_checkpointing=False, use_single_qubit_composition=False, transpilation_passes=None, algo_hevo=AlgoHEvo.EXP, ode_solver=SolverType.DP5_SE, n_steps_hevo=100, loop_expectation=False, noise=None, dropout_probability=0.0, dropout_mode=DropoutMode.ROTATIONAL)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BackendConfiguration</code></p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.algo_hevo","title":"<code>algo_hevo = AlgoHEvo.EXP</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine which kind of Hamiltonian evolution algorithm to use.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.dropout_mode","title":"<code>dropout_mode = DropoutMode.ROTATIONAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of quantum dropout to perform.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.dropout_probability","title":"<code>dropout_probability = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantum dropout probability (0 means no dropout).</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.loop_expectation","title":"<code>loop_expectation = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When computing batches of expectation values, only allocate one wavefunction.</p> <p>Loop over the batch of parameters to only allocate a single wavefunction at any given time.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.n_steps_hevo","title":"<code>n_steps_hevo = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default number of steps for the Hamiltonian evolution.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.noise","title":"<code>noise = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NoiseHandler containing readout noise applied in backend.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.ode_solver","title":"<code>ode_solver = SolverType.DP5_SE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine which ODE solver to use for time-dependent blocks.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.use_gradient_checkpointing","title":"<code>use_gradient_checkpointing = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use gradient checkpointing.</p> <p>Recommended for higher-order optimization tasks.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.use_single_qubit_composition","title":"<code>use_single_qubit_composition = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Composes chains of single qubit gates into a single matmul if possible.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.supported_gates","title":"<code>supported_gates = list(set(OpName.list()) - set([OpName.TDAGGER]))</code>  <code>module-attribute</code>","text":"<p>The set of supported gates.</p> <p>Tdagger is currently not supported.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_block","title":"<code>convert_block(block, n_qubits=None, config=None)</code>","text":"<p>Convert block to native Pyqtorch representation.</p> PARAMETER DESCRIPTION <code>block</code> <p>Block to convert.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Backend configuration instance. Defaults to None.</p> <p> TYPE: <code>Configuration</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>For non supported blocks.</p> RETURNS DESCRIPTION <code>Sequence[Module | Tensor | str | Expr]</code> <p>Sequence[Module | Tensor | str | sympy.Expr]: List of native operations.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_block(\n    block: AbstractBlock,\n    n_qubits: int = None,\n    config: Configuration = None,\n) -&gt; Sequence[Module | Tensor | str | sympy.Expr]:\n    \"\"\"Convert block to native Pyqtorch representation.\n\n    Args:\n        block (AbstractBlock): Block to convert.\n        n_qubits (int, optional): Number of qubits. Defaults to None.\n        config (Configuration, optional): Backend configuration instance. Defaults to None.\n\n    Raises:\n        NotImplementedError: For non supported blocks.\n\n    Returns:\n        Sequence[Module | Tensor | str | sympy.Expr]: List of native operations.\n    \"\"\"\n    if isinstance(block, (Tensor, str, sympy.Expr)):  # case for hamevo generators\n        if isinstance(block, Tensor):\n            block = block.permute(1, 2, 0)  # put batch size in the back\n        return [block]\n    qubit_support = block.qubit_support\n    if n_qubits is None:\n        n_qubits = max(qubit_support) + 1\n\n    if config is None:\n        config = Configuration()\n\n    noise: NoiseHandler | None = None\n    if hasattr(block, \"noise\") and block.noise:\n        noise = convert_digital_noise(block.noise)\n\n    if isinstance(block, ScaleBlock):\n        scaled_ops = convert_block(block.block, n_qubits, config)\n        scale = extract_parameter(block, config=config)\n\n        # replace underscore by dot when underscore is between two numbers in string\n        if isinstance(scale, str):\n            scale = replace_underscore_floats(scale)\n\n        if isinstance(scale, str) and not config._use_gate_params:\n            param = sympy_to_pyq(sympy.parse_expr(scale))\n        else:\n            param = scale\n\n        return [pyq.Scale(pyq.Sequence(scaled_ops), param)]\n\n    elif isinstance(block, TimeEvolutionBlock):\n        duration = block.duration  # type: ignore [attr-defined]\n        if getattr(block.generator, \"is_time_dependent\", False):\n            config._use_gate_params = False\n            duration = config.get_param_name(block)[1]\n            generator = convert_block(block.generator, config=config)[0]  # type: ignore [arg-type]\n        elif isinstance(block.generator, sympy.Basic):\n            generator = config.get_param_name(block)[1]\n\n        elif isinstance(block.generator, Tensor):\n            m = block.generator.to(dtype=cdouble)\n            generator = convert_block(\n                MatrixBlock(\n                    m,\n                    qubit_support=qubit_support,\n                    check_unitary=False,\n                    check_hermitian=True,\n                )\n            )[0]\n        else:\n            generator = convert_block(block.generator, n_qubits, config)[0]  # type: ignore[arg-type]\n        time_param = config.get_param_name(block)[0]\n\n        # convert noise operators here\n        noise_operators: list = [\n            convert_block(noise_block, config=config)[0] for noise_block in block.noise_operators\n        ]\n        if len(noise_operators) &gt; 0:\n            # squeeze batch size for noise operators\n            noise_operators = [\n                pyq_op.tensor(full_support=qubit_support).squeeze(-1) for pyq_op in noise_operators\n            ]\n\n        return [\n            pyq.HamiltonianEvolution(\n                qubit_support=qubit_support,\n                generator=generator,\n                time=time_param,\n                cache_length=0,\n                duration=duration,\n                solver=config.ode_solver,\n                steps=config.n_steps_hevo,\n                noise=noise_operators if len(noise_operators) &gt; 0 else None,\n            )\n        ]\n\n    elif isinstance(block, MatrixBlock):\n        return [pyq.primitives.Primitive(block.matrix, block.qubit_support, noise=noise)]\n    elif isinstance(block, CompositeBlock):\n        ops = list(flatten(*(convert_block(b, n_qubits, config) for b in block.blocks)))\n        if isinstance(block, AddBlock):\n            return [pyq.Add(ops)]  # add\n        elif is_single_qubit_chain(block) and config.use_single_qubit_composition:\n            return [pyq.Merge(ops)]  # for chains of single qubit ops on the same qubit\n        else:\n            return [pyq.Sequence(ops)]  # for kron and chain\n    elif isinstance(block, tuple(non_unitary_gateset)):\n        if isinstance(block, ProjectorBlock):\n            projector = getattr(pyq, block.name)\n            if block.name == OpName.N:\n                return [projector(target=qubit_support, noise=noise)]\n            else:\n                return [\n                    projector(\n                        qubit_support=qubit_support,\n                        ket=block.ket,\n                        bra=block.bra,\n                        noise=noise,\n                    )\n                ]\n        else:\n            return [getattr(pyq, block.name)(qubit_support[0])]\n    elif isinstance(block, tuple(single_qubit_gateset)):\n        pyq_cls = getattr(pyq, block.name)\n        if isinstance(block, ParametricBlock):\n            if isinstance(block, U):\n                op = pyq_cls(\n                    qubit_support[0],\n                    *config.get_param_name(block),\n                    noise=noise,\n                )\n            else:\n                param = extract_parameter(block, config)\n                op = pyq_cls(qubit_support[0], param, noise=noise)\n        else:\n            op = pyq_cls(qubit_support[0], noise=noise)  # type: ignore [attr-defined]\n        return [op]\n    elif isinstance(block, tuple(two_qubit_gateset)):\n        pyq_cls = getattr(pyq, block.name)\n        if isinstance(block, ParametricBlock):\n            op = pyq_cls(\n                qubit_support[0],\n                qubit_support[1],\n                extract_parameter(block, config),\n                noise=noise,\n            )\n        else:\n            op = pyq_cls(\n                qubit_support[0], qubit_support[1], noise=noise  # type: ignore [attr-defined]\n            )\n        return [op]\n    elif isinstance(block, tuple(three_qubit_gateset) + tuple(multi_qubit_gateset)):\n        block_name = block.name[1:] if block.name.startswith(\"M\") else block.name\n        pyq_cls = getattr(pyq, block_name)\n        if isinstance(block, ParametricBlock):\n            op = pyq_cls(\n                qubit_support[:-1],\n                qubit_support[-1],\n                extract_parameter(block, config),\n                noise=noise,\n            )\n        else:\n            if \"CSWAP\" in block_name:\n                op = pyq_cls(\n                    qubit_support[:-2], qubit_support[-2:], noise=noise  # type: ignore [attr-defined]\n                )\n            else:\n                op = pyq_cls(\n                    qubit_support[:-1], qubit_support[-1], noise=noise  # type: ignore [attr-defined]\n                )\n        return [op]\n    else:\n        raise NotImplementedError(\n            f\"Non supported operation of type {type(block)}. \"\n            \"In case you are trying to run an `AnalogBlock`, make sure you \"\n            \"specify the `device_specs` in your `Register` first.\"\n        )\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_digital_noise","title":"<code>convert_digital_noise(noise)</code>","text":"<p>Convert the digital noise into pyqtorch NoiseProtocol.</p> PARAMETER DESCRIPTION <code>noise</code> <p>Noise to convert.</p> <p> TYPE: <code>NoiseHandler</code> </p> RETURNS DESCRIPTION <code>DigitalNoiseProtocol | None</code> <p>pyq.noise.DigitalNoiseProtocol | None: Pyqtorch native noise protocol if there are any digital noise protocols.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_digital_noise(noise: NoiseHandler) -&gt; pyq.noise.DigitalNoiseProtocol | None:\n    \"\"\"Convert the digital noise into pyqtorch NoiseProtocol.\n\n    Args:\n        noise (NoiseHandler): Noise to convert.\n\n    Returns:\n        pyq.noise.DigitalNoiseProtocol | None: Pyqtorch native noise protocol\n            if there are any digital noise protocols.\n    \"\"\"\n    digital_part = noise.filter(NoiseProtocol.DIGITAL)\n    if digital_part is None:\n        return None\n    return pyq.noise.DigitalNoiseProtocol(\n        [\n            pyq.noise.DigitalNoiseProtocol(proto, option.get(\"error_probability\"))\n            for proto, option in zip(digital_part.protocol, digital_part.options)\n        ]\n    )\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_readout_noise","title":"<code>convert_readout_noise(n_qubits, noise)</code>","text":"<p>Convert the readout noise into pyqtorch ReadoutNoise.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Number of qubits</p> <p> TYPE: <code>int</code> </p> <code>noise</code> <p>Noise to convert.</p> <p> TYPE: <code>NoiseHandler</code> </p> RETURNS DESCRIPTION <code>ReadoutNoise | None</code> <p>pyq.noise.ReadoutNoise | None: Pyqtorch native ReadoutNoise instance if readout is is noise.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_readout_noise(n_qubits: int, noise: NoiseHandler) -&gt; pyq.noise.ReadoutNoise | None:\n    \"\"\"Convert the readout noise into pyqtorch ReadoutNoise.\n\n    Args:\n        n_qubits (int): Number of qubits\n        noise (NoiseHandler):  Noise to convert.\n\n    Returns:\n        pyq.noise.ReadoutNoise | None: Pyqtorch native ReadoutNoise instance\n            if readout is is noise.\n    \"\"\"\n    readout_part = noise.filter(NoiseProtocol.READOUT)\n    if readout_part is None:\n        return None\n\n    if readout_part.protocol[0] == NoiseProtocol.READOUT.INDEPENDENT:\n        return pyq.noise.ReadoutNoise(n_qubits, **readout_part.options[0])\n    else:\n        return pyq.noise.CorrelatedReadoutNoise(**readout_part.options[0])\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.extract_parameter","title":"<code>extract_parameter(block, config)</code>","text":"<p>Extract the parameter as string or its tensor value.</p> PARAMETER DESCRIPTION <code>block</code> <p>Block to extract parameter from.</p> <p> TYPE: <code>ScaleBlock | ParametricBlock</code> </p> <code>config</code> <p>Configuration instance.</p> <p> TYPE: <code>Configuration</code> </p> RETURNS DESCRIPTION <code>str | Tensor</code> <p>str | Tensor: Parameter value or symbol.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def extract_parameter(block: ScaleBlock | ParametricBlock, config: Configuration) -&gt; str | Tensor:\n    \"\"\"Extract the parameter as string or its tensor value.\n\n    Args:\n        block (ScaleBlock | ParametricBlock): Block to extract parameter from.\n        config (Configuration): Configuration instance.\n\n    Returns:\n        str | Tensor: Parameter value or symbol.\n    \"\"\"\n    if not block.is_parametric:\n        tensor_val = tensor([block.parameters.parameter], dtype=complex64)\n        return (\n            tensor([block.parameters.parameter], dtype=float64)\n            if torch.all(tensor_val.imag == 0)\n            else tensor_val\n        )\n\n    return config.get_param_name(block)[0]\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.replace_underscore_floats","title":"<code>replace_underscore_floats(s)</code>","text":"<p>Replace underscores with periods for all floats in given string.</p> <p>Needed for correct parsing of string by sympy parser.</p> PARAMETER DESCRIPTION <code>s</code> <p>string expression</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>transformed string expression</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def replace_underscore_floats(s: str) -&gt; str:\n    \"\"\"Replace underscores with periods for all floats in given string.\n\n    Needed for correct parsing of string by sympy parser.\n\n    Args:\n        s (str): string expression\n\n    Returns:\n        str: transformed string expression\n    \"\"\"\n\n    # Regular expression to match floats written with underscores instead of dots\n    float_with_underscore_pattern = r\"\"\"\n        (?&lt;!\\w)            # Negative lookbehind to ensure not part of a word\n        -?                 # Optional negative sign\n        \\d+                # One or more digits (before underscore)\n        _                  # The underscore acting as decimal separator\n        \\d+                # One or more digits (after underscore)\n        ([eE][-+]?\\d+)?    # Optional exponent part for scientific notation\n        (?!\\w)             # Negative lookahead to ensure not part of a word\n    \"\"\"\n\n    # Function to replace the underscore with a dot\n    def underscore_to_dot(match: re.Match) -&gt; Any:\n        return match.group(0).replace(\"_\", \".\")\n\n    # Compile the regular expression\n    pattern = re.compile(float_with_underscore_pattern, re.VERBOSE)\n\n    return pattern.sub(underscore_to_dot, s)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.sympy_to_pyq","title":"<code>sympy_to_pyq(expr)</code>","text":"<p>Convert sympy expression to pyqtorch ConcretizedCallable object.</p> PARAMETER DESCRIPTION <code>expr</code> <p>sympy expression</p> <p> TYPE: <code>Expr</code> </p> RETURNS DESCRIPTION <code>ConcretizedCallable</code> <p>expression encoded as ConcretizedCallable</p> <p> TYPE: <code>ConcretizedCallable | Tensor</code> </p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def sympy_to_pyq(expr: sympy.Expr) -&gt; ConcretizedCallable | Tensor:\n    \"\"\"Convert sympy expression to pyqtorch ConcretizedCallable object.\n\n    Args:\n        expr (sympy.Expr): sympy expression\n\n    Returns:\n        ConcretizedCallable: expression encoded as ConcretizedCallable\n    \"\"\"\n\n    # base case - independent argument\n    if len(expr.args) == 0:\n        try:\n            res = torch.as_tensor(float(expr))\n        except Exception as e:\n            res = str(expr)\n\n            if \"/\" in res:  # Found a rational\n                res = torch.as_tensor(float(sympy.Rational(res).evalf()))\n        return res\n\n    # Recursively iterate through current function arguments\n    all_results = []\n    for arg in expr.args:\n        res = sympy_to_pyq(arg)\n        all_results.append(res)\n\n    # deal with multi-argument (&gt;2) sympy functions: converting to nested\n    # ConcretizedCallable objects\n    if len(all_results) &gt; 2:\n\n        def fn(x: str | ConcretizedCallable, y: str | ConcretizedCallable) -&gt; Callable:\n            return partial(ConcretizedCallable, call_name=SYMPY_TO_PYQ_MAPPING[expr.func])(  # type: ignore [no-any-return]\n                abstract_args=[x, y]\n            )\n\n        concretized_callable = reduce(fn, all_results)\n    else:\n        concretized_callable = ConcretizedCallable(SYMPY_TO_PYQ_MAPPING[expr.func], all_results)\n    return concretized_callable\n</code></pre>"},{"location":"content/backends/","title":"Backends","text":"<p>Backends allow execution of Qadence abstract quantum circuits. They could be chosen from a variety of simulators, emulators and hardware and can enable circuit differentiability. The primary way to interact and configure a backend is via the high-level API <code>QuantumModel</code>.</p> <p>Not all backends are equivalent</p> <p>Not all backends support the same set of operations, especially while executing analog blocks. Qadence will throw descriptive errors in such cases.</p>"},{"location":"content/backends/#execution-backends","title":"Execution backends","text":"<p>PyQTorch: An efficient, large-scale simulator designed for quantum machine learning, seamlessly integrated with the popular PyTorch deep learning framework for automatic differentiability. It also offers analog computing for time-(in)dependent pulses. See <code>PyQTorchBackend</code>.</p> <p>Pulser: A Python library for pulse-level/analog control of neutral atom devices. Execution via QuTiP. See <code>PulserBackend</code>.</p> <p>More: Proprietary Qadence extensions provide more high-performance backends based on tensor networks or differentiation engines. For more enquiries, please contact: <code>info@pasqal.com</code>.</p>"},{"location":"content/backends/#differentiation-backend","title":"Differentiation backend","text":"<p>The <code>DifferentiableBackend</code> class enables different differentiation modes for the given backend. This can be chosen from two types:</p> <ul> <li>Automatic differentiation (AD): available for PyTorch based backends (PyQTorch).</li> <li>Parameter Shift Rules (PSR): available for all backends. See this section for more information on differentiability and PSR.</li> </ul> <p>In practice, only a <code>diff_mode</code> should be provided in the <code>QuantumModel</code>. Please note that <code>diff_mode</code> defaults to <code>None</code>:</p> <pre><code>import sympy\nimport torch\nfrom qadence import Parameter, RX, RZ, Z, CNOT, QuantumCircuit, QuantumModel, chain, BackendName, DiffMode\n\nx = Parameter(\"x\", trainable=False)\ny = Parameter(\"y\", trainable=False)\nfm = chain(\n    RX(0, 3 * x),\n    RX(0, x),\n    RZ(1, sympy.exp(y)),\n    RX(0, 3.14),\n    RZ(1, \"theta\")\n)\n\nansatz = CNOT(0, 1)\nblock = chain(fm, ansatz)\n\ncircuit = QuantumCircuit(2, block)\n\nobservable = Z(0)\n\n# DiffMode.GPSR is available for any backend.\n# DiffMode.AD is only available for natively differentiable backends.\nmodel = QuantumModel(circuit, observable, backend=BackendName.PYQTORCH, diff_mode=DiffMode.GPSR)\n\n# Get some values for the feature parameters.\nvalues = {\"x\": (x := torch.tensor([0.5], requires_grad=True)), \"y\": torch.tensor([0.1])}\n\n# Compute expectation.\nexp = model.expectation(values)\n\n# Differentiate the expectation wrt x.\ndexp_dx = torch.autograd.grad(exp, x, torch.ones_like(exp))\n</code></pre> <pre><code>dexp_dx = (tensor([3.6398]),)\n</code></pre>"},{"location":"content/backends/#low-level-backend_factory-interface","title":"Low-level <code>backend_factory</code> interface","text":"<p>Every backend in Qadence inherits from the abstract <code>Backend</code> class: <code>Backend</code> and implement the following methods:</p> <ul> <li><code>run</code>: propagate the initial state according to the quantum circuit and return the final wavefunction object.</li> <li><code>sample</code>: sample from a circuit.</li> <li><code>expectation</code>: computes the expectation of a circuit given an observable.</li> <li><code>convert</code>: convert the abstract <code>QuantumCircuit</code> object to its backend-native representation including a backend specific parameter embedding function.</li> </ul> <p>Backends are purely functional objects which take as input the values for the circuit parameters and return the desired output from a call to a method. In order to use a backend directly, embedded parameters must be supplied as they are returned by the backend specific embedding function.</p> <p>Here is a simple demonstration of the use of the PyQTorch backend to execute a circuit in non-differentiable mode:</p> <pre><code>from qadence import QuantumCircuit, FeatureParameter, RX, RZ, CNOT, hea, chain\n\n# Construct a feature map.\nx = FeatureParameter(\"x\")\nz = FeatureParameter(\"y\")\nfm = chain(RX(0, 3 * x), RZ(1, z), CNOT(0, 1))\n\n# Construct a circuit with an hardware-efficient ansatz.\ncircuit = QuantumCircuit(3, fm, hea(3,1))\n</code></pre> <p>The abstract <code>QuantumCircuit</code> can now be converted to its native representation via the PyQTorch backend.</p> <pre><code>from qadence import backend_factory\n\n# Use only PyQtorch in non-differentiable mode:\nbackend = backend_factory(\"pyqtorch\")\n\n# The `Converted` object\n# (contains a `ConvertedCircuit` with the original and native representation)\nconv = backend.convert(circuit)\n</code></pre> <pre><code>conv.circuit.original = ChainBlock(0,1,2)\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 RX(0) [params: ['3*x']]\n\u2502   \u251c\u2500\u2500 RZ(1) [params: ['y']]\n\u2502   \u2514\u2500\u2500 CNOT(0, 1)\n\u2514\u2500\u2500 ChainBlock(0,1,2) [tag: HEA]\n    \u251c\u2500\u2500 ChainBlock(0,1,2)\n    \u2502   \u251c\u2500\u2500 KronBlock(0,1,2)\n    \u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n    \u2502   \u2502   \u251c\u2500\u2500 RX(1) [params: ['theta_1']]\n    \u2502   \u2502   \u2514\u2500\u2500 RX(2) [params: ['theta_2']]\n    \u2502   \u251c\u2500\u2500 KronBlock(0,1,2)\n    \u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_3']]\n    \u2502   \u2502   \u251c\u2500\u2500 RY(1) [params: ['theta_4']]\n    \u2502   \u2502   \u2514\u2500\u2500 RY(2) [params: ['theta_5']]\n    \u2502   \u2514\u2500\u2500 KronBlock(0,1,2)\n    \u2502       \u251c\u2500\u2500 RX(0) [params: ['theta_6']]\n    \u2502       \u251c\u2500\u2500 RX(1) [params: ['theta_7']]\n    \u2502       \u2514\u2500\u2500 RX(2) [params: ['theta_8']]\n    \u2514\u2500\u2500 ChainBlock(0,1,2)\n        \u251c\u2500\u2500 KronBlock(0,1)\n        \u2502   \u2514\u2500\u2500 CNOT(0, 1)\n        \u2514\u2500\u2500 KronBlock(1,2)\n            \u2514\u2500\u2500 CNOT(1, 2)\nconv.circuit.native = QuantumCircuit(\n  (operations): ModuleList(\n    (0): Sequence(\n      (operations): ModuleList(\n        (0): Sequence(\n          (operations): ModuleList(\n            (0): RX(target: (0,), param: 830af221-5b50-411c-aed1-1ba91a919a85)\n            (1): RZ(target: (1,), param: aaf2f95b-cb3e-4c1c-a5c3-66fe442e4b77)\n            (2): CNOT(control: (0,), target: (1,))\n          )\n        )\n        (1): Sequence(\n          (operations): ModuleList(\n            (0): Sequence(\n              (operations): ModuleList(\n                (0): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (0,), param: b40d1b0a-3570-4025-980f-eae1ead879a4)\n                    (1): RY(target: (0,), param: 178c307c-f0d0-46e4-ac7a-d0bcc907b148)\n                    (2): RX(target: (0,), param: e3e6d0e6-0f5e-4555-a531-b674622b3bbd)\n                  )\n                )\n                (1): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (1,), param: 206305a8-fab3-4950-bb1e-3785703b8091)\n                    (1): RY(target: (1,), param: 38384b4f-f67f-46dc-9809-fee6d0959745)\n                    (2): RX(target: (1,), param: ce9f0234-ebd7-46fd-a681-c72e7d8e7cb9)\n                  )\n                )\n                (2): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (2,), param: b5b0d188-d247-4b5b-9aa9-311b888dfb1d)\n                    (1): RY(target: (2,), param: 622c21b7-511f-45a7-908b-192ea53ea078)\n                    (2): RX(target: (2,), param: 68f78757-3a9d-4306-8a9f-28ec3c0b1e55)\n                  )\n                )\n              )\n            )\n            (1): Sequence(\n              (operations): ModuleList(\n                (0): Sequence(\n                  (operations): ModuleList(\n                    (0): CNOT(control: (0,), target: (1,))\n                  )\n                )\n                (1): Sequence(\n                  (operations): ModuleList(\n                    (0): CNOT(control: (1,), target: (2,))\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n)\n</code></pre> <p>Additionally, <code>Converted</code> contains all fixed and variational parameters, as well as an embedding function which accepts feature parameters to construct a dictionary of circuit native parameters. These are needed as each backend uses a different representation of the circuit parameters:</p> <pre><code>import torch\n\n# Contains fixed parameters and variational (from the HEA)\nconv.params\n\ninputs = {\"x\": torch.tensor([1., 1.]), \"y\":torch.tensor([2., 2.])}\n\n# get all circuit parameters (including feature params)\nembedded = conv.embedding_fn(conv.params, inputs)\n</code></pre> <pre><code>conv.params = {\n  theta_5: tensor([0.5045], requires_grad=True)\n  theta_6: tensor([0.7569], requires_grad=True)\n  theta_8: tensor([0.5915], requires_grad=True)\n  theta_1: tensor([0.2272], requires_grad=True)\n  theta_4: tensor([0.1270], requires_grad=True)\n  theta_0: tensor([0.6965], requires_grad=True)\n  theta_7: tensor([0.7917], requires_grad=True)\n  theta_2: tensor([0.4572], requires_grad=True)\n  theta_3: tensor([0.3075], requires_grad=True)\n}\nembedded = {\n  830af221-5b50-411c-aed1-1ba91a919a85: tensor([3., 3.], grad_fn=&lt;ViewBackward0&gt;)\n  aaf2f95b-cb3e-4c1c-a5c3-66fe442e4b77: tensor([2., 2.])\n  b40d1b0a-3570-4025-980f-eae1ead879a4: tensor([0.6965], grad_fn=&lt;ViewBackward0&gt;)\n  178c307c-f0d0-46e4-ac7a-d0bcc907b148: tensor([0.3075], grad_fn=&lt;ViewBackward0&gt;)\n  e3e6d0e6-0f5e-4555-a531-b674622b3bbd: tensor([0.7569], grad_fn=&lt;ViewBackward0&gt;)\n  206305a8-fab3-4950-bb1e-3785703b8091: tensor([0.2272], grad_fn=&lt;ViewBackward0&gt;)\n  38384b4f-f67f-46dc-9809-fee6d0959745: tensor([0.1270], grad_fn=&lt;ViewBackward0&gt;)\n  ce9f0234-ebd7-46fd-a681-c72e7d8e7cb9: tensor([0.7917], grad_fn=&lt;ViewBackward0&gt;)\n  b5b0d188-d247-4b5b-9aa9-311b888dfb1d: tensor([0.4572], grad_fn=&lt;ViewBackward0&gt;)\n  622c21b7-511f-45a7-908b-192ea53ea078: tensor([0.5045], grad_fn=&lt;ViewBackward0&gt;)\n  68f78757-3a9d-4306-8a9f-28ec3c0b1e55: tensor([0.5915], grad_fn=&lt;ViewBackward0&gt;)\n}\n</code></pre> <p>With the embedded parameters, <code>QuantumModel</code> methods are accessible:</p> <pre><code>output = backend.run(conv.circuit, embedded)\nprint(f\"{output = }\")\n</code></pre> <pre><code>output = tensor([[ 0.2903+0.0211j,  0.1016-0.1596j,  0.2012+0.2426j, -0.1817+0.4495j,\n         -0.4665-0.2817j, -0.3051+0.1800j,  0.0967+0.1646j, -0.1523+0.2511j],\n        [ 0.2903+0.0211j,  0.1016-0.1596j,  0.2012+0.2426j, -0.1817+0.4495j,\n         -0.4665-0.2817j, -0.3051+0.1800j,  0.0967+0.1646j, -0.1523+0.2511j]],\n       grad_fn=&lt;TBackward0&gt;)\n</code></pre>"},{"location":"content/backends/#lower-level-the-backend-representation","title":"Lower-level: the <code>Backend</code> representation","text":"<p>If there is a requirement to work with a specific backend, it is possible to access directly the native circuit. For example, should one wish to use PyQtorch noise features directly instead of using the <code>NoiseHandler</code> interface from Qadence:</p> <pre><code>from pyqtorch.noise import Depolarizing\n\ninputs = {\"x\": torch.rand(1), \"y\":torch.rand(1)}\nembedded = conv.embedding_fn(conv.params, inputs)\n\n# Define a noise channel on qubit 0\nnoise = Depolarizing(0, error_probability=0.1)\n\n# Add noise to circuit\nconv.circuit.native.operations.append(noise)\n</code></pre> <p>When running With noise, one can see that the output is a density matrix:</p> <pre><code>density_result = backend.run(conv.circuit, embedded)\nprint(density_result.shape)\n</code></pre> <pre><code>torch.Size([1, 8, 8])\n</code></pre>"},{"location":"content/block_system/","title":"Block system","text":"<p>Quantum programs in Qadence are constructed using a block-system, with an emphasis on composability of primitive blocks to obtain larger, composite blocks. This functional approach is different from other frameworks which follow a more object-oriented way to construct circuits and express programs.</p>"},{"location":"content/block_system/#primitive-blocks","title":"Primitive blocks","text":"<p>A <code>PrimitiveBlock</code> represents a digital or an analog time-evolution quantum operation applied to a qubit support. Programs can always be decomposed down into a sequence of <code>PrimitiveBlock</code> elements.</p> <p>Two canonical examples of digital primitive blocks are the parametrized <code>RX</code> and the <code>CNOT</code> gates:</p> <pre><code>from qadence import chain, RX, CNOT\n\nrx = RX(0, 0.5)\ncnot = CNOT(0, 1)\n\nblock = chain(rx, cnot)\n</code></pre> %3 4fb39ed81e5047cc8381dfa211d824f2 0 a0cc22ef796748638d45f67da298c98b RX(0.5) 4fb39ed81e5047cc8381dfa211d824f2--a0cc22ef796748638d45f67da298c98b 56e877628bc44195858bdff3d17e38ea 1 94ea82f9722a4dc3a6c3a71d1f9b5b95 a0cc22ef796748638d45f67da298c98b--94ea82f9722a4dc3a6c3a71d1f9b5b95 97fb7ae65ba848c1a9a99eb8493a7bde 94ea82f9722a4dc3a6c3a71d1f9b5b95--97fb7ae65ba848c1a9a99eb8493a7bde cfd798eaa51f4f54a2a7049e90f8a364 8d8e47a135bb4d77a3c7b6bf26090c2f 56e877628bc44195858bdff3d17e38ea--8d8e47a135bb4d77a3c7b6bf26090c2f a35b05700273403bb482cda0831a90cc X 8d8e47a135bb4d77a3c7b6bf26090c2f--a35b05700273403bb482cda0831a90cc a35b05700273403bb482cda0831a90cc--94ea82f9722a4dc3a6c3a71d1f9b5b95 a35b05700273403bb482cda0831a90cc--cfd798eaa51f4f54a2a7049e90f8a364 <p>A list of all available primitive operations can be found here.</p> How to visualize blocks <p>There are two ways to display blocks in a Python interpreter: either as a tree in ASCII format using <code>print</code>:</p> <pre><code>from qadence import X, Y, kron\n\nkron_block = kron(X(0), Y(1))\nprint(kron_block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 Y(1)\n</code></pre> <p>Or using the visualization package:</p> <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nkron_block = kron(X(0), Y(1))\n# display(kron_block)\n</code></pre> %3 1b6696510dd74f54804b2a25d5aff584 0 c437e7c3702f42068e99357f5644e831 X 1b6696510dd74f54804b2a25d5aff584--c437e7c3702f42068e99357f5644e831 e96f8e0fb7864ae3803e20b5f113c161 1 cd06e6eb236c4891a7b7701d9e8e0966 c437e7c3702f42068e99357f5644e831--cd06e6eb236c4891a7b7701d9e8e0966 b77871de27f7418193679647adc9b89f 11804256a47e47499a38fafb4c0e4442 Y e96f8e0fb7864ae3803e20b5f113c161--11804256a47e47499a38fafb4c0e4442 11804256a47e47499a38fafb4c0e4442--b77871de27f7418193679647adc9b89f"},{"location":"content/block_system/#composite-blocks","title":"Composite Blocks","text":"<p>Programs can be expressed by composing blocks to result in a larger <code>CompositeBlock</code> using three fundamental operations: chain, kron, and add.</p> <ul> <li>chain applies a set of blocks in sequence, which can have overlapping qubit supports, and results in a <code>ChainBlock</code> type. It is akin to applying a matrix product of the sub-blocks, and can also be used with the <code>*</code> operator.</li> <li>kron applies a set of blocks in parallel, requiring disjoint qubit support, and results in a <code>KronBlock</code> type. This is akin to applying a tensor product of the sub-blocks, and can also be used with the <code>@</code> operator.</li> <li>add performs a direct sum of the operators, and results in an <code>AddBlock</code> type. Blocks constructed this way are typically non-unitary, as is the case for Hamiltonians which can be constructed through sums of Pauli strings. Addition can also be performed directly with the <code>+</code> operator.</li> </ul> <pre><code>from qadence import X, Y, chain, kron\n\nchain_0 = chain(X(0), Y(0))\nchain_1 = chain(X(1), Y(1))\n\nkron_block = kron(chain_0, chain_1)\n</code></pre> %3 07a8a5fb636b4cf982b74d80f7b150f4 0 a9ba2d47e8ab49c8aeacca535b26fa51 X 07a8a5fb636b4cf982b74d80f7b150f4--a9ba2d47e8ab49c8aeacca535b26fa51 d50933627b5a417bab829831d8e121ae 1 a50048ac88444c00919544c3145c37e1 Y a9ba2d47e8ab49c8aeacca535b26fa51--a50048ac88444c00919544c3145c37e1 6920ad085b934da49e4daf937ddacd76 a50048ac88444c00919544c3145c37e1--6920ad085b934da49e4daf937ddacd76 fefcbbcc6f2d4e7dbda514bcbcb351cd 076ca0871bd74ace942be8e496e7c453 X d50933627b5a417bab829831d8e121ae--076ca0871bd74ace942be8e496e7c453 8545673b2d44478e8939eab3b172040e Y 076ca0871bd74ace942be8e496e7c453--8545673b2d44478e8939eab3b172040e 8545673b2d44478e8939eab3b172040e--fefcbbcc6f2d4e7dbda514bcbcb351cd <p>All composition functions support list comprehension syntax. Below we exemplify the creation of an XY Hamiltonian for qubits laid out on a line.</p> <pre><code>from qadence import X, Y, add\n\ndef xy_int(i: int, j: int):\n    return (1/2) * (X(i)@X(j) + Y(i)@Y(j))\n\nn_qubits = 3\n\nxy_ham = add(xy_int(i, i+1) for i in range(n_qubits-1))\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 AddBlock(0,1)\n\u2502       \u251c\u2500\u2500 KronBlock(0,1)\n\u2502       \u2502   \u251c\u2500\u2500 X(0)\n\u2502       \u2502   \u2514\u2500\u2500 X(1)\n\u2502       \u2514\u2500\u2500 KronBlock(0,1)\n\u2502           \u251c\u2500\u2500 Y(0)\n\u2502           \u2514\u2500\u2500 Y(1)\n\u2514\u2500\u2500 [mul: 0.500] \n    \u2514\u2500\u2500 AddBlock(1,2)\n        \u251c\u2500\u2500 KronBlock(1,2)\n        \u2502   \u251c\u2500\u2500 X(1)\n        \u2502   \u2514\u2500\u2500 X(2)\n        \u2514\u2500\u2500 KronBlock(1,2)\n            \u251c\u2500\u2500 Y(1)\n            \u2514\u2500\u2500 Y(2)\n</code></pre> <p>Qadence blocks can be directly translated to matrix form by calling <code>block.tensor()</code>. Note that first dimension is the batch dimension, following PyTorch conventions. This becomes relevant if the block are parameterized and batched input values are passed, as we will see later.</p> <pre><code>from qadence import X, Y\n\nxy = (1/2) * (X(0)@X(1) + Y(0)@Y(1))\n\nprint(xy.tensor().real)\n</code></pre> <pre><code>tensor([[[0., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [0., 1., 0., 0.],\n         [0., 0., 0., 0.]]])\n</code></pre> <p>For a final example of the flexibility of functional block composition, below is an implementation of the Quantum Fourier Transform on an arbitrary qubit support.</p> <pre><code>from qadence import H, CPHASE, PI, chain, kron\n\ndef qft_layer(qs: tuple, l: int):\n    cphases = chain(CPHASE(qs[j], qs[l], PI/2**(j-l)) for j in range(l+1, len(qs)))\n    return H(qs[l]) * cphases\n\ndef qft(qs: tuple):\n    return chain(qft_layer(qs, l) for l in range(len(qs)))\n</code></pre> %3 293ebbfa374b4aedacd499e9d84ac213 0 53c02dd9450e4c87856f1b10c560504c H 293ebbfa374b4aedacd499e9d84ac213--53c02dd9450e4c87856f1b10c560504c de3379a0621c4abcaaa81a7038745137 1 c39ca2c072514a5f875369f5f27f3db9 PHASE(1.571) 53c02dd9450e4c87856f1b10c560504c--c39ca2c072514a5f875369f5f27f3db9 5d108ff0968045f68325a9169b4b65c6 PHASE(0.785) c39ca2c072514a5f875369f5f27f3db9--5d108ff0968045f68325a9169b4b65c6 e9fa11ffd8c84f099a922e722a9b8950 c39ca2c072514a5f875369f5f27f3db9--e9fa11ffd8c84f099a922e722a9b8950 d47ed4f363504c8785e96ac32a0cfc6b 5d108ff0968045f68325a9169b4b65c6--d47ed4f363504c8785e96ac32a0cfc6b 1e7ef39d3aa344f5ba45cc5132588467 5d108ff0968045f68325a9169b4b65c6--1e7ef39d3aa344f5ba45cc5132588467 bd333671f5514caf99d0dc6feb0520d2 d47ed4f363504c8785e96ac32a0cfc6b--bd333671f5514caf99d0dc6feb0520d2 b9b148863a6f49d89f1ae67f3ff31f86 bd333671f5514caf99d0dc6feb0520d2--b9b148863a6f49d89f1ae67f3ff31f86 52ce2dcd347b4a4f9114a913e4ef6655 b9b148863a6f49d89f1ae67f3ff31f86--52ce2dcd347b4a4f9114a913e4ef6655 6a14985b1c6a4a4da1b3cd36a58c277c a40d90db68ac46daa384860e77bcaa55 de3379a0621c4abcaaa81a7038745137--a40d90db68ac46daa384860e77bcaa55 ee6f5e0053b04e53b800e9a41b8ee5df 2 a40d90db68ac46daa384860e77bcaa55--e9fa11ffd8c84f099a922e722a9b8950 33f8b756d6184d94b84badaedbf044cf e9fa11ffd8c84f099a922e722a9b8950--33f8b756d6184d94b84badaedbf044cf 57d8cf80d429402d9e01c9ed4baedf41 H 33f8b756d6184d94b84badaedbf044cf--57d8cf80d429402d9e01c9ed4baedf41 d0775ff13ea64a288377749dd68a04a0 PHASE(1.571) 57d8cf80d429402d9e01c9ed4baedf41--d0775ff13ea64a288377749dd68a04a0 814fc560539342b894fdc3b70bd12df0 d0775ff13ea64a288377749dd68a04a0--814fc560539342b894fdc3b70bd12df0 439dbd3772fe40b7a635c330c5d02486 d0775ff13ea64a288377749dd68a04a0--439dbd3772fe40b7a635c330c5d02486 814fc560539342b894fdc3b70bd12df0--6a14985b1c6a4a4da1b3cd36a58c277c 6c19bcc5c2394bd8a7aca9b3235a9aed dcb9ff3ec2314b9d9f6bab04450f7c6d ee6f5e0053b04e53b800e9a41b8ee5df--dcb9ff3ec2314b9d9f6bab04450f7c6d b5c08a6f53b9497bba9dbc632f03d76e dcb9ff3ec2314b9d9f6bab04450f7c6d--b5c08a6f53b9497bba9dbc632f03d76e b5c08a6f53b9497bba9dbc632f03d76e--1e7ef39d3aa344f5ba45cc5132588467 e1099b5cd4d64c3fbc9ddd38c8214a50 1e7ef39d3aa344f5ba45cc5132588467--e1099b5cd4d64c3fbc9ddd38c8214a50 e1099b5cd4d64c3fbc9ddd38c8214a50--439dbd3772fe40b7a635c330c5d02486 ff6bf0cef1034854a8676f75098b4ac1 H 439dbd3772fe40b7a635c330c5d02486--ff6bf0cef1034854a8676f75098b4ac1 ff6bf0cef1034854a8676f75098b4ac1--6c19bcc5c2394bd8a7aca9b3235a9aed <p>Other functionalities are directly built in the block system. For example, the inverse operation can be created with the <code>dagger()</code> method.</p> <pre><code>qft_inv = qft((0, 1, 2)).dagger()\n</code></pre> %3 f00d6d4615ba41459f55c2d1ee161d7f 0 5f775717d6ec4e1ca0992bfec0d81073 f00d6d4615ba41459f55c2d1ee161d7f--5f775717d6ec4e1ca0992bfec0d81073 bc57915a4a5d4c3084805e6b178497ac 1 228dd41651ee4f75a55c4403b95d15a2 5f775717d6ec4e1ca0992bfec0d81073--228dd41651ee4f75a55c4403b95d15a2 cf0fc59c2d954773a058c8c2af1d4b0d 228dd41651ee4f75a55c4403b95d15a2--cf0fc59c2d954773a058c8c2af1d4b0d cd5047b39bb1439ab7299c72f0c329c7 PHASE(-0.785) cf0fc59c2d954773a058c8c2af1d4b0d--cd5047b39bb1439ab7299c72f0c329c7 9676d22047f340d68576d3ce755c4b1d PHASE(-1.571) cd5047b39bb1439ab7299c72f0c329c7--9676d22047f340d68576d3ce755c4b1d ee10c3329ea54019a065503631274640 cd5047b39bb1439ab7299c72f0c329c7--ee10c3329ea54019a065503631274640 3734e2eabfa84e06b75eab488ffd8cc9 H 9676d22047f340d68576d3ce755c4b1d--3734e2eabfa84e06b75eab488ffd8cc9 6cad4b10c7dc42f597e0e26341df996c 9676d22047f340d68576d3ce755c4b1d--6cad4b10c7dc42f597e0e26341df996c 768a962f0daf44faacf364d07137e8a5 3734e2eabfa84e06b75eab488ffd8cc9--768a962f0daf44faacf364d07137e8a5 33559673181e46c79890c3ed9825796e 1efc645b2f56409bad407c8f3b25bb9d bc57915a4a5d4c3084805e6b178497ac--1efc645b2f56409bad407c8f3b25bb9d 4e0a38479a16431a9982b2e559c6f12a 2 91d98a9a66644b4e838891dd13256d47 PHASE(-1.571) 1efc645b2f56409bad407c8f3b25bb9d--91d98a9a66644b4e838891dd13256d47 92458eb39d5942fdbee86cb7928c8178 H 91d98a9a66644b4e838891dd13256d47--92458eb39d5942fdbee86cb7928c8178 916635a052894369b6faa0b779ce8161 91d98a9a66644b4e838891dd13256d47--916635a052894369b6faa0b779ce8161 097bcc9ce8a9465391e860adf589e435 92458eb39d5942fdbee86cb7928c8178--097bcc9ce8a9465391e860adf589e435 097bcc9ce8a9465391e860adf589e435--6cad4b10c7dc42f597e0e26341df996c d9618c74f3ce4ea2809dc0820f1fef5b 6cad4b10c7dc42f597e0e26341df996c--d9618c74f3ce4ea2809dc0820f1fef5b d9618c74f3ce4ea2809dc0820f1fef5b--33559673181e46c79890c3ed9825796e bcf01d5ffd734fc1baf86191c1004480 9caa0fd849c7458abe8595cfc7a5ab2a H 4e0a38479a16431a9982b2e559c6f12a--9caa0fd849c7458abe8595cfc7a5ab2a 9caa0fd849c7458abe8595cfc7a5ab2a--916635a052894369b6faa0b779ce8161 b4829bf2cc5e4b08bd821e195f198796 916635a052894369b6faa0b779ce8161--b4829bf2cc5e4b08bd821e195f198796 b4829bf2cc5e4b08bd821e195f198796--ee10c3329ea54019a065503631274640 d1b771145967432bb6785db67cc32022 ee10c3329ea54019a065503631274640--d1b771145967432bb6785db67cc32022 5367738311b5468fa95d48d237730642 d1b771145967432bb6785db67cc32022--5367738311b5468fa95d48d237730642 5367738311b5468fa95d48d237730642--bcf01d5ffd734fc1baf86191c1004480"},{"location":"content/block_system/#digital-analog-composition","title":"Digital-analog composition","text":"<p>In Qadence, analog operations are first-class citizens. An analog operation is one whose unitary is best described by the evolution of some hermitian generator, or Hamiltonian, acting on an arbitrary number of qubits. Qadence provides the <code>HamEvo</code> class to initialize analog operations. For a time-independent generator \\(\\mathcal{H}\\) and some time variable \\(t\\), <code>HamEvo(H, t)</code> represents the evolution operator \\(\\exp(-i\\mathcal{H}t)\\).</p> <p>Analog operations constitute a generalization of digital operations, and all digital operations can also be represented as the evolution of some hermitian generator. For example, the <code>RX</code> gate is the evolution of <code>X</code>.</p> <pre><code>from qadence import X, RX, HamEvo, PI\nfrom torch import allclose\n\nangle = PI/2\n\nblock_digital = RX(0, angle)\n\nblock_analog = HamEvo(0.5*X(0), angle)\n\nprint(allclose(block_digital.tensor(), block_analog.tensor()))\n</code></pre> <pre><code>True\n</code></pre> <p>As seen in the previous section, arbitrary Hamiltonians can be constructed using Pauli operators. Their evolution can be combined with other arbitrary digital operations and incorporated into any quantum program.</p> <pre><code>from qadence import X, Y, RX, HamEvo\nfrom qadence import add, kron, PI\n\ndef xy_int(i: int, j: int):\n    return (1/2) * (X(i)@X(j) + Y(i)@Y(j))\n\nn_qubits = 3\n\nxy_ham = add(xy_int(i, i+1) for i in range(n_qubits-1))\n\nanalog_evo = HamEvo(xy_ham, 1.0)\n\ndigital_block = kron(RX(i, i*PI/2) for i in range(n_qubits))\n\nprogram = digital_block * analog_evo * digital_block\n</code></pre> %3 cluster_61d790a5badd424ea0d6dcb1af69e2f0 0b29a60b25504d8eb5ed1257d6789be4 0 dca135cd98d7464ea0a5323a52d686ee RX(0.0) 0b29a60b25504d8eb5ed1257d6789be4--dca135cd98d7464ea0a5323a52d686ee 1f1fb45fc15f4b95bacc2061a6561581 1 d9c2979a09624eb98a616ef70bb887ad HamEvo dca135cd98d7464ea0a5323a52d686ee--d9c2979a09624eb98a616ef70bb887ad aeeb3c58c72e41ad8e0e35a9f7ec780c RX(0.0) d9c2979a09624eb98a616ef70bb887ad--aeeb3c58c72e41ad8e0e35a9f7ec780c ada5cac533154f6f87977a7b535ef986 aeeb3c58c72e41ad8e0e35a9f7ec780c--ada5cac533154f6f87977a7b535ef986 478d11bd99194de38a2105f573f76562 742f172427a7400da6c3e83881b6ea47 RX(1.571) 1f1fb45fc15f4b95bacc2061a6561581--742f172427a7400da6c3e83881b6ea47 3969844de02e405fb8d6f8806251cf25 2 fadc2df22bc44d23ab7e528ba0967dc3 t = 1.000 742f172427a7400da6c3e83881b6ea47--fadc2df22bc44d23ab7e528ba0967dc3 36d80e130a0744548312289d893a5994 RX(1.571) fadc2df22bc44d23ab7e528ba0967dc3--36d80e130a0744548312289d893a5994 36d80e130a0744548312289d893a5994--478d11bd99194de38a2105f573f76562 d6df05fecebc49818c1f830173daaa88 6c26cb9bcf914967a8ef11a5aca73e2a RX(3.142) 3969844de02e405fb8d6f8806251cf25--6c26cb9bcf914967a8ef11a5aca73e2a 13bdab938130425d8fe5a2b19a5cb0c4 6c26cb9bcf914967a8ef11a5aca73e2a--13bdab938130425d8fe5a2b19a5cb0c4 fe2226c87dd0409f9abe19ecb57f03e5 RX(3.142) 13bdab938130425d8fe5a2b19a5cb0c4--fe2226c87dd0409f9abe19ecb57f03e5 fe2226c87dd0409f9abe19ecb57f03e5--d6df05fecebc49818c1f830173daaa88"},{"location":"content/block_system/#block-execution","title":"Block execution","text":"<p>To quickly run block operations and access wavefunctions, samples or expectation values of observables, one can use the convenience functions <code>run</code>, <code>sample</code> and <code>expectation</code>.</p> <pre><code>from qadence import kron, add, H, Z, run, sample, expectation\n\nn_qubits = 2\n\n# Prepares a uniform state\nh_block = kron(H(i) for i in range(n_qubits))\n\nwf = run(h_block)\n\nxs = sample(h_block, n_shots=1000)\n\nobs = add(Z(i) for i in range(n_qubits))\nex = expectation(h_block, obs)\n</code></pre> <pre><code>wf = tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\nxs = [OrderedCounter({'00': 277, '10': 246, '11': 243, '01': 234})]\nex = tensor([[0.]])\n</code></pre>"},{"location":"content/block_system/#execution-via-quantumcircuit-and-quantummodel","title":"Execution via <code>QuantumCircuit</code> and <code>QuantumModel</code>","text":"<p>More fine-grained control and better performance is provided via the high-level <code>QuantumModel</code> abstraction. Quantum programs in Qadence are constructed in two steps:</p> <ol> <li>Build a <code>QuantumCircuit</code> which ties together a composite block and a register.</li> <li>Define a <code>QuantumModel</code> which differentiates, compiles and executes the circuit.</li> </ol> <p>Execution of more complex Qadence programs will be explored in the next tutorials.</p>"},{"location":"content/block_system/#adding-noise-to-gates","title":"Adding noise to gates","text":"<p>It is possible to add noise to gates. Please refer to the noise tutorial here.</p>"},{"location":"content/hamiltonians/","title":"Constructing arbitrary Hamiltonians","text":"<p>At the heart of digital-analog quantum computing is the description and execution of analog blocks, which represent a set of interacting qubits under some interaction Hamiltonian. For this purpose, Qadence relies on the <code>hamiltonian_factory</code> function to create arbitrary Hamiltonian blocks to be used as generators of <code>HamEvo</code> or as observables to be measured.</p>"},{"location":"content/hamiltonians/#arbitrary-all-to-all-hamiltonians","title":"Arbitrary all-to-all Hamiltonians","text":"<p>Arbitrary all-to-all interaction Hamiltonians can be easily created by passing the number of qubits in the first argument. The type of <code>interaction</code> can be chosen from the available ones in the <code>Interaction</code> enum type.</p> <pre><code>from qadence import hamiltonian_factory\nfrom qadence import N, X, Y, Z\nfrom qadence import Interaction\n\nn_qubits = 3\n\nhamilt = hamiltonian_factory(n_qubits, interaction=Interaction.ZZ)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(2)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 Z(1)\n        \u2514\u2500\u2500 Z(2)\n</code></pre> <p>Alternatively, a custom interaction function can also be defined. The input should be two integer indices \\(i\\) and \\(j\\) and it should return a composition of pauli terms representing the interaction between qubits \\(i\\) and \\(j\\):</p> <pre><code>def custom_int(i: int, j: int):\n    return X(i) @ X(j) + Y(i) @ Y(j)\n\nn_qubits = 2\n\nhamilt = hamiltonian_factory(n_qubits, interaction=custom_int)\n</code></pre> <pre><code>AddBlock(0,1)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 AddBlock(0,1)\n        \u251c\u2500\u2500 KronBlock(0,1)\n        \u2502   \u251c\u2500\u2500 X(0)\n        \u2502   \u2514\u2500\u2500 X(1)\n        \u2514\u2500\u2500 KronBlock(0,1)\n            \u251c\u2500\u2500 Y(0)\n            \u2514\u2500\u2500 Y(1)\n</code></pre> <p>Single-qubit terms can also be added by passing the respective operator directly to the <code>detuning</code> argument. For example, the total magnetization is commonly used as an observable to be measured:</p> <pre><code>total_mag = hamiltonian_factory(n_qubits, detuning = Z)\n</code></pre> <pre><code>AddBlock(0,1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 Z(0)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 Z(1)\n</code></pre> <p>For further customization, arbitrary coefficients can be passed as arrays to the <code>interaction_strength</code> and <code>detuning_strength</code> arguments for the two-qubits and single-qubit terms respectively.</p> <pre><code>n_qubits = 3\n\nhamilt = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.ZZ,\n    detuning=Z,\n    interaction_strength=[0.5, 0.2, 0.1],\n    detuning_strength=[0.1, 0.5, -0.3]\n)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 0.100] \n\u2502   \u2514\u2500\u2500 Z(0)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: -0.30] \n\u2502   \u2514\u2500\u2500 Z(2)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: 0.200] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(2)\n\u2514\u2500\u2500 [mul: 0.100] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 Z(1)\n        \u2514\u2500\u2500 Z(2)\n</code></pre> <p>Ordering interaction strengths matters</p> <p>When passing interaction strengths as an array, the ordering must be identical to the one obtained from the <code>edges</code> property of a Qadence <code>Register</code>:</p> <pre><code>from qadence import Register\n\nprint(Register(n_qubits).edges)\n</code></pre> <pre><code>[(0, 1), (0, 2), (1, 2)]\n</code></pre> <p>For one more example, let's create a transverse-field Ising model,</p> <pre><code>n_qubits = 4\nn_edges = int(0.5 * n_qubits * (n_qubits - 1))\n\nz_terms = [1.0] * n_qubits\nzz_terms = [2.0] * n_edges\n\nzz_ham = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.ZZ,\n    detuning=Z,\n    interaction_strength=zz_terms,\n    detuning_strength=z_terms\n)\n\nx_terms = [-1.0] * n_qubits\nx_ham = hamiltonian_factory(n_qubits, detuning = X, detuning_strength = x_terms)\n\ntransverse_ising = zz_ham + x_ham\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 AddBlock(0,1,2,3)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(0)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(1)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(3)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(1)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(3)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502   \u2502       \u251c\u2500\u2500 Z(1)\n\u2502   \u2502       \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(1,3)\n\u2502   \u2502       \u251c\u2500\u2500 Z(1)\n\u2502   \u2502       \u2514\u2500\u2500 Z(3)\n\u2502   \u2514\u2500\u2500 [mul: 2.000] \n\u2502       \u2514\u2500\u2500 KronBlock(2,3)\n\u2502           \u251c\u2500\u2500 Z(2)\n\u2502           \u2514\u2500\u2500 Z(3)\n\u2514\u2500\u2500 AddBlock(0,1,2,3)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(0)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(1)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(2)\n    \u2514\u2500\u2500 [mul: -1.00] \n        \u2514\u2500\u2500 X(3)\n</code></pre> <p>Random interaction coefficients</p> <p>Random interaction coefficients can be chosen between -1 and 1 by simply passing <code>random_strength = True</code> instead of <code>detuning_strength</code> and <code>interaction_strength</code>.</p>"},{"location":"content/hamiltonians/#arbitrary-hamiltonian-topologies","title":"Arbitrary Hamiltonian topologies","text":"<p>Arbitrary interaction topologies can be created using the Qadence <code>Register</code>. Simply pass the register with the desired topology as the first argument to the <code>hamiltonian_factory</code>:</p> <pre><code>from qadence import Register\n\nreg = Register.square(qubits_side=2)\n\nsquare_hamilt = hamiltonian_factory(reg, interaction=Interaction.NN)\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(3)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(2)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(2,3)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(3)\n</code></pre>"},{"location":"content/hamiltonians/#adding-variational-parameters","title":"Adding variational parameters","text":"<p>Finally, fully parameterized Hamiltonians can be created by passing a string to the strength arguments, and used to prefix the name of the variational parameters.</p> <pre><code>n_qubits = 3\n\nnn_ham = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.NN,\n    detuning=N,\n    interaction_strength=\"c\",\n    detuning_strength=\"d\"\n)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: d_0] \n\u2502   \u2514\u2500\u2500 N(0)\n\u251c\u2500\u2500 [mul: d_1] \n\u2502   \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: d_2] \n\u2502   \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: c_01] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: c_02] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(2)\n\u2514\u2500\u2500 [mul: c_12] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 N(1)\n        \u2514\u2500\u2500 N(2)\n</code></pre> <p>Alternatively, fully customizable sympy functions can be passed in an array using the Qadence parameters. Furthermore, the <code>use_all_node_pairs = True</code> option can be passed so that interactions are created for every single node pair in the register, irrespectively of the topology of the edges. This is useful for creating Hamiltonians that depend on qubit distance.</p> <pre><code>from qadence import VariationalParameter, Register\n\n# Square register of 4 qubits with a dimensionless distance of 8.0\nreg = Register.square(2, spacing = 8.0)\n\n# Get the distances between all pairs of qubits\ndistance_dict = reg.distances\n\n# Create interaction strength with variational parameter and 1/r term\nstrength_list = []\nfor node_pair in reg.all_node_pairs:\n    param = VariationalParameter(\"x\" + f\"_{node_pair[0]}{node_pair[1]}\")\n    dist_factor = reg.distances[node_pair]\n    strength_list.append(param / dist_factor)\n\nnn_ham = hamiltonian_factory(\n    reg,\n    interaction=Interaction.NN,\n    interaction_strength=strength_list,\n    use_all_node_pairs=True,\n)\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 [mul: 0.125*x_01] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: 0.088*x_02] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: 0.125*x_03] \n\u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(3)\n\u251c\u2500\u2500 [mul: 0.125*x_12] \n\u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: 0.088*x_13] \n\u2502   \u2514\u2500\u2500 KronBlock(1,3)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(3)\n\u2514\u2500\u2500 [mul: 0.125*x_23] \n    \u2514\u2500\u2500 KronBlock(2,3)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(3)\n</code></pre>"},{"location":"content/overlap/","title":"Wavefunction overlaps","text":"<p>Qadence offers convenience functions for computing the overlap between the wavefunctions generated by two quantum circuits \\(U\\) and \\(W\\) as:</p> \\[ S = |\\langle \\psi_U | \\psi_W \\rangle|^2 \\quad \\textrm{where} \\quad \\psi_U = U|\\psi_0\\rangle \\] <p>Here is an example on how to compute the overlap between two very simple parametric circuits consisting of a single <code>RX</code> rotation on different qubits. The overlap is expected to be non-zero only when the rotation angle is different from \\(\\pi \\; \\textrm{mod}\\; 2\\pi\\) for both rotations:</p> <pre><code>import numpy as np\nfrom torch import tensor\nfrom qadence import Overlap, OverlapMethod, QuantumCircuit, H, RX, X, FeatureParameter, hea, PI\n\n\n# Create two quantum circuits\n# with a single qubit rotation on two random qubits\nn_qubits = 4\nqubits = np.random.choice(n_qubits, n_qubits, replace=False)\n\nphi = FeatureParameter(\"phi\")\ncircuit_bra = QuantumCircuit(n_qubits, RX(qubits[0], phi))\n\npsi = FeatureParameter(\"psi\")\ncircuit_ket = QuantumCircuit(n_qubits, RX(qubits[1], psi))\n\n# Values for the feature parameters\nvalues_bra = {\"phi\": tensor([PI / 2, PI])}\nvalues_ket = {\"psi\": tensor([PI / 2, PI])}\n\n# Calculate overlap by assigning values to the given bra and ket circuits\novrlp = Overlap(circuit_bra, circuit_ket)\novrlp = ovrlp(bra_param_values=values_bra, ket_param_values=values_ket)\n</code></pre> <pre><code>Overlap with exact method:\n tensor([[2.5000e-01, 1.8747e-33],\n        [1.8747e-33, 1.4058e-65]])\n</code></pre> <p>The <code>Overlap</code> class above inherits from <code>QuantumModel</code> and is executed through its inherited forward method for the given input parameter values. By default, the overlap is computed exactly by performing the dot product of the wavefunction propagated from bra and ket circuits.</p> <p>However, it is possible to choose a different method from the <code>OverlapMethod</code> enumeration to be passed via the <code>overlap_method</code> argument in the <code>Overlap</code> initializer. Currently, one can choose from:</p> <ul> <li><code>EXACT</code>: exact computation using the wavefunction matrix representation. Does not work with real devices since it assumes access to the complete qubit system wavefunction.</li> <li><code>COMPUTE_UNCOMPUTE</code>: exact or sampling-based computation using bra \\(U\\) and ket \\(W^{\\dagger}\\) unitaries.</li> <li><code>SWAP_TEST</code>: exact or sampling-based computation using the SWAP test method.</li> <li><code>HADAMARD_TEST</code>: exact or sampling-based computation using the Hadamard test method.</li> <li><code>JENSEN_SHANNON</code>: compute the overlap using the Jensen-Shannon divergence of the two probability distributions obtained by sampling the propagated circuits. This will yield a different result than the other methods.</li> </ul> <p>All methods (except for the <code>EXACT</code> method) take an optional <code>n_shots</code> argument which can be used to perform shot-based calculations.</p> <p>Warning</p> <p>If you select a finite number of shots, the overlap is not differentiable. Therefore, it cannot be used as output of a quantum model if gradients are required.</p> <pre><code># Calculate overlap with SWAP test\novrlp = Overlap(circuit_bra, circuit_ket, method=OverlapMethod.SWAP_TEST)\novrlp_ha = ovrlp(values_bra, values_ket)\n\n# Calculate overlap with SWAP test\n# using a finite number of shots\novrlp = Overlap(circuit_bra, circuit_ket, method=OverlapMethod.SWAP_TEST)\novrlp_ha = ovrlp(values_bra, values_ket, n_shots=10_000)\n</code></pre> <pre><code>Overlap with SWAP test:\n tensor([[ 2.5000e-01, -3.3307e-16],\n        [-3.3307e-16, -4.4409e-16]])\nOverlap with SWAP test with finite number of shots:\n tensor([[ 0.2412, -0.0070],\n        [-0.0106, -0.0046]])\n</code></pre>"},{"location":"content/parameters/","title":"Parametric programs","text":"<p>Qadence provides a flexible parameter system built on top of Sympy. Parameters can be of different types:</p> <ul> <li>Fixed parameter: a constant with a fixed, non-trainable value (e.g. \\(\\dfrac{\\pi}{2}\\)).</li> <li>Variational parameter: a trainable parameter which will be automatically picked up by the optimizer.</li> <li>Feature parameter: a non-trainable parameter which can be used to pass input values.</li> </ul>"},{"location":"content/parameters/#fixed-parameters","title":"Fixed parameters","text":"<p>Passing fixed parameters to blocks can be done by simply passing a Python numeric type or a <code>torch.Tensor</code>.</p> <pre><code>import torch\nfrom qadence import RX, run, PI\n\nwf = run(RX(0, torch.tensor(PI)))\n\nwf = run(RX(0, PI))\n</code></pre> <pre><code>wf = tensor([[6.1232e-17+0.j, 0.0000e+00-1.j]])\nwf = tensor([[6.1232e-17+0.j, 0.0000e+00-1.j]])\n</code></pre>"},{"location":"content/parameters/#variational-parameters","title":"Variational parameters","text":"<p>To parametrize a block a <code>VariationalParameter</code> instance is required. In most cases Qadence also accepts a Python string, which will be used to automatically initialize a <code>VariationalParameter</code>:</p> <pre><code>from qadence import RX, run, VariationalParameter\n\nblock = RX(0, VariationalParameter(\"theta\"))\nblock = RX(0, \"theta\")  # Equivalent\n\nwf = run(block)\n</code></pre> <pre><code>wf = tensor([[0.9599+0.0000j, 0.0000-0.2804j]])\n</code></pre> <p>By calling <code>run</code>, a random value for <code>\"theta\"</code> is initialized at execution. In a <code>QuantumModel</code>, variational parameters are stored in the underlying model parameter dictionary.</p>"},{"location":"content/parameters/#feature-parameters","title":"Feature parameters","text":"<p>A <code>FeatureParameter</code> type can also be used. It requires an input value or a batch of values. In most cases, Qadence accepts a <code>values</code> dictionary to set the input of feature parameters.</p> <pre><code>from torch import tensor\nfrom qadence import RX, PI, run, FeatureParameter\n\nblock = RX(0, FeatureParameter(\"phi\"))\n\nwf = run(block, values = {\"phi\": tensor([PI, PI/2])})\n</code></pre> <pre><code>wf = tensor([[6.1232e-17+0.0000j, 0.0000e+00-1.0000j],\n        [7.0711e-01+0.0000j, 0.0000e+00-0.7071j]])\n</code></pre> <p>Since a batch of input values was passed, the <code>run</code> function returns a batch of output states. Note that <code>FeatureParameter(\"x\")</code> and <code>VariationalParameter(\"x\")</code> are simply aliases for <code>Parameter(\"x\", trainable = False)</code> and <code>Parameter(\"x\", trainable = True)</code>.</p>"},{"location":"content/parameters/#multiparameter-expressions-and-analog-integration","title":"Multiparameter expressions and analog integration","text":"<p>The integration with Sympy becomes useful when one wishes to write arbitrary parameter compositions. Parameters can also be used as scaling coefficients in the block system, which is essential when defining arbitrary analog operations.</p> <pre><code>from torch import tensor\nfrom qadence import RX, Z, HamEvo, PI\nfrom qadence import VariationalParameter, FeatureParameter, run\nfrom sympy import sin\n\ntheta, phi = VariationalParameter(\"theta\"), FeatureParameter(\"phi\")\n\n# Arbitrary parameter composition\nexpr = PI * sin(theta + phi)\n\n# Use as unitary gate arguments\ngate = RX(0, expr)\n\n# Or as scaling coefficients for Hermitian operators\nh_op = expr * (Z(0) @ Z(1))\n\nwf = run(gate * HamEvo(h_op, 1.0), values = {\"phi\": tensor(PI)})\n</code></pre> <pre><code>wf = tensor([[-0.2707+0.3371j,  0.0000+0.0000j,  0.7031-0.5645j,  0.0000+0.0000j]])\n</code></pre>"},{"location":"content/parameters/#parameter-redundancy","title":"Parameter redundancy","text":"<p>Parameters are uniquely defined by their name and redundancy is allowed in composite blocks to assign the same value to different blocks. This is useful, for example, when defining layers of rotation gates typically used as feature maps.</p> <pre><code>from torch import tensor\nfrom qadence import RY, PI, run, kron, FeatureParameter\n\nn_qubits = 3\n\nparam = FeatureParameter(\"phi\")\n\nblock = kron(RY(i, (i+1) * param) for i in range(n_qubits))\n\nwf = run(block, values = {\"phi\": tensor(PI)})\n</code></pre> <pre><code>wf = tensor([[ 1.1248e-32+0.j,  6.1232e-17+0.j, -1.3775e-48+0.j, -7.4988e-33+0.j,\n          1.8370e-16+0.j,  1.0000e+00+0.j, -2.2496e-32+0.j, -1.2246e-16+0.j]])\n</code></pre>"},{"location":"content/parameters/#parametrized-circuits","title":"Parametrized circuits","text":"<p>Let's look at a final example of an arbitrary composition of digital and analog parameterized blocks:</p> <pre><code>import sympy\nfrom qadence import RX, RY, RZ, CNOT, CPHASE, Z, HamEvo\nfrom qadence import run, chain, add, kron, FeatureParameter, VariationalParameter, PI\n\nn_qubits = 3\n\nphi = FeatureParameter(\"\u03a6\")\ntheta = VariationalParameter(\"\u03b8\")\n\nrotation_block = kron(\n    RX(0, phi/theta),\n    RY(1, theta*2),\n    RZ(2, sympy.cos(phi))\n)\ndigital_entangler = CNOT(0, 1) * CPHASE(1, 2, PI)\n\nhamiltonian = add(theta * (Z(i) @ Z(i+1)) for i in range(n_qubits-1))\n\nanalog_evo = HamEvo(hamiltonian, phi)\n\nprogram = chain(rotation_block, digital_entangler, analog_evo)\n</code></pre> %3 cluster_d9c1588ac8d247daae1cd1d390899912 384c5c51a402401ba8561bff932a16f6 0 7c2ed688adbe400cbb70a077f74c8d7e RX(\u03a6/\u03b8) 384c5c51a402401ba8561bff932a16f6--7c2ed688adbe400cbb70a077f74c8d7e 6e87f4f279254b0b87317099abce9788 1 1d135a8203dd41dbba3ecd6555aac966 7c2ed688adbe400cbb70a077f74c8d7e--1d135a8203dd41dbba3ecd6555aac966 e45d743a3e4a46a8998c91bde489de37 1d135a8203dd41dbba3ecd6555aac966--e45d743a3e4a46a8998c91bde489de37 dd697f3514f44a59a273f7098807d9ce HamEvo e45d743a3e4a46a8998c91bde489de37--dd697f3514f44a59a273f7098807d9ce a73897d7e7db4bcfab77e9d3623db423 dd697f3514f44a59a273f7098807d9ce--a73897d7e7db4bcfab77e9d3623db423 5b45527977304d759347332660d3fb08 edfb9de9d5574fda83afe1641331d3ff RY(2*\u03b8) 6e87f4f279254b0b87317099abce9788--edfb9de9d5574fda83afe1641331d3ff ec7785a6304c42c1b33dea05ae99cd5d 2 003e8bc1541a40bbb2d4a094e10c7643 X edfb9de9d5574fda83afe1641331d3ff--003e8bc1541a40bbb2d4a094e10c7643 003e8bc1541a40bbb2d4a094e10c7643--1d135a8203dd41dbba3ecd6555aac966 60c99bd1ca83400f937cb329904cac05 003e8bc1541a40bbb2d4a094e10c7643--60c99bd1ca83400f937cb329904cac05 3b984407025348a6a2b0e568a74ef074 t = \u03a6 60c99bd1ca83400f937cb329904cac05--3b984407025348a6a2b0e568a74ef074 3b984407025348a6a2b0e568a74ef074--5b45527977304d759347332660d3fb08 d060ce933e6d4f8abbbc59ae1ac01865 c470df31398d4ad6b8909fbdbf8561d8 RZ(cos(\u03a6)) ec7785a6304c42c1b33dea05ae99cd5d--c470df31398d4ad6b8909fbdbf8561d8 c36464c541814324b7c9cb56670dbe1d c470df31398d4ad6b8909fbdbf8561d8--c36464c541814324b7c9cb56670dbe1d 13dc47a2b5c74d23bf623b1aaa52a150 PHASE(3.142) c36464c541814324b7c9cb56670dbe1d--13dc47a2b5c74d23bf623b1aaa52a150 13dc47a2b5c74d23bf623b1aaa52a150--60c99bd1ca83400f937cb329904cac05 9e950263509d403d8e697efdfacd8954 13dc47a2b5c74d23bf623b1aaa52a150--9e950263509d403d8e697efdfacd8954 9e950263509d403d8e697efdfacd8954--d060ce933e6d4f8abbbc59ae1ac01865 <p>Please note the different colors for the parametrization with different types. The default palette assigns blue for <code>VariationalParameter</code>, green for <code>FeatureParameter</code>, orange for numeric values, and shaded red for non-parametric gates.</p>"},{"location":"content/qml_constructors/","title":"Quantum machine learning constructors","text":"<p>Besides the arbitrary Hamiltonian constructors, Qadence also provides a complete set of program constructors useful for digital-analog quantum machine learning programs.</p>"},{"location":"content/qml_constructors/#feature-maps","title":"Feature maps","text":"<p>The <code>feature_map</code> function can easily create several types of data-encoding blocks. The two main types of feature maps use a Fourier basis or a Chebyshev basis.</p> <pre><code>from qadence import feature_map, BasisSet, chain\nfrom qadence.draw import display\n\nn_qubits = 3\n\nfourier_fm = feature_map(n_qubits, fm_type=BasisSet.FOURIER)\n\nchebyshev_fm = feature_map(n_qubits, fm_type=BasisSet.CHEBYSHEV)\n\nblock = chain(fourier_fm, chebyshev_fm)\n</code></pre> %3 cluster_506dd28aff0648a7b63f6a68ffc47f61 Constant Chebyshev FM cluster_b0bf56d74c794ddda8ea88fc5ea0845e Constant Fourier FM 8179251e20d9466290c5e9a5885f4e6c 0 ff9a9825b7ff41ae93546debdaa50575 RX(phi) 8179251e20d9466290c5e9a5885f4e6c--ff9a9825b7ff41ae93546debdaa50575 b2ce365126754eb7a543e13a1b7de398 1 99f8312b6db84a319440b77479d4d77c RX(acos(phi)) ff9a9825b7ff41ae93546debdaa50575--99f8312b6db84a319440b77479d4d77c 019e050e436f4446b1e1ce35b4a1d1c3 99f8312b6db84a319440b77479d4d77c--019e050e436f4446b1e1ce35b4a1d1c3 8d0dd4ffd48a43d1838821cfbfac88a5 3a31a0e12c2947269e414436b3733a8e RX(phi) b2ce365126754eb7a543e13a1b7de398--3a31a0e12c2947269e414436b3733a8e d2892ffa11b74ed39cbb6b64914e39c2 2 b23868b8156641a0900d0eb77445cdee RX(acos(phi)) 3a31a0e12c2947269e414436b3733a8e--b23868b8156641a0900d0eb77445cdee b23868b8156641a0900d0eb77445cdee--8d0dd4ffd48a43d1838821cfbfac88a5 39136ca920cc4f2ba35c2b935fe86d22 411ae961b92940b09deccb97dbe552c0 RX(phi) d2892ffa11b74ed39cbb6b64914e39c2--411ae961b92940b09deccb97dbe552c0 fd2b145f42f54690b81b899637d23505 RX(acos(phi)) 411ae961b92940b09deccb97dbe552c0--fd2b145f42f54690b81b899637d23505 fd2b145f42f54690b81b899637d23505--39136ca920cc4f2ba35c2b935fe86d22 <p>A custom encoding function can also be passed with <code>sympy</code></p> <pre><code>from sympy import asin, Function\n\nn_qubits = 3\n\n# Using a pre-defined sympy Function\ncustom_fm_0 = feature_map(n_qubits, fm_type=asin)\n\n# Creating a custom function\ndef custom_fn(x):\n    return asin(x) + x**2\n\ncustom_fm_1 = feature_map(n_qubits, fm_type=custom_fn)\n\nblock = chain(custom_fm_0, custom_fm_1)\n</code></pre> %3 cluster_8deb48cca941436a819f23a4d2704d49 Constant &lt;function custom_fn at 0x7f491c382710&gt; FM cluster_aa6e25379eda4597b604da1238878a8e Constant asin FM 48e0cbc3615c4e05b988fb0ab530d213 0 f686c1140de847329dc9fd0b09701e8e RX(asin(phi)) 48e0cbc3615c4e05b988fb0ab530d213--f686c1140de847329dc9fd0b09701e8e eefc6dce3cb049c6b74d20adec1fbfaa 1 16d91f710c7348f9bbd10d5ea39f2a1c RX(phi**2 + asin(phi)) f686c1140de847329dc9fd0b09701e8e--16d91f710c7348f9bbd10d5ea39f2a1c 10930bcab3064070ac9f6722a214fcd7 16d91f710c7348f9bbd10d5ea39f2a1c--10930bcab3064070ac9f6722a214fcd7 d213baa9f6cf4377b16008a78357fcf9 84636920048848eaa11f94e973c734e5 RX(asin(phi)) eefc6dce3cb049c6b74d20adec1fbfaa--84636920048848eaa11f94e973c734e5 58fe9eb4ac344c78b728b8d5dec2d4f7 2 4c273883c226471b9c9e468259cbad5e RX(phi**2 + asin(phi)) 84636920048848eaa11f94e973c734e5--4c273883c226471b9c9e468259cbad5e 4c273883c226471b9c9e468259cbad5e--d213baa9f6cf4377b16008a78357fcf9 2ddd28883e6e441381714658ba513d0c 02723935031f4e549c1bc1432aa72bdb RX(asin(phi)) 58fe9eb4ac344c78b728b8d5dec2d4f7--02723935031f4e549c1bc1432aa72bdb 5512e10a97a54c2ca2c9babcce55213b RX(phi**2 + asin(phi)) 02723935031f4e549c1bc1432aa72bdb--5512e10a97a54c2ca2c9babcce55213b 5512e10a97a54c2ca2c9babcce55213b--2ddd28883e6e441381714658ba513d0c <p>Furthermore, the <code>reupload_scaling</code> argument can be used to change the scaling applied to each qubit in the support of the feature map. The default scalings can be chosen from the <code>ReuploadScaling</code> enumeration.</p> <pre><code>from qadence import ReuploadScaling\nfrom qadence.draw import display\n\nn_qubits = 5\n\n# Default constant value\nfm_constant = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT)\n\n# Linearly increasing scaling\nfm_tower = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.TOWER)\n\n# Exponentially increasing scaling\nfm_exp = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.EXP)\n\nblock = chain(fm_constant, fm_tower, fm_exp)\n</code></pre> %3 cluster_96bc8f659eb2440fbf48f72878aee3f1 Exponential Fourier FM cluster_74d6a1ebddf343e98e20fe904f04c43c Constant Fourier FM cluster_1cc551be80a0484ea40dcede9a39e61e Tower Fourier FM c0862e58e04e42f69a7edad507638808 0 e7cc834a31774aa1a0f0fe4fb2ce0e40 RX(phi) c0862e58e04e42f69a7edad507638808--e7cc834a31774aa1a0f0fe4fb2ce0e40 1be6f2fe731045feb9b030060433bfcf 1 2c63be6809bd42aabb0a6934390dfb7b RX(1.0*phi) e7cc834a31774aa1a0f0fe4fb2ce0e40--2c63be6809bd42aabb0a6934390dfb7b a1f7678d31e2415a838460483c9b0a35 RX(1.0*phi) 2c63be6809bd42aabb0a6934390dfb7b--a1f7678d31e2415a838460483c9b0a35 08d8065d7caf4b05b21e8ea7814f006e a1f7678d31e2415a838460483c9b0a35--08d8065d7caf4b05b21e8ea7814f006e 0a6f4e5efe5440c9979671e4e15acb4b f386f550c6fe4427bf3cc99d497ba4e5 RX(phi) 1be6f2fe731045feb9b030060433bfcf--f386f550c6fe4427bf3cc99d497ba4e5 12a3efd6ca134c279c09e650a2681eac 2 3758a9c361494c37a391b5b93832c4d4 RX(2.0*phi) f386f550c6fe4427bf3cc99d497ba4e5--3758a9c361494c37a391b5b93832c4d4 680a174988f444a0b30a4e5d742214c7 RX(2.0*phi) 3758a9c361494c37a391b5b93832c4d4--680a174988f444a0b30a4e5d742214c7 680a174988f444a0b30a4e5d742214c7--0a6f4e5efe5440c9979671e4e15acb4b fb69dec3865f49369599a099dbbf66f0 f8bad1fc40eb46778be08fd216dff4d7 RX(phi) 12a3efd6ca134c279c09e650a2681eac--f8bad1fc40eb46778be08fd216dff4d7 83cb5f6d60aa4a10970d9faaea8b10da 3 3f5d4c74fc3e4147bfac5f423204a4c3 RX(3.0*phi) f8bad1fc40eb46778be08fd216dff4d7--3f5d4c74fc3e4147bfac5f423204a4c3 a9ee0da5e58149c6bc7ff91604e53417 RX(4.0*phi) 3f5d4c74fc3e4147bfac5f423204a4c3--a9ee0da5e58149c6bc7ff91604e53417 a9ee0da5e58149c6bc7ff91604e53417--fb69dec3865f49369599a099dbbf66f0 415dd89ae30e4dc6b6b1c4931db4468c fb4078e4e30e4aa59aac134b13e3f3d4 RX(phi) 83cb5f6d60aa4a10970d9faaea8b10da--fb4078e4e30e4aa59aac134b13e3f3d4 d97aa8dddb08448582405337ea31124f 4 39ac79d6bb0e4f8bba2c649c34e12586 RX(4.0*phi) fb4078e4e30e4aa59aac134b13e3f3d4--39ac79d6bb0e4f8bba2c649c34e12586 96203e2fde754db2b8389ae3dc3749c0 RX(8.0*phi) 39ac79d6bb0e4f8bba2c649c34e12586--96203e2fde754db2b8389ae3dc3749c0 96203e2fde754db2b8389ae3dc3749c0--415dd89ae30e4dc6b6b1c4931db4468c 84ebb2882da1438096f4941973c81bcd fd3113e94e4b40abb5cf45bfd06ad10d RX(phi) d97aa8dddb08448582405337ea31124f--fd3113e94e4b40abb5cf45bfd06ad10d c2ceb1c0e1ce4e6199dbb1f9b4555f6a RX(5.0*phi) fd3113e94e4b40abb5cf45bfd06ad10d--c2ceb1c0e1ce4e6199dbb1f9b4555f6a 5824c2b28e5a42b2b1e7d93111d1f255 RX(16.0*phi) c2ceb1c0e1ce4e6199dbb1f9b4555f6a--5824c2b28e5a42b2b1e7d93111d1f255 5824c2b28e5a42b2b1e7d93111d1f255--84ebb2882da1438096f4941973c81bcd <p>A custom scaling can also be defined with a function with an <code>int</code> input and <code>int</code> or <code>float</code> output.</p> <pre><code>n_qubits = 5\n\ndef custom_scaling(i: int) -&gt; int | float:\n    \"\"\"Sqrt(i+1)\"\"\"\n    return (i+1) ** (0.5)\n\n# Custom scaling function\nfm_custom = feature_map(n_qubits, fm_type=BasisSet.CHEBYSHEV, reupload_scaling=custom_scaling)\n</code></pre> %3 9f4cf76f6cd8433c91f75f673b6294b1 0 2e0dcd7bc2e248628a1d605fc07fc279 RX(1.0*acos(phi)) 9f4cf76f6cd8433c91f75f673b6294b1--2e0dcd7bc2e248628a1d605fc07fc279 c5d0b609b30c4cdab81a4eb53bca4bae 1 63afad27cb4f42d681ac261cdc2c8e77 2e0dcd7bc2e248628a1d605fc07fc279--63afad27cb4f42d681ac261cdc2c8e77 462c7ba4455e4d1a8f3ecfb8678a8e04 ae482fdcc6704f3493965ee85ec0d3df RX(1.414*acos(phi)) c5d0b609b30c4cdab81a4eb53bca4bae--ae482fdcc6704f3493965ee85ec0d3df 3e635aec9f40403194f08540d22027d0 2 ae482fdcc6704f3493965ee85ec0d3df--462c7ba4455e4d1a8f3ecfb8678a8e04 6a08ed9b3ed4411b8984f210b52568ae bf9cddc7213d4a78bffb4be62dda0301 RX(1.732*acos(phi)) 3e635aec9f40403194f08540d22027d0--bf9cddc7213d4a78bffb4be62dda0301 14624218d95a4890bac96d5f887f7b79 3 bf9cddc7213d4a78bffb4be62dda0301--6a08ed9b3ed4411b8984f210b52568ae 9ac08ad1d1be4f53bb1c1be92f5d31d4 121a0fb0bd55427ba7f488e9b0d7dd4f RX(2.0*acos(phi)) 14624218d95a4890bac96d5f887f7b79--121a0fb0bd55427ba7f488e9b0d7dd4f cc4d33276fd84543bfcfc29c1a9ccfad 4 121a0fb0bd55427ba7f488e9b0d7dd4f--9ac08ad1d1be4f53bb1c1be92f5d31d4 2202f5f8f9234eb284b1aa223195f4e8 765f639a1bbf4a95b63d19940034c092 RX(2.236*acos(phi)) cc4d33276fd84543bfcfc29c1a9ccfad--765f639a1bbf4a95b63d19940034c092 765f639a1bbf4a95b63d19940034c092--2202f5f8f9234eb284b1aa223195f4e8 <p>To add a trainable parameter that multiplies the feature parameter inside the encoding function, simply pass a <code>param_prefix</code> string:</p> <pre><code>n_qubits = 5\n\nfm_trainable = feature_map(\n    n_qubits,\n    fm_type=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.EXP,\n    param_prefix = \"w\",\n)\n</code></pre> %3 6b7177b173014d06b24d888e8a457b9c 0 7077824feebf48d6a46fdefa4531f8fe RX(1.0*phi*w\u2080) 6b7177b173014d06b24d888e8a457b9c--7077824feebf48d6a46fdefa4531f8fe 4ab204f2523d4461ad317799fba4771b 1 45a9114f8ea746988c8f2fa218dd995b 7077824feebf48d6a46fdefa4531f8fe--45a9114f8ea746988c8f2fa218dd995b 4ba6de14f109414b80ad9cb261bdd62f 01e3fd39e01f41b8bf33f331a374fbab RX(2.0*phi*w\u2081) 4ab204f2523d4461ad317799fba4771b--01e3fd39e01f41b8bf33f331a374fbab 592f42d3f37f4c6ca9fe7ff5abbce636 2 01e3fd39e01f41b8bf33f331a374fbab--4ba6de14f109414b80ad9cb261bdd62f 04b4c9766720495d9c59b568682b5f49 57f88c018ed747dcbd31cbc808ebf1d2 RX(4.0*phi*w\u2082) 592f42d3f37f4c6ca9fe7ff5abbce636--57f88c018ed747dcbd31cbc808ebf1d2 8db5ae74c67b427a88c60512ef2fee9e 3 57f88c018ed747dcbd31cbc808ebf1d2--04b4c9766720495d9c59b568682b5f49 aa0a7376ab204213900a2eedd8ce3af8 155961b911434a9890b543ff7511c5a5 RX(8.0*phi*w\u2083) 8db5ae74c67b427a88c60512ef2fee9e--155961b911434a9890b543ff7511c5a5 aa510485340b493d880c0b082f507727 4 155961b911434a9890b543ff7511c5a5--aa0a7376ab204213900a2eedd8ce3af8 04136252126443bd8a77cef7256b776b 9031c26e197d439f818133c4566cc278 RX(16.0*phi*w\u2084) aa510485340b493d880c0b082f507727--9031c26e197d439f818133c4566cc278 9031c26e197d439f818133c4566cc278--04136252126443bd8a77cef7256b776b <p>Note that for the Fourier feature map, the encoding function is simply \\(f(x)=x\\). For other cases, like the Chebyshev <code>acos()</code> encoding, the trainable parameter may cause the feature value to be outside the domain of the encoding function. This will eventually be fixed by adding range constraints to trainable parameters in Qadence.</p> <p>A full description of the remaining arguments can be found in the <code>feature_map</code> API reference. We provide an example below.</p> <pre><code>from qadence import RY\n\nn_qubits = 5\n\n# Custom scaling function\nfm_full = feature_map(\n    n_qubits = n_qubits,\n    support = tuple(reversed(range(n_qubits))), # Reverse the qubit support to run the scaling from bottom to top\n    param = \"x\", # Change the name of the parameter\n    op = RY, # Change the rotation gate between RX, RY, RZ or PHASE\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.EXP,\n    feature_range = (-1.0, 2.0), # Range from which the input data comes from\n    target_range = (1.0, 3.0), # Range the encoder assumes as the natural range\n    multiplier = 5.0, # Extra multiplier, which can also be a Parameter\n    param_prefix = \"w\", # Add trainable parameters\n)\n</code></pre> %3 dd18a0ec74b645a89d85ce822e14c4e8 0 fad16fda73e44f09916784377eac28d7 RY(80.0*acos(w\u2084*(0.667*x + 1.667))) dd18a0ec74b645a89d85ce822e14c4e8--fad16fda73e44f09916784377eac28d7 12d1b197b278492eabb3b8977712f283 1 71dfd105adc8485e914f6fbd763074e3 fad16fda73e44f09916784377eac28d7--71dfd105adc8485e914f6fbd763074e3 fd7ab06f5fda4a0b8bbe347b42a46689 dff6f54b8aba449e9bec146bd53f5ade RY(40.0*acos(w\u2083*(0.667*x + 1.667))) 12d1b197b278492eabb3b8977712f283--dff6f54b8aba449e9bec146bd53f5ade 924e0514967f47b1817bd75a7481d9de 2 dff6f54b8aba449e9bec146bd53f5ade--fd7ab06f5fda4a0b8bbe347b42a46689 ddea9e04596f4c5d80ca2c3bcc002d27 403a8f307f5d4c038e362eda4bd659b0 RY(20.0*acos(w\u2082*(0.667*x + 1.667))) 924e0514967f47b1817bd75a7481d9de--403a8f307f5d4c038e362eda4bd659b0 16cbe17dc68947c4b203038e93daaa4f 3 403a8f307f5d4c038e362eda4bd659b0--ddea9e04596f4c5d80ca2c3bcc002d27 c21ae6454365406b91d74cc7297c2b0b 7e92a9ae7f214872b511fc263365912e RY(10.0*acos(w\u2081*(0.667*x + 1.667))) 16cbe17dc68947c4b203038e93daaa4f--7e92a9ae7f214872b511fc263365912e f3e11f0ccbc14508934c78fef08d2349 4 7e92a9ae7f214872b511fc263365912e--c21ae6454365406b91d74cc7297c2b0b 06f5879ae4a94f71a666c1990d7d5ad7 7c27c932ae064836b9b98fc10e4ae025 RY(5.0*acos(w\u2080*(0.667*x + 1.667))) f3e11f0ccbc14508934c78fef08d2349--7c27c932ae064836b9b98fc10e4ae025 7c27c932ae064836b9b98fc10e4ae025--06f5879ae4a94f71a666c1990d7d5ad7"},{"location":"content/qml_constructors/#hardware-efficient-ansatz","title":"Hardware-efficient ansatz","text":"<p>Ansatze blocks for quantum machine-learning are typically built following the Hardware-Efficient Ansatz formalism (HEA). Both fully digital and digital-analog HEAs can easily be built with the <code>hea</code> function. By default, the digital version is returned:</p> <pre><code>from qadence import hea\nfrom qadence.draw import display\n\nn_qubits = 3\ndepth = 2\n\nansatz = hea(n_qubits, depth)\n</code></pre> %3 7e891bc99d7e4ce1b3d1a87a094d71c0 0 d54f70aa35db41e6ad484211c586b692 RX(theta\u2080) 7e891bc99d7e4ce1b3d1a87a094d71c0--d54f70aa35db41e6ad484211c586b692 5c25e7b00a2844abb8fdb75f821ff20b 1 f1fe1bcc303b47d8b14b75399011c896 RY(theta\u2083) d54f70aa35db41e6ad484211c586b692--f1fe1bcc303b47d8b14b75399011c896 a6ace2e4892643cdac68f3e3e578d1f0 RX(theta\u2086) f1fe1bcc303b47d8b14b75399011c896--a6ace2e4892643cdac68f3e3e578d1f0 732490ce160d4bb78f1a45221cb822c3 a6ace2e4892643cdac68f3e3e578d1f0--732490ce160d4bb78f1a45221cb822c3 1aa6e8de59874555a9a5c4383e6f022c 732490ce160d4bb78f1a45221cb822c3--1aa6e8de59874555a9a5c4383e6f022c 2bbec2604c7c43b2ae5f2a58a61b7d1e RX(theta\u2089) 1aa6e8de59874555a9a5c4383e6f022c--2bbec2604c7c43b2ae5f2a58a61b7d1e a3f283aca51148b58bc8c9a00ad4d645 RY(theta\u2081\u2082) 2bbec2604c7c43b2ae5f2a58a61b7d1e--a3f283aca51148b58bc8c9a00ad4d645 1cd0e3ba189f42049f28437aea847913 RX(theta\u2081\u2085) a3f283aca51148b58bc8c9a00ad4d645--1cd0e3ba189f42049f28437aea847913 d29821270dd64fc2838713efc18ae5d9 1cd0e3ba189f42049f28437aea847913--d29821270dd64fc2838713efc18ae5d9 925bab62669342c48a5ca4eb00e94ec4 d29821270dd64fc2838713efc18ae5d9--925bab62669342c48a5ca4eb00e94ec4 fb39fa4cb13d401890ae2fa4095b1ebf 925bab62669342c48a5ca4eb00e94ec4--fb39fa4cb13d401890ae2fa4095b1ebf 8c642d09474b474b99ac36b56e8505d9 777f6245c3af43d2b11ed05ba944de32 RX(theta\u2081) 5c25e7b00a2844abb8fdb75f821ff20b--777f6245c3af43d2b11ed05ba944de32 5ee1b93c577142089d63b22cf6784d06 2 6443e2f55a784c69b495b63b08a97ce7 RY(theta\u2084) 777f6245c3af43d2b11ed05ba944de32--6443e2f55a784c69b495b63b08a97ce7 8d91824984ac47e396a58ef02ecf765f RX(theta\u2087) 6443e2f55a784c69b495b63b08a97ce7--8d91824984ac47e396a58ef02ecf765f e040b62732bc42efbe514c593e81758f X 8d91824984ac47e396a58ef02ecf765f--e040b62732bc42efbe514c593e81758f e040b62732bc42efbe514c593e81758f--732490ce160d4bb78f1a45221cb822c3 27827c78f5ba42fc8d6d40e060e309b6 e040b62732bc42efbe514c593e81758f--27827c78f5ba42fc8d6d40e060e309b6 ad0dff40d8af4bb2941e79f327a43c0a RX(theta\u2081\u2080) 27827c78f5ba42fc8d6d40e060e309b6--ad0dff40d8af4bb2941e79f327a43c0a 0f899adfb2894ae1881023d2cf94f710 RY(theta\u2081\u2083) ad0dff40d8af4bb2941e79f327a43c0a--0f899adfb2894ae1881023d2cf94f710 50df27cc3518481a92cd064867105756 RX(theta\u2081\u2086) 0f899adfb2894ae1881023d2cf94f710--50df27cc3518481a92cd064867105756 2ddc16944f1540fdbcbcd9be694f6c30 X 50df27cc3518481a92cd064867105756--2ddc16944f1540fdbcbcd9be694f6c30 2ddc16944f1540fdbcbcd9be694f6c30--d29821270dd64fc2838713efc18ae5d9 8f23215f4ac448c6b67520b387c1ab1f 2ddc16944f1540fdbcbcd9be694f6c30--8f23215f4ac448c6b67520b387c1ab1f 8f23215f4ac448c6b67520b387c1ab1f--8c642d09474b474b99ac36b56e8505d9 4ac832d37d7d40a0a90142fe5a5afd2d ebeb8d8f622343359b6b5dc19da8654e RX(theta\u2082) 5ee1b93c577142089d63b22cf6784d06--ebeb8d8f622343359b6b5dc19da8654e 0aa70047b6014ed88466448362b341ad RY(theta\u2085) ebeb8d8f622343359b6b5dc19da8654e--0aa70047b6014ed88466448362b341ad 94f2447c60bd4139b9f0d8e9409d97fd RX(theta\u2088) 0aa70047b6014ed88466448362b341ad--94f2447c60bd4139b9f0d8e9409d97fd f56f67ba92e94d4eaa09445783ba6887 94f2447c60bd4139b9f0d8e9409d97fd--f56f67ba92e94d4eaa09445783ba6887 0b1e75a5607f424ca59e189f902d8b07 X f56f67ba92e94d4eaa09445783ba6887--0b1e75a5607f424ca59e189f902d8b07 0b1e75a5607f424ca59e189f902d8b07--27827c78f5ba42fc8d6d40e060e309b6 90daaa42d9934060994280eceabf5b7d RX(theta\u2081\u2081) 0b1e75a5607f424ca59e189f902d8b07--90daaa42d9934060994280eceabf5b7d 6b49f12a78144c63b89f6f0c97357d80 RY(theta\u2081\u2084) 90daaa42d9934060994280eceabf5b7d--6b49f12a78144c63b89f6f0c97357d80 c556539210a343fda91b137e0b0f812b RX(theta\u2081\u2087) 6b49f12a78144c63b89f6f0c97357d80--c556539210a343fda91b137e0b0f812b c02139374b5d46c7ae47160e938e3425 c556539210a343fda91b137e0b0f812b--c02139374b5d46c7ae47160e938e3425 e4e6a768a6304f7ab0138e4995854e39 X c02139374b5d46c7ae47160e938e3425--e4e6a768a6304f7ab0138e4995854e39 e4e6a768a6304f7ab0138e4995854e39--8f23215f4ac448c6b67520b387c1ab1f e4e6a768a6304f7ab0138e4995854e39--4ac832d37d7d40a0a90142fe5a5afd2d <p>As seen above, the rotation layers are automatically parameterized, and the prefix <code>\"theta\"</code> can be changed with the <code>param_prefix</code> argument.</p> <p>Furthermore, both the single-qubit rotations and the two-qubit entangler can be customized with the <code>operations</code> and <code>entangler</code> argument. The operations can be passed as a list of single-qubit rotations, while the entangler should be either <code>CNOT</code>, <code>CZ</code>, <code>CRX</code>, <code>CRY</code>, <code>CRZ</code> or <code>CPHASE</code>.</p> <pre><code>from qadence import RX, RY, CPHASE\n\nansatz = hea(\n    n_qubits=n_qubits,\n    depth=depth,\n    param_prefix=\"phi\",\n    operations=[RX, RY, RX],\n    entangler=CPHASE\n)\n</code></pre> %3 b1f616ca4fdb40208ada24d20f3e4c9b 0 f88af1f235b149e9b285f5f211d96ab7 RX(phi\u2080) b1f616ca4fdb40208ada24d20f3e4c9b--f88af1f235b149e9b285f5f211d96ab7 65da93b4db72468c8873ae13d2691798 1 c93e7c28f2204e6299020ec9f0530678 RY(phi\u2083) f88af1f235b149e9b285f5f211d96ab7--c93e7c28f2204e6299020ec9f0530678 69a3b26665ee4a569925c90a4d0497cd RX(phi\u2086) c93e7c28f2204e6299020ec9f0530678--69a3b26665ee4a569925c90a4d0497cd 47a84a126fa44cd6aec1a2bce1694dd2 69a3b26665ee4a569925c90a4d0497cd--47a84a126fa44cd6aec1a2bce1694dd2 15576694b7334b9190d17e9eff04bdfb 47a84a126fa44cd6aec1a2bce1694dd2--15576694b7334b9190d17e9eff04bdfb 2018c964b7574233b67df835c0c87ab2 RX(phi\u2089) 15576694b7334b9190d17e9eff04bdfb--2018c964b7574233b67df835c0c87ab2 91c13105d6594a839a501d956ac5d4aa RY(phi\u2081\u2082) 2018c964b7574233b67df835c0c87ab2--91c13105d6594a839a501d956ac5d4aa c10e1dc5f7e64675bc011e39e135a690 RX(phi\u2081\u2085) 91c13105d6594a839a501d956ac5d4aa--c10e1dc5f7e64675bc011e39e135a690 0e53241aac2542c4a37e66e45ca6dd5c c10e1dc5f7e64675bc011e39e135a690--0e53241aac2542c4a37e66e45ca6dd5c d3ded36e53d24d0385c3cb07024ec368 0e53241aac2542c4a37e66e45ca6dd5c--d3ded36e53d24d0385c3cb07024ec368 d7a299a19f994c6daa339399f0dac6a9 d3ded36e53d24d0385c3cb07024ec368--d7a299a19f994c6daa339399f0dac6a9 25b54e2160814b4ebce181da2ed33084 d31d2ccf22b34e6cb89e84c7bb65716c RX(phi\u2081) 65da93b4db72468c8873ae13d2691798--d31d2ccf22b34e6cb89e84c7bb65716c 8af77f5df2274ac895be7bd7fd3017d1 2 124695bf3f9842d4abd8dbbfef75ed76 RY(phi\u2084) d31d2ccf22b34e6cb89e84c7bb65716c--124695bf3f9842d4abd8dbbfef75ed76 084f45567c8c46108f032b343e2d3a35 RX(phi\u2087) 124695bf3f9842d4abd8dbbfef75ed76--084f45567c8c46108f032b343e2d3a35 b14edbcbe4154fe2ae5e2f370366b5b2 PHASE(phi_ent\u2080) 084f45567c8c46108f032b343e2d3a35--b14edbcbe4154fe2ae5e2f370366b5b2 b14edbcbe4154fe2ae5e2f370366b5b2--47a84a126fa44cd6aec1a2bce1694dd2 b5ffb76aed3a45d1ab700802443ddc42 b14edbcbe4154fe2ae5e2f370366b5b2--b5ffb76aed3a45d1ab700802443ddc42 80e70737127c48d78035e120f8663ed5 RX(phi\u2081\u2080) b5ffb76aed3a45d1ab700802443ddc42--80e70737127c48d78035e120f8663ed5 b046ac196ea84c4eab02936da9c87644 RY(phi\u2081\u2083) 80e70737127c48d78035e120f8663ed5--b046ac196ea84c4eab02936da9c87644 7195df55ab0a4e658318590bcac16c1c RX(phi\u2081\u2086) b046ac196ea84c4eab02936da9c87644--7195df55ab0a4e658318590bcac16c1c a4f596ce13c4469688c6ec8eddc1c6ad PHASE(phi_ent\u2082) 7195df55ab0a4e658318590bcac16c1c--a4f596ce13c4469688c6ec8eddc1c6ad a4f596ce13c4469688c6ec8eddc1c6ad--0e53241aac2542c4a37e66e45ca6dd5c bd481f71a07341a9995b574ee807bd3c a4f596ce13c4469688c6ec8eddc1c6ad--bd481f71a07341a9995b574ee807bd3c bd481f71a07341a9995b574ee807bd3c--25b54e2160814b4ebce181da2ed33084 c1f7141c796a4471b465db0af7325d4a e9a872fc916e4d7692be7a58dbef5f9a RX(phi\u2082) 8af77f5df2274ac895be7bd7fd3017d1--e9a872fc916e4d7692be7a58dbef5f9a af67bde03dab435a927814ca2e64b8d6 RY(phi\u2085) e9a872fc916e4d7692be7a58dbef5f9a--af67bde03dab435a927814ca2e64b8d6 b3904681f0fe466488d3e9fd3adf153e RX(phi\u2088) af67bde03dab435a927814ca2e64b8d6--b3904681f0fe466488d3e9fd3adf153e 6819aecef2964507b0b2e1ee49f5d5b9 b3904681f0fe466488d3e9fd3adf153e--6819aecef2964507b0b2e1ee49f5d5b9 8025478be82444cf870dd7ea903c7fe7 PHASE(phi_ent\u2081) 6819aecef2964507b0b2e1ee49f5d5b9--8025478be82444cf870dd7ea903c7fe7 8025478be82444cf870dd7ea903c7fe7--b5ffb76aed3a45d1ab700802443ddc42 9c82205bbf294f08923a8a8b498b55bf RX(phi\u2081\u2081) 8025478be82444cf870dd7ea903c7fe7--9c82205bbf294f08923a8a8b498b55bf e29903675f8a4e97875ae6001652c116 RY(phi\u2081\u2084) 9c82205bbf294f08923a8a8b498b55bf--e29903675f8a4e97875ae6001652c116 233334224766446594abc8696e1fa7a0 RX(phi\u2081\u2087) e29903675f8a4e97875ae6001652c116--233334224766446594abc8696e1fa7a0 4096209234554deca0deb3de2c0e85f0 233334224766446594abc8696e1fa7a0--4096209234554deca0deb3de2c0e85f0 0a5511d89d6c42e9a880a1e75b3fe0a5 PHASE(phi_ent\u2083) 4096209234554deca0deb3de2c0e85f0--0a5511d89d6c42e9a880a1e75b3fe0a5 0a5511d89d6c42e9a880a1e75b3fe0a5--bd481f71a07341a9995b574ee807bd3c 0a5511d89d6c42e9a880a1e75b3fe0a5--c1f7141c796a4471b465db0af7325d4a <p>Having a truly hardware-efficient ansatz means that the entangling operation can be chosen according to each device's native interactions. Besides digital operations, in Qadence it is also possible to build digital-analog HEAs with the entanglement produced by the natural evolution of a set of interacting qubits, as natively implemented in neutral atom devices. As with other digital-analog functions, this can be controlled with the <code>strategy</code> argument which can be chosen from the <code>Strategy</code> enum type. Currently, only <code>Strategy.DIGITAL</code> and <code>Strategy.SDAQC</code> are available. By default, calling <code>strategy = Strategy.SDAQC</code> will use a global entangling Hamiltonian with Ising-like \\(NN\\) interactions and constant interaction strength,</p> <pre><code>from qadence import Strategy\n\nansatz = hea(\n    n_qubits,\n    depth=depth,\n    strategy=Strategy.SDAQC\n)\n</code></pre> %3 cluster_a83973e8b7e649c99adce6ee25218c9c cluster_51f61fbed12143b5a41e7b556464e584 3a4e2b2304f54aefa017ae24b9e60650 0 2b39a08a17b54f76b00e9ae159752b13 RX(theta\u2080) 3a4e2b2304f54aefa017ae24b9e60650--2b39a08a17b54f76b00e9ae159752b13 bec3012000f84912a5a0e3d1a568d61d 1 feb6db11e5da468ba9e772a7500a8317 RY(theta\u2083) 2b39a08a17b54f76b00e9ae159752b13--feb6db11e5da468ba9e772a7500a8317 29c7c70d3be84d57b9b5245797aff97a RX(theta\u2086) feb6db11e5da468ba9e772a7500a8317--29c7c70d3be84d57b9b5245797aff97a 501ad047f2724551840d91f894c05a92 HamEvo 29c7c70d3be84d57b9b5245797aff97a--501ad047f2724551840d91f894c05a92 a62d7725dbd84063ba2e07dff3fa1a69 RX(theta\u2089) 501ad047f2724551840d91f894c05a92--a62d7725dbd84063ba2e07dff3fa1a69 119a3df1f5c944ef9a65e28e0009c3ab RY(theta\u2081\u2082) a62d7725dbd84063ba2e07dff3fa1a69--119a3df1f5c944ef9a65e28e0009c3ab 059b841c3ea042849ebcb67330d6a62a RX(theta\u2081\u2085) 119a3df1f5c944ef9a65e28e0009c3ab--059b841c3ea042849ebcb67330d6a62a 9fbe6c0af62649728f4a3d83dbf91506 HamEvo 059b841c3ea042849ebcb67330d6a62a--9fbe6c0af62649728f4a3d83dbf91506 b7c54132c6b344dda1a9c688ab1b1673 9fbe6c0af62649728f4a3d83dbf91506--b7c54132c6b344dda1a9c688ab1b1673 1a8ef1d4301b46769055c77cc6aeb9d9 2dfb704984474cecb0b174eba5953454 RX(theta\u2081) bec3012000f84912a5a0e3d1a568d61d--2dfb704984474cecb0b174eba5953454 3ff851b2f2884247b87bd3d827493c07 2 1e2aa38de1e247feb7df62a466c2ac58 RY(theta\u2084) 2dfb704984474cecb0b174eba5953454--1e2aa38de1e247feb7df62a466c2ac58 c6f854ee30014331b3519e4af4c592b1 RX(theta\u2087) 1e2aa38de1e247feb7df62a466c2ac58--c6f854ee30014331b3519e4af4c592b1 b60491d4115f47da96a571cec1eb99b3 t = theta_t\u2080 c6f854ee30014331b3519e4af4c592b1--b60491d4115f47da96a571cec1eb99b3 b8d13a0d02334e0a8ad3ca1f35334c35 RX(theta\u2081\u2080) b60491d4115f47da96a571cec1eb99b3--b8d13a0d02334e0a8ad3ca1f35334c35 caeb1feeaef449deb98cc6be120596b3 RY(theta\u2081\u2083) b8d13a0d02334e0a8ad3ca1f35334c35--caeb1feeaef449deb98cc6be120596b3 76b1597650794530ad508755cd6694d9 RX(theta\u2081\u2086) caeb1feeaef449deb98cc6be120596b3--76b1597650794530ad508755cd6694d9 e56f8678fc61485b80ef29dcb3a916d9 t = theta_t\u2081 76b1597650794530ad508755cd6694d9--e56f8678fc61485b80ef29dcb3a916d9 e56f8678fc61485b80ef29dcb3a916d9--1a8ef1d4301b46769055c77cc6aeb9d9 3d968c8c638e45be99da9aa5911eee03 320a32b818ef46558fc8623e6c16d47b RX(theta\u2082) 3ff851b2f2884247b87bd3d827493c07--320a32b818ef46558fc8623e6c16d47b da3b74f31ca54e77ac39b2b1fd6852f0 RY(theta\u2085) 320a32b818ef46558fc8623e6c16d47b--da3b74f31ca54e77ac39b2b1fd6852f0 adbf9949494f4ca0bac01e757fac4e24 RX(theta\u2088) da3b74f31ca54e77ac39b2b1fd6852f0--adbf9949494f4ca0bac01e757fac4e24 939d40ae14c44d6d9e472d2b9a261c64 adbf9949494f4ca0bac01e757fac4e24--939d40ae14c44d6d9e472d2b9a261c64 5942dd7a4afa46a98462a825242de9d3 RX(theta\u2081\u2081) 939d40ae14c44d6d9e472d2b9a261c64--5942dd7a4afa46a98462a825242de9d3 37f437a51f5d4374840b821ce9920721 RY(theta\u2081\u2084) 5942dd7a4afa46a98462a825242de9d3--37f437a51f5d4374840b821ce9920721 1f31053e4f6548fba2f458e08227b8c4 RX(theta\u2081\u2087) 37f437a51f5d4374840b821ce9920721--1f31053e4f6548fba2f458e08227b8c4 62425793e67c4ec686f80302ff729604 1f31053e4f6548fba2f458e08227b8c4--62425793e67c4ec686f80302ff729604 62425793e67c4ec686f80302ff729604--3d968c8c638e45be99da9aa5911eee03 <p>Note that, by default, only the time-parameter is automatically parameterized when building a digital-analog HEA. However, as described in the Hamiltonians tutorial, arbitrary interaction Hamiltonians can be easily built with the <code>hamiltonian_factory</code> function, with both customized or fully parameterized interactions, and these can be directly passed as the <code>entangler</code> for a customizable digital-analog HEA.</p> <pre><code>from qadence import hamiltonian_factory, Interaction, N, Register, hea\n\n# Build a parameterized neutral-atom Hamiltonian following a honeycomb_lattice:\nregister = Register.honeycomb_lattice(1, 1)\n\nentangler = hamiltonian_factory(\n    register,\n    interaction=Interaction.NN,\n    detuning=N,\n    interaction_strength=\"e\",\n    detuning_strength=\"n\"\n)\n\n# Build a fully parameterized Digital-Analog HEA:\nn_qubits = register.n_qubits\ndepth = 2\n\nansatz = hea(\n    n_qubits=register.n_qubits,\n    depth=depth,\n    operations=[RX, RY, RX],\n    entangler=entangler,\n    strategy=Strategy.SDAQC\n)\n</code></pre> %3 cluster_eb8cbe3ce7a0412e8caa00f9249b5fe8 cluster_a7f326aaa68f4d5e9850bea0a9f95f40 76ed5e59828c4659a40433c9c03d224c 0 7905074ab2c34153a9268d4ede218f55 RX(theta\u2080) 76ed5e59828c4659a40433c9c03d224c--7905074ab2c34153a9268d4ede218f55 2d0202abd97149b081134a448cde7015 1 18cbb4f49aa94c7ca6047b4f5f2ddfdb RY(theta\u2086) 7905074ab2c34153a9268d4ede218f55--18cbb4f49aa94c7ca6047b4f5f2ddfdb 9084f06d63304db08f226a740b1ec172 RX(theta\u2081\u2082) 18cbb4f49aa94c7ca6047b4f5f2ddfdb--9084f06d63304db08f226a740b1ec172 e120e02873f1498986b85c25f1b30884 9084f06d63304db08f226a740b1ec172--e120e02873f1498986b85c25f1b30884 d8793cd2608a468c856b7fc6f7ea943d RX(theta\u2081\u2088) e120e02873f1498986b85c25f1b30884--d8793cd2608a468c856b7fc6f7ea943d 2bffba80a65d48748307e2c8acac35d9 RY(theta\u2082\u2084) d8793cd2608a468c856b7fc6f7ea943d--2bffba80a65d48748307e2c8acac35d9 dcb84d8ce1564d3ba8e0e18076d50482 RX(theta\u2083\u2080) 2bffba80a65d48748307e2c8acac35d9--dcb84d8ce1564d3ba8e0e18076d50482 8a640e5d9c09439b812912666571c933 dcb84d8ce1564d3ba8e0e18076d50482--8a640e5d9c09439b812912666571c933 ddcd53eecb164b76b7f722e5e7351c61 8a640e5d9c09439b812912666571c933--ddcd53eecb164b76b7f722e5e7351c61 e1d51ecfcc0b406ba87492f83e9caae9 eb4547b7ae684af4937aa52dd09aaebf RX(theta\u2081) 2d0202abd97149b081134a448cde7015--eb4547b7ae684af4937aa52dd09aaebf a50051f090d247128a355f5beb200971 2 8018690130df48048b8e7753f04b61d4 RY(theta\u2087) eb4547b7ae684af4937aa52dd09aaebf--8018690130df48048b8e7753f04b61d4 d2b83e13d6a9480c9cd6f685de9975fb RX(theta\u2081\u2083) 8018690130df48048b8e7753f04b61d4--d2b83e13d6a9480c9cd6f685de9975fb 7157bee2b0a84c7cb539adae71850ca7 d2b83e13d6a9480c9cd6f685de9975fb--7157bee2b0a84c7cb539adae71850ca7 f94282408d8e437499f672095e4cef20 RX(theta\u2081\u2089) 7157bee2b0a84c7cb539adae71850ca7--f94282408d8e437499f672095e4cef20 6cd205a438d349859b2ce9782af79028 RY(theta\u2082\u2085) f94282408d8e437499f672095e4cef20--6cd205a438d349859b2ce9782af79028 b3d22e785a70468db8415973f815abe3 RX(theta\u2083\u2081) 6cd205a438d349859b2ce9782af79028--b3d22e785a70468db8415973f815abe3 06ced0369d494b438105ba8124cda5e5 b3d22e785a70468db8415973f815abe3--06ced0369d494b438105ba8124cda5e5 06ced0369d494b438105ba8124cda5e5--e1d51ecfcc0b406ba87492f83e9caae9 442293b260cf465b8f4a342e9bdb0603 db9812c8940440fd813e10f5399d3b88 RX(theta\u2082) a50051f090d247128a355f5beb200971--db9812c8940440fd813e10f5399d3b88 411cb5f49a584762b7642e2f87725bb6 3 b14c4d3558de4c52a0b8116a5bc26fa1 RY(theta\u2088) db9812c8940440fd813e10f5399d3b88--b14c4d3558de4c52a0b8116a5bc26fa1 2b298bddeb174b0885b6a56a2632944c RX(theta\u2081\u2084) b14c4d3558de4c52a0b8116a5bc26fa1--2b298bddeb174b0885b6a56a2632944c d6bc5e78232b4ccda3e144826e9d8731 HamEvo 2b298bddeb174b0885b6a56a2632944c--d6bc5e78232b4ccda3e144826e9d8731 a5321586226b48a8ac6437718333a27e RX(theta\u2082\u2080) d6bc5e78232b4ccda3e144826e9d8731--a5321586226b48a8ac6437718333a27e 25cf3b1f87494725b3a85969cc3e54bd RY(theta\u2082\u2086) a5321586226b48a8ac6437718333a27e--25cf3b1f87494725b3a85969cc3e54bd c36d2e74469b41a4b0bc3d8bb63d8d30 RX(theta\u2083\u2082) 25cf3b1f87494725b3a85969cc3e54bd--c36d2e74469b41a4b0bc3d8bb63d8d30 2447ea47ef2f4caca4a8517ba890de8b HamEvo c36d2e74469b41a4b0bc3d8bb63d8d30--2447ea47ef2f4caca4a8517ba890de8b 2447ea47ef2f4caca4a8517ba890de8b--442293b260cf465b8f4a342e9bdb0603 75cb19b5309443fe89f3d5253f0fe508 2e72f954141d46ecaac96e20f2d5e39d RX(theta\u2083) 411cb5f49a584762b7642e2f87725bb6--2e72f954141d46ecaac96e20f2d5e39d e8c63b459ba1498ab8b08823034b38fe 4 591bdb874e834b278fbf86d0aa20e677 RY(theta\u2089) 2e72f954141d46ecaac96e20f2d5e39d--591bdb874e834b278fbf86d0aa20e677 9764a8933287434ca281f94e99ad71b2 RX(theta\u2081\u2085) 591bdb874e834b278fbf86d0aa20e677--9764a8933287434ca281f94e99ad71b2 bce9bfbc37c946c191b834d0097f5dad t = theta_t\u2080 9764a8933287434ca281f94e99ad71b2--bce9bfbc37c946c191b834d0097f5dad a53136c983964ce09f1dfa6a69de3afd RX(theta\u2082\u2081) bce9bfbc37c946c191b834d0097f5dad--a53136c983964ce09f1dfa6a69de3afd 6ed629c0126d4132b30eb74d444a75f9 RY(theta\u2082\u2087) a53136c983964ce09f1dfa6a69de3afd--6ed629c0126d4132b30eb74d444a75f9 8b6edeb388a549749f10194c5c971ffe RX(theta\u2083\u2083) 6ed629c0126d4132b30eb74d444a75f9--8b6edeb388a549749f10194c5c971ffe 8654af75fcb24119bf450acac825e3b4 t = theta_t\u2081 8b6edeb388a549749f10194c5c971ffe--8654af75fcb24119bf450acac825e3b4 8654af75fcb24119bf450acac825e3b4--75cb19b5309443fe89f3d5253f0fe508 c1f14a7b3661477ca5541e10d7635e0c 85f13c2c68164e72955b4349f137ff69 RX(theta\u2084) e8c63b459ba1498ab8b08823034b38fe--85f13c2c68164e72955b4349f137ff69 b864117111af4453af250660d98de619 5 39fa6a251a18488f82e2041110e39899 RY(theta\u2081\u2080) 85f13c2c68164e72955b4349f137ff69--39fa6a251a18488f82e2041110e39899 e599b8ff24b84634b04d938099dcb7e9 RX(theta\u2081\u2086) 39fa6a251a18488f82e2041110e39899--e599b8ff24b84634b04d938099dcb7e9 8655fbfe0ab1498caba94cdaeee89909 e599b8ff24b84634b04d938099dcb7e9--8655fbfe0ab1498caba94cdaeee89909 aed5e19646f74cc6868529018b31535a RX(theta\u2082\u2082) 8655fbfe0ab1498caba94cdaeee89909--aed5e19646f74cc6868529018b31535a d03e0a5f769e488aa734a4664c517c25 RY(theta\u2082\u2088) aed5e19646f74cc6868529018b31535a--d03e0a5f769e488aa734a4664c517c25 b555ccc8f9dd430c86891fa79c5ec509 RX(theta\u2083\u2084) d03e0a5f769e488aa734a4664c517c25--b555ccc8f9dd430c86891fa79c5ec509 0976bdcc31294e188520248247ad2d50 b555ccc8f9dd430c86891fa79c5ec509--0976bdcc31294e188520248247ad2d50 0976bdcc31294e188520248247ad2d50--c1f14a7b3661477ca5541e10d7635e0c b165cf998f09424bbc0239877296eaec 4b81bdbb778844e1bda5d298d9a26cb0 RX(theta\u2085) b864117111af4453af250660d98de619--4b81bdbb778844e1bda5d298d9a26cb0 9c3de41545284f63b66468057e2187fc RY(theta\u2081\u2081) 4b81bdbb778844e1bda5d298d9a26cb0--9c3de41545284f63b66468057e2187fc 6a9f12e0b91542449001fbba09e29d7b RX(theta\u2081\u2087) 9c3de41545284f63b66468057e2187fc--6a9f12e0b91542449001fbba09e29d7b e3c445e36941413a87af378eba3b2355 6a9f12e0b91542449001fbba09e29d7b--e3c445e36941413a87af378eba3b2355 f5686c7d4f424f43aad1d235f88b136f RX(theta\u2082\u2083) e3c445e36941413a87af378eba3b2355--f5686c7d4f424f43aad1d235f88b136f 7502480c7dc94bd0a80f4c851d62ce2e RY(theta\u2082\u2089) f5686c7d4f424f43aad1d235f88b136f--7502480c7dc94bd0a80f4c851d62ce2e c9d974d2a0da4f568f9fe14022057f66 RX(theta\u2083\u2085) 7502480c7dc94bd0a80f4c851d62ce2e--c9d974d2a0da4f568f9fe14022057f66 69c28e2279714eceac493b52f6bbc583 c9d974d2a0da4f568f9fe14022057f66--69c28e2279714eceac493b52f6bbc583 69c28e2279714eceac493b52f6bbc583--b165cf998f09424bbc0239877296eaec"},{"location":"content/qml_constructors/#identity-initialized-ansatz","title":"Identity-initialized ansatz","text":"<p>It is widely known that parametrized quantum circuits are characterized by barren plateaus, where the gradient becomes exponentially small in the number of qubits. Here we include one of many techniques that have been proposed in recent years to mitigate this effect and facilitate <code>QNN</code>s training: Grant et al. showed that initializing the weights of a <code>QNN</code> so that each block of the circuit evaluates to identity reduces the effect of barren plateaus in the initial stage of training. In a similar fashion to <code>hea</code>, such circuit can be created via calling the associated function, <code>identity_initialized_ansatz</code>:</p> <pre><code>from qadence.constructors import identity_initialized_ansatz\nfrom qadence.draw import display\n\nn_qubits = 3\ndepth = 2\n\nansatz = identity_initialized_ansatz(n_qubits, depth)\n</code></pre> %3 cluster_46666f91baa548aebbaf715e1b2f01e3 BPMA-1 cluster_4c53af4b531942c58e7b5289c467bb36 BPMA-0 47cfe747bdc5412997b33390bc3a4a38 0 a17cde668a35411da6537b7a898e0b0e RX(iia_\u03b1\u2080\u2080) 47cfe747bdc5412997b33390bc3a4a38--a17cde668a35411da6537b7a898e0b0e 788f6149130d4089874c6ad6ea58c2f0 1 0154c4e7b03d4654a4ebdc750c78edcd RY(iia_\u03b1\u2080\u2083) a17cde668a35411da6537b7a898e0b0e--0154c4e7b03d4654a4ebdc750c78edcd 86f431a57a334f2a8831be5761d290c3 0154c4e7b03d4654a4ebdc750c78edcd--86f431a57a334f2a8831be5761d290c3 9950032fee7346099838e79bde883860 86f431a57a334f2a8831be5761d290c3--9950032fee7346099838e79bde883860 0300424ba9664ae1a69f5a9b252f6c1c RX(iia_\u03b3\u2080\u2080) 9950032fee7346099838e79bde883860--0300424ba9664ae1a69f5a9b252f6c1c 7cded3bec795407bae864dc3ee79e194 0300424ba9664ae1a69f5a9b252f6c1c--7cded3bec795407bae864dc3ee79e194 38f9f58b550e4bdf8f3d507a2159de25 7cded3bec795407bae864dc3ee79e194--38f9f58b550e4bdf8f3d507a2159de25 ebc9072c4a574dc8940ad972ef246f3b RY(iia_\u03b2\u2080\u2083) 38f9f58b550e4bdf8f3d507a2159de25--ebc9072c4a574dc8940ad972ef246f3b 213291cd496144b7885200d506117d1b RX(iia_\u03b2\u2080\u2080) ebc9072c4a574dc8940ad972ef246f3b--213291cd496144b7885200d506117d1b 65d85d58217c4c378eebc8dcdf484a03 RX(iia_\u03b1\u2081\u2080) 213291cd496144b7885200d506117d1b--65d85d58217c4c378eebc8dcdf484a03 4b607eae992c416e860699b937a23543 RY(iia_\u03b1\u2081\u2083) 65d85d58217c4c378eebc8dcdf484a03--4b607eae992c416e860699b937a23543 7b21d5e331b84fd2befd547d90db723b 4b607eae992c416e860699b937a23543--7b21d5e331b84fd2befd547d90db723b 978ec8c6863147aabfe0b6844bb3504b 7b21d5e331b84fd2befd547d90db723b--978ec8c6863147aabfe0b6844bb3504b 1be5bc96f10f4d4f826604ca9bf3815e RX(iia_\u03b3\u2081\u2080) 978ec8c6863147aabfe0b6844bb3504b--1be5bc96f10f4d4f826604ca9bf3815e 25e7464ef28a4b6c8be14f489da8b222 1be5bc96f10f4d4f826604ca9bf3815e--25e7464ef28a4b6c8be14f489da8b222 0aac8d903e294dd087c4fc813c036878 25e7464ef28a4b6c8be14f489da8b222--0aac8d903e294dd087c4fc813c036878 a870aeac05164cedbd6df19e5a0e3409 RY(iia_\u03b2\u2081\u2083) 0aac8d903e294dd087c4fc813c036878--a870aeac05164cedbd6df19e5a0e3409 828711353fe04b489c311cf8c540a3e9 RX(iia_\u03b2\u2081\u2080) a870aeac05164cedbd6df19e5a0e3409--828711353fe04b489c311cf8c540a3e9 b879b7f1d25541e0bddd55facee2bc6e 828711353fe04b489c311cf8c540a3e9--b879b7f1d25541e0bddd55facee2bc6e e07106e1fc0d421a880f5ce1d18d548a c0df652d58d54568858128911b5c251c RX(iia_\u03b1\u2080\u2081) 788f6149130d4089874c6ad6ea58c2f0--c0df652d58d54568858128911b5c251c 75ebc378d42d40d894123f14671758cd 2 c13de69de442406f9d713d3ad708c3d7 RY(iia_\u03b1\u2080\u2084) c0df652d58d54568858128911b5c251c--c13de69de442406f9d713d3ad708c3d7 80a19f0427e0469780f014fb38c2bdd6 X c13de69de442406f9d713d3ad708c3d7--80a19f0427e0469780f014fb38c2bdd6 80a19f0427e0469780f014fb38c2bdd6--86f431a57a334f2a8831be5761d290c3 eb16a63a98b241c7b73539e8c4ee989c 80a19f0427e0469780f014fb38c2bdd6--eb16a63a98b241c7b73539e8c4ee989c ca2532ba39d14893998efd4f548a8c63 RX(iia_\u03b3\u2080\u2081) eb16a63a98b241c7b73539e8c4ee989c--ca2532ba39d14893998efd4f548a8c63 87a5f933a5b6458c82bc2c3f5a3ece11 ca2532ba39d14893998efd4f548a8c63--87a5f933a5b6458c82bc2c3f5a3ece11 94f00dc9218249aa957c9fe4fce58574 X 87a5f933a5b6458c82bc2c3f5a3ece11--94f00dc9218249aa957c9fe4fce58574 94f00dc9218249aa957c9fe4fce58574--38f9f58b550e4bdf8f3d507a2159de25 7df72d858cff4fca8597ef17ccad42f8 RY(iia_\u03b2\u2080\u2084) 94f00dc9218249aa957c9fe4fce58574--7df72d858cff4fca8597ef17ccad42f8 034470111bef4b6ebb854b83771e298c RX(iia_\u03b2\u2080\u2081) 7df72d858cff4fca8597ef17ccad42f8--034470111bef4b6ebb854b83771e298c fb4901915e54402fa946ed6235033b82 RX(iia_\u03b1\u2081\u2081) 034470111bef4b6ebb854b83771e298c--fb4901915e54402fa946ed6235033b82 a3029b4719de41c0b2ba92a693512334 RY(iia_\u03b1\u2081\u2084) fb4901915e54402fa946ed6235033b82--a3029b4719de41c0b2ba92a693512334 df1462b9036846e2a912856110fff27b X a3029b4719de41c0b2ba92a693512334--df1462b9036846e2a912856110fff27b df1462b9036846e2a912856110fff27b--7b21d5e331b84fd2befd547d90db723b 82962e49e18341f0b4c014269d4cd687 df1462b9036846e2a912856110fff27b--82962e49e18341f0b4c014269d4cd687 65e2ff2ceef44b96b497c08578e9d02d RX(iia_\u03b3\u2081\u2081) 82962e49e18341f0b4c014269d4cd687--65e2ff2ceef44b96b497c08578e9d02d 014a07090c6a470e87e537c56ff5cfdb 65e2ff2ceef44b96b497c08578e9d02d--014a07090c6a470e87e537c56ff5cfdb eac42883b3534902a06516ea41be3b27 X 014a07090c6a470e87e537c56ff5cfdb--eac42883b3534902a06516ea41be3b27 eac42883b3534902a06516ea41be3b27--0aac8d903e294dd087c4fc813c036878 0402e746cb2b4b0ca1586747517396fb RY(iia_\u03b2\u2081\u2084) eac42883b3534902a06516ea41be3b27--0402e746cb2b4b0ca1586747517396fb 4b800533b39e412fb84a40f6b3da7755 RX(iia_\u03b2\u2081\u2081) 0402e746cb2b4b0ca1586747517396fb--4b800533b39e412fb84a40f6b3da7755 4b800533b39e412fb84a40f6b3da7755--e07106e1fc0d421a880f5ce1d18d548a 2c544a47314d47e489f5bdea1e1d4ace 301c1981803242b8a071f664a01bedb3 RX(iia_\u03b1\u2080\u2082) 75ebc378d42d40d894123f14671758cd--301c1981803242b8a071f664a01bedb3 df1bddd3d9524837baf51e37cc7db231 RY(iia_\u03b1\u2080\u2085) 301c1981803242b8a071f664a01bedb3--df1bddd3d9524837baf51e37cc7db231 940ec4c65cc54ce39345b3d64c49e3c3 df1bddd3d9524837baf51e37cc7db231--940ec4c65cc54ce39345b3d64c49e3c3 507c4379afce4e648843478e82535c07 X 940ec4c65cc54ce39345b3d64c49e3c3--507c4379afce4e648843478e82535c07 507c4379afce4e648843478e82535c07--eb16a63a98b241c7b73539e8c4ee989c f55df5235d6b4003b7c2ba7dafc750be RX(iia_\u03b3\u2080\u2082) 507c4379afce4e648843478e82535c07--f55df5235d6b4003b7c2ba7dafc750be 8f325bd00ad840b8afb7714d5f01f0bc X f55df5235d6b4003b7c2ba7dafc750be--8f325bd00ad840b8afb7714d5f01f0bc 8f325bd00ad840b8afb7714d5f01f0bc--87a5f933a5b6458c82bc2c3f5a3ece11 3cc5ba174a464e1ca1fdd3617e3a0455 8f325bd00ad840b8afb7714d5f01f0bc--3cc5ba174a464e1ca1fdd3617e3a0455 bc31d0b5b7a74db78cbde07b89265070 RY(iia_\u03b2\u2080\u2085) 3cc5ba174a464e1ca1fdd3617e3a0455--bc31d0b5b7a74db78cbde07b89265070 26dc04ae09a5478d831307ef22d36d0b RX(iia_\u03b2\u2080\u2082) bc31d0b5b7a74db78cbde07b89265070--26dc04ae09a5478d831307ef22d36d0b 72db784988964816a610dafa138a14eb RX(iia_\u03b1\u2081\u2082) 26dc04ae09a5478d831307ef22d36d0b--72db784988964816a610dafa138a14eb ed29d466fa5a46fdb5e10a5b334ca4e5 RY(iia_\u03b1\u2081\u2085) 72db784988964816a610dafa138a14eb--ed29d466fa5a46fdb5e10a5b334ca4e5 9cc8bb3c9ed140ea9253ec202162c254 ed29d466fa5a46fdb5e10a5b334ca4e5--9cc8bb3c9ed140ea9253ec202162c254 b9702b4b3cee49ccbe6f0a5c4ffc46e7 X 9cc8bb3c9ed140ea9253ec202162c254--b9702b4b3cee49ccbe6f0a5c4ffc46e7 b9702b4b3cee49ccbe6f0a5c4ffc46e7--82962e49e18341f0b4c014269d4cd687 d17a208004d24d8c8aa813e549307fac RX(iia_\u03b3\u2081\u2082) b9702b4b3cee49ccbe6f0a5c4ffc46e7--d17a208004d24d8c8aa813e549307fac 8d1a7a037163431ea9fe3381c8129fc8 X d17a208004d24d8c8aa813e549307fac--8d1a7a037163431ea9fe3381c8129fc8 8d1a7a037163431ea9fe3381c8129fc8--014a07090c6a470e87e537c56ff5cfdb 400712ef25764e7fafdf963af68fb518 8d1a7a037163431ea9fe3381c8129fc8--400712ef25764e7fafdf963af68fb518 8d1878617d5e4a0b9d4e587837668736 RY(iia_\u03b2\u2081\u2085) 400712ef25764e7fafdf963af68fb518--8d1878617d5e4a0b9d4e587837668736 b6aee338489e4080aed0d3b3913e1fb0 RX(iia_\u03b2\u2081\u2082) 8d1878617d5e4a0b9d4e587837668736--b6aee338489e4080aed0d3b3913e1fb0 b6aee338489e4080aed0d3b3913e1fb0--2c544a47314d47e489f5bdea1e1d4ace"},{"location":"content/quantummodels/","title":"Quantum models","text":"<p>A quantum program can be expressed and executed using the <code>QuantumModel</code> type. It serves three primary purposes:</p> <p>Parameter handling: by conveniently handling and embedding the two parameter types that Qadence supports: feature and variational (see more details in the previous section).</p> <p>Differentiability: by enabling a differentiable backend that supports two differentiable modes: automatic differentiation (AD) and parameter shift rules (PSR). The former is used general differentiation in statevector simulators based on PyTorch and JAX. The latter is a quantum specific method used to differentiate gate parameters, and is enabled for all backends.</p> <p>Execution: by defining which backend the program is expected to be executed on. Qadence supports circuit compilation to the native backend representation.</p> <p>Backends</p> <p>The goal is for quantum models to be executed seemlessly on a number of different purpose backends: simulators, emulators or real hardware. By default, Qadence executes on the PyQTorch backend which implements a state vector simulator. Currently, this is the most feature rich backend. The Pulser backend is being developed, and currently supports a more limited set of functionalities (pulse sequences on programmable neutral atom arrays). The Horqrux backend, built on JAX, is also available, but currently not supported with the <code>QuantumModel</code> interface. For more information see the backend section.</p> <p>The base <code>QuantumModel</code> exposes the following methods:</p> <ul> <li><code>QuantumModel.run()</code>: To extract the wavefunction after circuit execution. Not supported by all backends.</li> <li><code>QuantumModel.sample()</code>: Sample a bitstring from the resulting quantum state after circuit execution. Supported by all backends.</li> <li><code>QuantumModel.expectation()</code>: Compute the expectation value of an observable.</li> </ul> <p>Every <code>QuantumModel</code> is an instance of a <code>torch.nn.Module</code> that enables differentiability for its <code>expectation</code> method. For statevector simulators, AD also works for the statevector itself.</p> <p>To construct a <code>QuantumModel</code>, the program block must first be initialized into a <code>QuantumCircuit</code> instance by combining it with a <code>Register</code>. An integer number can also be passed for the total number of qubits, which instantiates a <code>Register</code> automatically. The qubit register also includes topological information on the qubit layout, essential for digital-analog computations. However, we will explore that in a later tutorial. For now, let's construct a simple parametrized quantum circuit.</p> <pre><code>from qadence import QuantumCircuit, RX, RY, chain, kron\nfrom qadence import FeatureParameter, VariationalParameter\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, theta), RY(1, theta)),\n    kron(RX(0, phi), RY(1, phi)),\n)\n\ncircuit = QuantumCircuit(2, block)\nunique_params = circuit.unique_parameters\n</code></pre> <pre><code>unique_params = [theta, phi]\n</code></pre> <p>The model can then be instantiated. Similarly to the direct execution functions shown in the previous tutorial, the <code>run</code>, <code>sample</code> and <code>expectation</code> methods are available directly from the model.</p> <pre><code>import torch\nfrom qadence import QuantumModel, PI, Z\n\nobservable = Z(0) + Z(1)\n\nmodel = QuantumModel(circuit, observable)\n\nvalues = {\"phi\": torch.tensor([PI, PI/2])}\n\nwf = model.run(values)\nxs = model.sample(values, n_shots=100)\nex = model.expectation(values)\n</code></pre> <pre><code>wf = tensor([[ 0.1563+0.0000j, -0.3632+0.0000j,  0.0000+0.3632j,  0.0000-0.8437j],\n        [ 0.1368+0.0000j,  0.3437+0.0000j,  0.0000-0.3437j,  0.0000-0.8632j]])\nxs = [OrderedCounter({'11': 76, '10': 15, '01': 7, '00': 2}), OrderedCounter({'11': 75, '10': 14, '01': 7, '00': 4})]\nex = tensor([[-1.3747],\n        [-1.4527]])\n</code></pre> <p>By default, the <code>forward</code> method of <code>QuantumModel</code> calls <code>model.run()</code>. To define custom quantum models, the best way is to inherit from <code>QuantumModel</code> and override the <code>forward</code> method, as typically done with custom PyTorch Modules.</p> <p>The <code>QuantumModel</code> class provides convenience methods to manipulate parameters. Being a <code>torch.nn.Module</code>, all torch methods are also available.</p> <pre><code># To pass onto a torch optimizer\nparameter_generator = model.parameters()\n\n# Number of variational parameters\nnum_vparams = model.num_vparams\n\n# Dictionary to easily inspect variational parameter values\nvparams_values = model.vparams\n</code></pre> <pre><code>vparams_values = OrderedDict([('theta', tensor([0.8130]))])\n</code></pre>"},{"location":"content/quantummodels/#model-output","title":"Model output","text":"<p>The output of a quantum model is typically encoded in the measurement of an expectation value. In Qadence, one way to customize the number of outputs is by batching the number of observables at model creation by passing a list of blocks.</p> <pre><code>from torch import tensor\nfrom qadence import chain, kron, VariationalParameter, FeatureParameter\nfrom qadence import QuantumModel, QuantumCircuit, PI, Z, RX, CNOT\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, phi), RX(1, phi)),\n    CNOT(0, 1)\n)\n\ncircuit = QuantumCircuit(2, block)\n\nmodel = QuantumModel(circuit, [Z(0), Z(0) + Z(1)])\n\nvalues = {\"phi\": tensor(PI)}\n\nex = model.expectation(values)\n</code></pre> <pre><code>ex = tensor([[-1.0000e+00, -7.4988e-33]])\n</code></pre> <p>As mentioned in the previous tutorial, blocks can also be arbitrarily parameterized through multiplication, which allows the inclusion of trainable parameters in the definition of the observable.</p> <pre><code>from qadence import I, Z\n\na = VariationalParameter(\"a\")\nb = VariationalParameter(\"b\")\n\n# Magnetization with a trainable shift and scale\nobservable = a * I(0) + b * Z(0)\n\nmodel = QuantumModel(circuit, observable)\n</code></pre>"},{"location":"content/quantummodels/#quantum-neural-network-qnn","title":"Quantum Neural Network (QNN)","text":"<p>The <code>QNN</code> is a subclass of the <code>QuantumModel</code> geared towards quantum machine learning and parameter optimisation. See the quantum machine learning section section or the <code>QNN</code> API reference for more detailed information. There are three main differences in interface when compared with the <code>QuantumModel</code>:</p> <ul> <li>It is initialized with a list of the input parameter names, and then supports direct <code>torch.Tensor</code> inputs instead of the values dictionary shown above. The ordering of the input values should respect the order given in the input names.</li> <li>Passing an observable is mandatory.</li> <li>The <code>forward</code> method calls <code>model.expectation()</code>.</li> </ul> <pre><code>from torch import tensor\nfrom qadence import chain, kron, VariationalParameter, FeatureParameter\nfrom qadence import QNN, QuantumCircuit, PI, Z, RX, RY, CNOT\n\ntheta = FeatureParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, phi), RX(1, phi)),\n    kron(RY(0, theta), RY(1, theta)),\n    CNOT(0, 1)\n)\n\ncircuit = QuantumCircuit(2, block)\nobservable = Z(0) + Z(1)\n\nmodel = QNN(circuit, observable, inputs = [\"phi\", \"theta\"])\n\n# \"phi\" = PI, PI/2, \"theta\" = 0.0, 1.0\nvalues = tensor([[PI, 0.0], [PI/2, 1.0]])\n\nex = model(values)\n</code></pre> <pre><code>ex = tensor([[-7.4988e-33],\n        [ 1.1102e-16]])\n</code></pre>"},{"location":"content/register/","title":"Quantum registers","text":"<p>In Qadence, quantum programs can be executed by specifying the layout of a register of resources as a lattice. Built-in <code>Register</code> types can be used or constructed for arbitrary topologies. Common register topologies are available and illustrated in the plot below.</p> 2025-01-28T16:50:10.371281 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"content/register/#building-and-drawing-registers","title":"Building and drawing registers","text":"<p>Built-in topologies are directly accessible in the <code>Register</code> methods:</p> <pre><code>from qadence import Register\n\nreg = Register.all_to_all(n_qubits = 4)\nreg_line = Register.line(n_qubits = 4)\nreg_circle = Register.circle(n_qubits = 4)\nreg_squre = Register.square(qubits_side = 2)\nreg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\nreg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\nreg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n</code></pre> <p>The <code>Register</code> class builds on top of the NetworkX <code>Graph</code>, and the graphs can be visualized with the <code>reg.draw()</code> method</p> <p>Qubit coordinates are saved as node properties in the underlying NetworkX graph, but can be accessed directly with the <code>coords</code> property.</p> <p><pre><code>reg = Register.square(2)\nprint(reg.coords)\n</code></pre> <pre><code>{0: (0.5, -0.5), 1: (0.5, 0.5), 2: (-0.5, 0.5), 3: (-0.5, -0.5)}\n</code></pre>  By default, the coords are scaled such that the minimum distance between any two qubits is 1, unless the register is created directly from specific coordinates as shown below. The <code>spacing</code> argument can be used to set the minimum spacing. The <code>rescale_coords</code> method can be used to create a new register by rescaling the coordinates of an already created register.</p> <pre><code>scaled_reg_1 = Register.square(2, spacing = 4.0)\nscaled_reg_2 = reg.rescale_coords(scaling = 4.0)\nprint(scaled_reg_1.coords)\nprint(scaled_reg_2.coords)\n</code></pre> <pre><code>{0: (2.0, -2.0), 1: (2.0, 2.0), 2: (-2.0, 2.0), 3: (-2.0, -2.0)}\n{0: (2.0, -2.0), 1: (2.0, 2.0), 2: (-2.0, 2.0), 3: (-2.0, -2.0)}\n</code></pre> <p>The distance between qubits can also be directly accessed with the <code>distances</code> and <code>edge_distances</code> properties.</p> <pre><code>print(reg.distances)\nprint(reg.edge_distances)\n</code></pre> <pre><code>Distance between all qubit pairs:\n{(0, 1): 1.0, (0, 2): 1.4142135623730951, (0, 3): 1.0, (1, 2): 1.0, (1, 3): 1.4142135623730951, (2, 3): 1.0}\nDistance between qubits connect by an edge in the graph\n{(0, 1): 1.0, (0, 3): 1.0, (1, 2): 1.0, (2, 3): 1.0}\n</code></pre> <p>By calling the <code>Register</code> directly, either the number of nodes or a specific graph can be given as input. If passing a custom graph directly, the node positions will not be defined automatically, and should be previously saved in the <code>\"pos\"</code> node property. If not, <code>reg.coords</code> will return empty tuples and all distances will be 0.</p> <pre><code>import networkx as nx\n\n# Same as Register.all_to_all(n_qubits = 2):\nreg = Register(2)\n\n# Register from a custom graph:\ngraph = nx.complete_graph(3)\n\n# Set node positions, in this case a simple line:\nfor i, node in enumerate(graph.nodes):\n    graph.nodes[node][\"pos\"] = (1.0 * i, 0.0)\n\nreg = Register(graph)\n\nprint(reg.distances)\n</code></pre> <pre><code>{(0, 1): 1.0, (0, 2): 2.0, (1, 2): 1.0}\n</code></pre> <p>Alternatively, arbitrarily shaped registers can also be constructed by providing the node coordinates. In this case, there will be no edges automatically created in the connectivity graph.</p> <pre><code>import numpy as np\nfrom qadence import Register, PI\n\nreg = Register.from_coordinates(\n    [(x, np.sin(x)) for x in np.linspace(0, 2*PI, 10)]\n)\n\nreg.draw(show=False)\n</code></pre> 2025-01-28T16:50:10.534788 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>Units for qubit coordinates</p> <p>In general, Qadence makes no assumption about the units for qubit coordinates and distances. However, if used in the context of a Hamiltonian coefficient, care should be taken by the user to guarantee the quantity \\(H.t\\) is dimensionless for exponentiation in the PyQTorch backend, where it is assumed that \\(\\hbar = 1\\). For registers passed to the Pulser backend, coordinates are in \\(\\mu \\textrm{m}\\).</p>"},{"location":"content/register/#connectivity-graphs","title":"Connectivity graphs","text":"<p>Register topology is often assumed in digital simulations to be an all-to-all qubit connectivity. When running on real devices that enable the digital-analog computing paradigm, qubit interactions must be specified either by specifying distances between qubits, or by defining edges in the register connectivity graph.</p> <p>The abstract graph nodes and edges are accessible for direct usage.</p> <pre><code>from qadence import Register\n\nreg = Register.rectangular_lattice(2,3)\n</code></pre> <pre><code>reg.nodes = NodeView((0, 1, 2, 3, 4, 5))\nreg.edges = EdgeView([(0, 2), (0, 1), (1, 3), (2, 4), (2, 3), (3, 5), (4, 5)])\n</code></pre> <p>There is also an <code>all_node_pairs</code> property for convenience:</p> <pre><code>print(reg.all_node_pairs)\n</code></pre> <pre><code>[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n</code></pre> <p>More details about the usage of <code>Register</code> types in the digital-analog paradigm can be found in the digital-analog basics section.</p>"},{"location":"content/serializ_and_prep/","title":"Serialization","text":"<p>Qadence offers convenience functions for serializing and deserializing any quantum program. This is useful for storing quantum programs and sending them for execution over the network via an API.</p> <p>Note</p> <p>Qadence currently uses a custom JSON serialization as interchange format. Support for QASM format for digital quantum programs is currently under consideration.</p> <ul> <li><code>serialize/deserialize</code>: serialize and deserialize a Qadence object into a dictionary</li> <li><code>save/load</code>: save and load a Qadence object to a file with one of the supported   formats. Currently, these are <code>.json</code> and the PyTorch-compatible <code>.pt</code> format.</li> </ul> <p>Let's start with serialization into a dictionary.</p> <pre><code>import torch\nfrom qadence import QuantumCircuit, QuantumModel, DiffMode\nfrom qadence import chain, hamiltonian_factory, feature_map, hea, Z\nfrom qadence.serialization import serialize, deserialize\n\nn_qubits = 4\n\nmy_block = chain(feature_map(n_qubits, param=\"x\"), hea(n_qubits, depth=2))\nobs = hamiltonian_factory(n_qubits, detuning=Z)\n\n# Use the block defined above to create a quantum circuit\n# serialize/deserialize it\nqc = QuantumCircuit(n_qubits, my_block)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n# Let's wrap it in a QuantumModel\n# and serialize it\nqm = QuantumModel(qc, obs, diff_mode=DiffMode.AD)\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n\n# Check if the loaded QuantumModel returns the same expectation\nvalues = {\"x\": torch.rand(10)}\nassert torch.allclose(qm.expectation(values=values), qm_deserialized.expectation(values=values))\n</code></pre> <p>Finally, we can save the quantum circuit and the model with the two supported formats.</p> <pre><code>from qadence.serialization import serialize, deserialize, save, load, SerializationFormat\n\nqc_fname = \"circuit\"\nsave(qc, folder=\".\", file_name=qc_fname, format=SerializationFormat.PT)\nloaded_qc = load(f\"{qc_fname}.pt\")\nassert qc == loaded_qc\n\nqm_fname = \"model\"\nsave(qm, folder=\".\", file_name=qm_fname, format=SerializationFormat.JSON)\nmodel = load(f\"{qm_fname}.json\")\nassert isinstance(model, QuantumModel)\n</code></pre>"},{"location":"content/state_conventions/","title":"State Conventions","text":"<p>Here is an overview of the state conventions used in Qadence together with practical examples.</p>"},{"location":"content/state_conventions/#qubit-register-order","title":"Qubit register order","text":"<p>Qubit registers in quantum computing are often indexed in increasing or decreasing order from left to right. In Qadence, the convention is qubit indexation in increasing order. For example, a register of four qubits in bra-ket notation reads:</p> \\[|q_0, q_1, q_2, q_3\\rangle\\] <p>Furthermore, when displaying a quantum circuit, qubits are ordered from top to bottom.</p>"},{"location":"content/state_conventions/#basis-state-order","title":"Basis state order","text":"<p>Basis state ordering refers to how basis states are ordered when considering the conversion from bra-ket notation to the standard linear algebra basis. In Qadence, basis states are ordered in the following manner:</p> \\[ \\begin{align} |00\\rangle = [1, 0, 0, 0]^T\\\\ |01\\rangle = [0, 1, 0, 0]^T\\\\ |10\\rangle = [0, 0, 1, 0]^T\\\\ |11\\rangle = [0, 0, 0, 1]^T \\end{align} \\]"},{"location":"content/state_conventions/#endianness","title":"Endianness","text":"<p>Endianness refers to the storage convention for binary information (in bytes) in a classical memory register. In quantum computing, information is either stored in bits or in qubits. The most commonly used conventions are:</p> <ul> <li>A big-endian system stores the most significant bit of a binary word at the smallest memory address.</li> <li>A little-endian system stores the least significant bit of a binary word at the smallest memory address.</li> </ul> <p>Given the register convention in Qadence, the integer \\(2\\) written in binary big-endian as \\(10\\) can be encoded in a qubit register in both big-endian as \\(|10\\rangle\\) or little-endian as \\(|01\\rangle\\).</p> <p>The convention for Qadence is big-endian.</p>"},{"location":"content/state_conventions/#quantum-states","title":"Quantum states","text":"<p>In practical scenarios, conventions regarding register order, basis state order and endianness are very much intertwined, and identical results can be obtained by fixing or varying any of them. In Qadence, we assume that qubit ordering and basis state ordering is fixed, and allow an <code>endianness</code> argument that can be passed to control the expected result. Here are a few examples:</p> <p>A simple and direct way to exemplify the endianness convention is using convenience functions for state preparation.</p> <p>Bitstring convention as inputs</p> <p>When a bitstring is passed as input to a function for state preparation, it has to be understood in big-endian convention.</p> <pre><code>from qadence import Endianness, product_state\n\n# The state |10&gt;, the 3rd basis state.\nstate_big = product_state(\"10\", endianness=Endianness.BIG) # or just \"Big\"\n\n# The state |01&gt;, the 2nd basis state.\nstate_little = product_state(\"10\", endianness=Endianness.LITTLE) # or just \"Little\"\n</code></pre> <pre><code>State in big endian = tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]])\nState in little endian = tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> <p>Here, a bitword expressed as a Python string to encode the integer 2 in big-endian is used to create the respective basis state in both conventions. However, note that the same results can be obtained by fixing the endianness convention as big-endian (thus creating the state \\(|10\\rangle\\) in both cases), and changing the basis state ordering. A similar argument holds for fixing both endianness and basis state ordering and simply changing the qubit index order.</p> <p>Another example where endianness directly comes into play is when measuring a register. A big- or little-endian measurement will choose the first or the last qubit, respectively, as the most significant bit. Let's see this in an example:</p> <pre><code>from qadence import I, H, sample\n\n# Create superposition state: |00&gt; + |01&gt; (normalized)\nblock = I(0) @ H(1)  # Identity on qubit 0, Hadamard on qubit 1\n\n# Generate bitword samples following both conventions\n# Samples \"00\" and \"01\"\nresult_big = sample(block, endianness=Endianness.BIG)\n# Samples \"00\" and \"10\"\nresult_little = sample(block, endianness=Endianness.LITTLE)\n</code></pre> <pre><code>Sample in big endian = [OrderedCounter({'01': 57, '00': 43})]\nSample in little endian = [Counter({'00': 51, '10': 49})]\n</code></pre> <p>In Qadence, endianness can be flipped for many relevant objects:</p> <pre><code>from qadence import invert_endianness\n\n# Equivalent to sampling in little-endian.\nflip_big_sample = invert_endianness(result_big)\n\n# Equivalent to a state created in little-endian.\nflip_big_state = invert_endianness(state_big)\n</code></pre> <pre><code>Flipped sample = [Counter({'10': 57, '00': 43})]\nFlipped state = tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre>"},{"location":"content/state_conventions/#quantum-operations","title":"Quantum operations","text":"<p>When looking at the matricial form of quantum operations, the usage of the term endianness becomes slightly abusive. To exemplify, we may consider the <code>CNOT</code> operation with <code>control = 0</code> and <code>target = 1</code>. This operation is often described with two different matrices:</p> \\[ \\text{CNOT(0, 1)} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\qquad \\text{or} \\qquad \\text{CNOT(0, 1)} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] <p>The difference can be easily explained either by considering a different ordering of the qubit indices, or a different ordering of the basis states. In Qadence, both can be retrieved through the <code>endianness</code> argument:</p> <pre><code>from qadence import block_to_tensor, CNOT\n\nmatrix_big = block_to_tensor(CNOT(0, 1), endianness=Endianness.BIG)\nmatrix_little = block_to_tensor(CNOT(0, 1), endianness=Endianness.LITTLE)\n</code></pre> <pre><code>CNOT matrix in big endian =\n\ntensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]])\n\nCNOT matrix in little endian =\n\ntensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]]])\n</code></pre>"},{"location":"content/state_conventions/#backends","title":"Backends","text":"<p>An important part of having clear state conventions is that we need to make sure our results are consistent accross different computational backends, which may have their own conventions. In Qadence, this is taken care of automatically: by calling operations for different backends, the result is expected to be equivalent up to qubit ordering.</p> <pre><code>from qadence import BackendName, RX, run, sample, PI\n\n# RX(PI/4) on qubit 1\nn_qubits = 2\nop = RX(1, PI/4)\n</code></pre> <pre><code>Same sampling order in big endian:\n\nOn PyQTorch = [OrderedCounter({'00': 92, '01': 8})]\nOn Pulser = [Counter({'00': 87, '01': 13})]\n\nSame wavefunction order:\n\nOn PyQTorch = tensor([[0.9239+0.0000j, 0.0000-0.3827j, 0.0000+0.0000j, 0.0000+0.0000j]])\nOn Pulser = tensor([[0.9239+0.0000j, 0.0000-0.3826j, 0.0000+0.0000j, 0.0000+0.0000j]])\n</code></pre>"},{"location":"content/state_init/","title":"State initialization","text":"<p>Qadence offers convenience routines for preparing initial quantum states. These routines are divided into two approaches:</p> <ul> <li>As a dense matrix.</li> <li>From a suitable quantum circuit. This is available for every backend and it should be added in front of the desired quantum circuit to simulate.</li> </ul> <p>Let's illustrate the usage of the state preparation routine.</p> <pre><code>from qadence import random_state, product_state, is_normalized, StateGeneratorType\n\n# Random initial state.\n# the default `type` is StateGeneratorType.HaarMeasureFast\nstate = random_state(n_qubits=2, type=StateGeneratorType.RANDOM_ROTATIONS)\n\n# Check the normalization.\nassert is_normalized(state)\n\n# Product state from a given bitstring.\n# NB: Qadence follows the big endian convention.\nstate = product_state(\"01\")\n</code></pre> <pre><code>Random initial state generated with rotations:\n\nstate = [0.98111062+0.j         0.08438519+0.j         0.        +0.17343176j\n 0.        +0.01491684j]\n\nProduct state corresponding to bitstring '01':\n\nstate = [0.+0.j 1.+0.j 0.+0.j 0.+0.j]\n</code></pre> <p>Now we see how to generate the product state corresponding to the one above with a suitable quantum circuit.</p> <p><pre><code>from qadence import product_block, tag, hea, QuantumCircuit\nfrom qadence.draw import display\n\nstate_prep_block = product_block(\"01\")\n# display(state_prep_block)\n\n# Let's now prepare a circuit.\nn_qubits = 4\n\nstate_prep_block = product_block(\"0001\")\ntag(state_prep_block, \"Prep block\")\n\ncircuit_block = tag(hea(n_qubits, depth = 2), \"Circuit block\")\n\nqc_with_state_prep = QuantumCircuit(n_qubits, state_prep_block, circuit_block)\n</code></pre> %3 cluster_afaef9aac5de4cc5b28f43cb537ce0cc Circuit block cluster_75bcf2f28e6a41d1abfc0ab2ebfd0adc Prep block cfad5fcadc294a5c890ff2de256f7eef 0 2bc5769d554b4963a829848c804bbd85 cfad5fcadc294a5c890ff2de256f7eef--2bc5769d554b4963a829848c804bbd85 d43f06ebf6b049c4ac2b2e14ef66d48b 1 54fff406b4274ca4bbe0f11e6dfe7e48 RX(theta\u2080) 2bc5769d554b4963a829848c804bbd85--54fff406b4274ca4bbe0f11e6dfe7e48 83e4947cc3954b739bd4b5dba6b379c5 RY(theta\u2084) 54fff406b4274ca4bbe0f11e6dfe7e48--83e4947cc3954b739bd4b5dba6b379c5 52ed7b2424f64432be239e07cce81066 RX(theta\u2088) 83e4947cc3954b739bd4b5dba6b379c5--52ed7b2424f64432be239e07cce81066 9b427bd5148a4f579f874a006a2ed453 52ed7b2424f64432be239e07cce81066--9b427bd5148a4f579f874a006a2ed453 bb6476075c9545e6b1c225d8ff48e0d3 9b427bd5148a4f579f874a006a2ed453--bb6476075c9545e6b1c225d8ff48e0d3 454d41f70d2b4b06b880d405ee57ba5a RX(theta\u2081\u2082) bb6476075c9545e6b1c225d8ff48e0d3--454d41f70d2b4b06b880d405ee57ba5a c82c8b8824264114a26d5a6d2326fd22 RY(theta\u2081\u2086) 454d41f70d2b4b06b880d405ee57ba5a--c82c8b8824264114a26d5a6d2326fd22 7d19e6cc1dad476cb53c13888a953db5 RX(theta\u2082\u2080) c82c8b8824264114a26d5a6d2326fd22--7d19e6cc1dad476cb53c13888a953db5 2b91091e2a924e4ba8d916cbde4596b2 7d19e6cc1dad476cb53c13888a953db5--2b91091e2a924e4ba8d916cbde4596b2 ce02a40c557a41d18abc02da38041c31 2b91091e2a924e4ba8d916cbde4596b2--ce02a40c557a41d18abc02da38041c31 8539094df6174952a186767e778f1c91 ce02a40c557a41d18abc02da38041c31--8539094df6174952a186767e778f1c91 b4aebab4f9a44fa5aae1c08bafe99ef5 7765ad7bc2114068ab64613d74902cda d43f06ebf6b049c4ac2b2e14ef66d48b--7765ad7bc2114068ab64613d74902cda 907df7c4ea3547f78f204e516c22cda0 2 8b755f8a1b704915aff6aa8088013de1 RX(theta\u2081) 7765ad7bc2114068ab64613d74902cda--8b755f8a1b704915aff6aa8088013de1 97d10a74a4c840109b8a7fd403564bb9 RY(theta\u2085) 8b755f8a1b704915aff6aa8088013de1--97d10a74a4c840109b8a7fd403564bb9 9ada60d8e1ec4e7ea97d0ed67e40f5c9 RX(theta\u2089) 97d10a74a4c840109b8a7fd403564bb9--9ada60d8e1ec4e7ea97d0ed67e40f5c9 fcfa538221c346299717d2742f5632ed X 9ada60d8e1ec4e7ea97d0ed67e40f5c9--fcfa538221c346299717d2742f5632ed fcfa538221c346299717d2742f5632ed--9b427bd5148a4f579f874a006a2ed453 57413f60c0864f418782741097a1a3dd fcfa538221c346299717d2742f5632ed--57413f60c0864f418782741097a1a3dd 41d4946cfe174e81a934b3abd5450ebb RX(theta\u2081\u2083) 57413f60c0864f418782741097a1a3dd--41d4946cfe174e81a934b3abd5450ebb 1f819e87fdcf4d9eb0e29195ff5f73fe RY(theta\u2081\u2087) 41d4946cfe174e81a934b3abd5450ebb--1f819e87fdcf4d9eb0e29195ff5f73fe ec60ad5c75724f139543aea16383adcb RX(theta\u2082\u2081) 1f819e87fdcf4d9eb0e29195ff5f73fe--ec60ad5c75724f139543aea16383adcb e72acb5afc6f4b5b9117d9894ac27989 X ec60ad5c75724f139543aea16383adcb--e72acb5afc6f4b5b9117d9894ac27989 e72acb5afc6f4b5b9117d9894ac27989--2b91091e2a924e4ba8d916cbde4596b2 a970e1c4db864d09b58102d9df7e8fa5 e72acb5afc6f4b5b9117d9894ac27989--a970e1c4db864d09b58102d9df7e8fa5 a970e1c4db864d09b58102d9df7e8fa5--b4aebab4f9a44fa5aae1c08bafe99ef5 e70687a548bc4faaab8897e82682bee3 8c522278d23f45efb428a6a9b00968ce 907df7c4ea3547f78f204e516c22cda0--8c522278d23f45efb428a6a9b00968ce 85b9718d33194a7a9e74fae708e2c229 3 83f5932bce594b4cbb6a5ead5c31a3d1 RX(theta\u2082) 8c522278d23f45efb428a6a9b00968ce--83f5932bce594b4cbb6a5ead5c31a3d1 e6101c174949414a87094a7a38575b9a RY(theta\u2086) 83f5932bce594b4cbb6a5ead5c31a3d1--e6101c174949414a87094a7a38575b9a d001bb58e8614be3801a963965b48ee9 RX(theta\u2081\u2080) e6101c174949414a87094a7a38575b9a--d001bb58e8614be3801a963965b48ee9 4aefbb1e1d624edd97a42d19bac4ec88 d001bb58e8614be3801a963965b48ee9--4aefbb1e1d624edd97a42d19bac4ec88 22bd7641cab74cb5884026db2f3c17a6 X 4aefbb1e1d624edd97a42d19bac4ec88--22bd7641cab74cb5884026db2f3c17a6 22bd7641cab74cb5884026db2f3c17a6--57413f60c0864f418782741097a1a3dd 0b50d19ff84747a7a5eeac3e9086b804 RX(theta\u2081\u2084) 22bd7641cab74cb5884026db2f3c17a6--0b50d19ff84747a7a5eeac3e9086b804 b1df69d88c10455494a953c417c77b78 RY(theta\u2081\u2088) 0b50d19ff84747a7a5eeac3e9086b804--b1df69d88c10455494a953c417c77b78 17eae20d08e246358a51d3dfc964c995 RX(theta\u2082\u2082) b1df69d88c10455494a953c417c77b78--17eae20d08e246358a51d3dfc964c995 7f8ad99c7cc549bd948e872ab03dc6bf 17eae20d08e246358a51d3dfc964c995--7f8ad99c7cc549bd948e872ab03dc6bf b03d061883c44d1483b3622d0ce502f2 X 7f8ad99c7cc549bd948e872ab03dc6bf--b03d061883c44d1483b3622d0ce502f2 b03d061883c44d1483b3622d0ce502f2--a970e1c4db864d09b58102d9df7e8fa5 b03d061883c44d1483b3622d0ce502f2--e70687a548bc4faaab8897e82682bee3 c9b4a546fa8e44d1a7ff9427c2064173 322ec9ed6d284054b385636ec2623dd7 X 85b9718d33194a7a9e74fae708e2c229--322ec9ed6d284054b385636ec2623dd7 b89f62b3054648938818e1e832df73cb RX(theta\u2083) 322ec9ed6d284054b385636ec2623dd7--b89f62b3054648938818e1e832df73cb 777f755e02c84a048b43943c6368ef53 RY(theta\u2087) b89f62b3054648938818e1e832df73cb--777f755e02c84a048b43943c6368ef53 11be39d03b104af3a0bdc47fb3af7056 RX(theta\u2081\u2081) 777f755e02c84a048b43943c6368ef53--11be39d03b104af3a0bdc47fb3af7056 1a16ccc8f7b949aea3408b2021e6bc82 X 11be39d03b104af3a0bdc47fb3af7056--1a16ccc8f7b949aea3408b2021e6bc82 1a16ccc8f7b949aea3408b2021e6bc82--4aefbb1e1d624edd97a42d19bac4ec88 cc4c60bbb09b4d1f86dffeed87830200 1a16ccc8f7b949aea3408b2021e6bc82--cc4c60bbb09b4d1f86dffeed87830200 54a85eb1543442bba175957a354b35d6 RX(theta\u2081\u2085) cc4c60bbb09b4d1f86dffeed87830200--54a85eb1543442bba175957a354b35d6 bec423b730e04a26a33ed60e3a4b56c0 RY(theta\u2081\u2089) 54a85eb1543442bba175957a354b35d6--bec423b730e04a26a33ed60e3a4b56c0 f2b813e60e59486f9bb8d0b15ec3d7d3 RX(theta\u2082\u2083) bec423b730e04a26a33ed60e3a4b56c0--f2b813e60e59486f9bb8d0b15ec3d7d3 5ee1854df468423ca7b3a95ef4bf0599 X f2b813e60e59486f9bb8d0b15ec3d7d3--5ee1854df468423ca7b3a95ef4bf0599 5ee1854df468423ca7b3a95ef4bf0599--7f8ad99c7cc549bd948e872ab03dc6bf 208dfcb3ebbc4da5abf311a1526721e7 5ee1854df468423ca7b3a95ef4bf0599--208dfcb3ebbc4da5abf311a1526721e7 208dfcb3ebbc4da5abf311a1526721e7--c9b4a546fa8e44d1a7ff9427c2064173  Several standard quantum states can be conveniently initialized in Qadence, both in statevector form as well as in block form as shown in following.</p>"},{"location":"content/state_init/#state-vector-initialization","title":"State vector initialization","text":"<p>Qadence offers a number of constructor functions for state vector preparation.</p> <pre><code>from qadence import uniform_state, zero_state, one_state\n\nn_qubits = 3\nbatch_size = 2\n\nuniform_state = uniform_state(n_qubits, batch_size)\nzero_state = zero_state(n_qubits, batch_size)\none_state = one_state(n_qubits, batch_size)\n</code></pre> <pre><code>Uniform state = \n\ntensor([[0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,\n         0.3536+0.j],\n        [0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,\n         0.3536+0.j]])\nZero state = \n\ntensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\nOne state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> <p>As already seen, product states can be easily created, even in batches:</p> <pre><code>from qadence import product_state, rand_product_state\n\n# From a bitsring \"100\"\nprod_state = product_state(\"100\", batch_size)\n\n# Or a random product state\nrand_state = rand_product_state(n_qubits, batch_size)\n</code></pre> <pre><code>Product state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n\nRandom state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> <p>Creating a GHZ state:</p> <pre><code>from qadence import ghz_state\n\nghz = ghz_state(n_qubits, batch_size)\n</code></pre> <pre><code>GHZ state = \n\ntensor([[0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j,\n         0.7071+0.j],\n        [0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j,\n         0.7071+0.j]])\n</code></pre> <p>Creating a random state uniformly sampled from a Haar measure:</p> <pre><code>from qadence import random_state\n\nrand_haar_state = random_state(n_qubits, batch_size)\n</code></pre> <pre><code>Random state from Haar = \n\ntensor([[-0.1732-0.0990j,  0.2022+0.2079j, -0.1102+0.2475j, -0.2171+0.4750j,\n          0.1075-0.0152j, -0.0820+0.1841j, -0.1133-0.1981j, -0.6522-0.0096j],\n        [-0.0571+0.1382j, -0.6252+0.0486j,  0.2354-0.4191j, -0.0668+0.0385j,\n          0.0395-0.1282j,  0.1214-0.4909j,  0.0412+0.0087j,  0.2465-0.1057j]])\n</code></pre> <p>Custom initial states can then be passed to either <code>run</code>, <code>sample</code> and <code>expectation</code> through the <code>state</code> argument</p> <pre><code>from qadence import random_state, product_state, CNOT, run\n\ninit_state = product_state(\"10\")\nfinal_state = run(CNOT(0, 1), state=init_state)\n</code></pre> <pre><code>Final state = tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre>"},{"location":"content/state_init/#density-matrices-conversion","title":"Density matrices conversion","text":"<p>It is also possible to obtain density matrices from statevectors. They can be passed as inputs to quantum programs performing density matrix based operations such as noisy simulations, when the backend allows such as PyQTorch.</p> <pre><code>from qadence import product_state, density_mat\n\ninit_state = product_state(\"10\")\ninit_density_matrix = density_mat(init_state)\n\nfinal_density_matrix = run(CNOT(0, 1), state=init_density_matrix)\n</code></pre> <pre><code>Initial = DensityMatrix([[[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]]])\nFinal = DensityMatrix([[[0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 1.-0.j]]])\n</code></pre>"},{"location":"content/state_init/#block-initialization","title":"Block initialization","text":"<p>Not all backends support custom statevector initialization, however previous utility functions have their counterparts to initialize the respective blocks:</p> <pre><code>from qadence import uniform_block, one_block\n\nn_qubits = 3\n\nuniform_block = uniform_block(n_qubits)\n\none_block = one_block(n_qubits)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 H(0)\n\u251c\u2500\u2500 H(1)\n\u2514\u2500\u2500 H(2)\nKronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> <p>Similarly, for product states:</p> <pre><code>from qadence import product_block, rand_product_block\n\nproduct_block = product_block(\"100\")\n\nrand_product_block = rand_product_block(n_qubits)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 I(1)\n\u2514\u2500\u2500 I(2)\nKronBlock(0,1,2)\n\u251c\u2500\u2500 I(0)\n\u251c\u2500\u2500 I(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> <p>And GHZ states:</p> <pre><code>from qadence import ghz_block\n\nghz_block = ghz_block(n_qubits)\n</code></pre> <pre><code>ChainBlock(0,1,2)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 ChainBlock(0,1,2)\n    \u251c\u2500\u2500 CNOT(0, 1)\n    \u2514\u2500\u2500 CNOT(1, 2)\n</code></pre> <p>Initial state blocks can simply be chained at the start of a given circuit.</p>"},{"location":"content/state_init/#utility-functions","title":"Utility functions","text":"<p>Some state vector utility functions are also available. We can easily create the probability mass function of a given statevector using <code>torch.distributions.Categorical</code></p> <pre><code>from qadence import random_state, pmf\n\nn_qubits = 3\n\nstate = random_state(n_qubits)\ndistribution = pmf(state)\n</code></pre> <pre><code>Categorical(probs: torch.Size([1, 8]))\n</code></pre> <p>We can also check if a state is normalized:</p> <pre><code>from qadence import random_state, is_normalized\n\nstate = random_state(n_qubits)\nprint(is_normalized(state))\n</code></pre> <pre><code>True\n</code></pre> <p>Or normalize a state:</p> <pre><code>import torch\nfrom qadence import normalize, is_normalized\n\nstate = torch.tensor([[1, 1, 1, 1]], dtype = torch.cdouble)\nprint(normalize(state))\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre>"},{"location":"content/time_dependent/","title":"Time-dependent generators","text":"<p>For use cases when the Hamiltonian of the system is time-dependent, Qadence provides a special parameter <code>TimeParameter(\"t\")</code> that denotes the explicit time dependence. Using this time parameter, one can define a parameterized block acting as the generator passed to <code>HamEvo</code> that encapsulates the required time dependence function.</p>"},{"location":"content/time_dependent/#noiseless-time-dependent-hamiltonian-evolution","title":"Noiseless time-dependent Hamiltonian evolution","text":"<pre><code>from qadence import X, Y, HamEvo, TimeParameter, FeatureParameter, run\nfrom pyqtorch.utils import SolverType\nimport torch\n\n# Simulation parameters\node_solver = SolverType.DP5_SE  # time-dependent Schrodinger equation solver method\nn_steps_hevo = 500  # integration time steps used by solver\n\n# Define block parameters\nt = TimeParameter(\"t\")\nomega_param = FeatureParameter(\"omega\")\n\n# Arbitrarily compose a time-dependent generator\ntd_generator = omega_param * (t * X(0) + t**2 * Y(1))\n\n# Create parameterized HamEvo block\nhamevo = HamEvo(td_generator, t)\n</code></pre> <p>Note that when using <code>HamEvo</code> with a time-dependent generator, the actual time parameter that was used to construct the generator must be passed for the second argument <code>parameter</code>.</p> <p>By default, the code above will initialize an internal parameter <code>FeatureParameter(\"duration\")</code> in the <code>HamEvo</code>. Alternatively, the <code>duration</code> argument can be used to rename this parameter, or to pass a fixed value directly. If no fixed value is passed, it must then be set in the <code>values</code> dictionary at runtime.</p> <p>Future improvements</p> <p>Currently it is only possible to pass a single value for the duration, and the only result obtained will be the one corresponding to the state at end of the integration. In the future we will change the interface to allow directly passing some array of save values to obtain expectation values or statevectors at intermediate steps during the evolution.</p> <pre><code>values = {\"omega\": torch.tensor(10.0), \"duration\": torch.tensor(1.0)}\n\nconfig = {\"ode_solver\": ode_solver, \"n_steps_hevo\": n_steps_hevo}\n\nout_state = run(hamevo, values = values, configuration = config)\n\nprint(out_state)\n</code></pre> <pre><code>tensor([[-0.2785+0.0000j, -0.0541+0.0000j,  0.0000-0.9414j,  0.0000-0.1827j]])\n</code></pre> <p>Note that Qadence makes no assumption on units. The unit of passed duration value \\(\\tau\\) must be aligned with the units of other parameters in the time-dependent generator so that the integral of generator \\(\\overset{\\tau}{\\underset{0}{\\int}}\\mathcal{\\hat{H}}(t){\\rm d}t\\) is dimensionless.</p>"},{"location":"content/time_dependent/#noisy-time-dependent-hamiltonian-evolution","title":"Noisy time-dependent Hamiltonian evolution","text":"<p>To perform noisy time-dependent Hamiltonian evolution, one needs to pass a list of noise operators to the <code>noise_operators</code> argument in <code>HamEvo</code>. They correspond to the jump operators used within the time-dependent Schrodinger equation solver method <code>SolverType.DP5_ME</code>:</p> <pre><code>from qadence import X, Y, HamEvo, TimeParameter, FeatureParameter, run\nfrom pyqtorch.utils import SolverType\nimport torch\n\n# Simulation parameters\node_solver = SolverType.DP5_ME  # time-dependent Schrodinger equation solver method\nn_steps_hevo = 500  # integration time steps used by solver\n\n# Define block parameters\nt = TimeParameter(\"t\")\nomega_param = FeatureParameter(\"omega\")\n\n# Arbitrarily compose a time-dependent generator\ntd_generator = omega_param * (t * X(0) + t**2 * Y(1))\n\n# Create parameterized HamEvo block\nnoise_operators = [X(i) for i in td_generator.qubit_support]\nhamevo = HamEvo(td_generator, t, noise_operators = noise_operators)\n\nvalues = {\"omega\": torch.tensor(10.0), \"duration\": torch.tensor(1.0)}\n\nconfig = {\"ode_solver\": ode_solver, \"n_steps_hevo\": n_steps_hevo}\n\nout_state = run(hamevo, values = values, configuration = config)\n\nprint(out_state)\n</code></pre>   DensityMatrix([[[0.2734+0.0000j, 0.0297+0.0000j, 0.0000-0.0227j, 0.0000-0.0025j],                 [0.0297+0.0000j, 0.1698+0.0000j, 0.0000-0.0025j, 0.0000-0.0141j],                 [0.0000+0.0227j, 0.0000+0.0025j, 0.3435+0.0000j, 0.0373+0.0000j],                 [0.0000+0.0025j, 0.0000+0.0141j, 0.0373+0.0000j, 0.2133+0.0000j]]])    <p>Noise operators definition</p> <p>Note it is not possible to define <code>noise_operators</code> with parametric operators. If you want to do so, we recommend obtaining the tensors via run and set <code>noise_operators</code> using <code>MatrixBlock</code>. Also, <code>noise_operators</code> should have the same or a subset of the qubit support of the <code>HamEvo</code> instance.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"getting_started/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"getting_started/CONTRIBUTING/","title":"How to contribute","text":"<p>We're grateful for your interest in participating in Qadence. Please follow our guidelines to ensure a smooth contribution process.</p>"},{"location":"getting_started/CONTRIBUTING/#reporting-an-issue-or-proposing-a-feature","title":"Reporting an issue or proposing a feature","text":"<p>Your course of action will depend on your objective, but generally, you should start by creating an issue. If you've discovered a bug or have a feature you'd like to see added to qadence, feel free to create an issue on qadence's GitHub issue tracker. Here are some steps to take:</p> <ol> <li>Quickly search the existing issues using relevant keywords to ensure your issue hasn't been addressed already.</li> <li> <p>If your issue is not listed, create a new one. Try to be as detailed and clear as possible in your description.</p> </li> <li> <p>If you're merely suggesting an improvement or reporting a bug, that's already excellent! We thank you for it. Your issue will be listed and, hopefully, addressed at some point.</p> </li> <li>However, if you're willing to be the one solving the issue, that would be even better! In such instances, you would proceed by preparing a Pull Request.</li> </ol>"},{"location":"getting_started/CONTRIBUTING/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>We're excited that you're eager to contribute to Qadence. To contribute, fork the <code>main</code> branch of qadence repository and once you are satisfied with your feature and all the tests pass create a Pull Request.</p> <p>Here's the process for making a contribution:</p> <p>Click the \"Fork\" button at the upper right corner of the repo page to create a new GitHub repo at <code>https://github.com/USERNAME/qadence</code>, where <code>USERNAME</code> is your GitHub ID. Then, <code>cd</code> into the directory where you want to place your new fork and clone it:</p> <pre><code>git clone https://github.com/USERNAME/qadence.git\n</code></pre> <p>Next, navigate to your new qadence fork directory and mark the main qadence repository as the <code>upstream</code>:</p> <pre><code>git remote add upstream https://github.com/pasqal-io/qadence.git\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>We recommended to use <code>hatch</code> for managing environments:</p> <p>To develop within qadence, use: <pre><code>pip install hatch\nhatch -v shell\n</code></pre></p> <p>To run qadence tests, use:</p> <pre><code>hatch -e tests run test\n</code></pre> <p>If you don't want to use <code>hatch</code>, you can use the environment manager of your choice (e.g. Conda) and execute the following:</p> <pre><code>pip install pytest\npip install -e .\npytest\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#useful-things-for-your-workflow-linting-and-testing","title":"Useful things for your workflow: linting and testing","text":"<p>Use <code>pre-commit</code> to lint your code and run the unit tests before pushing a new commit.</p> <p>Using <code>hatch</code>, it's simply:</p> <pre><code>hatch -e tests run pre-commit run --all-files\nhatch -e tests run test\n</code></pre> <p>Our CI/CD pipeline will also test if the documentation can be built correctly. To test it locally, please run:</p> <pre><code>hatch -e docs run mkdocs build --clean --strict\n</code></pre> <p>Without <code>hatch</code>, <code>pip</code> install those libraries first: \"mkdocs\", \"mkdocs-material\", \"mkdocstrings\", \"mkdocstrings-python\", \"mkdocs-section-index\", \"mkdocs-jupyter\", \"mkdocs-exclude\", \"markdown-exec\"</p> <p>And then:</p> <pre><code> mkdocs build --clean --strict\n</code></pre>"},{"location":"getting_started/LICENSE/","title":"Apache License","text":"<p>Version 2.0, January 2004</p> <p>http://www.apache.org/licenses/</p>"},{"location":"getting_started/LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"getting_started/LICENSE/#1-definitions","title":"1. Definitions:","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"getting_started/LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"getting_started/LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"getting_started/LICENSE/#4-redistribution","title":"4. Redistribution.","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ul> <li> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> </li> <li> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> </li> <li> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> </li> <li> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> </li> </ul> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"getting_started/LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions.","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"getting_started/LICENSE/#6-trademarks","title":"6. Trademarks.","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"getting_started/LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty.","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"getting_started/LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability.","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"getting_started/LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability.","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"getting_started/LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":""},{"location":"getting_started/LICENSE/#appendix-how-to-apply-the-apache-license-to-your-work","title":"APPENDIX: How to apply the Apache License to your work.","text":"<p>To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!)  The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.</p> <p>Copyright [yyyy] Pasqal</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>Qadence is fully tested on Linux/MacOS operating systems. For Windows users, we recommend using WSL2 to install a Linux distribution of choice.</p>"},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>Qadence can be installed from PyPI with <code>pip</code> as follows:</p> <pre><code>pip install qadence\n</code></pre> <p>By default, this will also install PyQTorch, a differentiable state vector simulator which serves as the main numerical backend for Qadence.</p> <p>It is possible to install additional backends and the circuit visualization library using the following extras:</p> <ul> <li><code>visualization</code>: to display quantum circuits.</li> <li><code>pulser</code>: the Pulser backend for composing, simulating and executing pulse sequences for neutral-atom quantum devices (in development).</li> </ul> <p>To install other backends or the visualization tool, please use:</p> <pre><code>pip install \"qadence[pulser, visualization]\"\n</code></pre> <p>Note</p> <p>In order to correctly install the <code>visualization</code> extra, the <code>graphviz</code> package needs to be installed in your system:</p> <pre><code># on Ubuntu\nsudo apt install graphviz\n\n# on MacOS\nbrew install graphviz\n\n# via conda\nconda install python-graphviz\n</code></pre>"},{"location":"getting_started/installation/#install-from-source","title":"Install from source","text":"<p>We recommend to use the <code>hatch</code> environment manager to install <code>qadence</code> from source:</p> <pre><code>python -m pip install hatch\n\n# get into a shell with all the dependencies\npython -m hatch shell\n\n# run a command within the virtual environment with all the dependencies\npython -m hatch run python my_script.py\n</code></pre> <p>Warning</p> <p><code>hatch</code> will not combine nicely with other environment managers such Conda. If you want to use Conda, install it from source using <code>pip</code>:</p> <pre><code># within the Conda environment\npython -m pip install -e .\n</code></pre>"},{"location":"getting_started/installation/#citation","title":"Citation","text":"<p>If you use Qadence for a publication, we kindly ask you to cite our work using the following BibTex entry:</p> <pre><code>@article{qadence2024pasqal,\n  title = {Qadence: a differentiable interface for digital-analog programs.},\n  author={Dominik Seitz and Niklas Heim and Jo\u00e3o P. Moutinho and Roland Guichard and Vytautas Abramavicius and Aleksander Wennersteen and Gert-Jan Both and Anton Quelle and Caroline de Groot and Gergana V. Velikova and Vincent E. Elfving and Mario Dagrada},\n  journal={arXiv:2401.09915},\n  url = {https://github.com/pasqal-io/qadence},\n  year = {2024}\n}\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section is undergoing changes and most of the information here will be reorganized.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/advanced_tutorials/","title":"Advanced Tutorials","text":"<p>In this section, advanced programming concepts and implementations in Qadence are examplified.</p>"},{"location":"tutorials/advanced_tutorials/cloud-submission/","title":"Submission of Qadence Jobs to Pasqal Cloud","text":"<p>It is possible to submit quantum computational jobs to execute remotely on Pasqal's cloud platform from Qadence. This feature can only be used if you have an account on the cloud platform, which has access to the Qadence workload. The qadence module <code>qadence.pasqal_cloud_connection</code> offers functionality to specify the computation easily, upload the specification and retrieve the result when the computation has finished execution on the cloud platform. In this tutorial, a simple quantum circuit will be defined as an example to showcase the submission process for remote computations. The same process can be applied to run more complex quantum circuits on the cloud platform.</p> <p>Let's first define a very simple quantum circuit that creates a Bell state.</p> <pre><code>from qadence import CNOT, H, QuantumCircuit\n\ncircuit = QuantumCircuit(2, H(0), CNOT(0, 1))\n</code></pre> <p>If we want to upload this circuit to the cloud platform we need to follow 4 steps: - Authentication and connection to cloud - Defining workload specification - Submission - Retrieval of results</p>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#authentication-and-connection","title":"Authentication and connection","text":"<p>To setup a connection the cloud platform, use the <code>SDK</code> object present in <code>qadence.pasqal_cloud_connection</code>. The email and password are the ones used to login to the webportal. The project-id can be found in the webportal under \"Projects\".</p> <pre><code>from qadence.pasqal_cloud_connection import SDK\n\nconnection = SDK(\"john.doe@email.com\", \"my-password\", project_id=\"proj1\")\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#defining-workload-specification","title":"Defining workload specification","text":"<p>A workload is a quantum calculation to execute on a Pasqal cloud backend. To create a workload specification, we need some extra information on top the circuit itself. We need to specify the backend, chosen here to be PyQTorch. The cloud platform currently only supports PyQTorch. Moreover, the requested result type needs to be defined. Based on the workload specification, the appropriate run methods (<code>run</code>, <code>sample</code> or <code>expectation</code>) will be called by the <code>QuantumModel</code> by passing them through the enum value <code>ResultTypes</code> argument. Moreover, the requested result type needs to be defined. These are provided in a list, so that multiple result types can be requested in a single submission.</p> <pre><code>from qadence import BackendName\nfrom qadence.pasqal_cloud_connection import WorkloadSpec, ResultType\n\nworkload = WorkloadSpec(circuit, BackendName.PYQTORCH, [ResultType.SAMPLE, ResultType.RUN])\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#using-a-quantum-model","title":"Using a Quantum Model","text":"<p>If you already have your quantum computation defined as a <code>QuantumModel</code>, it is possible to create a workload specification directly from the model using <code>get_workload_spec</code>. Then, the circuit and backend specifications will be extracted from the model, the other values need to be provided as extra arguments.</p> <pre><code>from qadence import QuantumModel\nfrom qadence.pasqal_cloud_connection import get_workload_spec\n\nmodel = QuantumModel(circuit)\nworkload = get_workload_spec(model, [ResultType.SAMPLE])\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#observable-expectation-value","title":"Observable Expectation Value","text":"<p>For the result type <code>ResultType.EXPECTATION</code> it is mandatory to provide an observable to the workload specification. In the example below we use the trivial identity observable <code>I(0) @ I(1)</code>.</p> <pre><code>from qadence import I\n\nworkload = WorkloadSpec(circuit, BackendName.PYQTORCH, [ResultType.EXPECTATION], observable=I(0)*I(1))\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#parametric-circuits","title":"Parametric Circuits","text":"<p>In the case of a parametric circuit, i.e. a circuit that contains feature parameters or variational parameters, values for these parameters need to be provided. The parameter values are defined in a dictionary, where keys are the parameter name and values are parameter value passed as torch tensors. The parameter values are defined in a dictionary, where keys are the parameter name and values are parameter value passed as torch tensors. It is possible to set multiple values by using a 1-D tensor, to the parameters, in that case the computation is executed for each value in the tensor. A mix of 0-D and 1-D tensors can be provided to keep some parameters constant and others changed during this process. However, all 1-D tensors need to have the same length. An exception will be raised if the dimensions and lengths are invalid.</p> <pre><code>from torch import tensor\n\nparametric_circuit = ...\nparameter_values = {\"param1\": tensor(0), \"param2\": tensor([0, 1, 2]), \"param3\": tensor([5, 6, 7])}\nworkload = WorkloadSpec(parametric_circuit, BackendName.PYQTORCH, [ResultType.SAMPLE], parameter_values=parameter_values)\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#submission","title":"Submission","text":"<p>Submission to the cloud platform is done very easily using the <code>submit_workload</code> function. The workload id will be provided by executing the function. This id is needed later, to request the status of the given workload.</p> <pre><code>from qadence.pasqal_cloud_connection import submit_workload\n\nworkload_id = submit_workload(connection, workload)\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#check-workload-status","title":"Check Workload Status","text":"<p>The status of a workload can be: done, pending, running, paused, canceled, timed out or error. The <code>check_status</code> function can be used to see if the workload is finished already. If so, the results of the computation will be provided in a <code>WorkloadResult</code> object. The result of the computation itself can be found in the <code>result</code> attribute of this object. If the workload has not finished yet, or resulted in an error, <code>check_status</code> will raise an exception, either a <code>WorkloadStoppedError</code> or <code>WorkloadNotDoneError</code>.</p> <pre><code>from qadence.pasqal_cloud_connection import check_status\n\nworkload_result = check_status(connection, workload_id)\nprint(workload_result.result)\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#retrieval-of-results","title":"Retrieval of Results","text":"<p>If you wish to wait for the workload to be finished, before moving further with your code, you can use the <code>get_result</code> function. This function checks in set intervals the status of the workload until the workload is finished or the function has timed out. The polling rate as well as the time out duration can be set optionally.</p> <pre><code>from qadence.pasqal_cloud_connection import get_result\n\nworkload_result = get_result(connection, workload_id, timeout=60, refresh_time=1)\nprint(workload_result.result)\n</code></pre>"},{"location":"tutorials/advanced_tutorials/custom-models/","title":"Custom quantum models","text":"<p>In <code>qadence</code>, the <code>QuantumModel</code> is the central class point for executing <code>QuantumCircuit</code>s.  The idea of a <code>QuantumModel</code> is to decouple the backend execution from the management of circuit parameters and desired quantum computation output.</p> <p>In the following, we create a custom <code>QuantumModel</code> instance which introduces some additional optimizable parameters: *  an adjustable scaling factor in front of the observable to measured *  adjustable scale and shift factors to be applied to the model output before returning the result</p> <p>This can be easily done using PyTorch flexible model definition, and it will automatically work with the rest of <code>qadence</code> infrastructure.</p> <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit\n\n\nclass CustomQuantumModel(QuantumModel):\n\n    def __init__(self, circuit: QuantumCircuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\"):\n        super().__init__(circuit, observable=observable, backend=backend, diff_mode=diff_mode)\n\n        self.n_qubits = circuit.n_qubits\n\n        # define some additional parameters which will scale and shift (variationally) the\n        # output of the QuantumModel\n        # you can use all torch machinery for building those\n        self.scale_out = torch.nn.Parameter(torch.ones(1))\n        self.shift_out = torch.nn.Parameter(torch.ones(1))\n\n    # override the forward pass of the model\n    # the forward pass is the output of your QuantumModel and in this case\n    # it's the (scaled) expectation value of the total magnetization with\n    # a variable coefficient in front\n    def forward(self, values: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n\n        # scale the observable\n        res = self.expectation(values)\n\n        # scale and shift the result before returning\n        return self.shift_out + res * self.scale_out\n</code></pre> <p>The custom model can be used like any other <code>QuantumModel</code>: <pre><code>from qadence import Parameter, RX, CNOT, QuantumCircuit\nfrom qadence import chain, kron, hamiltonian_factory, Z\nfrom sympy import acos\n\ndef quantum_circuit(n_qubits):\n\n    x = Parameter(\"x\", trainable=False)\n    fm = kron(RX(i, acos(x) * (i+1)) for i in range(n_qubits))\n\n    ansatz = kron(RX(i, f\"theta{i}\") for i in range(n_qubits))\n    ansatz = chain(ansatz, CNOT(0, n_qubits-1))\n\n    block = chain(fm, ansatz)\n    block.tag = \"circuit\"\n    return QuantumCircuit(n_qubits, block)\n\nn_qubits = 4\nbatch_size = 10\ncircuit = quantum_circuit(n_qubits)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)  # Total magnetization\n\nmodel = CustomQuantumModel(circuit, observable, backend=\"pyqtorch\")\n\nvalues = {\"x\": torch.rand(batch_size)}\nres = model(values)\nprint(\"Model output: \", res)\nassert len(res) == batch_size\n</code></pre> <pre><code>Model output:  tensor([[-0.5954],\n        [-0.4010],\n        [-0.5833],\n        [-0.6110],\n        [ 0.1211],\n        [-0.4482],\n        [-0.0071],\n        [-0.5930],\n        [-0.6037],\n        [ 0.0406]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> </p>"},{"location":"tutorials/advanced_tutorials/custom-models/#quantum-model-with-wavefunction-overlaps","title":"Quantum model with wavefunction overlaps","text":"<p><code>QuantumModel</code>'s can also use different quantum operations in their forward pass, such as wavefunction overlaps described here. Beware that the resulting overlap tensor has to be differentiable to apply gradient-based optimization. This is only applicable to the <code>\"EXACT\"</code> overlap method.</p> <p>Here we show how to use overlap calculation when fitting a parameterized quantum circuit to act as a standard Hadamard gate.</p> <pre><code>from qadence import RY, RX, H, Overlap\n\n# create a quantum model which acts as an Hadamard gate after training\nclass LearnHadamard(QuantumModel):\n    def __init__(\n        self,\n        train_circuit: QuantumCircuit,\n        target_circuit: QuantumCircuit,\n        backend=\"pyqtorch\",\n    ):\n        super().__init__(circuit=train_circuit, backend=backend)\n        self.overlap_fn = Overlap(train_circuit, target_circuit, backend=backend, method=\"exact\", diff_mode='ad')\n\n    def forward(self):\n        return self.overlap_fn()\n\n    # compute the wavefunction of the associated train circuit\n    def wavefunction(self):\n        return model.overlap_fn.run({})\n\n\ntrain_circuit = QuantumCircuit(1, chain(RX(0, \"phi\"), RY(0, \"theta\")))\ntarget_circuit = QuantumCircuit(1, H(0))\n\nmodel = LearnHadamard(train_circuit, target_circuit)\n\n# get the overlap between model and target circuit wavefunctions\nprint(model())\n</code></pre> <pre><code>tensor([[0.6715]], grad_fn=&lt;UnsqueezeBackward0&gt;)\n</code></pre> <p>This model can then be trained with the standard Qadence helper functions.</p> <pre><code>from qadence import run\nfrom qadence.ml_tools import Trainer, TrainConfig\nTrainer.set_use_grad(True)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n\ndef loss_fn(model: LearnHadamard, _unused) -&gt; tuple[torch.Tensor, dict]:\n    loss = criterion(torch.tensor([[1.0]]), model())\n    return loss, {}\n\nconfig = TrainConfig(max_iter=2500)\ntrainer = Trainer(\n    model, optimizer, config, loss_fn\n)\nmodel, optimizer = trainer.fit()\n\nwf_target = run(target_circuit)\nassert torch.allclose(wf_target, model.wavefunction(), atol=1e-2)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/advanced_tutorials/differentiability/","title":"Differentiability","text":"<p>Many application in quantum computing and quantum machine learning more specifically requires the differentiation of a quantum circuit with respect to its parameters.</p> <p>In Qadence, we perform quantum computations via the <code>QuantumModel</code> interface. The derivative of the outputs of quantum models with respect to feature and variational parameters in the quantum circuit can be implemented in Qadence with two different modes:</p> <ul> <li>Automatic differentiation (AD) mode <sup>1</sup>. This mode allows to differentiation both <code>run()</code> and <code>expectation()</code> methods of the <code>QuantumModel</code> and it is the fastest available differentiation method. Under the hood, it is based on the PyTorch autograd engine wrapped by the <code>DifferentiableBackend</code> class. This mode is not working on quantum devices.</li> <li>Generalized parameter shift rule (GPSR) mode. This is general implementation of the well known parameter  shift rule algorithm <sup>2</sup> which works for arbitrary quantum operations <sup>3</sup>. This mode is only applicable to  the <code>expectation()</code> method of <code>QuantumModel</code> but it is compatible with execution or quantum devices.</li> </ul>"},{"location":"tutorials/advanced_tutorials/differentiability/#automatic-differentiation","title":"Automatic differentiation","text":"<p>Automatic differentiation <sup>1</sup> is a procedure to derive a complex function defined as a sequence of elementary mathematical operations in the form of a computer program. Automatic differentiation is a cornerstone of modern machine learning and a crucial ingredient of its recent successes. In its so-called reverse mode, it follows this sequence of operations in reverse order by systematically applying the chain rule to recover the exact value of derivative. Reverse mode automatic differentiation is implemented in Qadence leveraging the PyTorch <code>autograd</code> engine.</p> <p>Only available via the PyQTorch or Horqrux backends</p> <p>Currently, automatic differentiation mode is only available when the <code>pyqtorch</code> or <code>horqrux</code> backends are selected.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#generalized-parameter-shift-rule","title":"Generalized parameter shift rule","text":"<p>The generalized parameter shift rule implementation in Qadence was introduced in <sup>3</sup>. Here the standard parameter shift rules, which only works for quantum operations whose generator has a single gap in its eigenvalue spectrum, was generalized to work with arbitrary generators of quantum operations.</p> <p>For this, we define the differentiable function as quantum expectation value</p> \\[ f(x) = \\left\\langle 0\\right|\\hat{U}^{\\dagger}(x)\\hat{C}\\hat{U}(x)\\left|0\\right\\rangle \\] <p>where \\(\\hat{U}(x)={\\rm exp}{\\left( -i\\frac{x}{2}\\hat{G}\\right)}\\) is the quantum evolution operator with generator \\(\\hat{G}\\) representing the structure of the underlying quantum circuit and \\(\\hat{C}\\) is the cost operator. Then using the eigenvalue spectrum \\(\\left\\{ \\lambda_n\\right\\}\\) of the generator \\(\\hat{G}\\) we calculate the full set of corresponding unique non-zero spectral gaps \\(\\left\\{ \\Delta_s\\right\\}\\) (differences between eigenvalues). It can be shown that the final expression of derivative of \\(f(x)\\) is then given by the following expression:</p> <p>\\(\\begin{equation} \\frac{{\\rm d}f\\left(x\\right)}{{\\rm d}x}=\\overset{S}{\\underset{s=1}{\\sum}}\\Delta_{s}R_{s}, \\end{equation}\\)</p> <p>where \\(S\\) is the number of unique non-zero spectral gaps and \\(R_s\\) are real quantities that are solutions of a system of linear equations</p> <p>\\(\\begin{equation} \\begin{cases} F_{1} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{1}\\Delta_{s}}{2}\\right)R_{s},\\\\ F_{2} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{2}\\Delta_{s}}{2}\\right)R_{s},\\\\  &amp; ...\\\\ F_{S} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{M}\\Delta_{s}}{2}\\right)R_{s}. \\end{cases} \\end{equation}\\)</p> <p>Here \\(F_s=f(x+\\delta_s)-f(x-\\delta_s)\\) denotes the difference between values of functions evaluated at shifted arguments \\(x\\pm\\delta_s\\).</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#adjoint-differentiation","title":"Adjoint Differentiation","text":"<p>Qadence also offers a memory-efficient, non-device compatible alternative to automatic differentation, called 'Adjoint Differentiation' <sup>4</sup> and allows for precisely calculating the gradients of variational parameters in O(P) time and using O(1) state-vectors. Adjoint Differentation is currently only supported by the Torch Engine and allows for first-order derivatives only.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#usage","title":"Usage","text":""},{"location":"tutorials/advanced_tutorials/differentiability/#basics","title":"Basics","text":"<p>In Qadence, the differentiation modes can be selected via the <code>diff_mode</code> argument of the QuantumModel class. It either accepts a <code>DiffMode</code>(<code>DiffMode.GSPR</code>, <code>DiffMode.AD</code> or <code>DiffMode.ADJOINT</code>) or a string (<code>\"gpsr\"\"</code>, <code>\"ad\"</code> or <code>\"adjoint\"</code>). The code in the box below shows how to create <code>QuantumModel</code> instances with all available differentiation modes.</p> <pre><code>from qadence import (FeatureParameter, RX, Z, hea, chain,\n                    hamiltonian_factory, QuantumCircuit,\n                    QuantumModel, BackendName, DiffMode)\nimport torch\n\nn_qubits = 2\n\n# Define a symbolic parameter to differentiate with respect to\nx = FeatureParameter(\"x\")\n\nblock = chain(hea(n_qubits, 1), RX(0, x))\n\n# create quantum circuit\ncircuit = QuantumCircuit(n_qubits, block)\n\n# create total magnetization cost operator\nobs = hamiltonian_factory(n_qubits, detuning=Z)\n\n# create models with AD, ADJOINT and GPSR differentiation engines\nmodel_ad = QuantumModel(circuit, obs,\n                        backend=BackendName.PYQTORCH,\n                        diff_mode=DiffMode.AD)\nmodel_adjoint = QuantumModel(circuit, obs,\n                        backend=BackendName.PYQTORCH,\n                        diff_mode=DiffMode.ADJOINT)\nmodel_gpsr = QuantumModel(circuit, obs,\n                          backend=BackendName.PYQTORCH,\n                          diff_mode=DiffMode.GPSR)\n\n# Create concrete values for the parameter we want to differentiate with respect to\nxs = torch.linspace(0, 2*torch.pi, 100, requires_grad=True)\nvalues = {\"x\": xs}\n\n# calculate function f(x)\nexp_val_ad = model_ad.expectation(values)\nexp_val_adjoint = model_adjoint.expectation(values)\nexp_val_gpsr = model_gpsr.expectation(values)\n\n# calculate derivative df/dx using the PyTorch\n# autograd engine\ndexpval_x_ad = torch.autograd.grad(\n    exp_val_ad, values[\"x\"], torch.ones_like(exp_val_ad), create_graph=True\n)[0]\ndexpval_x_adjoint = torch.autograd.grad(\n    exp_val_adjoint, values[\"x\"], torch.ones_like(exp_val_ad), create_graph=True\n)[0]\ndexpval_x_gpsr = torch.autograd.grad(\n    exp_val_gpsr, values[\"x\"], torch.ones_like(exp_val_gpsr), create_graph=True\n)[0]\n</code></pre> <p>We can plot the resulting derivatives and see that in both cases they coincide.</p> <pre><code>import matplotlib.pyplot as plt\n\n# plot f(x) and df/dx derivatives calculated using AD ,ADJOINT and GPSR\n# differentiation engines\nfig, ax = plt.subplots()\nax.scatter(xs.detach().numpy(),\n           exp_val_ad.detach().numpy(),\n           label=\"f(x)\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_ad.detach().numpy(),\n           label=\"df/dx AD\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_adjoint.detach().numpy(),\n           label=\"df/dx ADJOINT\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_gpsr.detach().numpy(),\n           s=5,\n           label=\"df/dx GPSR\")\nplt.legend()\n</code></pre> 2025-01-28T16:50:28.422962 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/advanced_tutorials/differentiability/#low-level-control-on-the-shift-values","title":"Low-level control on the shift values","text":"<p>In order to get a finer control over the GPSR differentiation engine we can use the low-level Qadence API to define a <code>DifferentiableBackend</code>.</p> <pre><code>from qadence.engines.torch import DifferentiableBackend\nfrom qadence.backends.pyqtorch import Backend as PyQBackend\n\n# define differentiable quantum backend\nquantum_backend = PyQBackend()\nconv = quantum_backend.convert(circuit, obs)\npyq_circ, pyq_obs, embedding_fn, params = conv\ndiff_backend = DifferentiableBackend(quantum_backend, diff_mode=DiffMode.GPSR, shift_prefac=0.2)\n\n# calculate function f(x)\nexpval = diff_backend.expectation(pyq_circ, pyq_obs, embedding_fn(params, values))\n</code></pre> <p>Here we passed an additional argument <code>shift_prefac</code> to the <code>DifferentiableBackend</code> instance that governs the magnitude of shifts \\(\\delta\\equiv\\alpha\\delta^\\prime\\) shown in equation (2) above. In this relation \\(\\delta^\\prime\\) is set internally and \\(\\alpha\\) is the value passed by <code>shift_prefac</code> and the resulting shift value \\(\\delta\\) is then used in all the following GPSR calculations.</p> <p>Tuning parameter \\(\\alpha\\) is useful to improve results when the generator \\(\\hat{G}\\) or the quantum operation is a dense matrix, for example a complex <code>HamEvo</code> operation; if many entries of this matrix are sufficiently larger than 0 the operation is equivalent to a strongly interacting system. In such case parameter \\(\\alpha\\) should be gradually lowered in order to achieve exact derivative values.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#low-level-differentiation-of-qadence-circuits-using-jax","title":"Low-level differentiation of qadence circuits using JAX","text":"<p>For users interested in using the <code>JAX</code> engine instead, we show how to run and differentiate qadence programs using the <code>horqrux</code> backend under qadence examples.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#references","title":"References","text":"<ol> <li> <p>A. G. Baydin et al., Automatic Differentiation in Machine Learning: a Survey \u21a9\u21a9</p> </li> <li> <p>Schuld et al., Evaluating analytic gradients on quantum hardware (2018). \u21a9</p> </li> <li> <p>Kyriienko et al., General quantum circuit differentiation rules \u21a9\u21a9</p> </li> <li> <p>Tyson et al., Efficient calculation of gradients in classical simulations of variational quantum algorithms \u21a9</p> </li> </ol>"},{"location":"tutorials/advanced_tutorials/profiling-and-debugging/","title":"Profiling and debugging on CUDA devices","text":"<p>For this to work, you'll have to have the right to access perf counters on your machine (for example with sudo or inside a Docker container). See: https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/index.html.</p> <pre><code>$ pip install nvidia-pyindex\n$ pip install nvidia-dlprof[pytorch]\n</code></pre> <p>Make sure that your entrypoint is an executable script. That means that it must start with a she-bang on the top line, e.g. <pre><code>#!/bin/python\n</code></pre> and have execution rights <pre><code>$ chmod +x your_script.py\n</code></pre></p> <p>Lastly it's recommended to add <pre><code>import nvidia_dlprof_pytorch_nvtx\nnvidia_dlprof_pytorch_nvtx.init()\n</code></pre> To your script in the beginning enable extra annotations of PyTorch functions.</p> <p>You can then use dlprof to profile. <pre><code>dlprof --mode=pytorch your_script.py\n</code></pre></p> <pre><code>PYQ_LOG_LEVEL=info QADENCE_LOG_LEVEL=debug dlprof --mode=pytorch examples/backends/differentiable_backend.py\n</code></pre> <p>For example to achieve this through Docker we can start a session in the shell, also mounting our local Qadence version and PyQTorch (Both optional, but you probably need to mount your script at least) <pre><code>$ docker run --rm --gpus=1 --shm-size=1g --ulimit memlock=-1 \\\n --ulimit stack=67108864 -it -p8000:8000 -v./:/opt/qadence -v ../PyQ:/opt/pyqtorch pytorch:24.02-py3 bash\n</code></pre> (You may need to jump through extra hoops to make Docker access the GPUs if you have error messages like <code>docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].</code>)</p> <p>After this you should have a shell inside the container and you can <pre><code>root@2a85826c4e7b:/workspace# cd /opt/qadence/\nroot@2a85826c4e7b:/opt/qadence# pip3 install -e .\nroot@2a85826c4e7b:/opt/qadence# pip3 install -e ../pyqtorch/\nroot@2a85826c4e7b:/opt/qadence# pip3 install nvidia-dlprof[pytorch]\nroot@2a85826c4e7b:/opt/qadence# PYQ_LOG_LEVEL=debug QADENCE_LOG_LEVEL=debug dlprof --mode=pytorch --nsys_opts=\"-t cuda,nvtx,cublas,cusparse,cusparse-verbose,cublas-verbose --force-overwrite true\" examples/models/quantum_model.py\n</code></pre></p> <p>Where we have <code>--force-overwrite true</code> to always store the latest profiling result (hence you must rename the file) if you wish to keep several. We add <code>cublas,cusparse,cusparse-verbose,cublas-verbose</code> do get more details about the numerical backend pacakges being used.</p>"},{"location":"tutorials/advanced_tutorials/projectors/","title":"Projector blocks","text":"<p>This section introduces the <code>ProjectorBlock</code> as an implementation for the quantum mechanical projection operation onto the subspace spanned by \\(|a\\rangle\\): \\(\\mathbb{\\hat{P}}=|a\\rangle \\langle a|\\). It evaluates the outer product for bras and kets expressed as bitstrings for a given qubit support. They have to possess matching lengths.</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence.operations import Projector  # Projector as an operation.\n\n# Define a projector for |1&gt; onto the qubit labelled 0.\nprojector_block = Projector(ket=\"1\", bra=\"1\", qubit_support=0)\n\n# As any block, the matrix representation can be retrieved.\nprojector_matrix = block_to_tensor(projector_block)\n</code></pre> <pre><code>projector matrix = tensor([[[0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j]]])\n</code></pre> <p>Other standard operations are expressed as projectors in Qadence. For instance, the number operator is the projector onto the 1-subspace, \\(N=|1\\rangle\\langle 1|\\).</p> <p>In fact, projectors can be used to compose any arbitrary operator. For example, the <code>CNOT</code> can be defined as \\(\\textrm{CNOT}(i,j)=|0\\rangle\\langle 0|_i\\otimes \\mathbb{I}_j+|1\\rangle\\langle 1|_i\\otimes X_j\\) and we can compare its matrix representation with the native one in Qadence:</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence import kron, I, X, CNOT\n\n# Define a projector for |0&gt; onto the qubit labelled 0.\nprojector0 = Projector(ket=\"0\", bra=\"0\", qubit_support=0)\n\n# Define a projector for |1&gt; onto the qubit labelled 0.\nprojector1 = Projector(ket=\"1\", bra=\"1\", qubit_support=0)\n\n# Construct the projector controlled CNOT.\nprojector_cnot = kron(projector0, I(1)) + kron(projector1, X(1))\n\n# Get the underlying unitary.\nprojector_cnot_matrix = block_to_tensor(projector_cnot)\n\n# Qadence CNOT unitary.\nqadence_cnot_matrix = block_to_tensor(CNOT(0,1))\n</code></pre> <pre><code>projector cnot matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]])\nqadence cnot matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Another example is the canonical SWAP unitary that can be defined as \\(SWAP=|00\\rangle\\langle 00|+|01\\rangle\\langle 10|+|10\\rangle\\langle 01|+|11\\rangle\\langle 11|\\). Indeed, it can be shown that their matricial representations are again identical:</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence import SWAP\n\n# Define all projectors.\nprojector00 = Projector(ket=\"00\", bra=\"00\", qubit_support=(0, 1))\nprojector01 = Projector(ket=\"01\", bra=\"10\", qubit_support=(0, 1))\nprojector10 = Projector(ket=\"10\", bra=\"01\", qubit_support=(0, 1))\nprojector11 = Projector(ket=\"11\", bra=\"11\", qubit_support=(0, 1))\n\n# Construct the SWAP gate.\nprojector_swap = projector00 + projector10 + projector01 + projector11\n\n# Get the underlying unitary.\nprojector_swap_matrix = block_to_tensor(projector_swap)\n\n# Qadence SWAP unitary.\nqadence_swap_matrix = block_to_tensor(SWAP(0,1))\n</code></pre> <pre><code>projector swap matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]])\nqadence swap matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]], grad_fn=&lt;UnsafeViewBackward0&gt;)\n</code></pre> <p>Warning</p> <p>Projectors are non-unitary operators, only supported by the PyQTorch backend.</p> <p>To examplify this point, let's run some non-unitary computation involving projectors.</p> <pre><code>from qadence import chain, run\nfrom qadence.operations import H, CNOT\n\n# Define a projector for |1&gt; onto the qubit labelled 1.\nprojector_block = Projector(ket=\"1\", bra=\"1\", qubit_support=1)\n\n# Some non-unitary computation.\nnon_unitary_block = chain(H(0), CNOT(0,1), projector_block)\n\n# Projected wavefunction becomes unnormalized\nprojected_wf = run(non_unitary_block)  # Run on PyQTorch.\n</code></pre> <pre><code>projected_wf = tensor([[0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j]])\n</code></pre>"},{"location":"tutorials/development/architecture/","title":"Architecture and sharp bits","text":"<p>Qadence as a software library mixes functional and object-oriented programming. We do that by maintaining core objects and operating on them with functions.</p> <p>Furthermore, Qadence strives at keeping the lower level abstraction layers for automatic differentiation and quantum computation fully stateless while only the frontend layer which is the main user-facing interface is stateful.</p> <p>Code design philosopy</p> <p>Functional, stateless core with object-oriented, stateful user interface.</p>"},{"location":"tutorials/development/architecture/#abstraction-layers","title":"Abstraction layers","text":"<p>In Qadence there are 4 main objects spread across 3 different levels of abstraction:</p> <ul> <li> <p>Frontend layer: The user facing layer and encompasses two objects:</p> <ul> <li><code>QuantumCircuit</code>: A class representing an abstract quantum   circuit not tight not any particular framework. Parameters are represented symbolically using   <code>sympy</code> expressions.</li> <li><code>QuantumModel</code>: The models are higher-level abstraction   providing an interface for executing different kinds of common quantum computing models such   quantum neural networks (QNNs), quantum kernels etc.</li> </ul> </li> <li> <p>Differentiation layer: Intermediate layer has the purpose of integrating quantum   computation with a given automatic differentiation engine. It is meant to be purely stateless and   contains one object:</p> <ul> <li><code>DifferentiableBackend</code>:   An abstract class whose concrete implementation wraps a quantum backend and make it   automatically differentiable using different engines (e.g. PyTorch or Jax).   Note, that today only PyTorch is supported but there is plan to add also a Jax   differentiable backend which will require some changes in the base class implementation.</li> </ul> </li> <li> <p>Quantum layer: The lower-level layer which directly interfaces with quantum emulators   and processing units. It is meant to be purely stateless and it contains one base object which is   specialized for each supported backend:</p> <ul> <li><code>Backend</code>: An abstract class whose concrete implementation   enables the execution of quantum circuit with a variety of quantum backends (normally non   automatically differentiable by default) such as PyQTorch, or Pulser.</li> </ul> </li> </ul>"},{"location":"tutorials/development/architecture/#main-components","title":"Main components","text":""},{"location":"tutorials/development/architecture/#quantumcircuit","title":"<code>QuantumCircuit</code>","text":"<p>We consider <code>QuantumCircuit</code> to be an abstract object, i.e. it is not tied to any backend. However, it blocks are even more abstract. This is because we consider <code>QuantumCircuit</code>s \"real\", whereas the blocks are largely considered just syntax.</p> <p>Unitary <code>QuantumCircuits</code> (this encompasses digital, or gate-based, circuits as well as analog circuits) are constructed by [<code>PrimitiveBlocks</code>] using a syntax that allows you to execute them in sequence, dubbed <code>ChainBlock</code> in the code, or in parallel (i.e. at the same time) where applicable, dubbed <code>KronBlock</code> in the code. Notice that this differs from other packages by providing more control of the layout of the circuit than conventional packages like Qiskit, and from Yao where the blocks are the primary type.</p>"},{"location":"tutorials/development/architecture/#quantummodel","title":"<code>QuantumModel</code>","text":"<p><code>QuantumModel</code>s are meant to be the main entry point for quantum computations in <code>qadence</code>. In general, they take one or more quantum circuit as input and they wrap all the necessary boiler plate code to make the circuit executable and differentiable on the chosen backend.</p> <p>Models are meant to be specific for a certain kind of quantum problem or algorithm and you can easily create new ones starting from the base class <code>QuantumModel</code>, as explained in the custom model tutorial. Currently, Qadence offers a <code>QNN</code> model class which provides convenient methods to work with quantum neural networks with multi-dimensional inputs and outputs.</p>"},{"location":"tutorials/development/architecture/#differentiablebackend","title":"<code>DifferentiableBackend</code>","text":"<p>The differentiable backend is a thin wrapper which takes as input a <code>QuantumCircuit</code> instance and a chosen quantum backend and make the circuit execution routines (expectation value, overalap, etc.) differentiable. Qadence offers both a PyTorch and Jax differentiation engine.</p>"},{"location":"tutorials/development/architecture/#quantum-backend","title":"Quantum <code>Backend</code>","text":"<p>For execution the primary object is the <code>Backend</code>. Backends maintain the same user-facing interface, and internally connects to other libraries to execute circuits. Those other libraries can execute the code on QPUs and local or cloud-based emulators. The <code>Backends</code> use PyTorch tensors to represent data and leverages PyTorchs autograd to help compute derivatives of circuits.</p>"},{"location":"tutorials/development/architecture/#symbolic-parameters","title":"Symbolic parameters","text":"<p>To illustrate how parameters work in Qadence, let's consider the following simple block composed of just two rotations:</p> <pre><code>import sympy\nfrom qadence import Parameter, RX\n\nparam = Parameter(\"phi\", trainable=False)\nblock = RX(0, param) * RX(1, sympy.acos(param))\n</code></pre> <p>The rotation angles assigned to <code>RX</code> (and to any Qadence quantum operation) are defined as arbitrary expressions of <code>Parameter</code>'s. <code>Parameter</code> is a subclass of <code>sympy.Symbol</code>, thus fully interoperable with it.</p> <p>To assign values of the parameter <code>phi</code> in a quantum model, one should use a dictionary containing the a key with parameter name and the corresponding values values:</p> <pre><code>import torch\nfrom qadence import run\n\nvalues = {\"phi\": torch.rand(10)}\nwf = run(block, values=values)\n</code></pre> <p>This is the only interface for parameter assignment exposed to the user. Under the hood, parameters applied to every quantum operation are identified in different ways:</p> <ul> <li> <p>By default, with a stringified version of the <code>sympy</code> expression supplied to the quantum operation. Notice that multiple operations can have the same expression.</p> </li> <li> <p>In certain case, e.g. for constructing parameter shift rules, one must access a unique identifier of the parameter for each quantum operation. Therefore, Qadence also creates unique identifiers for each parametrized operation (see the <code>ParamMap</code> class).</p> </li> </ul> <p>By default, when one constructs a new backend, the parameter identifiers are the <code>sympy</code> expressions which are used when converting an abstract block into a native circuit for the chosen backend. However, one can use the unique identifiers as parameter names by setting the private flag <code>_use_gate_params</code> to <code>True</code> in the backend configuration <code>BackendConfiguration</code>. This is automatically set when PSR differentiation is selected (see next section for more details).</p> <p>You can see the logic for choosing the parameter identifier in <code>get_param_name</code>.</p>"},{"location":"tutorials/development/architecture/#differentiation-with-parameter-shift-rules-psr","title":"Differentiation with parameter shift rules (PSR)","text":"<p>In Qadence, parameter shift rules are applied by implementing a custom <code>torch.autograd.Function</code> class for PyTorch and the <code>custom_vjp</code> in the Jax Engine, respectively.</p> <p>A custom PyTorch <code>Function</code> looks like this:</p> <pre><code>import torch\nfrom torch.autograd import Function\n\nclass CustomFunction(Function):\n\n    # forward pass implementation giving the output of the module\n    @staticmethod\n    def forward(ctx, inputs: torch.Tensor, params: torch.Tensor):\n        ctx.save_for_backward(inputs, params)\n        ...\n\n    # backward pass implementation giving the derivative of the module\n    # with respect to the parameters. This must return the whole vector-jacobian\n    # product to integrate within the autograd engine\n    @staticmethod\n    def backward(ctx, grad_output: torch.Tensor):\n        inputs, params = ctx.saved_tensors\n        ...\n</code></pre> <p>The class <code>PSRExpectation</code> under <code>qadence.engines.torch.differentiable_expectation</code> implements parameter shift rules for all parameters using a custom function as the one above. There are a few implementation details to keep in mind if you want to modify the PSR code:</p> <ul> <li> <p>PyTorch <code>Function</code> only works with tensor arguments. Parameters in Qadence are passed around as   dictionaries with parameter names as keys and current parameter values (tensors)   as values. This works for both variational and feature parameters. However, the <code>Function</code> class   only work with PyTorch tensors as input, not dictionaries. Therefore, the forward pass of   <code>PSRExpectation</code> accepts one argument <code>param_keys</code> with the   parameter keys and a variadic positional argument <code>param_values</code> with the parameter values one by   one. The dictionary is reconstructed within the <code>forward()</code> pass body.</p> </li> <li> <p>Higher-order derivatives with PSR. Higher-order PSR derivatives can be tricky. Parameter shift   rules calls, under the hood, the <code>QuantumBackend</code> expectation value routine that usually yield a   non-differentiable output. Therefore, a second call to the backward pass would not work. However,   Qadence employs a very simple trick to make higher-order derivatives work: instead of using   directly the expectation value of the quantum backend, the PSR backward pass uses the PSR forward   pass itself as expectation value function (see the code below). In this way, multiple calls to the   backward pass are allowed since the <code>expectation_fn</code> routine is always differentiable by   definition. Notice that this implementation is simple but suboptimal since, in some corner cases,   higher-order derivates might include some repeated terms that, with this implementation, are   always recomputed.</p> </li> </ul> <pre><code># expectation value used in the PSR backward pass\ndef expectation_fn(params: dict[str, Tensor]) -&gt; Tensor:\n    return PSRExpectation.apply(\n        ctx.expectation_fn,\n        ctx.param_psrs,\n        params.keys(),\n        *params.values(),\n    )\n</code></pre> <ul> <li> <p>Operation parameters must be uniquely identified for PSR to work. Parameter shift rules work at the level of individual quantum operations. This means that, given a parameter <code>x</code>, one needs to sum the contributions from shifting the parameter values of all the operation where the parameter <code>x</code> appears. When constructing the PSR rules, one must access a unique parameter identifier for each operation even if the corresponding user-facing parameter is the same. Therefore, when PSR differentiation is selected, the flag <code>_use_gate_params</code> is automatically set to <code>True</code> in the backend configuration <code>BackendConfiguration</code> (see previous section).</p> </li> <li> <p>PSR must not be applied to observable parameters. In Qadence, Pauli observables can also be parametrized. However, the tunable parameters of observables are purely classical and should not be included in the differentiation with PSRs. However, the quantum expectation value depends on them, thus they still need to enter into the PSR evaluation. To solve this issue, the code sets the <code>requires_grad</code> attribute of all observable parameters to <code>False</code> when constructing the PSRs for the circuit as in the snippet below:</p> </li> </ul> <pre><code>for obs in observable:\n    for param_id, _ in uuid_to_eigen(obs).items():\n        param_to_psr[param_id] = lambda x: torch.tensor([0.0], requires_grad=False)\n</code></pre>"},{"location":"tutorials/development/draw/","title":"<code>qadence.draw</code> example plots","text":"<p>Mostly for quick, manual checking of correct plotting output.</p> <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\n</code></pre> %3 5169fa77ea8f4ba9930c397c768ffa14 0 834aa9acf0d24c66933c0905939d4096 X 5169fa77ea8f4ba9930c397c768ffa14--834aa9acf0d24c66933c0905939d4096 3d5462b7fc0648228da0ec4ae3ca62ca 1 7fe4a475704a479fbccc338f6166a0cb 834aa9acf0d24c66933c0905939d4096--7fe4a475704a479fbccc338f6166a0cb 1a7bb2afb5b44b33a748a9b81e274d2c c3aef1da7b5a4cbeb65b3589751edd2e Y 3d5462b7fc0648228da0ec4ae3ca62ca--c3aef1da7b5a4cbeb65b3589751edd2e c3aef1da7b5a4cbeb65b3589751edd2e--1a7bb2afb5b44b33a748a9b81e274d2c <pre><code>from qadence import X, Y, chain\nfrom qadence.draw import display\n\nb = chain(X(0), Y(0))\n</code></pre> %3 01651670aaf84636bf3246b107c97169 0 f96610536155427c9e79cd0723a58acd X 01651670aaf84636bf3246b107c97169--f96610536155427c9e79cd0723a58acd 28778a6b65274283bd6d28ab1e4acaa0 Y f96610536155427c9e79cd0723a58acd--28778a6b65274283bd6d28ab1e4acaa0 6ec6df2e13194223ad89a97d2e52d203 28778a6b65274283bd6d28ab1e4acaa0--6ec6df2e13194223ad89a97d2e52d203 <pre><code>from qadence import X, Y, chain\nfrom qadence.draw import display\n\nb = chain(X(0), Y(1))\n</code></pre> %3 8a6a51499c2849ca8165e15a092f3d41 0 8ef5ce82edaa4d5db45e61d4486cf654 X 8a6a51499c2849ca8165e15a092f3d41--8ef5ce82edaa4d5db45e61d4486cf654 df4907eeebf14516a96dabb372ac6e64 1 4eb86d5482964f9194bf85d2fa05b4de 8ef5ce82edaa4d5db45e61d4486cf654--4eb86d5482964f9194bf85d2fa05b4de 82107f467f344027b926868e18121924 4eb86d5482964f9194bf85d2fa05b4de--82107f467f344027b926868e18121924 39296710ca1f47b5847c656baf9a9f01 ff25e08afbb84c838f9d876f6bbeac46 df4907eeebf14516a96dabb372ac6e64--ff25e08afbb84c838f9d876f6bbeac46 4c1627e6013d42319868f23581146e22 Y ff25e08afbb84c838f9d876f6bbeac46--4c1627e6013d42319868f23581146e22 4c1627e6013d42319868f23581146e22--39296710ca1f47b5847c656baf9a9f01 <pre><code>from qadence import X, Y, add\nfrom qadence.draw import display\n\nb = add(X(0), Y(1), X(2))\n</code></pre> %3 cluster_ab1cc49044b74e1a8d465121bb681fe7 3fbc11aa81ec454b8b1144600b3a5e79 0 54be5e78c4d7490daa2fe870d31d5d79 3fbc11aa81ec454b8b1144600b3a5e79--54be5e78c4d7490daa2fe870d31d5d79 865fe3075b25405891c3d919560bc80c 1 499903a66b184098984855f6bb976941 54be5e78c4d7490daa2fe870d31d5d79--499903a66b184098984855f6bb976941 3f0165d5c21240c59a4e31c7b9569a6d 057221b1be9e46419b47ef63a2ac9105 AddBlock 865fe3075b25405891c3d919560bc80c--057221b1be9e46419b47ef63a2ac9105 14121ba001e34541bca2d02b2ddb2915 2 057221b1be9e46419b47ef63a2ac9105--3f0165d5c21240c59a4e31c7b9569a6d 7f83400b3fac44c296e97bd8f2db6ab7 f106c26e915247e49df6b9880c28ab39 14121ba001e34541bca2d02b2ddb2915--f106c26e915247e49df6b9880c28ab39 f106c26e915247e49df6b9880c28ab39--7f83400b3fac44c296e97bd8f2db6ab7 <pre><code>from qadence import CNOT, RX, HamEvo, X, Y, Z, chain, kron\n\nrx = kron(RX(3,0.5), RX(2, \"x\"))\nrx.tag = \"rx\"\ngen = chain(Z(i) for i in range(4))\n\n# `chain` puts things in sequence\nblock = chain(\n    kron(X(0), Y(1), rx),\n    CNOT(2,3),\n    HamEvo(gen, 10)\n)\n</code></pre> %3 cluster_10cbc520f1164e67abed97412c5e085f cluster_9e6159c518204210a22912041c395205 rx 9924e22f26c0499a8cd0b37fb4af19bf 0 0f1d666f2a964838908c54119744bffd X 9924e22f26c0499a8cd0b37fb4af19bf--0f1d666f2a964838908c54119744bffd a6b0442c1b324a2f9938f40c1a365bac 1 e7797dd8dabb4cfb8131e50487f69de9 0f1d666f2a964838908c54119744bffd--e7797dd8dabb4cfb8131e50487f69de9 5103a89b86ee40e8bcd4df5cca509ce8 e7797dd8dabb4cfb8131e50487f69de9--5103a89b86ee40e8bcd4df5cca509ce8 9ceaeec7806e444ba5a0afc292af5218 5103a89b86ee40e8bcd4df5cca509ce8--9ceaeec7806e444ba5a0afc292af5218 710cbf6908c2451b98482272715e669d 05ac1ec028934474abab7ac29aeede45 Y a6b0442c1b324a2f9938f40c1a365bac--05ac1ec028934474abab7ac29aeede45 6e94c2dbf34a42c68ae32c070ae1dc90 2 a241ca2c444942659c4c16dd04a08084 05ac1ec028934474abab7ac29aeede45--a241ca2c444942659c4c16dd04a08084 a8f7800e3d0440059b694d031324a1c7 HamEvo a241ca2c444942659c4c16dd04a08084--a8f7800e3d0440059b694d031324a1c7 a8f7800e3d0440059b694d031324a1c7--710cbf6908c2451b98482272715e669d 02199127bff84ea4a8efdbbb23b6ee72 af7494a2dd3643818f5a84fa9ea58d4a RX(x) 6e94c2dbf34a42c68ae32c070ae1dc90--af7494a2dd3643818f5a84fa9ea58d4a 7fcd5c22fc24453ca2c270ebeb845092 3 396577f7f88545df9677a630815382f8 af7494a2dd3643818f5a84fa9ea58d4a--396577f7f88545df9677a630815382f8 f54c45eb652d4ecfb4eb4828de9c044f t = 10 396577f7f88545df9677a630815382f8--f54c45eb652d4ecfb4eb4828de9c044f f54c45eb652d4ecfb4eb4828de9c044f--02199127bff84ea4a8efdbbb23b6ee72 e801dea818f844c09eb9cb0a4be30b70 894bdfbaee084860bc7bd21dc7baf73b RX(0.5) 7fcd5c22fc24453ca2c270ebeb845092--894bdfbaee084860bc7bd21dc7baf73b 595719327edd4424bbc1740405668145 X 894bdfbaee084860bc7bd21dc7baf73b--595719327edd4424bbc1740405668145 595719327edd4424bbc1740405668145--396577f7f88545df9677a630815382f8 e230ec8f4470430a9c3fa3ad6b8a05dd 595719327edd4424bbc1740405668145--e230ec8f4470430a9c3fa3ad6b8a05dd e230ec8f4470430a9c3fa3ad6b8a05dd--e801dea818f844c09eb9cb0a4be30b70 <pre><code>from qadence import feature_map, hea, chain\n\nblock = chain(feature_map(4, reupload_scaling=\"Tower\"), hea(4,2))\n</code></pre> %3 cluster_97e7e860ec8948019fba92fc56fe7a9c HEA cluster_8720137b01d446e4820c60dae17c3d6b Tower Fourier FM 0d3fc45be0cc42e9be1112dd72328be9 0 bae5ad713c484ce38268ff44be5d34c5 RX(1.0*phi) 0d3fc45be0cc42e9be1112dd72328be9--bae5ad713c484ce38268ff44be5d34c5 487f6ff4ea0e4a2f8a0f2834e6521750 1 bb581cea378f4c459dc26dc3a3fb0fe1 RX(theta\u2080) bae5ad713c484ce38268ff44be5d34c5--bb581cea378f4c459dc26dc3a3fb0fe1 d1ad45a2839e4208b95fc6413a0cbba3 RY(theta\u2084) bb581cea378f4c459dc26dc3a3fb0fe1--d1ad45a2839e4208b95fc6413a0cbba3 65d9a39eae444b9d8d6e301194a24803 RX(theta\u2088) d1ad45a2839e4208b95fc6413a0cbba3--65d9a39eae444b9d8d6e301194a24803 cc8b07ba149248888842bb371660dffe 65d9a39eae444b9d8d6e301194a24803--cc8b07ba149248888842bb371660dffe 8b977cb23c5d4dea8f903b8dc08fc6c0 cc8b07ba149248888842bb371660dffe--8b977cb23c5d4dea8f903b8dc08fc6c0 c1ff44029c814f50ba44f904674342a9 RX(theta\u2081\u2082) 8b977cb23c5d4dea8f903b8dc08fc6c0--c1ff44029c814f50ba44f904674342a9 ecd1695d7aca43ec9235e9eeeaddab0e RY(theta\u2081\u2086) c1ff44029c814f50ba44f904674342a9--ecd1695d7aca43ec9235e9eeeaddab0e c26237e5fc6f4f54a94762b668a61896 RX(theta\u2082\u2080) ecd1695d7aca43ec9235e9eeeaddab0e--c26237e5fc6f4f54a94762b668a61896 257196d7d88a41d5a09023dd47e04862 c26237e5fc6f4f54a94762b668a61896--257196d7d88a41d5a09023dd47e04862 737920bbfad9401cb318f0a409db4934 257196d7d88a41d5a09023dd47e04862--737920bbfad9401cb318f0a409db4934 37611f50f8224462957be1f8137228f4 737920bbfad9401cb318f0a409db4934--37611f50f8224462957be1f8137228f4 8bdcdc5d7c884262bf850da46c53850e c47278effe4b475b94458224dccfc711 RX(2.0*phi) 487f6ff4ea0e4a2f8a0f2834e6521750--c47278effe4b475b94458224dccfc711 fccc3adce43d4865b0e5c65d79f769b6 2 a63b95aabfff45ad8e16f2cf2d3c36bd RX(theta\u2081) c47278effe4b475b94458224dccfc711--a63b95aabfff45ad8e16f2cf2d3c36bd b5e6db08689048c68b7c9c8f9f38c4b4 RY(theta\u2085) a63b95aabfff45ad8e16f2cf2d3c36bd--b5e6db08689048c68b7c9c8f9f38c4b4 b9669e20d4aa48349b6ccf2207270d95 RX(theta\u2089) b5e6db08689048c68b7c9c8f9f38c4b4--b9669e20d4aa48349b6ccf2207270d95 4e81f436471b4921bdb604f183c4bd59 X b9669e20d4aa48349b6ccf2207270d95--4e81f436471b4921bdb604f183c4bd59 4e81f436471b4921bdb604f183c4bd59--cc8b07ba149248888842bb371660dffe c8f1a1884b9e4034a1386271c7c98f20 4e81f436471b4921bdb604f183c4bd59--c8f1a1884b9e4034a1386271c7c98f20 795cb673c1d741339960e04956469f29 RX(theta\u2081\u2083) c8f1a1884b9e4034a1386271c7c98f20--795cb673c1d741339960e04956469f29 435cc06d95c24c35b24d5607b782972f RY(theta\u2081\u2087) 795cb673c1d741339960e04956469f29--435cc06d95c24c35b24d5607b782972f 771dcfecb3264543935859dc700a4245 RX(theta\u2082\u2081) 435cc06d95c24c35b24d5607b782972f--771dcfecb3264543935859dc700a4245 01d64a2e21064d61be3d0a1aef974d4c X 771dcfecb3264543935859dc700a4245--01d64a2e21064d61be3d0a1aef974d4c 01d64a2e21064d61be3d0a1aef974d4c--257196d7d88a41d5a09023dd47e04862 f889520b4cdc440ea8feab6f57aa6e56 01d64a2e21064d61be3d0a1aef974d4c--f889520b4cdc440ea8feab6f57aa6e56 f889520b4cdc440ea8feab6f57aa6e56--8bdcdc5d7c884262bf850da46c53850e 21ecb55aa677493a9243a25ab982052a 3a9370f7ff444b90b315ef8baa281ac9 RX(3.0*phi) fccc3adce43d4865b0e5c65d79f769b6--3a9370f7ff444b90b315ef8baa281ac9 7239341656fb4747924fb4f52eb518ff 3 21f91f3db3aa44318ac7d7b131b54f69 RX(theta\u2082) 3a9370f7ff444b90b315ef8baa281ac9--21f91f3db3aa44318ac7d7b131b54f69 629d4635c0024d3894b0f0ccc84ff8cc RY(theta\u2086) 21f91f3db3aa44318ac7d7b131b54f69--629d4635c0024d3894b0f0ccc84ff8cc 2d80e3a436114220a23d91ea81f02332 RX(theta\u2081\u2080) 629d4635c0024d3894b0f0ccc84ff8cc--2d80e3a436114220a23d91ea81f02332 cb1151a9cfc644479222fc1912333ca7 2d80e3a436114220a23d91ea81f02332--cb1151a9cfc644479222fc1912333ca7 f71193e58e454f99b24b77a283b89469 X cb1151a9cfc644479222fc1912333ca7--f71193e58e454f99b24b77a283b89469 f71193e58e454f99b24b77a283b89469--c8f1a1884b9e4034a1386271c7c98f20 70b708a7c8ed40268b7c3172292514a3 RX(theta\u2081\u2084) f71193e58e454f99b24b77a283b89469--70b708a7c8ed40268b7c3172292514a3 4d1d04e2ff214df58ae88a53e22e73e9 RY(theta\u2081\u2088) 70b708a7c8ed40268b7c3172292514a3--4d1d04e2ff214df58ae88a53e22e73e9 a54563ec6068488daf1cba2648a72822 RX(theta\u2082\u2082) 4d1d04e2ff214df58ae88a53e22e73e9--a54563ec6068488daf1cba2648a72822 ef565cf219a54a7d950bb058d20874f7 a54563ec6068488daf1cba2648a72822--ef565cf219a54a7d950bb058d20874f7 f33d02ce2c8a421cbd4de42f83487bc7 X ef565cf219a54a7d950bb058d20874f7--f33d02ce2c8a421cbd4de42f83487bc7 f33d02ce2c8a421cbd4de42f83487bc7--f889520b4cdc440ea8feab6f57aa6e56 f33d02ce2c8a421cbd4de42f83487bc7--21ecb55aa677493a9243a25ab982052a 79584573a6b74da4bd6802af7fc566d5 e31fc2df213b44b69c1d958d69e14d3e RX(4.0*phi) 7239341656fb4747924fb4f52eb518ff--e31fc2df213b44b69c1d958d69e14d3e 469b2edf0ad3460a91398fcf6908e31f RX(theta\u2083) e31fc2df213b44b69c1d958d69e14d3e--469b2edf0ad3460a91398fcf6908e31f 22c389ec16e641c8996ebed0512345df RY(theta\u2087) 469b2edf0ad3460a91398fcf6908e31f--22c389ec16e641c8996ebed0512345df c9e1d3ada3134d118125b7f7ec765287 RX(theta\u2081\u2081) 22c389ec16e641c8996ebed0512345df--c9e1d3ada3134d118125b7f7ec765287 48805f61cd784376906f5aed5d7502dc X c9e1d3ada3134d118125b7f7ec765287--48805f61cd784376906f5aed5d7502dc 48805f61cd784376906f5aed5d7502dc--cb1151a9cfc644479222fc1912333ca7 2844cc8372c2419ca4c7b232eda6d5a6 48805f61cd784376906f5aed5d7502dc--2844cc8372c2419ca4c7b232eda6d5a6 bed557b6174d40179c46ad6b118ea9eb RX(theta\u2081\u2085) 2844cc8372c2419ca4c7b232eda6d5a6--bed557b6174d40179c46ad6b118ea9eb 9c518762214443ea87bd252bc416ed47 RY(theta\u2081\u2089) bed557b6174d40179c46ad6b118ea9eb--9c518762214443ea87bd252bc416ed47 79d0d3601b7f40be9247aed42b2c4b60 RX(theta\u2082\u2083) 9c518762214443ea87bd252bc416ed47--79d0d3601b7f40be9247aed42b2c4b60 1c00f70d8a1d4875ace028e7a6a52536 X 79d0d3601b7f40be9247aed42b2c4b60--1c00f70d8a1d4875ace028e7a6a52536 1c00f70d8a1d4875ace028e7a6a52536--ef565cf219a54a7d950bb058d20874f7 e93d9912724f4c8b8003d9739dda9d12 1c00f70d8a1d4875ace028e7a6a52536--e93d9912724f4c8b8003d9739dda9d12 e93d9912724f4c8b8003d9739dda9d12--79584573a6b74da4bd6802af7fc566d5 <pre><code>from qadence import QuantumModel, QuantumCircuit, total_magnetization, hea\n\nmodel = QuantumModel(QuantumCircuit(3, hea(3,2)), total_magnetization(3))\n</code></pre> %3 cluster_ad2994cf555241499abe5321e33fefc6 Obs. cluster_79600a7cfa1947bf8022d2d8461b70d4 cluster_2ee17acdd07d413f8679e6fcf13f8472 HEA 72f162764ca54c628dd073e5cd7874fb 0 bd40b714d09a4fd2b1d358cf7b64d034 RX(theta\u2080) 72f162764ca54c628dd073e5cd7874fb--bd40b714d09a4fd2b1d358cf7b64d034 c06d4f049912437d90a8ce488ba5f224 1 3c8d630cac8340afa50ab19202608f22 RY(theta\u2083) bd40b714d09a4fd2b1d358cf7b64d034--3c8d630cac8340afa50ab19202608f22 6b8d1594d96749b09590354477207dc2 RX(theta\u2086) 3c8d630cac8340afa50ab19202608f22--6b8d1594d96749b09590354477207dc2 9375cfe09f534737aeb648855580c265 6b8d1594d96749b09590354477207dc2--9375cfe09f534737aeb648855580c265 a28f74d4afc04785968a6af738b5e7cf 9375cfe09f534737aeb648855580c265--a28f74d4afc04785968a6af738b5e7cf 9368f684b35e4184992d35885f4bd9d6 RX(theta\u2089) a28f74d4afc04785968a6af738b5e7cf--9368f684b35e4184992d35885f4bd9d6 b4b34a53bcdd4165a758bfb06939d9d0 RY(theta\u2081\u2082) 9368f684b35e4184992d35885f4bd9d6--b4b34a53bcdd4165a758bfb06939d9d0 93f8e7c7d2a44ba891afcbcf896d3c01 RX(theta\u2081\u2085) b4b34a53bcdd4165a758bfb06939d9d0--93f8e7c7d2a44ba891afcbcf896d3c01 116aad07351b43a7b34753583076b77b 93f8e7c7d2a44ba891afcbcf896d3c01--116aad07351b43a7b34753583076b77b 1aa0ee66b6a540bbb92f2c1d722f574b 116aad07351b43a7b34753583076b77b--1aa0ee66b6a540bbb92f2c1d722f574b a7a5d446365b47d5be1e120c1ddeaabd 1aa0ee66b6a540bbb92f2c1d722f574b--a7a5d446365b47d5be1e120c1ddeaabd 083636af32a74574a7ced379d423ea45 a7a5d446365b47d5be1e120c1ddeaabd--083636af32a74574a7ced379d423ea45 9ad634eafc894f83a43456c1f9e5432e a3b1775d1dac4873bb609e292a8f30e8 RX(theta\u2081) c06d4f049912437d90a8ce488ba5f224--a3b1775d1dac4873bb609e292a8f30e8 bebcb492d5194adf8e4111907d78fa2a 2 3b7ec544873841b09e2eebb285250aef RY(theta\u2084) a3b1775d1dac4873bb609e292a8f30e8--3b7ec544873841b09e2eebb285250aef 17ddb8291fd84398bf6ab833106b1a40 RX(theta\u2087) 3b7ec544873841b09e2eebb285250aef--17ddb8291fd84398bf6ab833106b1a40 f6c3d23b9ad8496c978a8ff08c94c760 X 17ddb8291fd84398bf6ab833106b1a40--f6c3d23b9ad8496c978a8ff08c94c760 f6c3d23b9ad8496c978a8ff08c94c760--9375cfe09f534737aeb648855580c265 33333a3907b14764819bfb886cd5f658 f6c3d23b9ad8496c978a8ff08c94c760--33333a3907b14764819bfb886cd5f658 f8163e6a924146a2ac7bc26aa978c90b RX(theta\u2081\u2080) 33333a3907b14764819bfb886cd5f658--f8163e6a924146a2ac7bc26aa978c90b 7cad4fe86a81413aae975e27cd9836d8 RY(theta\u2081\u2083) f8163e6a924146a2ac7bc26aa978c90b--7cad4fe86a81413aae975e27cd9836d8 4afb55c52c5b4d418209391bfd184052 RX(theta\u2081\u2086) 7cad4fe86a81413aae975e27cd9836d8--4afb55c52c5b4d418209391bfd184052 4a648bace39f4d0b98390561add5581e X 4afb55c52c5b4d418209391bfd184052--4a648bace39f4d0b98390561add5581e 4a648bace39f4d0b98390561add5581e--116aad07351b43a7b34753583076b77b 587f43f4b4444bd082859e413005d3e1 4a648bace39f4d0b98390561add5581e--587f43f4b4444bd082859e413005d3e1 9ec7c7e1a77c49a594890ca5c027116c AddBlock 587f43f4b4444bd082859e413005d3e1--9ec7c7e1a77c49a594890ca5c027116c 9ec7c7e1a77c49a594890ca5c027116c--9ad634eafc894f83a43456c1f9e5432e bb4cb0a7dde542e8b0087fee637836e4 54b2a37cb88c465792468704782ab933 RX(theta\u2082) bebcb492d5194adf8e4111907d78fa2a--54b2a37cb88c465792468704782ab933 1f3cd11583ff4bc2bfc9de8e3c1070c4 RY(theta\u2085) 54b2a37cb88c465792468704782ab933--1f3cd11583ff4bc2bfc9de8e3c1070c4 825cb9fa0f354f069fd2884d9c2482ed RX(theta\u2088) 1f3cd11583ff4bc2bfc9de8e3c1070c4--825cb9fa0f354f069fd2884d9c2482ed a80e5b591522413b8d9b71405fa6da07 825cb9fa0f354f069fd2884d9c2482ed--a80e5b591522413b8d9b71405fa6da07 bc43a373130e462290a9a78a1d7ad039 X a80e5b591522413b8d9b71405fa6da07--bc43a373130e462290a9a78a1d7ad039 bc43a373130e462290a9a78a1d7ad039--33333a3907b14764819bfb886cd5f658 129b27d19a7b4b02be5b3c5f65f3c291 RX(theta\u2081\u2081) bc43a373130e462290a9a78a1d7ad039--129b27d19a7b4b02be5b3c5f65f3c291 4d90c392c59341aba65b18dae3a25e3a RY(theta\u2081\u2084) 129b27d19a7b4b02be5b3c5f65f3c291--4d90c392c59341aba65b18dae3a25e3a 1e4ca633eff34415b61944c0dd8b9c6d RX(theta\u2081\u2087) 4d90c392c59341aba65b18dae3a25e3a--1e4ca633eff34415b61944c0dd8b9c6d 63281dff7b954d468ad235b71d20bd4a 1e4ca633eff34415b61944c0dd8b9c6d--63281dff7b954d468ad235b71d20bd4a 4f278df99dfd4e498cceb49a8a122a0c X 63281dff7b954d468ad235b71d20bd4a--4f278df99dfd4e498cceb49a8a122a0c 4f278df99dfd4e498cceb49a8a122a0c--587f43f4b4444bd082859e413005d3e1 8d8385207dc94d0589b454c5f979fcd7 4f278df99dfd4e498cceb49a8a122a0c--8d8385207dc94d0589b454c5f979fcd7 8d8385207dc94d0589b454c5f979fcd7--bb4cb0a7dde542e8b0087fee637836e4 <pre><code>from qadence import *\n\nb = chain(SWAP(0,1), SWAP(0,3))\n</code></pre> %3 c47e43301182430e8419064d6441df72 0 1d6147762aaf48fdb3c157be04fd11bf c47e43301182430e8419064d6441df72--1d6147762aaf48fdb3c157be04fd11bf 0cfb8f15c46249f59486e86a72d0364e 1 e48c089cd48645d28c63648e07aa06c4 49c53d2da0734b04a6c20ced7b132c37 1d6147762aaf48fdb3c157be04fd11bf--49c53d2da0734b04a6c20ced7b132c37 055a48e477fe4c1b81bed65db1abfcb7 e48c089cd48645d28c63648e07aa06c4--055a48e477fe4c1b81bed65db1abfcb7 97ae454ca55a48e488ed37eef47adf0a 0f0c433b7b204f9aab3538c908e62d24 055a48e477fe4c1b81bed65db1abfcb7--0f0c433b7b204f9aab3538c908e62d24 89cc294487f54b2abd48f379855afd27 97ae454ca55a48e488ed37eef47adf0a--89cc294487f54b2abd48f379855afd27 c32b00767b73410c99db6fdf239dd9b1 3366640fef7944e182ceb7951e757f2c 0cfb8f15c46249f59486e86a72d0364e--3366640fef7944e182ceb7951e757f2c e391f71ead1c4ec59245022f7061687a 2 3366640fef7944e182ceb7951e757f2c--e48c089cd48645d28c63648e07aa06c4 ed6f759e56704c18bc972af4ed8edc86 49c53d2da0734b04a6c20ced7b132c37--ed6f759e56704c18bc972af4ed8edc86 f2adfe9bdc9d4b2980148089840990d3 ed6f759e56704c18bc972af4ed8edc86--f2adfe9bdc9d4b2980148089840990d3 f2adfe9bdc9d4b2980148089840990d3--c32b00767b73410c99db6fdf239dd9b1 1bfc95309e6d4f67ada2e5e12eb09d31 4b059725dc7f408d998248d78ec04722 e391f71ead1c4ec59245022f7061687a--4b059725dc7f408d998248d78ec04722 46d96b0c78a94455b8cdf094decd7fcb 3 1e125b0603014c94b693e9da02065705 4b059725dc7f408d998248d78ec04722--1e125b0603014c94b693e9da02065705 28b45e5b38554f7ca1c55e85bf0192ef 1e125b0603014c94b693e9da02065705--28b45e5b38554f7ca1c55e85bf0192ef 31aba041d07c4a7eaf6b455d6c8bddb1 28b45e5b38554f7ca1c55e85bf0192ef--31aba041d07c4a7eaf6b455d6c8bddb1 31aba041d07c4a7eaf6b455d6c8bddb1--1bfc95309e6d4f67ada2e5e12eb09d31 79e1ae0b071140feb1f844633e4b7b67 281be78c3e524a18909679d3f2e99451 46d96b0c78a94455b8cdf094decd7fcb--281be78c3e524a18909679d3f2e99451 fd14d72977e646618aabbfc8222de809 281be78c3e524a18909679d3f2e99451--fd14d72977e646618aabbfc8222de809 b76215a9752a4f03b4441d46be33169d fd14d72977e646618aabbfc8222de809--b76215a9752a4f03b4441d46be33169d b76215a9752a4f03b4441d46be33169d--97ae454ca55a48e488ed37eef47adf0a 0f0c433b7b204f9aab3538c908e62d24--79e1ae0b071140feb1f844633e4b7b67 <pre><code>from qadence import *\n\nb = chain(CPHASE(0, 1, 0.5), CPHASE(0, 2, 0.5), CPHASE(0, 3, 0.5))\n</code></pre> %3 e740d83ea6824560af39f1c11cc79f52 0 90ff8ea39faa4285a96c56a7cfca4f1f e740d83ea6824560af39f1c11cc79f52--90ff8ea39faa4285a96c56a7cfca4f1f 6325c4c1cd7b4f58855272dfe46e7643 1 b96657de48a34ccca0a6653dbd6a45d9 90ff8ea39faa4285a96c56a7cfca4f1f--b96657de48a34ccca0a6653dbd6a45d9 11513b6aca4443a18d56547850a44364 b96657de48a34ccca0a6653dbd6a45d9--11513b6aca4443a18d56547850a44364 f199a61ea29440e09c17b89b97de836e 11513b6aca4443a18d56547850a44364--f199a61ea29440e09c17b89b97de836e d5dd69d622204f5492e9d426c2496393 179d9a9ae7044086af12abbda3c38a4b PHASE(0.5) 6325c4c1cd7b4f58855272dfe46e7643--179d9a9ae7044086af12abbda3c38a4b d7c008a4ad1449faa3f82cf5a01024db 2 179d9a9ae7044086af12abbda3c38a4b--90ff8ea39faa4285a96c56a7cfca4f1f 4b406674ea6e4a5888bb09e253f16827 179d9a9ae7044086af12abbda3c38a4b--4b406674ea6e4a5888bb09e253f16827 e912f262bd6f4a8ab124aa59b8f04790 4b406674ea6e4a5888bb09e253f16827--e912f262bd6f4a8ab124aa59b8f04790 e912f262bd6f4a8ab124aa59b8f04790--d5dd69d622204f5492e9d426c2496393 2bd1ccd150c14609b1215b4710603f9c 501c42b85c704672b395d852d65db294 d7c008a4ad1449faa3f82cf5a01024db--501c42b85c704672b395d852d65db294 19466cfe11e74db6a5460b9afcfa42be 3 7ba92278a92342aaaaa8ac7ee38d7cd0 PHASE(0.5) 501c42b85c704672b395d852d65db294--7ba92278a92342aaaaa8ac7ee38d7cd0 7ba92278a92342aaaaa8ac7ee38d7cd0--b96657de48a34ccca0a6653dbd6a45d9 7155000fe87a4edda97faa93d4838eb9 7ba92278a92342aaaaa8ac7ee38d7cd0--7155000fe87a4edda97faa93d4838eb9 7155000fe87a4edda97faa93d4838eb9--2bd1ccd150c14609b1215b4710603f9c a8a95d0403034313980b85da9267da7c 20cb37591d384a95b15689743b41077a 19466cfe11e74db6a5460b9afcfa42be--20cb37591d384a95b15689743b41077a 242bc31065a34660a9c3c4fc625908e5 20cb37591d384a95b15689743b41077a--242bc31065a34660a9c3c4fc625908e5 f74fec97ca7e41479ccdfd05939dbccb PHASE(0.5) 242bc31065a34660a9c3c4fc625908e5--f74fec97ca7e41479ccdfd05939dbccb f74fec97ca7e41479ccdfd05939dbccb--11513b6aca4443a18d56547850a44364 f74fec97ca7e41479ccdfd05939dbccb--a8a95d0403034313980b85da9267da7c"},{"location":"tutorials/development/draw/#developer-documentation","title":"Developer documentation","text":"<p>This section contains examples in pure graphviz that can be used to understand roughly what is done in the actual drawing backend.</p> <pre><code>import graphviz\n\nfont_name = \"Sans-Serif\"\nfont_size = \"8\"\n\ngraph_attr = {\n    \"rankdir\": \"LR\",  # LR = left to right, TB = top to bottom\n    \"nodesep\": \"0.1\",  # In inches, tells distance between nodes without edges\n    \"compound\": \"true\",  # Needed to draw properly edges in hamevo when content is hidden\n    \"splines\": \"false\",  # Needed to draw control gates vertical lines one over the other\n}  # These are the default values for graphs\n\nnode_attr = {\n    \"shape\": \"box\",  # 'box' for normal nodes, 'point' for control gates or 'plaintext' for starting nodes (the qubit label).\n    \"style\": \"rounded\",  # Unfortunately we can't specify the radius of the rounded, at least for this version\n    \"fontname\": font_name,\n    \"fontsize\": font_size,\n    \"width\": \"0.1\",  # In inches, it doesn't get tinier than the label font.\n    \"height\": \"0.1\"  # In inches, it doesn't get tinier than the label font.\n}  # These are the defaults values that can be overridden at node declaration.\n\ndefault_cluster_attr = {\n    \"fontname\": font_name,\n    \"fontsize\": font_size,\n    \"labelloc\": \"b\",  # location of cluster label. b as bottom, t as top\n    \"style\": \"rounded\"\n} # These are the defaults values that can be overridden at sub graph declaration\n\nhamevo_cluster_attr = {\n    \"label\": \"HamEvo(t=10)\"\n}\nhamevo_cluster_attr.update(default_cluster_attr)\n\nh = graphviz.Graph(graph_attr=graph_attr, node_attr=node_attr)\nh.node(\"Hello World!\")\nh\n</code></pre> <pre><code>\n</code></pre> <pre><code># Define graph\nh = graphviz.Graph(node_attr=node_attr, graph_attr=graph_attr)\n\n# Add start and end nodes\nfor i in range(4):\n    h.node(f's{i}', shape=\"plaintext\", label=f'{i}', group=f\"{i}\")\n    h.node(f'e{i}', style='invis', group=f\"{i}\")\n\n# Add nodes\nh.node('X', group=\"0\")\nh.node('Y', group=\"1\")\n\n# Add hamevo and its nodes\nhamevo = graphviz.Graph(name='cluster_hamevo', graph_attr=hamevo_cluster_attr)\nfor i in range(4):\n    hamevo.node(f'z{i}', shape=\"box\", style=\"invis\", label=f'{i}', group=f\"{i}\")\nh.subgraph(hamevo)\n\n# Add rx gates cluster and its nodes\ncluster_attr = {\"label\": \"RX gates\"}\ncluster_attr.update(default_cluster_attr)\ncluster = graphviz.Graph(name=\"cluster_0\", graph_attr=cluster_attr)\ncluster.node('RX(x)', group=\"2\")\ncluster.node('RX(0.5)', group=\"3\")\nh.subgraph(cluster)\n\nh.node('cnot0', label='', shape='point', width='0.1', group='0')\nh.node('cnot1', label='X', group='1')\nh.node('cnot2', label='', shape='point', width='0.1', group='2')\nh.node('cnot3', label='', shape='point', width='0.1', group='3')\n\n# Add edges\nh.edge('s0', 'X')\nh.edge('X', 'cnot0')\nh.edge('cnot0', 'z0', lhead='cluster_hamevo')\nh.edge('z0', 'e0', ltail='cluster_hamevo')\nh.edge('s1', 'Y')\nh.edge('Y', 'cnot1')\nh.edge('cnot1', 'z1', lhead='cluster_hamevo')\nh.edge('z1', 'e1', ltail='cluster_hamevo')\nh.edge('s2', 'RX(x)')\nh.edge('RX(x)', 'cnot2')\nh.edge('cnot2', 'z2', lhead='cluster_hamevo')\nh.edge('z2', 'e2', ltail='cluster_hamevo')\nh.edge('s3', 'RX(0.5)')\nh.edge('RX(0.5)', 'cnot3')\nh.edge('cnot3', 'z3', lhead='cluster_hamevo')\nh.edge('z3', 'e3', ltail='cluster_hamevo')\nh.edge('cnot1', 'cnot0', constraint='false')  # constraint: false is needed to draw vertical edges\nh.edge('cnot1', 'cnot2', constraint='false')  # constraint: false is needed to draw vertical edges\nh.edge('cnot1', 'cnot3', constraint='false')  # constraint: false is needed to draw vertical edges\nh\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/development/draw/#example-of-cluster-of-clusters","title":"Example of cluster of clusters","text":"<pre><code># Define graph\nh = graphviz.Graph(node_attr=node_attr, graph_attr=graph_attr)\n\n# Define start and end nodes\nfor i in range(4):\n    h.node(f's{i}', shape=\"plaintext\", label=f'{i}', group=f\"{i}\")\n    h.node(f'e{i}', style='invis', group=f\"{i}\")\n\n# Define outer cluster\ncluster_attr = {\"label\": \"Outer cluster\"}\ncluster_attr.update(default_cluster_attr)\nouter_cluster = graphviz.Graph(name=\"cluster_outer\", graph_attr=cluster_attr)\n\n# Define inner cluster 1 and its nodes\ncluster_attr = {\"label\": \"Inner cluster 1\"}\ncluster_attr.update(default_cluster_attr)\ninner1_cluster = graphviz.Graph(name=\"cluster_inner1\", graph_attr=cluster_attr)\ninner1_cluster.node(\"a0\", group=\"0\")\ninner1_cluster.node(\"a1\", group=\"1\")\nouter_cluster.subgraph(inner1_cluster)\n\n# Define inner cluster 2 and its nodes\ncluster_attr = {\"label\": \"Inner cluster 2\"}\ncluster_attr.update(default_cluster_attr)\ninner2_cluster = graphviz.Graph(name=\"cluster_inner2\", graph_attr=cluster_attr)\ninner2_cluster.node(\"a2\", group=\"2\")\ninner2_cluster.node(\"a3\", group=\"3\")\nouter_cluster.subgraph(inner2_cluster)\n\n# This has to be done here, after inner clusters definitions\nh.subgraph(outer_cluster)\n\n# Define more nodes\nfor i in range(4):\n    h.node(f\"b{i}\", group=f\"{i}\")\n\nfor i in range(4):\n    h.edge(f's{i}', f'a{i}')\n    h.edge(f'a{i}', f'b{i}')\n    h.edge(f'b{i}', f'e{i}')\n\nh\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/digital_analog_qc/","title":"Digital-Analog Quantum Computation","text":"<p>Digital-analog quantum computation (DAQC) is a universal quantum computing paradigm<sup>1</sup>, based on two primary computations:</p> <ul> <li>Fast single-qubit operations (digital).</li> <li>Multi-partite entangling operations acting on all qubits (analog).</li> </ul> <p>A promising quantum computing platform for the implementation of the DAQC paradigm is neutral-atoms, where both these computations are realizable.</p>"},{"location":"tutorials/digital_analog_qc/#digital-analog-emulation","title":"Digital-analog emulation","text":"<p>Qadence simplifies the execution of DAQC programs on either emulated or real devices by providing a simplified interface for customizing interactions and interfacing with pulse-level programming in <code>Pulser</code><sup>3</sup>.</p>"},{"location":"tutorials/digital_analog_qc/#digital-analog-transformation","title":"Digital-analog transformation","text":"<p>Furthermore, the essence of digital-analog computation is the ability to represent any analog operation, i.e. any arbitrary Hamiltonian, using an auxiliary device-amenable Hamiltonian, such as the ubiquitous Ising model<sup>2</sup>. This is at the core of the DAQC implementation in Qadence.</p>"},{"location":"tutorials/digital_analog_qc/#execution-on-rydberg-atom-arrays-with-restriced-addressability","title":"Execution on Rydberg atom arrays with restriced addressability","text":"<p>Finally, Qadence offers some convenience constructors and interfaces to execute programs compatible with a DAQC flavor featuring only a restricted access to individual qubit addressability with always-on interaction. This regime is common in currently available neutral atom quantum computers.</p>"},{"location":"tutorials/digital_analog_qc/#references","title":"References","text":"<ol> <li> <p>Dodd et al., Universal quantum computation and simulation using any entangling Hamiltonian and local unitaries, PRA 65, 040301 (2002). \u21a9</p> </li> <li> <p>Pulser: An open-source package for the design of pulse sequences in programmable neutral-atom arrays \u21a9</p> </li> <li> <p>Parra-Rodriguez et al., Digital-Analog Quantum Computation, PRA 101, 022305 (2020). \u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/analog-basics/","title":"Basic operations on neutral-atoms","text":"<p>Warning</p> <p>The digital-analog emulation framework is under construction and more changes to the interface may still occur.</p> <p>Qadence includes primitives for the construction of programs implemented on a set of interacting qubits. The goal is to build digital-analog programs that better represent the reality of interacting qubit platforms, such as neutral-atoms, while maintaining a simplified interface for users coming from a digital quantum computing background that may not be as familiar with pulse-level programming.</p> <p>To build the intuition for the interface in Qadence, it is important to go over some of the underlying physics. We can write a general Hamiltonian for a set of \\(n\\) interacting qubits as</p> \\[ \\mathcal{H} = \\sum_{i=0}^{n-1}\\left(\\mathcal{H}^\\text{d}_{i}(t) + \\sum_{j&lt;i}\\mathcal{H}^\\text{int}_{ij}\\right), \\] <p>where the driving Hamiltonian \\(\\mathcal{H}^\\text{d}_{i}\\) describes the pulses used to control single-qubit rotations, and the interaction Hamiltonian \\(\\mathcal{H}^\\text{int}_{ij}\\) describes the natural interaction between qubits.</p>"},{"location":"tutorials/digital_analog_qc/analog-basics/#rydberg-atoms","title":"Rydberg atoms","text":"<p>For the purpose of digital-analog emulation of neutral-atom systems in Qadence, we now consider a simplified time-independent global driving Hamiltonian, written as</p> \\[ \\mathcal{H}^\\text{d}_{i} = \\frac{\\Omega}{2}\\left(\\cos(\\phi) X_i - \\sin(\\phi) Y_i \\right) - \\delta N_i \\] <p>where \\(\\Omega\\) is the Rabi frequency, \\(\\delta\\) is the detuning, \\(\\phi\\) is the phase, \\(X_i\\) and \\(Y_i\\) are the standard Pauli operators, and \\(N_i=\\frac{1}{2}(I_i-Z_i)\\) is the number operator. This Hamiltonian allows arbitrary global single-qubit rotations to be written, meaning that the values set for \\((\\Omega,\\phi,\\delta)\\) are the same accross the qubit support.</p> <p>For the interaction term, Rydberg atoms typically allow both an Ising and an XY mode of operation. For now, we focus on the Ising interaction, where the Hamiltonian is written as</p> \\[ \\mathcal{H}^\\text{int}_{ij} = \\frac{C_6}{r_{ij}^6}N_iN_j \\] <p>where \\(r_{ij}\\) is the distance between atoms \\(i\\) and \\(j\\), and \\(C_6\\) is a coefficient depending on the specific Rydberg level of the excited state used in the computational logic states. A typical value for rydberg level of 60 is \\(C_6\\approx 866~[\\text{rad} . \\mu \\text{m}^6 / \\text{ns}]\\).</p> <p>For a given register of atoms prepared in some spatial coordinates, the Hamiltonians described will generate the dynamics of some unitary operation as</p> \\[ U(t, \\Omega, \\delta, \\phi) = \\exp(-i\\mathcal{H}t) \\] <p>where we specify the final parameter \\(t\\), the duration of the operation.</p> <p>Qadence uses the following units for user-specified parameters:</p> <ul> <li>Rabi frequency and detuning \\(\\Omega\\), \\(\\delta\\): \\([\\text{rad}/\\mu \\text{s}]\\)</li> <li>Phase \\(\\phi\\): \\([\\text{rad}]\\)</li> <li>Duration \\(t\\): \\([\\text{ns}]\\)</li> <li>Atom coordinates: \\([\\mu \\text{m}]\\)</li> </ul>"},{"location":"tutorials/digital_analog_qc/analog-basics/#in-practice","title":"In practice","text":"<p>Given the Hamiltonian description in the previous section, we will now go over a few examples of the standard operations available in Qadence.</p>"},{"location":"tutorials/digital_analog_qc/analog-basics/#arbitrary-rotation","title":"Arbitrary rotation","text":"<p>To start, we will exemplify the a general rotation on a set of atoms. To create an arbitrary register of atoms, we refer the user to the register creation tutorial. Below, we create a line register of three qubits with a separation of \\(8~\\mu\\text{m}\\). This is a typical value used in combination with a standard experimental setup of neutral atoms such that the interaction term in the Hamiltonian can effectively be used for computations.</p> <pre><code>from qadence import Register\n\nreg = Register.line(3, spacing=8.0)  # Atom spacing in \u03bcm\n</code></pre> <p>Currently, the most general rotation operation uses the <code>AnalogRot</code> operation, which essentially implements \\(U(t, \\Omega, \\delta, \\phi)\\) defined above.</p> <pre><code>from qadence import AnalogRot, PI\n\nrot_op = AnalogRot(\n    duration = 500., # [ns]\n    omega = PI, # [rad/\u03bcs]\n    delta = PI, # [rad/\u03bcs]\n    phase = PI, # [rad]\n)\n</code></pre> <p>Note that in the code above a specific qubit support is not defined. By default this operation applies a global rotation on all qubits. We can define a circuit using the 3-qubit register and run it in the pyqtorch backend:</p> <pre><code>from qadence import BackendName, run\n\nwf = run(reg, rot_op, backend = BackendName.PYQTORCH)\n\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4248-0.2411j, -0.1687+0.3156j, -0.1696+0.2676j, -0.2040-0.2671j,\n         -0.1687+0.3156j,  0.0014-0.2721j, -0.2040-0.2671j,  0.3034-0.1130j]])\n</code></pre> Under the hood of AnalogRot      To be fully explicit about what goes on under the hood of `AnalogRot`, we can look at the example     code below.      <pre><code>from qadence import BackendName, HamEvo, X, Y, N, add, run, PI\nfrom qadence.analog.constants import C6_DICT\nfrom math import cos, sin\n\n# Following the 3-qubit register above\nn_qubits = 3\ndx = 8.0\n\n# Parameters used in the AnalogRot\nduration = 500.\nomega = PI\ndelta = PI\nphase = PI\n\n# Building the terms in the driving Hamiltonian\nh_x = (omega / 2) * cos(phase) * add(X(i) for i in range(n_qubits))\nh_y = (-1.0 * omega / 2) * sin(phase) * add(Y(i) for i in range(n_qubits))\nh_n = -1.0 * delta * add(N(i) for i in range(n_qubits))\n\n# Building the interaction Hamiltonian\n\n# Dictionary of coefficient values for each Rydberg level, which is 60 by default\nc_6 = C6_DICT[60]\n\nh_int = c_6 * (\n    1/(dx**6) * (N(0)@N(1)) +\n    1/(dx**6) * (N(1)@N(2)) +\n    1/((2*dx)**6) * (N(0)@N(2))\n)\n\nhamiltonian = h_x + h_y + h_n + h_int\n\n# Convert duration to \u00b5s due to the units of the Hamiltonian\nexplicit_rot = HamEvo(hamiltonian, duration / 1000)\n\nwf = run(n_qubits, explicit_rot, backend = BackendName.PYQTORCH)\n\n# We get the same final wavefunction\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4248-0.2411j, -0.1687+0.3156j, -0.1696+0.2676j, -0.2040-0.2671j,\n         -0.1687+0.3156j,  0.0014-0.2721j, -0.2040-0.2671j,  0.3034-0.1130j]])\n</code></pre> <p>When sending the <code>AnalogRot</code> operation to the pyqtorch backend, Qadence automatically builds the correct Hamiltonian and the corresponding <code>HamEvo</code> operation with the added qubit interactions, as shown explicitly in the minimized section above. However, this operation is also supported in the Pulser backend, where the correct pulses are automatically created.</p> <pre><code>wf = run(\n    reg,\n    rot_op,\n    backend = BackendName.PULSER,\n)\n\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4253-0.2408j, -0.1688+0.3157j, -0.1698+0.2678j, -0.2044-0.2667j,\n         -0.1688+0.3157j,  0.0011-0.2721j, -0.2044-0.2667j,  0.3026-0.1137j]])\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#rx-ry-rz-rotations","title":"RX / RY / RZ rotations","text":"<p>The <code>AnalogRot</code> provides full control over the parameters of \\(\\mathcal{H}^\\text{d}\\), but users coming from a digital quantum computing background may be more familiar with the standard <code>RX</code>, <code>RY</code> and <code>RZ</code> rotations, also available in Qadence. For the emulated analog interface, Qadence provides alternative <code>AnalogRX</code>, <code>AnalogRY</code> and <code>AnalogRZ</code> operations which call <code>AnalogRot</code> under the hood to represent the rotations accross the respective axis.</p> <p>For a given angle of rotation \\(\\theta\\) provided to each of these operations, currently a set of hardcoded assumptions are made on the tunable Hamiltonian parameters:</p> \\[ \\begin{aligned} \\text{RX}:&amp; \\quad \\Omega = \\pi, \\quad \\delta = 0, \\quad \\phi = 0, \\quad t = (\\theta/\\Omega)\\times 10^3 \\\\ \\text{RY}:&amp; \\quad \\Omega = \\pi, \\quad \\delta = 0, \\quad \\phi = -\\pi/2, \\quad t = (\\theta/\\Omega)\\times 10^3 \\\\ \\text{RZ}:&amp; \\quad \\Omega = 0, \\quad \\delta = \\pi, \\quad \\phi = 0, \\quad t = (\\theta/\\delta)\\times 10^3 \\\\ \\end{aligned} \\] <p>Note that the \\(\\text{RZ}\\) operation as defined above includes a global phase compared to the standard \\(\\text{RZ}\\) rotation since it evolves \\(\\exp\\left(-i\\frac{\\theta}{2}\\frac{I-Z}{2}\\right)\\) instead of \\(\\exp\\left(-i\\frac{\\theta}{2}Z\\right)\\) given the detuning operator in \\(\\mathcal{H}^\\text{d}\\).</p> <p>Warning</p> <p>As shown above, the values of \\(\\Omega\\) and \\(\\delta\\) are currently hardcoded in these operators, and the effective angle of rotation is controlled by varying the duration of the evolution. Currently, the best way to overcome this is to use <code>AnalogRot</code> directly, but more general and convenient options will be provided soon in an improved interface.</p> <p>Below we exemplify the usage of <code>AnalogRX</code>:</p> <pre><code>from qadence import Register, BackendName\nfrom qadence import RX, AnalogRX, random_state, equivalent_state, kron, run, PI\n\nn_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\n# Rotation angle\ntheta = PI\n\n# Analog rotation using the Rydberg Hamiltonian\nrot_analog = AnalogRX(angle = theta)\n\n# Equivalent full-digital global rotation\nrot_digital = kron(RX(i, theta) for i in range(n_qubits))\n\n# Some random initial state\ninit_state = random_state(n_qubits)\n\n# Compare the final state using the full digital and the AnalogRX\nwf_analog_pyq = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\n\nwf_digital_pyq = run(\n    reg,\n    rot_digital,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_digital_pyq, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  False\n</code></pre> <p>As we can see, running a global <code>RX</code> or the <code>AnalogRX</code> does not result in equivalent states at the end, given that the digital <code>RX</code> operation does not include the interaction between the qubits. By setting <code>dx</code> very high in the code above the interaction will be less significant and the results will match.</p> <p>However, if we compare with the Pulser backend, we see that the results for <code>AnalogRX</code> are consistent with the expected results from a real device:</p> <pre><code>wf_analog_pulser = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PULSER,\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_analog_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#evolving-the-interaction-term","title":"Evolving the interaction term","text":"<p>Finally, besides applying specific qubit rotations, we can also choose to evolve only the interaction term \\(\\mathcal{H}^\\text{int}\\), equivalent to setting \\(\\Omega = \\delta = \\phi = 0\\). To do so, Qadence provides the function <code>AnalogInteraction</code> which does exactly this.</p> <pre><code>from qadence import Register, BackendName, random_state, equivalent_state, AnalogInteraction, run\n\nn_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\nduration = 1000.\nop = AnalogInteraction(duration = duration)\n\ninit_state = random_state(n_qubits)\n\nwf_pyq = run(reg, op, state = init_state, backend = BackendName.PYQTORCH)\nwf_pulser = run(reg, op, state = init_state, backend = BackendName.PULSER)\n\nbool_equiv = equivalent_state(wf_pyq, wf_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#device-specifications-in-qadence","title":"Device specifications in Qadence","text":"<p>As a way to control other specifications of the interacting Rydberg atoms, Qadence provides a <code>RydbergDevice</code> class, which is currently used for both the pyqtorch and the pulser backends. Below we initialize a Rydberg device showcasing all the possible options.</p> <pre><code>from qadence import RydbergDevice, DeviceType, Interaction, PI\n\ndevice_specs = RydbergDevice(\n    interaction=Interaction.NN, # Or Interaction.XY, supported only for pyqtorch\n    rydberg_level=60, # Integer value affecting the C_6 coefficient\n    coeff_xy=3700.00, # C_3 coefficient for the XY interaction\n    max_detuning=2 * PI * 4, # Max value for delta, currently only used in pulser\n    max_amp=2 * PI * 3, # Max value for omega, currently only used in pulser\n    pattern=None, # Semi-local addressing pattern, see the relevant tutorial\n    type=DeviceType.IDEALIZED, # Pulser device to which the qadence device is converted in that backend\n)\n</code></pre> <p>The values above are the defaults when simply running <code>device_specs = RydbergDevice()</code>. The convenience wrappers <code>IdealDevice()</code> or <code>RealisticDevice()</code> can also be used which simply change the <code>type</code> for the Pulser backend, but also allow an <code>AddressingPattern</code> passed in the <code>pattern</code> argument (see the relevant tutorial here).</p> <p>Warning</p> <p>Currently, the options above are not fully integrated in both backends and this class should mostly be used if a user wishes to experiment with a different <code>rydberg_level</code>, or to change the device type for the pulser backend.</p> <p>Planned features to add to the RydbergDevice include the definition of custom interaction functions, the control of other drive Hamiltonian parameters so that \\(\\Omega\\), \\(\\delta\\) and \\(\\phi\\) are not hardcoded when doing analog rotations, and the usage of the <code>max_detuning</code> and <code>max_amp</code> to control those respective parameters when training models in the pyqtorch backend.</p> <p>Finally, to change a given simulation, the device specifications are integrated in the Qadence <code>Register</code>. By default, all registers initialize an <code>IdealDevice()</code> under the hood. Below we run a quick test for a different rydberg level.</p> <pre><code>from qadence import Register, BackendName, random_state, equivalent_state, run\nfrom qadence import AnalogRX, RydbergDevice, PI\n\ndevice_specs = RydbergDevice(rydberg_level = 70)\n\nn_qubits_side = 2\nreg = Register.square(\n    n_qubits_side,\n    spacing = 8.0,\n    device_specs = device_specs\n)\n\nrot_analog = AnalogRX(angle = PI)\n\ninit_state = random_state(n_qubits = 4)\n\nwf_analog_pyq = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\nwf_analog_pulser = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PULSER\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_analog_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#technical-details","title":"Technical details","text":"<p>Warning</p> <p>The details described here are relevant in the current version but will be lifted soon for the next version of the emulated analog interface.</p> <p>In the previous section we have exemplified the main ingredients of the current user-facing functionalities of the emulated analog interface, and in the next tutorial on Quantum Circuit Learning we will exmplify its usage in a simple QML example. Here we specify some extra details of this interface.</p> <p>In the block system, all analog rotation operators initialize a <code>ConstantAnalogRotation</code> block, while the <code>AnalogInteraction</code> operation initializes an <code>InteractionBlock</code>. As we have shown, by default, these blocks use a global qubit support, which can be passed explicitly by setting <code>qubit_support = QubitSupportType.GLOBAL</code>. However, composing blocks using <code>kron</code> with local qubit supports and different durations is not allowed.</p> <pre><code>from qadence import AnalogRX, AnalogRY, Register, kron\n\ndx = 8.0\nreg = Register.from_coordinates([(0, 0), (dx, 0)])\n\n# Does not work (the angle affects the duration, as seen above):\nrot_0 = AnalogRX(angle = 1.0, qubit_support = (0,))\nrot_1 = AnalogRY(angle = 2.0, qubit_support = (1,))\n\ntry:\n    block = kron(rot_0, rot_1)\nexcept ValueError as error:\n    print(\"Error:\", error)\n\n# Works:\nrot_0 = AnalogRX(angle = 1.0, qubit_support = (0,))\nrot_1 = AnalogRY(angle = 1.0, qubit_support = (1,))\n\nblock = kron(rot_0, rot_1)\n</code></pre> <pre><code>Error: Kron'ed blocks have to have same duration.\n</code></pre> <p>Using <code>chain</code> is only supported between analog blocks with global qubit support:</p> <pre><code>from qadence import chain\n\nrot_0 = AnalogRX(angle = 1.0, qubit_support = \"global\")\nrot_1 = AnalogRY(angle = 2.0, qubit_support = \"global\")\n\nblock = chain(rot_0, rot_1)\n</code></pre> <p>The restrictions above only apply to the analog blocks, and analog and digital blocks can currently be composed.</p> <pre><code>from qadence import RX\n\nrot_0 = AnalogRX(angle = 1.0, qubit_support = \"global\")\nrot_1 = AnalogRY(angle = 2.0, qubit_support = (0,))\nrot_digital = RX(1, 1.0)\n\nblock_0 = chain(rot_0, rot_digital)\nblock_1 = kron(rot_1, rot_digital)\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-blocks-qcl/","title":"Fitting a function with analog blocks","text":"<p>Analog blocks can be parametrized in the usual Qadence manner. Like any other parameters, they can be optimized. The next snippet exemplifies the creation of an analog and parameterized ansatz to fit a simple function. First, define a register and feature map block. We again use a default spacing of \\(8~\\mu\\text{m}\\) as done in the basic tutorial.</p> <pre><code>from qadence import Register, FeatureParameter, chain\nfrom qadence import AnalogRX, AnalogRY, AnalogRZ, AnalogInteraction\nfrom sympy import acos\n\n# Line register\nn_qubits = 2\nregister = Register.line(n_qubits, spacing = 8.0)\n\n# The input feature x for the circuit to learn f(x)\nx = FeatureParameter(\"x\")\n\n# Feature map with a few global analog rotations\nfm = chain(\n    AnalogRX(x),\n    AnalogRY(2*x),\n    AnalogRZ(3*x),\n)\n</code></pre> <p>Next, we define the ansatz with parameterized rotations.</p> <pre><code>from qadence import hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel, BackendName, DiffMode\nfrom qadence import VariationalParameter\n\nt_0 = 1000. * VariationalParameter(\"t_0\")\nt_1 = 1000. * VariationalParameter(\"t_1\")\nt_2 = 1000. * VariationalParameter(\"t_2\")\n\n# Creating the ansatz with parameterized rotations and wait time\nansatz = chain(\n    AnalogRX(\"tht_0\"),\n    AnalogRY(\"tht_1\"),\n    AnalogRZ(\"tht_2\"),\n    AnalogInteraction(t_0),\n    AnalogRX(\"tht_3\"),\n    AnalogRY(\"tht_4\"),\n    AnalogRZ(\"tht_5\"),\n    AnalogInteraction(t_1),\n    AnalogRX(\"tht_6\"),\n    AnalogRY(\"tht_7\"),\n    AnalogRZ(\"tht_8\"),\n    AnalogInteraction(t_2),\n)\n</code></pre> <p>We define the measured observable as the total magnetization, and build the <code>QuantumModel</code>.</p> <pre><code># Total magnetization observable\nobservable = hamiltonian_factory(n_qubits, detuning = Z)\n\n# Defining the circuit and observable\ncircuit = QuantumCircuit(register, fm, ansatz)\n\nmodel = QuantumModel(\n    circuit,\n    observable = observable,\n    backend = BackendName.PYQTORCH,\n    diff_mode = DiffMode.AD\n)\n</code></pre> <p>Now we can define the function to fit as well as our training and test data.</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Function to fit:\ndef f(x):\n    return x**2\n\nx_test = torch.linspace(-1.0, 1.0, steps=100)\ny_test = f(x_test)\n\nx_train = torch.linspace(-1.0, 1.0, steps=10)\ny_train = f(x_train)\n\n# Initial prediction from the model, to be visualized later\ny_pred_initial = model.expectation({\"x\": x_test}).detach()\n</code></pre> <p>Finally we define a simple loss function and training loop.</p> <pre><code>mse_loss = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(x_train, y_train):\n    out = model.expectation({\"x\": x_train})\n    loss = mse_loss(out.squeeze(), y_train)\n    return loss\n\nn_epochs = 200\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_fn(x_train, y_train)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>And with the model trained we can plot the final results.</p> <pre><code>y_pred_final = model.expectation({\"x\": x_test}).detach()\n\nplt.plot(x_test, y_pred_initial, label = \"Initial prediction\")\nplt.plot(x_test, y_pred_final, label = \"Final prediction\")\nplt.scatter(x_train, y_train, label = \"Training points\")\n</code></pre> 2025-01-28T16:50:50.891049 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/analog-qubo/","title":"Solve a QUBO problem","text":"<p>In this notebook, we solve a quadratic unconstrained binary optimization (QUBO) problem with Qadence. QUBOs are very popular combinatorial optimization problems with a wide range of applications. Here, we solve the problem using the QAOA <sup>1</sup> variational algorithm by embedding the QUBO problem weights onto a register as standard for neutral atom quantum devices.</p> <p>Additional background information on QUBOs can be found here, directly solved using the pulse-level interface Pulser.</p>"},{"location":"tutorials/digital_analog_qc/analog-qubo/#define-and-solve-qubo","title":"Define and solve QUBO","text":"Pre-requisite: optimal register coordinates for embedding the QUBO problem <p>A basic ingredient for solving a QUBO problem with a neutral atom device is to embed the problem onto the atomic register. In short, embedding algorithms cast the problem onto a graph mapped onto the register by optimally finding atomic coordinates. A discussion on the embedding algorithms is beyond the scope of this tutorial and a simplified version taken from here is added below.</p> <pre><code>import numpy as np\nimport numpy.typing as npt\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist, squareform\nfrom qadence import RydbergDevice\n\ndef qubo_register_coords(Q: np.ndarray, device: RydbergDevice) -&gt; list:\n    \"\"\"Compute coordinates for register.\"\"\"\n\n    def evaluate_mapping(new_coords, *args):\n        \"\"\"Cost function to minimize. Ideally, the pairwise\n        distances are conserved\"\"\"\n        Q, shape = args\n        new_coords = np.reshape(new_coords, shape)\n        interaction_coeff = device.coeff_ising\n        new_Q = squareform(interaction_coeff / pdist(new_coords) ** 6)\n        return np.linalg.norm(new_Q - Q)\n\n    shape = (len(Q), 2)\n    np.random.seed(0)\n    x0 = np.random.random(shape).flatten()\n    res = minimize(\n        evaluate_mapping,\n        x0,\n        args=(Q, shape),\n        method=\"Nelder-Mead\",\n        tol=1e-6,\n        options={\"maxiter\": 200000, \"maxfev\": None},\n    )\n    return [(x, y) for (x, y) in np.reshape(res.x, (len(Q), 2))]\n</code></pre> <p>With the embedding routine define above, we can translate a matrix defining a QUBO problem to a set of atom coordinates for the register. The QUBO problem is initially defined by a graph of weighted edges and a cost function to be optimized. The weighted edges are represented by a real-valued symmetric matrix <code>Q</code> which is used throughout the tutorial.</p> <pre><code>import torch\nfrom qadence import QuantumModel\n\nseed = 0\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# QUBO problem weights (real-value symmetric matrix)\nQ = np.array(\n    [\n        [-10.0, 19.7365809, 19.7365809, 5.42015853, 5.42015853],\n        [19.7365809, -10.0, 20.67626392, 0.17675796, 0.85604541],\n        [19.7365809, 20.67626392, -10.0, 0.85604541, 0.17675796],\n        [5.42015853, 0.17675796, 0.85604541, -10.0, 0.32306662],\n        [5.42015853, 0.85604541, 0.17675796, 0.32306662, -10.0],\n    ]\n)\n\n# Loss function to guide the optimization routine\ndef loss(model: QuantumModel, *args) -&gt; tuple[torch.Tensor, dict]:\n    to_arr_fn = lambda bitstring: np.array(list(bitstring), dtype=int)\n    cost_fn = lambda arr: arr.T @ Q @ arr\n    samples = model.sample({}, n_shots=1000)[0]\n    cost_fn = sum(samples[key] * cost_fn(to_arr_fn(key)) for key in samples)\n    return torch.tensor(cost_fn / sum(samples.values())), {}\n</code></pre> <p>The QAOA algorithm needs a variational quantum circuit with optimizable parameters. For that purpose, we use a fully analog circuit composed of two global rotations per layer on different axes of the Bloch sphere. The first rotation corresponds to the mixing Hamiltonian and the second one to the embedding Hamiltonian <sup>1</sup>. In this setting, the embedding is realized by the appropriate register coordinates and the resulting qubit interaction. Details on the analog blocks used here can be found in the analog basics tutorial.</p> Rydberg level <p>The Rydberg level is set to 70. We initialize the weighted register graph from the QUBO definition similarly to what is done in the original tutorial, and set the device specifications with the updated Rydberg level.</p> <pre><code>from qadence import QuantumCircuit, Register, RydbergDevice\nfrom qadence import chain, AnalogRX, AnalogRZ\n\n# Device specification and atomic register\ndevice = RydbergDevice(rydberg_level=70)\n\nreg = Register.from_coordinates(\n    qubo_register_coords(Q, device), device_specs=device\n)\n\n# Analog variational quantum circuit\nlayers = 2\nblock = chain(*[AnalogRX(f\"t{i}\") * AnalogRZ(f\"s{i}\") for i in range(layers)])\ncircuit = QuantumCircuit(reg, block)\n</code></pre> <p>By initializing the <code>QuantumModel</code> with this circuit we can check the initial counts where no clear solution can be found.</p> <pre><code>model = QuantumModel(circuit)\ninitial_counts = model.sample({}, n_shots=1000)[0]\n</code></pre> <pre><code>initial_counts = OrderedCounter({'00000': 101, '10000': 87, '01000': 75, '00110': 72, '00100': 70, '01010': 64, '01001': 62, '00101': 53, '00010': 51, '00011': 48, '01011': 46, '00001': 45, '10010': 45, '00111': 38, '10001': 34, '11000': 29, '10100': 18, '01100': 14, '01110': 11, '01111': 10, '10110': 7, '11001': 7, '11010': 6, '01101': 4, '10101': 2, '10011': 1})\n</code></pre> <p>Finally, we can proceed with the variational optimization. The cost function defined above is derived from bitstring computations and therefore non differentiable. We use Qadence ML facilities to run gradient-free optimizations using the <code>nevergrad</code> library.</p> <pre><code>from qadence.ml_tools import Trainer, TrainConfig, num_parameters\nimport nevergrad as ng\n\nTrainer.set_use_grad(False)\n\nconfig = TrainConfig(max_iter=100)\n\noptimizer = ng.optimizers.NGOpt(\n    budget=config.max_iter, parametrization=num_parameters(model)\n)\n\ntrainer = Trainer(model, optimizer, config, loss)\n\ntrainer.fit()\n\noptimal_counts = model.sample({}, n_shots=1000)[0]\n</code></pre> <p>Finally, let's plot the solution. The expected bitstrings are marked in red.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Known solutions to the QUBO problem.\nsolution_bitstrings = [\"01011\", \"00111\"]\n\ndef plot_distribution(C, ax, title):\n    C = dict(sorted(C.items(), key=lambda item: item[1], reverse=True))\n    color_dict = {key: \"r\" if key in solution_bitstrings else \"b\" for key in C}\n    ax.set_xlabel(\"bitstrings\")\n    ax.set_ylabel(\"counts\")\n    ax.set_xticks([i for i in range(len(C.keys()))], C.keys(), rotation=90)\n    ax.bar(C.keys(), C.values(), color=color_dict.values())\n    ax.set_title(title)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nplot_distribution(initial_counts, axs[0], \"Initial counts\")\nplot_distribution(optimal_counts, axs[1], \"Optimal counts\")\n</code></pre> 2025-01-28T16:50:57.830637 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/analog-qubo/#references","title":"References","text":"<ol> <li> <p>Edward Farhi, Jeffrey Goldstone, Sam Gutmann, A Quantum Approximate Optimization Algorithm, arXiv:1411.4028 (2014) \u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/","title":"<code>CNOT</code> with interacting qubits","text":"<p>Digital-analog quantum computing focuses on using single qubit digital gates combined with more complex and device-dependent analog interactions to represent quantum programs. This paradigm has been shown to be universal for quantum computation<sup>1</sup>. However, while this approach may have advantages when adapting quantum programs to real devices, known quantum algorithms are very often expressed in a fully digital paradigm. As such, it is also important to have concrete ways to transform from one paradigm to another.</p> <p>This tutorial will exemplify the DAQC transformation starting with the representation of a simple digital <code>CNOT</code> using the universality of the Ising Hamiltonian<sup>2</sup>.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#cnot-with-cphase","title":"<code>CNOT</code> with <code>CPHASE</code>","text":"<p>Let's look at a single example of how the digital-analog transformation can be used to perform a <code>CNOT</code> on two qubits inside a register of globally interacting qubits.</p> <p>First, note that the <code>CNOT</code> can be decomposed with two Hadamard and a <code>CPHASE</code> gate with \\(\\phi=\\pi\\):</p> <pre><code>import torch\nfrom qadence import chain, sample, product_state\n\nfrom qadence.draw import display\nfrom qadence import X, I, Z, H, N, CPHASE, CNOT, HamEvo, PI\n\nn_qubits = 2\n\n# CNOT gate\ncnot_gate = CNOT(0, 1)\n\n# CNOT decomposed\nphi = PI\ncnot_decomp = chain(H(1), CPHASE(0, 1, phi), H(1))\n\ninit_state = product_state(\"10\")\n</code></pre> <pre><code>sample from CNOT gate and 100 shots = [OrderedCounter({'11': 100})]\nsample from decomposed CNOT gate and 100 shots = [OrderedCounter({'11': 100})]\n</code></pre> <p>The <code>CPHASE</code> matrix is diagonal, and can be implemented by exponentiating an Ising-like Hamiltonian, or generator,</p> \\[\\text{CPHASE}(i,j,\\phi)=\\text{exp}\\left(-i\\phi \\mathcal{H}_\\text{CP}(i, j)\\right)\\] \\[\\begin{aligned} \\mathcal{H}_\\text{CP}&amp;=-\\frac{1}{4}(I_i-Z_i)(I_j-Z_j)\\\\ &amp;=-N_iN_j \\end{aligned}\\] <p>where the number operator \\(N_i = \\frac{1}{2}(I_i-Z_i)=\\hat{n}_i\\) is used, leading to an Ising-like interaction \\(\\hat{n}_i\\hat{n}_j\\) realisable in neutral-atom systems. Let's rebuild the <code>CNOT</code> using this evolution.</p> <pre><code>from qadence import kron, block_to_tensor\n\n# Hamiltonian for the CPHASE gate\nh_cphase = (-1.0) * kron(N(0), N(1))\n\n# Exponentiating and time-evolving the Hamiltonian until t=phi.\ncphase_evo = HamEvo(h_cphase, phi)\n\n# Check that we have the CPHASE gate:\ncphase_matrix = block_to_tensor(CPHASE(0, 1, phi))\ncphase_evo_matrix = block_to_tensor(cphase_evo)\n</code></pre> <pre><code>cphase_matrix == cphase_evo_matrix: True\n</code></pre> <p>Now that the <code>CPHASE</code> generator is checked, it can be applied to the <code>CNOT</code>:</p> <pre><code># CNOT with Hamiltonian Evolution\ncnot_evo = chain(\n    H(1),\n    cphase_evo,\n    H(1)\n)\n\n# Initialize state to check CNOTs sample outcomes.\ninit_state = product_state(\"10\")\n</code></pre> <pre><code>sample cnot_gate = [OrderedCounter({'11': 100})]\nsample cnot_evo = [OrderedCounter({'11': 100})]\n</code></pre> <p>Thus, a <code>CNOT</code> gate can be created by combining a few single-qubit gates together with a two-qubit Ising interaction between the control and the target qubit which is the essence of the Ising transform proposed in the seminal DAQC paper<sup>2</sup> for \\(ZZ\\) interactions. In Qadence, both \\(ZZ\\) and \\(NN\\) interactions are supported.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#cnot-in-an-interacting-system-of-three-qubits","title":"<code>CNOT</code> in an interacting system of three qubits","text":"<p>Consider a simple experimental setup with \\(n=3\\) interacting qubits laid out in a triangular grid. For the sake of simplicity, all qubits interact with each other with an \\(NN\\)-Ising interaction of constant strength \\(g_\\text{int}\\). The Hamiltonian for the system can be written by summing interaction terms over all pairs:</p> \\[\\mathcal{H}_\\text{sys}=\\sum_{i=0}^{n}\\sum_{j=0}^{i-1}g_\\text{int}N_iN_j,\\] <p>which in this case leads to only three interaction terms,</p> \\[\\mathcal{H}_\\text{sys}=g_\\text{int}(N_0N_1+N_1N_2+N_0N_2)\\] <p>This generator can be easily built in Qadence:</p> <pre><code>from qadence import add, kron\nn_qubits = 3\n\n# Interaction strength.\ng_int = 1.0\n\n# Build a list of interactions.\ninteraction_list = []\nfor i in range(n_qubits):\n    for j in range(i):\n        interaction_list.append(g_int * kron(N(i), N(j)))\n\nh_sys = add(*interaction_list)\n</code></pre> <pre><code>h_sys = AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(0)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(2)\n\u2502       \u2514\u2500\u2500 N(0)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(1)\n</code></pre> <p>Now let's consider that the experimental system is fixed, and qubits can not be isolated one from another. The options are:</p> <ul> <li>Turn on or off the global system Hamiltonian.</li> <li>Perform local single-qubit rotations.</li> </ul> <p>To perform a fully digital <code>CNOT(0,1)</code>, the interacting control on qubit 0 and target on qubit 1 must be isolated from the third one to implement the gate directly. While this can be achieved for a three-qubit system, it becomes experimentally untractable when scaling the qubit count.</p> <p>However, this is not the case within the digital-analog paradigm. In fact, the two qubit Ising interaction required for the <code>CNOT</code> can be represented with a combination of the global system Hamiltonian and a specific set of single-qubit rotations. Full details about this transformation are to be found in the DAQC paper<sup>2</sup> but a more succint yet in-depth description takes place in the next section. It is conveniently available in Qadence by calling the <code>daqc_transform</code> function.</p> <p>In the most general sense, the <code>daqc_transform</code> function will return a circuit that represents the evolution of a target Hamiltonian \\(\\mathcal{H}_\\text{target}\\) (here the unitary of the gate) until a specified time \\(t_f\\) by using only the evolution of a build Hamiltonian \\(\\mathcal{H}_\\text{build}\\) (here \\(\\mathcal{H}_\\text{sys}\\)) together with local \\(X\\)-gates. In Qadence, <code>daqc_transform</code> is applicable for \\(\\mathcal{H}_\\text{target}\\) and \\(\\mathcal{H}_\\text{build}\\) composed only of \\(ZZ\\)- or \\(NN\\)-interactions. These generators are parsed by the <code>daqc_transform</code> function and the appropriate type is automatically determined together with the appropriate single-qubit detunings and global phases.</p> <p>Let's apply it for the <code>CNOT</code> implementation:</p> <pre><code>from qadence import daqc_transform, Strategy\n\n# Settings for the target CNOT operation\ni = 0  # Control qubit\nj = 1  # Target qubit\nk = 2  # The extra qubit\n\n# Define the target CNOT operation\n# by composing with identity on the extra qubit.\ncnot_target = kron(CNOT(i, j), I(k))\n\n# The two-qubit NN-Ising interaction term for the CPHASE\nh_int = (-1.0) * kron(N(i), N(j))\n\n# Transforming the two-qubit Ising interaction using only our system Hamiltonian\ntransformed_ising = daqc_transform(\n    n_qubits=3,        # Total number of qubits in the transformation\n    gen_target=h_int,  # The target Ising generator\n    t_f=PI,            # The target evolution time\n    gen_build=h_sys,   # The building block Ising generator to be used\n    strategy=Strategy.SDAQC,   # Currently only sDAQC is implemented\n    ignore_global_phases=False  # Global phases from mapping between Z and N\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_e552e081e16c49a6a1541637eb267252 cluster_1dc21d2ce6154345a9013a6ff1d35d96 cluster_a5ad4ba89b69449b87731952203dba44 cluster_b0f5464fb1b54d4d8ff8e28b61e0b353 cluster_e7508c1ecfcd43c4a321780857a5bb1a cluster_79ed2cd7f15944e2b80e74a54a087281 cluster_fd9843f1b7e7485cbac8a59c1bda61e4 512c87e6799a4c1e9d799ed5cb3f45e7 0 e20a089b91984944822b5d9515866d4f HamEvo 512c87e6799a4c1e9d799ed5cb3f45e7--e20a089b91984944822b5d9515866d4f 9428ef8b6033402b8818764d3e5efc65 1 b98a9e7126c74f4aa6dd79bd2f2adc90 HamEvo e20a089b91984944822b5d9515866d4f--b98a9e7126c74f4aa6dd79bd2f2adc90 bdd571ed507b4b55a0e29370a37b1af8 HamEvo b98a9e7126c74f4aa6dd79bd2f2adc90--bdd571ed507b4b55a0e29370a37b1af8 408627fbcaa04cb09c08808ae72372d2 X bdd571ed507b4b55a0e29370a37b1af8--408627fbcaa04cb09c08808ae72372d2 f4b14ad4e6fb43fd9f3f7c48da4f31c5 HamEvo 408627fbcaa04cb09c08808ae72372d2--f4b14ad4e6fb43fd9f3f7c48da4f31c5 7eae56bfc85b4b2e8af8a28fa0c0ffbc HamEvo f4b14ad4e6fb43fd9f3f7c48da4f31c5--7eae56bfc85b4b2e8af8a28fa0c0ffbc c0be34440e88418389c4d9097c5a916c X 7eae56bfc85b4b2e8af8a28fa0c0ffbc--c0be34440e88418389c4d9097c5a916c 0d584a6bf9a444d9819cf6d682cc36e7 c0be34440e88418389c4d9097c5a916c--0d584a6bf9a444d9819cf6d682cc36e7 3b6a752198bc44fb8b881f0a8f38682d HamEvo 0d584a6bf9a444d9819cf6d682cc36e7--3b6a752198bc44fb8b881f0a8f38682d 603071d3c3db4b0ca4c67ecd7d850e8f HamEvo 3b6a752198bc44fb8b881f0a8f38682d--603071d3c3db4b0ca4c67ecd7d850e8f e2b1674a9c5c4883b4cbf09d35e3c0e9 603071d3c3db4b0ca4c67ecd7d850e8f--e2b1674a9c5c4883b4cbf09d35e3c0e9 1dafdf9e2f7d47d889696aa36d983359 e2b1674a9c5c4883b4cbf09d35e3c0e9--1dafdf9e2f7d47d889696aa36d983359 0b30ff18b9ca4d3b81ef6a031df72f47 6ffab37a297d434bbe6514f5d7e83451 t = -3.14 9428ef8b6033402b8818764d3e5efc65--6ffab37a297d434bbe6514f5d7e83451 1d61a6fd89d94186bf71905b0e0c33e7 2 70c2a7f3a47443c281296301b450963f t = 3.142 6ffab37a297d434bbe6514f5d7e83451--70c2a7f3a47443c281296301b450963f c2ff43bab96c4c95b8978cd7596a969d t = -3.14 70c2a7f3a47443c281296301b450963f--c2ff43bab96c4c95b8978cd7596a969d d5671eee64d84f83bfc62d1d2a2f7ecd c2ff43bab96c4c95b8978cd7596a969d--d5671eee64d84f83bfc62d1d2a2f7ecd 23ae09d964104270aa1ea72ae725d7f9 t = 1.571 d5671eee64d84f83bfc62d1d2a2f7ecd--23ae09d964104270aa1ea72ae725d7f9 41c1a9ff782445a99fc318d1df3b2120 t = 1.571 23ae09d964104270aa1ea72ae725d7f9--41c1a9ff782445a99fc318d1df3b2120 5225ca72b14844b78d113ade8ad74614 41c1a9ff782445a99fc318d1df3b2120--5225ca72b14844b78d113ade8ad74614 e40e2ad7f19c4ab882ecef1a2b83ffc5 X 5225ca72b14844b78d113ade8ad74614--e40e2ad7f19c4ab882ecef1a2b83ffc5 bc73c33589024c978466893d4641f8d5 t = 1.571 e40e2ad7f19c4ab882ecef1a2b83ffc5--bc73c33589024c978466893d4641f8d5 b9b70569ce144be5979b4ea2325afba2 t = 1.571 bc73c33589024c978466893d4641f8d5--b9b70569ce144be5979b4ea2325afba2 53bcba2903f34d4bb433fba0dbf9cce9 X b9b70569ce144be5979b4ea2325afba2--53bcba2903f34d4bb433fba0dbf9cce9 53bcba2903f34d4bb433fba0dbf9cce9--0b30ff18b9ca4d3b81ef6a031df72f47 e9fe63d016334c54900fdebb115a9f7b 6a2684a3b2334826981c9126a8561606 1d61a6fd89d94186bf71905b0e0c33e7--6a2684a3b2334826981c9126a8561606 41b920d2e2864a4a8a17104d92cf6468 6a2684a3b2334826981c9126a8561606--41b920d2e2864a4a8a17104d92cf6468 7850e919896a4c7c8051213bdc3967ff 41b920d2e2864a4a8a17104d92cf6468--7850e919896a4c7c8051213bdc3967ff f54472ac2a1244c39604e90d7a7c30e1 X 7850e919896a4c7c8051213bdc3967ff--f54472ac2a1244c39604e90d7a7c30e1 617cb34957c84c16914c6625e2043128 f54472ac2a1244c39604e90d7a7c30e1--617cb34957c84c16914c6625e2043128 1ec26a3680674c9c9097391720d04981 617cb34957c84c16914c6625e2043128--1ec26a3680674c9c9097391720d04981 389dc18b8ae04b8ca6d56d4347816a02 X 1ec26a3680674c9c9097391720d04981--389dc18b8ae04b8ca6d56d4347816a02 86fd8ab003a5453393c666e88bc2f684 X 389dc18b8ae04b8ca6d56d4347816a02--86fd8ab003a5453393c666e88bc2f684 78ff083425f44e8ba0f3af612441b59a 86fd8ab003a5453393c666e88bc2f684--78ff083425f44e8ba0f3af612441b59a 42d15a8891ab4534b3ecfc7004d5545f 78ff083425f44e8ba0f3af612441b59a--42d15a8891ab4534b3ecfc7004d5545f 798c98c6a98a461294aa81ed6ae52128 X 42d15a8891ab4534b3ecfc7004d5545f--798c98c6a98a461294aa81ed6ae52128 798c98c6a98a461294aa81ed6ae52128--e9fe63d016334c54900fdebb115a9f7b <p>The output circuit displays three groups of system Hamiltonian evolutions which account for global-phases and single-qubit detunings related to the mapping between the \\(Z\\) and \\(N\\) operators. Optionally, global phases can be ignored.</p> <p>In general, the mapping of a \\(n\\)-qubit Ising Hamiltonian to another will require at most \\(n(n-1)\\) evolutions. The transformed circuit performs these evolutions for specific times that are computed from the solution of a linear system of equations involving the set of interactions in the target and build Hamiltonians.</p> <p>In this case, the mapping is exact when using the step-wise DAQC strategy (<code>Strategy.SDAQC</code>) available in Qadence. In banged DAQC (<code>Strategy.BDAQC</code>) the mapping is approximate, but easier to implement on a physical device with always-on interactions such as neutral-atom systems.</p> <p>Just as before, the transformed Ising circuit can be checked to exactly recover the <code>CPHASE</code> gate:</p> <pre><code># CPHASE on (i, j), Identity on third qubit:\ncphase_matrix = block_to_tensor(kron(CPHASE(i, j, phi), I(k)))\n\n# CPHASE using the transformed circuit:\ncphase_evo_matrix = block_to_tensor(transformed_ising)\n\n# Check that it implements the CPHASE.\n# Will fail if global phases are ignored.\n</code></pre> <pre><code>cphase_matrix == cphase_evo_matrix : True\n</code></pre> <p>The <code>CNOT</code> gate can now finally be built:</p> <pre><code>from qadence import equivalent_state, run, sample\n\ncnot_daqc = chain(\n    H(j),\n    transformed_ising,\n    H(j)\n)\n\n# And finally apply the CNOT on a specific 3-qubit initial state:\ninit_state = product_state(\"101\")\n\n# Check we get an equivalent wavefunction\nwf_cnot = run(n_qubits, block=cnot_target, state=init_state)\nwf_daqc = run(n_qubits, block=cnot_daqc, state=init_state)\n\n# Visualize the CNOT bit-flip in samples.\n</code></pre> <pre><code>wf_cnot == wf_dacq : True\nsample cnot_target = [OrderedCounter({'111': 100})]\nsample cnot_dacq = [OrderedCounter({'111': 100})]\n</code></pre> <p>As one can see, a <code>CNOT</code> operation has been succesfully implemented on the desired target qubits by using only the global system as the building block Hamiltonian and single-qubit rotations. Decomposing a single digital gate into an Ising Hamiltonian serves as a proof of principle for the potential of this technique to represent universal quantum computation.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#technical-details-on-the-daqc-transformation","title":"Technical details on the DAQC transformation","text":"<ul> <li>The mapping between target generator and final circuit is performed by solving a linear system of size \\(n(n-1)\\) where \\(n\\) is the number of qubits, so it can be computed efficiently (i.e., with a polynomial cost in the number of qubits).</li> <li>The linear system to be solved is actually not invertible for \\(n=4\\) qubits. This is very specific edge case requiring a workaround, that is currently not yet implemented.</li> <li>As mentioned, the final circuit has at most \\(n(n-1)\\) slices, so there is at most a quadratic overhead in circuit depth.</li> </ul> <p>Finally, and most important to its usage:</p> <ul> <li>The target Hamiltonian should be sufficiently represented in the building block Hamiltonian.</li> </ul> <p>To illustrate this point, consider the following target and build Hamiltonians:</p> <pre><code># Interaction between qubits 0 and 1\ngen_target = 1.0 * (Z(0) @ Z(1))\n\n# Fixed interaction between qubits 1 and 2, and customizable between 0 and 1\ndef gen_build(g_int):\n    return g_int * (Z(0) @ Z(1)) + 1.0 * (Z(1) @ Z(2))\n</code></pre> <p>And now we perform the DAQC transform by setting <code>g_int=1.0</code>, exactly matching the target Hamiltonian:</p> <pre><code>transformed_ising = daqc_transform(\n    n_qubits=3,\n    gen_target=gen_target,\n    t_f=1.0,\n    gen_build=gen_build(g_int=1.0),\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_d40ec51c22f140b6a6f30d77c44b4888 cluster_3e89b93c39664eb9a9c08ce2dea9e5b9 16b98e9576be4337b001848a71bc3902 0 ad879e9452844a438757ebc64c87fc38 X 16b98e9576be4337b001848a71bc3902--ad879e9452844a438757ebc64c87fc38 03b9ffb35e374dd8a33575d7f2920e19 1 ac24da1a4bb94a38b22444b6644c6376 HamEvo ad879e9452844a438757ebc64c87fc38--ac24da1a4bb94a38b22444b6644c6376 a0e342af3dac4769aa3d01ce74aa4008 X ac24da1a4bb94a38b22444b6644c6376--a0e342af3dac4769aa3d01ce74aa4008 26e1f18bc3794c1a8dcfbbcce1460e26 a0e342af3dac4769aa3d01ce74aa4008--26e1f18bc3794c1a8dcfbbcce1460e26 bc7d582b64254af9a1dcdc6717d282a0 HamEvo 26e1f18bc3794c1a8dcfbbcce1460e26--bc7d582b64254af9a1dcdc6717d282a0 8119652ec8664d49bbb68b1724593270 bc7d582b64254af9a1dcdc6717d282a0--8119652ec8664d49bbb68b1724593270 9a0579737fb54a1aa0fd20536fa5d54d 8119652ec8664d49bbb68b1724593270--9a0579737fb54a1aa0fd20536fa5d54d e9a5de76c56940d882bc6d62200d00db 93f16b263e824a8db3221943f8daa91a 03b9ffb35e374dd8a33575d7f2920e19--93f16b263e824a8db3221943f8daa91a d8357694280443bbbe6128a3d912331e 2 a773548da6b34fa59486790a4499e79e t = -0.50 93f16b263e824a8db3221943f8daa91a--a773548da6b34fa59486790a4499e79e f45b615b170a4a5c97793587de348243 a773548da6b34fa59486790a4499e79e--f45b615b170a4a5c97793587de348243 f291f6108d594241b696468738b2d9f8 X f45b615b170a4a5c97793587de348243--f291f6108d594241b696468738b2d9f8 255dc87734f24e26ba8127b4903baedc t = -0.50 f291f6108d594241b696468738b2d9f8--255dc87734f24e26ba8127b4903baedc 26fddce5f3a84930825ddbf6633d2e61 X 255dc87734f24e26ba8127b4903baedc--26fddce5f3a84930825ddbf6633d2e61 26fddce5f3a84930825ddbf6633d2e61--e9a5de76c56940d882bc6d62200d00db edc97a7f152649ecb876020622b21c28 baa6eec458c64906b15e5e776bd09c82 X d8357694280443bbbe6128a3d912331e--baa6eec458c64906b15e5e776bd09c82 2513703a3ee649c9926e91c4bb0259de baa6eec458c64906b15e5e776bd09c82--2513703a3ee649c9926e91c4bb0259de 9b4cb5d91e8c4fe58a06715773f42a4a X 2513703a3ee649c9926e91c4bb0259de--9b4cb5d91e8c4fe58a06715773f42a4a 4be8bb419c8d4dfc91346d3bb0498cdd X 9b4cb5d91e8c4fe58a06715773f42a4a--4be8bb419c8d4dfc91346d3bb0498cdd c9dcc16656734569a289184ac01e734f 4be8bb419c8d4dfc91346d3bb0498cdd--c9dcc16656734569a289184ac01e734f 45e307c06ec1402e83ff66dee63570aa X c9dcc16656734569a289184ac01e734f--45e307c06ec1402e83ff66dee63570aa 45e307c06ec1402e83ff66dee63570aa--edc97a7f152649ecb876020622b21c28 <p>Now, if the interaction between qubits 0 and 1 is weakened in the build Hamiltonian:</p> <pre><code>transformed_ising = daqc_transform(\n    n_qubits=3,\n    gen_target=gen_target,\n    t_f=1.0,\n    gen_build=gen_build(g_int=0.001),\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_7638bdee73df40259e2a78dcb7138c9c cluster_d89c1668040949b59814dc29d3d1f422 1f32b3b4571e427b99d5d452fe7dc8ac 0 f59733b4e6c9405c9c620d4b14497cc9 X 1f32b3b4571e427b99d5d452fe7dc8ac--f59733b4e6c9405c9c620d4b14497cc9 5260cfafda674232a9faf06ac9099a10 1 c6d0b1211f2b4462bf83bd7dc944572e HamEvo f59733b4e6c9405c9c620d4b14497cc9--c6d0b1211f2b4462bf83bd7dc944572e 821dd4ce12f446598f6a6590466afca4 X c6d0b1211f2b4462bf83bd7dc944572e--821dd4ce12f446598f6a6590466afca4 a450d82c64c946669a70750211b4ddcd 821dd4ce12f446598f6a6590466afca4--a450d82c64c946669a70750211b4ddcd 41a02326b01d47808e4d2c004430a116 HamEvo a450d82c64c946669a70750211b4ddcd--41a02326b01d47808e4d2c004430a116 9bbd27bb2f2c4050b8695169dfb3e928 41a02326b01d47808e4d2c004430a116--9bbd27bb2f2c4050b8695169dfb3e928 53be2b797859498db94e87af97624959 9bbd27bb2f2c4050b8695169dfb3e928--53be2b797859498db94e87af97624959 9cf0b43917a0406fa9e0dbf08e459f85 c20b91f337224795963bad913e5747f2 5260cfafda674232a9faf06ac9099a10--c20b91f337224795963bad913e5747f2 c4ffc9a79ef949c3a9f65d63bc9b48d8 2 505fc176acb7451eac070fbf046685a7 t = -500. c20b91f337224795963bad913e5747f2--505fc176acb7451eac070fbf046685a7 bb8f1bf151194347886ed45e2786e0f8 505fc176acb7451eac070fbf046685a7--bb8f1bf151194347886ed45e2786e0f8 02dde272797f41cc982309a98341e9d4 X bb8f1bf151194347886ed45e2786e0f8--02dde272797f41cc982309a98341e9d4 fbbd1f362f624f1fb945bb7b667b5ef7 t = -500. 02dde272797f41cc982309a98341e9d4--fbbd1f362f624f1fb945bb7b667b5ef7 9ec5ef1c91434ad4b982e202625ee1b7 X fbbd1f362f624f1fb945bb7b667b5ef7--9ec5ef1c91434ad4b982e202625ee1b7 9ec5ef1c91434ad4b982e202625ee1b7--9cf0b43917a0406fa9e0dbf08e459f85 42907cb7f3d8416393954ba8e02d59fe 3f9c25e55dac44b3996066fc4847cb39 X c4ffc9a79ef949c3a9f65d63bc9b48d8--3f9c25e55dac44b3996066fc4847cb39 400ec32f3e144c20aeb3ccafa14958c7 3f9c25e55dac44b3996066fc4847cb39--400ec32f3e144c20aeb3ccafa14958c7 bae29b62b89b4adb9a2f7a27183eac2f X 400ec32f3e144c20aeb3ccafa14958c7--bae29b62b89b4adb9a2f7a27183eac2f 5658bb6589cf493ab98abe3c3b914bf7 X bae29b62b89b4adb9a2f7a27183eac2f--5658bb6589cf493ab98abe3c3b914bf7 d3607906ee8d406497926ad11c04d311 5658bb6589cf493ab98abe3c3b914bf7--d3607906ee8d406497926ad11c04d311 a9709b5c064c463b97e5d2b1882c5bd4 X d3607906ee8d406497926ad11c04d311--a9709b5c064c463b97e5d2b1882c5bd4 a9709b5c064c463b97e5d2b1882c5bd4--42907cb7f3d8416393954ba8e02d59fe <p>The times slices using the build Hamiltonian need now to evolve for much longer to represent the same interaction since it is not sufficiently represented in the building block Hamiltonian.</p> <p>In the limit where that interaction is not present, the transform will not work:</p> <pre><code>try:\n    transformed_ising = daqc_transform(\n        n_qubits=3,\n        gen_target=gen_target,\n        t_f=1.0,\n        gen_build=gen_build(g_int = 0.0),\n    )\nexcept ValueError as error:\n    print(\"Error:\", error)\n</code></pre> <pre><code>Error: Incompatible interactions between target and build Hamiltonians.\n</code></pre>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#references","title":"References","text":"<ol> <li> <p>Dodd et al., Universal quantum computation and simulation using any entangling Hamiltonian and local unitaries, PRA 65, 040301 (2002). \u21a9</p> </li> <li> <p>Parra-Rodriguez et al., Digital-Analog Quantum Computation, PRA 101, 022305 (2020). \u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/","title":"Fitting a function with a Hamiltonian ansatz","text":"<p>In the analog QCL tutorial we used analog blocks to learn a function of interest. The analog blocks are a direct abstraction of device execution with global addressing. However, we may want to directly program an Hamiltonian-level ansatz to have a finer control on our model. In Qadence this can easily be done through digital-analog programs. In this tutorial we will solve a simple QCL problem with this approach.</p>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#setting-up-the-problem","title":"Setting up the problem","text":"<p>The example problem considered is to fit a function of interest in a specified range. Below we define and plot the function \\(f(x)=x^5\\).</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Function to fit:\ndef f(x):\n    return x**5\n\nxmin = -1.0\nxmax = 1.0\nn_test = 100\n\nx_test = torch.linspace(xmin, xmax, steps = n_test)\ny_test = f(x_test)\n\nplt.plot(x_test, y_test)\nplt.xlim((-1.1, 1.1))\nplt.ylim((-1.1, 1.1))\n</code></pre> 2025-01-28T16:50:58.207805 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#digital-analog-ansatz","title":"Digital-Analog Ansatz","text":"<p>We start by defining the register of qubits. The topology we use now will define the interactions in the entangling Hamiltonian. As an example, we can define a rectangular lattice with 6 qubits.</p> <pre><code>from qadence import Register\n\nreg = Register.rectangular_lattice(\n    qubits_row = 3,\n    qubits_col = 2,\n)\n</code></pre> <p>Inspired by the Ising interaction mode of Rydberg atoms, we can now define an interaction Hamiltonian as \\(\\mathcal{H}_{ij}=\\frac{1}{r_{ij}^6}N_iN_j\\), where \\(N_i=(1/2)(I_i-Z_i)\\) is the number operator and and \\(r_{ij}\\) is the distance between qubits \\(i\\) and \\(j\\). We can easily instatiate this interaction Hamiltonian from the register information:</p> <pre><code>from qadence import N, add\n\ndef h_ij(i: int, j: int):\n    return N(i)@N(j)\n\nh_int = add(h_ij(*edge)/r**6 for edge, r in reg.edge_distances.items())\n</code></pre> <p>To build the digital-analog ansatz we can make use of the standard <code>hea</code> function by specifying we want to use the <code>Strategy.SDAQC</code> and passing the Hamiltonian we created as the entangler, as see in the QML constructors tutorial. The entangling operation will be replaced by the evolution of this Hamiltonian <code>HamEvo(h_int, t)</code>, where the time parameter <code>t</code> is considered to be a variational parameter at each layer.</p> <pre><code>from qadence import hea, Strategy, RX, RY\n\ndepth = 2\n\nda_ansatz = hea(\n    n_qubits = reg.n_qubits,\n    depth = depth,\n    operations = [RX, RY, RX],\n    entangler = h_int,\n    strategy = Strategy.SDAQC,\n)\n\nprint(html_string(da_ansatz))\n</code></pre> %3 cluster_c638c34d3639499e83c337577fa03527 cluster_c7418e7737884b9e9a7445f9611915aa 00c9caa206ae49f5a142194da32e2b2d 0 b4e6d34c5433491692ee6788d3bb1305 RX(theta\u2080) 00c9caa206ae49f5a142194da32e2b2d--b4e6d34c5433491692ee6788d3bb1305 94cfc23f69f14827b02ee7a192abd5db 1 598cfe0ab2c2451d89402e7a3b36bbbb RY(theta\u2086) b4e6d34c5433491692ee6788d3bb1305--598cfe0ab2c2451d89402e7a3b36bbbb 7384f2e22e3a42c0a2b4f50e7ba6796b RX(theta\u2081\u2082) 598cfe0ab2c2451d89402e7a3b36bbbb--7384f2e22e3a42c0a2b4f50e7ba6796b d5023dd1a8574cd4b6a87ea05e707f23 7384f2e22e3a42c0a2b4f50e7ba6796b--d5023dd1a8574cd4b6a87ea05e707f23 2ac7e57487e94297b58166bdc68257d9 RX(theta\u2081\u2088) d5023dd1a8574cd4b6a87ea05e707f23--2ac7e57487e94297b58166bdc68257d9 a1f333af91a949dd99d9835e4109fad1 RY(theta\u2082\u2084) 2ac7e57487e94297b58166bdc68257d9--a1f333af91a949dd99d9835e4109fad1 f8f6d0f21c274c72a4fd69c0f10d355c RX(theta\u2083\u2080) a1f333af91a949dd99d9835e4109fad1--f8f6d0f21c274c72a4fd69c0f10d355c 0f20a2a89d85475cb3fc845a30f20a97 f8f6d0f21c274c72a4fd69c0f10d355c--0f20a2a89d85475cb3fc845a30f20a97 e4cbb09593f44df2abafe076bc7de291 0f20a2a89d85475cb3fc845a30f20a97--e4cbb09593f44df2abafe076bc7de291 cdf9790965b64c80a35287e99a9ac70e 32e3039e4b9440a9a1f83f5b557c2f3a RX(theta\u2081) 94cfc23f69f14827b02ee7a192abd5db--32e3039e4b9440a9a1f83f5b557c2f3a b2dae95547ba4504940ad1ab679e63a6 2 1969fcae73064fdaad0ddc14089f4984 RY(theta\u2087) 32e3039e4b9440a9a1f83f5b557c2f3a--1969fcae73064fdaad0ddc14089f4984 97de4ce592914dcdb51da8bd98bf5d00 RX(theta\u2081\u2083) 1969fcae73064fdaad0ddc14089f4984--97de4ce592914dcdb51da8bd98bf5d00 c4fd73b44c064d08b9f88d40d7a8d9e8 97de4ce592914dcdb51da8bd98bf5d00--c4fd73b44c064d08b9f88d40d7a8d9e8 1cbd0679fcd84179bf3fee79d9d0a604 RX(theta\u2081\u2089) c4fd73b44c064d08b9f88d40d7a8d9e8--1cbd0679fcd84179bf3fee79d9d0a604 98dbf73061684716979bec80a7138f12 RY(theta\u2082\u2085) 1cbd0679fcd84179bf3fee79d9d0a604--98dbf73061684716979bec80a7138f12 e1886c5cd36a4ff3bb70251d668a35cb RX(theta\u2083\u2081) 98dbf73061684716979bec80a7138f12--e1886c5cd36a4ff3bb70251d668a35cb be197eb7ac66412e86d840454200e245 e1886c5cd36a4ff3bb70251d668a35cb--be197eb7ac66412e86d840454200e245 be197eb7ac66412e86d840454200e245--cdf9790965b64c80a35287e99a9ac70e 3757f32c21474464a1f2dd5819c2ad17 d168fe8e8dc04433b4cdbb99517a1389 RX(theta\u2082) b2dae95547ba4504940ad1ab679e63a6--d168fe8e8dc04433b4cdbb99517a1389 b295d4142ec24e75b005b45d33c2fa50 3 3705d29e1e224a3d9e0ec716687a95ca RY(theta\u2088) d168fe8e8dc04433b4cdbb99517a1389--3705d29e1e224a3d9e0ec716687a95ca f152caa769a7411b8163ea799501eb93 RX(theta\u2081\u2084) 3705d29e1e224a3d9e0ec716687a95ca--f152caa769a7411b8163ea799501eb93 f70fe047a2f04c448e10b669caa0ff84 HamEvo f152caa769a7411b8163ea799501eb93--f70fe047a2f04c448e10b669caa0ff84 803ab0391c9541a99d5b1342e7e91735 RX(theta\u2082\u2080) f70fe047a2f04c448e10b669caa0ff84--803ab0391c9541a99d5b1342e7e91735 d4cbc4b5296b4f86b469e62cb07ce16d RY(theta\u2082\u2086) 803ab0391c9541a99d5b1342e7e91735--d4cbc4b5296b4f86b469e62cb07ce16d 75acc861a4034c51b514c9ad51dd88cc RX(theta\u2083\u2082) d4cbc4b5296b4f86b469e62cb07ce16d--75acc861a4034c51b514c9ad51dd88cc 43414aae0fca452893d2252ab5003b0d HamEvo 75acc861a4034c51b514c9ad51dd88cc--43414aae0fca452893d2252ab5003b0d 43414aae0fca452893d2252ab5003b0d--3757f32c21474464a1f2dd5819c2ad17 8ba758f7b33d409a8183369e33d539ba d924069f56e24f10ab488ecfaa6093e2 RX(theta\u2083) b295d4142ec24e75b005b45d33c2fa50--d924069f56e24f10ab488ecfaa6093e2 ea41b5e3495f4c699b3a157463105426 4 d4cf487769454f038371c19f87319b01 RY(theta\u2089) d924069f56e24f10ab488ecfaa6093e2--d4cf487769454f038371c19f87319b01 5e95fbf44924490387b68ed79fc54b55 RX(theta\u2081\u2085) d4cf487769454f038371c19f87319b01--5e95fbf44924490387b68ed79fc54b55 823ab1415c814d019971aa40a2108eb1 t = theta_t\u2080 5e95fbf44924490387b68ed79fc54b55--823ab1415c814d019971aa40a2108eb1 8e1d0634f0954842a3dcf86b89a02af5 RX(theta\u2082\u2081) 823ab1415c814d019971aa40a2108eb1--8e1d0634f0954842a3dcf86b89a02af5 d8ae1b58a0d04cf798170cc7d8cc4c7e RY(theta\u2082\u2087) 8e1d0634f0954842a3dcf86b89a02af5--d8ae1b58a0d04cf798170cc7d8cc4c7e 92fdad102deb4039bb4a2973aea2322a RX(theta\u2083\u2083) d8ae1b58a0d04cf798170cc7d8cc4c7e--92fdad102deb4039bb4a2973aea2322a 446116dc3a154c518782308856b61d1a t = theta_t\u2081 92fdad102deb4039bb4a2973aea2322a--446116dc3a154c518782308856b61d1a 446116dc3a154c518782308856b61d1a--8ba758f7b33d409a8183369e33d539ba 2d0989c063064387a2bf6353d08ee46f 2ff521a0c84740cbb4732cb7ed3ae7a9 RX(theta\u2084) ea41b5e3495f4c699b3a157463105426--2ff521a0c84740cbb4732cb7ed3ae7a9 89e9f3f72ace4d3ea0cc095c7809fd59 5 6894012cd34748dd8aeb688fb37242cf RY(theta\u2081\u2080) 2ff521a0c84740cbb4732cb7ed3ae7a9--6894012cd34748dd8aeb688fb37242cf 04bd8e2eeb014046ac52d6aec6b44a2d RX(theta\u2081\u2086) 6894012cd34748dd8aeb688fb37242cf--04bd8e2eeb014046ac52d6aec6b44a2d b8938ad5ddf5451db1f6199631c88296 04bd8e2eeb014046ac52d6aec6b44a2d--b8938ad5ddf5451db1f6199631c88296 0810bfe53ca245d7af2ceeaceed0efd6 RX(theta\u2082\u2082) b8938ad5ddf5451db1f6199631c88296--0810bfe53ca245d7af2ceeaceed0efd6 37f13d32199e47d29b0de635c322c225 RY(theta\u2082\u2088) 0810bfe53ca245d7af2ceeaceed0efd6--37f13d32199e47d29b0de635c322c225 916a78532d56459b828069f4de54db24 RX(theta\u2083\u2084) 37f13d32199e47d29b0de635c322c225--916a78532d56459b828069f4de54db24 3558b9a0705f4b2fadb381d0e88fec3d 916a78532d56459b828069f4de54db24--3558b9a0705f4b2fadb381d0e88fec3d 3558b9a0705f4b2fadb381d0e88fec3d--2d0989c063064387a2bf6353d08ee46f 5b4affb1f95d48d1b734135358374d5e ee8b8d85983f470fb2227b0b2c427c44 RX(theta\u2085) 89e9f3f72ace4d3ea0cc095c7809fd59--ee8b8d85983f470fb2227b0b2c427c44 ce511675a2e3465cb385ec59b446e54c RY(theta\u2081\u2081) ee8b8d85983f470fb2227b0b2c427c44--ce511675a2e3465cb385ec59b446e54c cc5d3301f46647d0a9570cf6d9299bb7 RX(theta\u2081\u2087) ce511675a2e3465cb385ec59b446e54c--cc5d3301f46647d0a9570cf6d9299bb7 652552cee834423584ab7c3884eafabf cc5d3301f46647d0a9570cf6d9299bb7--652552cee834423584ab7c3884eafabf 6ec96fd644964933aaa79e9c3540e82d RX(theta\u2082\u2083) 652552cee834423584ab7c3884eafabf--6ec96fd644964933aaa79e9c3540e82d cddb3eb646bb41ea9c7c8005e03e912a RY(theta\u2082\u2089) 6ec96fd644964933aaa79e9c3540e82d--cddb3eb646bb41ea9c7c8005e03e912a a7bda1635f36436db668a45b10cd798a RX(theta\u2083\u2085) cddb3eb646bb41ea9c7c8005e03e912a--a7bda1635f36436db668a45b10cd798a cf22e846482d49938b7ec72001a8c89b a7bda1635f36436db668a45b10cd798a--cf22e846482d49938b7ec72001a8c89b cf22e846482d49938b7ec72001a8c89b--5b4affb1f95d48d1b734135358374d5e"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#creating-the-quantummodel","title":"Creating the QuantumModel","text":"<p>The rest of the procedure is the same as any other Qadence workflow. We start by defining a feature map for input encoding and an observable for output decoding.</p> <pre><code>from qadence import feature_map, BasisSet, ReuploadScaling\nfrom qadence import Z, I\n\nfm = feature_map(\n    n_qubits = reg.n_qubits,\n    param = \"x\",\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.TOWER,\n)\n\n# Total magnetization\nobservable = add(Z(i) for i in range(reg.n_qubits))\n</code></pre> <p>And we have all the ingredients to initialize the <code>QuantumModel</code>:</p> <pre><code>from qadence import QuantumCircuit, QuantumModel\n\ncircuit = QuantumCircuit(reg, fm, da_ansatz)\n\nmodel = QuantumModel(circuit, observable = observable)\n</code></pre>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#training-the-model","title":"Training the model","text":"<p>We can now train the model. We use a set of 20 equally spaced training points.</p> <pre><code># Chebyshev FM does not accept x = -1, 1\nxmin = -0.99\nxmax = 0.99\nn_train = 20\n\nx_train = torch.linspace(xmin, xmax, steps = n_train)\ny_train = f(x_train)\n\n# Initial model prediction\ny_pred_initial = model.expectation({\"x\": x_test}).detach()\n</code></pre> <p>And we use a simple custom training loop.</p> <pre><code>criterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nn_epochs = 200\n\ndef loss_fn(x_train, y_train):\n    out = model.expectation({\"x\": x_train})\n    loss = criterion(out.squeeze(), y_train)\n    return loss\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_fn(x_train, y_train)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#results","title":"Results","text":"<p>Finally we can plot the resulting trained model.</p> <pre><code>y_pred_final = model.expectation({\"x\": x_test}).detach()\n\nplt.plot(x_test, y_pred_initial, label = \"Initial prediction\")\nplt.plot(x_test, y_pred_final, label = \"Final prediction\")\nplt.scatter(x_train, y_train, label = \"Training points\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.xlim((-1.1, 1.1))\nplt.ylim((-1.1, 1.1))\n</code></pre> 2025-01-28T16:51:06.435797 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/","title":"Pulse-level programming with Pulser","text":"<p>Qadence offers a direct interface with Pulser<sup>1</sup>, an open-source pulse-level interface written in Python and specifically designed for programming neutral atom quantum computers.</p> <p>Using directly Pulser requires advanced knowledge on pulse-level programming and on how neutral atom devices work. Qadence abstracts this complexity out by using the familiar block-based interface for building pulse sequences in Pulser while leaving the possibility to directly manipulate them if required by, for instance, optimal pulse shaping.</p> <p>Note</p> <p>The Pulser backend is still experimental and the interface might change in the future. Please note that it does not support <code>DiffMode.AD</code>.</p> <p>Note</p> <p>With the Pulser backend, <code>qadence</code> simulations can be executed on the cloud emulators available on the PASQAL cloud platform. In order to do so, make to have valid credentials for the PASQAL cloud platform and use the following configuration for the Pulser backend:</p> <pre><code>config = {\n    \"cloud_configuration\": {\n        \"username\": \"&lt;changeme&gt;\",\n        \"password\": \"&lt;changeme&gt;\",\n        \"project_id\": \"&lt;changeme&gt;\",  # the project should have access to emulators\n        \"platform\": \"EMU_FREE\"  # choose between `EMU_TN` and `EMU_FREE`\n    }\n}\n</code></pre> <p>For inquiries and more details on the cloud credentials, please contact info@pasqal.com.</p>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#default-qubit-interaction","title":"Default qubit interaction","text":"<p>When simulating pulse sequences written using Pulser, the underlying constructed Hamiltonian is equivalent to a digital-analog quantum computing program (see digital-analog emulation for more details) with the following interaction term:</p> \\[ \\mathcal{H}_{\\textrm{int}} = \\sum_{i&lt;j} \\frac{C_6}{|R_i - R_j|^6} \\hat{n}_i \\hat{n}_j \\] <p>where \\(C_6\\) is an interaction strength coefficient dependent on the principal quantum number of chosen the neutral atom system, \\(R_i\\) are atomic positions in Cartesian coordinates and \\(\\hat{n} = \\frac{1-\\sigma^z_i}{2}\\) the number operator.</p> <p>Note</p> <p>The Ising interaction is always-on for all computations performed with the Pulser backend. It cannot be switched off.</p>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#available-quantum-operations","title":"Available quantum operations","text":"<p>Currently, the Pulser backend supports the following operations:</p> gate description trainable parameter <code>RX</code>, <code>RY</code> Single qubit rotations. Notice that the interaction is on and this affects the resulting gate fidelity. rotation angle <code>AnalogRX</code>, <code>AnalogRY</code>, <code>AnalogRZ</code> Span a single qubit rotation among the entire register. rotation angle <code>entangle</code> Fully entangle the register. interaction time <code>AnalogInteraction</code> An idle block to to free-evolve for a duration according to the interaction. free evolution time"},{"location":"tutorials/digital_analog_qc/pulser-basic/#sequence-the-bell-state-on-a-two-qubit-register","title":"Sequence the Bell state on a two qubit register","text":"<p>The next example illustrates how to create a pulse sequence to prepare a Bell state. This is a sequence of an entanglement operation, represented as an <code>entangle</code> gate (using <code>CZ</code> interactions) in the \\(X\\)-basis and a \\(Y\\) rotation for readout in the \\(Z\\)-basis:</p> <pre><code>from qadence import chain, entangle, RY\n\nbell_state = chain(\n   entangle(\"t\", qubit_support=(0,1)),\n   RY(0, \"y\"),\n)\n</code></pre> <pre><code>bell_state = ChainBlock(0,1)\n\u251c\u2500\u2500 AnalogEntanglement(t=0.25022735767019744, support=(0, 1))\n\u2514\u2500\u2500 RY(0) [params: ['y']]\n</code></pre> <p>Next, a <code>Register</code> with two qubits is combined with the resulting <code>ChainBlock</code> to form a circuit. Then, the <code>QuantumModel</code> converts the circuit into a proper parametrized pulse sequence with the Pulser backend. Supplying the parameter values allows to sample the pulse sequence outcome:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom qadence import Register, QuantumCircuit, QuantumModel, PI\n\nregister = Register.line(2, spacing = 8.0)  # Two qubits with a distance of 8\u00b5m\ncircuit = QuantumCircuit(register, bell_state)\nmodel = QuantumModel(circuit, backend=\"pulser\", diff_mode=\"gpsr\")\n\nparams = {\n    \"t\": torch.tensor([1000]),  # ns\n    \"y\": torch.tensor([3*PI/2]),\n}\n\n# Return the final state vector\nfinal_vector = model.run(params)\n\n# Sample from the result state vector\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> <pre><code>final_vector = tensor([[-0.7114-0.0169j, -0.0338+0.0155j,  0.0110-0.0457j,  0.6631-0.2245j]])\nsample = Counter({'11': 26, '00': 23, '10': 1})\n</code></pre> <p>Plot the distribution:</p> <p><pre><code>\n</code></pre> 2025-01-28T16:51:06.642956 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/  One can visualise the pulse sequence with different parameters using the <code>assign_paramters</code> method.</p> <pre><code>model.assign_parameters(params).draw(show=False)\n</code></pre> 2025-01-28T16:51:06.761777 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#change-device-specifications","title":"Change device specifications","text":"<p>At variance with other backends, Pulser provides the concept of <code>Device</code>. A <code>Device</code> instance encapsulates all the properties for the definition of a real neutral atoms processor, including but not limited to the maximum laser amplitude for pulses, the maximum distance between two qubits and the maximum duration of the pulse. For more information, please check this tutorial.</p> <p>Qadence offers a simplified interface with only two devices which are detailed here:</p> <ul> <li><code>IDEALIZED</code> (default): ideal device which should be used only for testing purposes. It does not restrict the simulation of pulse sequences.</li> <li><code>REALISTIC</code>: device specification close to real neutral atom quantum processors.</li> </ul> <p>Note</p> <p>If you want to perform simulations closer to the specifications of real neutral atom machines, always select the <code>REALISTIC</code> device.</p> <p>One can use the <code>Configuration</code> of the Pulser backend to select the appropriate device:</p> <pre><code>from qadence import BackendName, DiffMode\nfrom qadence import RealisticDevice\n\n# Choose a realistic device\nregister = Register.line(2, spacing = 8.0, device_specs = RealisticDevice())\n\ncircuit = QuantumCircuit(register, bell_state)\n\nmodel = QuantumModel(\n    circuit,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR,\n)\n\nparams = {\n    \"t\": torch.tensor([1000]),  # ns\n    \"y\": torch.tensor([3*PI/2]),\n}\n\n# Sample from the result state vector\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> <pre><code>sample = Counter({'00': 27, '11': 22, '10': 1})\n</code></pre>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#create-a-custom-gate","title":"Create a custom gate","text":"<p>A major advantage of the block-based interface in Qadence is the ease to compose complex operations from a restricted set of primitive ones. In the following, a custom entanglement operation is used as an example.</p> <p>The operation consists of moving all the qubits to the \\(X\\)-basis. This is realized when the atomic interaction performs a controlled-\\(Z\\) operation during the free evolution. As seen before, this is implemented with the <code>AnalogInteraction</code> and <code>AnalogRY</code> blocks together with appropriate parameters.</p> <pre><code>from qadence import AnalogRY, chain, AnalogInteraction\n\n# Custom entanglement operation.\ndef my_entanglement(duration):\n    return chain(\n        AnalogRY(-PI / 2),\n        AnalogInteraction(duration)\n    )\n\nprotocol = chain(\n   my_entanglement(\"t\"),\n   RY(0, \"y\"),\n)\n\nregister = Register.line(2, spacing = 8.0)\ncircuit = QuantumCircuit(register, protocol)\nmodel = QuantumModel(circuit, backend=BackendName.PULSER, diff_mode=DiffMode.GPSR)\n\nparams = {\n    \"t\": torch.tensor([500]),  # ns\n    \"y\": torch.tensor([PI / 2]),\n}\n\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> 2025-01-28T16:51:07.136918 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#digital-analog-qnn-circuit","title":"Digital-analog QNN circuit","text":"<p>Finally, let's put all together by constructing a digital-analog version of a quantum neural network circuit with feature map and variational ansatz.</p> <pre><code>from qadence import kron, feature_map, BasisSet\nfrom qadence.operations import RX, RY, AnalogRX\n\nhea_one_layer = chain(\n    kron(RY(0, \"th00\"), RY(1, \"th01\")),\n    kron(RX(0, \"th10\"), RX(1, \"th11\")),\n    kron(RY(0, \"th20\"), RY(1, \"th21\")),\n    entangle(\"t\", qubit_support=(0,1)),\n)\n\nprotocol = chain(\n    feature_map(1, param=\"x\", fm_type=BasisSet.FOURIER),\n    hea_one_layer,\n    AnalogRX(PI/4)\n)\n\nregister = Register.line(2, spacing=8.0)\ncircuit = QuantumCircuit(register, protocol)\nmodel = QuantumModel(circuit, backend=BackendName.PULSER, diff_mode=DiffMode.GPSR)\n\nparams = {\n    \"x\": torch.tensor([0.8]), # rad\n    \"t\": torch.tensor([900]), # ns\n    \"th00\":  torch.rand(1), # rad\n    \"th01\":  torch.rand(1), # rad\n    \"th10\":  torch.rand(1), # rad\n    \"th11\":  torch.rand(1), # rad\n    \"th20\":  torch.rand(1), # rad\n    \"th21\":  torch.rand(1), # rad\n}\n\nmodel.assign_parameters(params).draw(draw_phase_area=True, show=False)\n</code></pre> 2025-01-28T16:51:07.294700 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#references","title":"References","text":"<ol> <li> <p>Pulser: An open-source package for the design of pulse sequences in programmable neutral-atom arrays \u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/","title":"Restricted local addressability","text":""},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#physics-behind-semi-local-addressing-patterns","title":"Physics behind semi-local addressing patterns","text":"<p>Recall that in Qadence the general neutral-atom Hamiltonian for a set of \\(n\\) interacting qubits is given by expression</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm drive} + \\mathcal{H}_{\\rm int} = \\sum_{i=0}^{n-1}\\left(\\mathcal{H}^\\text{d}_{i}(t) + \\sum_{j&lt;i}\\mathcal{H}^\\text{int}_{ij}\\right) \\] <p>as is described in detail in the analog interface basics documentation.</p> <p>The driving Hamiltonian term in priciple can model any local single-qubit rotation by addressing each qubit individually. However, some neutral-atom devices offer restricted local addressability using devices called spatial light modulators (SLMs).</p> <p>We refer to this regime as semi-local addressability. In this regime, the individual qubit addressing is restricted to a pattern of targeted qubits which is kept fixed during the execution of the quantum circuit. More formally, the addressing pattern appears as an additional term in the neutral-atom Hamiltonian:</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm drive} + \\mathcal{H}_{\\rm int} + \\mathcal{H}_{\\rm local} \\] <p>where \\(\\mathcal{H}_{\\rm pattern}\\) is given by</p> \\[ \\mathcal{H}_{\\rm local} = \\sum_{i=0}^{n-1}\\left(-\\Delta w_i^{\\rm det} \\hat{n}_i + \\Gamma w_i^{\\rm drive} \\hat{\\sigma}^x_i\\right). \\] <p>Here \\(\\Delta\\) specifies the maximal negative detuning that each qubit in the register can be exposed to. The weight \\(w_i^{\\rm det}\\in [0, 1]\\) determines the actual value of detuning that \\(i\\)-th qubit feels and this way the detuning pattern is emulated. Similarly, for the amplitude pattern \\(\\Gamma\\) determines the maximal additional positive drive that acts on qubits. In this case the corresponding weights \\(w_i^{\\rm drive}\\) can vary in the interval \\([0, 1]\\).</p> <p>Using the detuning and amplitude patterns described above one can modify the behavior of a selected set of qubits, thus achieving semi-local addressing.</p> <p>Qadence implements semi-local addressing in two different flavors of increasing complexity: either as a circuit constructor or directly as a pattern added to the general evolution Hamiltonian described by the circuit.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#using-circuit-constructors","title":"Using circuit constructors","text":"<p>The <code>rydberg_hea</code> constructor routine allows to build a circuit instance implementing a basic version of the Hamiltonian evolution described above where both \\(\\Delta\\) and \\(\\tilde{\\Omega}\\) coefficients are considered constants. Furthemore, no global drive and detuning are explicitly added to the Hamiltonian. Therefore, the final Hamiltonian generator of the circuit reads as follows:</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm local}(w^{\\rm drive}, w^{\\rm det}) + \\mathcal{H}_{\\textrm{int}} \\] <p>This implementation does not perform any checks on the weights normalization, thus making it not realistic. This implies that global drive and detuning can be retrieved by appropriately choosing the weights.</p> <p>You can easily create a Rydberg hardware efficient ansatz implementing multiple layers of the evolution generated by the local addressing Hamiltonian:</p> \\[ \\mathcal{H}_{\\rm evo} = \\sum_j \\mathcal{H}_{\\textrm{local}}(w_{j}^{\\rm drive}, w_{j}^{\\rm det}) \\] <p>Notice that in real-device implementation, one layer only is usually achievable.</p> <pre><code>import qadence as qd\nfrom qadence import rydberg_hea, rydberg_hea_layer\n\nn_qubits = 4\nn_layers = 2\nregister = qd.Register.line(n_qubits)\n\n# ansatz constructor\n# the evolution time is parametrized for each layer of the evolution\nansatz = rydberg_hea(\n    register,\n    n_layers=n_layers,  # number of subsequent layers of Hamiltonian evolution\n    addressable_detuning=True,  # make the local detuning weights w_i^{det} as variational parameters\n    addressable_drive=True, # make the local drive weights w_i^{drv} as variational parameters\n    tunable_phase=True, # make the phase \\phi as a variational parameter\n)\n\n# alternatively, a single ansatz layer can also be created for\n# better flexibility\n\n# these can be variational parameters\ntevo_drive = 1.0  # evolution time for the locally addressed drive term\ntevo_det = 1.0 # evolution time for the locally addressed detuning term\ntevo_int = 1.0  # evolution time for the interaction term\n\n# these can be list of variational parameters\nweights_drive = [0.0, 0.25, 0.5, 0.25]\nweights_det = [0.0, 0.0, 0.5, 0.5]\n\nansatz_layer = rydberg_hea_layer(\n    register,\n    tevo_det,\n    tevo_drive,\n    tevo_int,\n    detunings=weights_det,\n    drives=weights_drive,\n)\n</code></pre> <pre><code>\n</code></pre> <p>This circuit constructor is meant to be used with fully differentiable backends such as <code>pyqtorch</code> and mainly for quick experimentation with neutral atom compatible ansatze.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#using-addressing-patterns","title":"Using addressing patterns","text":"<p>In Qadence semi-local addressing patterns can be created by either specifying fixed values for the weights of the qubits being addressed or defining them as trainable parameters that can be optimized later in some training loop. Semi-local addressing patterns can be defined with the <code>AddressingPattern</code> dataclass.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#fixed-weights","title":"Fixed weights","text":"<p>With fixed weights, detuning/amplitude addressing patterns can be defined in the following way:</p> <pre><code>import torch\nfrom qadence.analog import AddressingPattern\n\nn_qubits = 3\n\nw_det = {0: 0.9, 1: 0.5, 2: 1.0}\nw_amp = {0: 0.1, 1: 0.4, 2: 0.8}\ndet = 9.0\namp = 6.5\npattern = AddressingPattern(\n    n_qubits=n_qubits,\n    det=det,\n    amp=amp,\n    weights_det=w_det,\n    weights_amp=w_amp,\n)\n</code></pre> <p>If only detuning or amplitude pattern is needed - the corresponding weights for all qubits can be set to 0.</p> <p>The created addressing pattern can now be passed as an argument to any Qadence device class, or to the <code>IdealDevice</code> or <code>RealisticDevice</code> to make use of the pre-defined options in those devices,</p> <pre><code>import torch\nfrom qadence import (\n    AnalogRX,\n    AnalogRY,\n    BackendName,\n    DiffMode,\n    Parameter,\n    QuantumCircuit,\n    QuantumModel,\n    Register,\n    chain,\n    total_magnetization,\n    IdealDevice,\n    PI\n)\n\n# define register and circuit\nspacing = 8.0\nx = Parameter(\"x\")\nblock = chain(AnalogRX(3 * x), AnalogRY(0.5 * x))\n\ndevice_specs = IdealDevice(pattern = pattern)\n\nreg = Register.line(\n    n_qubits,\n    spacing=spacing,\n    device_specs=device_specs,\n)\n\ncirc = QuantumCircuit(reg, block)\n\nobs = total_magnetization(n_qubits)\n\nmodel_pyq = QuantumModel(\n    circuit=circ, observable=obs, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD\n)\n\n# calculate expectation value of the circuit for random input value\nvalue = {\"x\": 1.0 + torch.rand(1)}\nexpval_pyq = model_pyq.expectation(values = value)\n</code></pre>   Expectation value on PyQ:  tensor([2.1555])     <p>The same configuration can also be seamlessly used to create a model with the Pulser backend.</p> <pre><code>model_pulser = QuantumModel(\n    circuit=circ,\n    observable=obs,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR\n)\n\n# calculate expectation value of the circuit for same random input value\nexpval_pulser = model_pulser.expectation(values = value)\n</code></pre>   Expectation value on Pulser:  tensor([2.1534])     <p>Note that by default the addressing pattern terms are added to every analog operation in the circuit. However, it is possible to turn the addressing pattern off for specific operations by passing <code>add_pattern=False</code> in the operation. For example <code>AnalogRX(pi)</code> will get the extra addressing pattern term, but <code>AnalogRX(pi, add_pattern=False)</code> will not. This is currently only implemented for the PyQTorch backend. If an addressing pattern is specified for the Pulser backend, it will be added to all the blocks.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#trainable-weights","title":"Trainable weights","text":"<p>Note</p> <p>Trainable parameters currently are supported only by <code>pyqtorch</code> backend.</p> <p>Since both the maximum detuning/amplitude value of the addressing pattern and the corresponding weights can be user specified, they can be variationally used in some QML setting. This can be achieved by defining pattern weights as trainable <code>Parameter</code> instances or strings specifying weight names.</p> <pre><code>n_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\n# some random target function value\nf_value = torch.rand(1)\n\n# define trainable addressing pattern\nw_amp = {i: f\"w_amp{i}\" for i in range(n_qubits)}\nw_det = {i: f\"w_det{i}\" for i in range(n_qubits)}\namp = \"max_amp\"\ndet = \"max_det\"\n\npattern = AddressingPattern(\n    n_qubits=n_qubits,\n    det=det,\n    amp=amp,\n    weights_det=w_det,\n    weights_amp=w_amp,\n)\n\n# some fixed analog operation\nblock = AnalogRX(PI)\n\ndevice_specs = IdealDevice(pattern = pattern)\n\nreg = Register.line(\n    n_qubits,\n    spacing=spacing,\n    device_specs=device_specs,\n)\n\ncirc = QuantumCircuit(reg, block)\n\n# define quantum model\nobs = total_magnetization(n_qubits)\nmodel = QuantumModel(circuit=circ, observable=obs, backend=BackendName.PYQTORCH)\n\n# prepare for training\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_criterion = torch.nn.MSELoss()\nn_epochs = 200\nloss_save = []\n\n# train model\nfor _ in range(n_epochs):\n    optimizer.zero_grad()\n    out = model.expectation()\n    loss = loss_criterion(f_value, out)\n    loss.backward()\n    optimizer.step()\n    loss_save.append(loss.item())\n\n# get final results\nf_value_model = model.expectation().detach()\n\nassert torch.isclose(f_value, f_value_model, atol=0.01)\n</code></pre>   The target function value:  tensor([0.5492]) The trained function value:  tensor([[0.5492]])    <p>Here, the expectation value of the circuit is fitted by varying the parameters of the addressing pattern.</p>"},{"location":"tutorials/qml/","title":"Variational quantum algorithms","text":"<p>Variational algorithms on noisy devices and quantum machine learning (QML)[^1] in particular are one of the main target applications for Qadence. For this purpose, the library offers both flexible symbolic expressions for the quantum circuit parameters via <code>sympy</code> (see here for more details) and native automatic differentiation via integration with PyTorch deep learning framework.</p> <p>Furthermore, Qadence offers a wide range of utilities for helping building and researching quantum machine learning algorithms, including:</p> <ul> <li>a set of constructors for circuits commonly used in quantum machine learning such as feature maps and ansatze</li> <li>a set of tools for training and optimizing quantum neural networks and loading classical data into a QML algorithm</li> </ul>"},{"location":"tutorials/qml/#some-simple-examples","title":"Some simple examples","text":"<p>Qadence symbolic parameter interface allows to create arbitrary feature maps to encode classical data into quantum circuits with an arbitrary non-linear function embedding for the input values:</p> <pre><code>import qadence as qd\nfrom qadence.operations import *\nimport torch\nfrom sympy import acos\n\nn_qubits = 4\n\n# Example feature map, also directly available with the `feature_map` function\nfp = qd.FeatureParameter(\"phi\")\nfm = qd.kron(RX(i, acos(fp)) for i in range(n_qubits))\n\n# the key in the dictionary must correspond to\n# the name of the assigned to the feature parameter\ninputs = {\"phi\": torch.rand(3)}\nsamples = qd.sample(fm, values=inputs)\n</code></pre> <pre><code>samples = OrderedCounter({'0000': 99, '0010': 1})\n</code></pre> <p>The <code>constructors.feature_map</code> module provides convenience functions to build commonly used feature maps where the input parameter is encoded in the single-qubit gates rotation angle. This function will be further demonstrated in the QML constructors tutorial.</p> <p>Furthermore, Qadence is natively integrated with PyTorch automatic differentiation engine thus Qadence quantum models can be used seamlessly in a PyTorch workflow.</p> <p>Let's create a quantum neural network model using the feature map just defined, a digital-analog variational ansatz (also explained here) and a simple observable \\(X(0) \\otimes X(1)\\). We use the convenience <code>QNN</code> quantum model abstraction.</p> <pre><code>ansatz = qd.hea(n_qubits, strategy=\"sDAQC\")\ncircuit = qd.QuantumCircuit(n_qubits, fm, ansatz)\nobservable = qd.kron(X(0), X(1))\n\nmodel = qd.QNN(circuit, observable)\n\n# NOTE: the `QNN` is a torch.nn.Module\nassert isinstance(model, torch.nn.Module)\n</code></pre> <pre><code>True\n</code></pre> <p>Differentiation works the same way as any other PyTorch module:</p> <pre><code>values = {\"phi\": torch.rand(10, requires_grad=True)}\n\n# the forward pass of the quantum model returns the expectation\n# value of the input observable\nout = model(values)\n\n# you can compute the gradient with respect to inputs using\n# PyTorch autograd differentiation engine\ndout = torch.autograd.grad(out, values[\"phi\"], torch.ones_like(out), create_graph=True)[0]\nprint(f\"First-order derivative w.r.t. the feature parameter: \\n{dout}\")\n\n# you can also call directly a backward pass to compute derivatives with respect\n# to the variational parameters and use it for implementing variational\n# optimization\nout.sum().backward()\n</code></pre> <pre><code>Quantum model output: \ntensor([[ 0.0231],\n        [-0.0133],\n        [ 0.3699],\n        [ 0.4348],\n        [ 0.3164],\n        [ 0.2681],\n        [-0.0471],\n        [ 0.1624],\n        [ 0.2670],\n        [ 0.3393]], grad_fn=&lt;CatBackward0&gt;)\n\nFirst-order derivative w.r.t. the feature parameter: \ntensor([ 0.6922,  0.6359,  0.5828, -0.0646, -2.0352,  0.7593,  0.5677,  0.7926,\n         0.7603,  0.6604], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>To run QML on real devices, Qadence offers generalized parameter shift rules (GPSR) <sup>1</sup> for arbitrary quantum operations which can be selected when constructing the <code>QNN</code> model:</p> <pre><code>model = qd.QNN(circuit, observable, diff_mode=\"gpsr\")\nout = model(values)\n\ndout = torch.autograd.grad(out, values[\"phi\"], torch.ones_like(out), create_graph=True)[0]\nprint(f\"First-order derivative w.r.t. the feature parameter: \\n{dout}\")\n</code></pre> <pre><code>First-order derivative w.r.t. the feature parameter: \ntensor([ 0.6922,  0.6359,  0.5828, -0.0646, -2.0352,  0.7593,  0.5677,  0.7926,\n         0.7603,  0.6604], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>See here for more details on how the parameter shift rules implementation works in Qadence.</p>"},{"location":"tutorials/qml/#references","title":"References","text":"<p>[^1] Schuld, Petruccione, Machine learning on Quantum Computers, Springer Nature (2021)</p> <ol> <li> <p>Kyriienko et al., General quantum circuit differentiation rules \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/config_qnn/","title":"Configuring a QNN","text":"<p>In <code>qadence</code>, the <code>QNN</code> is a variational quantum model that can potentially take multi-dimensional input.</p> <p>The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>The circuit has two parts, the feature map and the ansatz. The feature map is responsible for encoding the input data into the quantum state, while the ansatz is responsible for the variational part of the model. In addition, a third part of the QNN is the observables, which is (a list of) operators that are measured at the end of the circuit.</p> <p>In QML Constructors we have seen how to construct the feature map and the ansatz. In this tutorial, we will see how to do the same using configs.</p> <p>One convenient way to construct these three parts of the model is to use the config classes, namely, <code>ObservableConfig</code>, <code>FeatureMapConfig</code>, <code>AnsatzConfig</code>. These classes allow you to specify the type of circuit and the parameters of the circuit in a structured way.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-observable","title":"Defining the Observable","text":"<p>The model output is the expectation value of the defined observable(s). We use the <code>ObservableConfig</code> class to specify the observable.</p> <p>We can specify any Hamiltonian that we want to measure at the end of the circuit. Let us say we want to measure the \\(Z\\) operator.</p> <pre><code>from qadence import observable_from_config, ObservableConfig, Z\n\nobservable_config = ObservableConfig(\n    detuning=Z,\n    scale=3.0,\n    shift=-1.0,\n)\n\nobservable = observable_from_config(register=4, config=observable_config)\n</code></pre> %3 cluster_541f4ee92e034c3c89082a770053cb0d 867da886b0084a798831678396a94deb 0 6f8fb82de2594399957891d96801e45f 867da886b0084a798831678396a94deb--6f8fb82de2594399957891d96801e45f 6fd60cb557a34839b0f617ec514e98ab 1 6697aa9808e540bf9f8fce7e51a008c7 6f8fb82de2594399957891d96801e45f--6697aa9808e540bf9f8fce7e51a008c7 e553aa631b574357a3bf29d7fc6b72d4 e8fc72f438da4120922d34ac858ba262 AddBlock 6fd60cb557a34839b0f617ec514e98ab--e8fc72f438da4120922d34ac858ba262 8aba3ceb2d7c45c38d67adadbced7741 2 e8fc72f438da4120922d34ac858ba262--e553aa631b574357a3bf29d7fc6b72d4 cb557fd1f6e7464bbacdf0ad576ef63f 2170748c5176451f823b94851d8bf53f 8aba3ceb2d7c45c38d67adadbced7741--2170748c5176451f823b94851d8bf53f 76081653a84a4361a25168b62956b4ed 3 2170748c5176451f823b94851d8bf53f--cb557fd1f6e7464bbacdf0ad576ef63f 67fad262bf7346a8b0d98bdc16c55918 c0404d94c0be4916bd9f22187443a932 76081653a84a4361a25168b62956b4ed--c0404d94c0be4916bd9f22187443a932 c0404d94c0be4916bd9f22187443a932--67fad262bf7346a8b0d98bdc16c55918 <p>We have specified the observable Hamiltonian to be one with \\(Z\\)-detuning. The result is linearly scaled by 3.0 and shifted by -1.0. These parameters can optionally also be FeatureParameter or VariationalParameter</p> <p>One can also specify the observable as a list of observables, in which case the QNN will output a list of values.</p> <p>For full details on the <code>ObservableConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-feature-map","title":"Defining the Feature Map","text":"<p>Let us say we want to build a 4-qubit QNN that takes two inputs, namely, the \\(x\\) and the \\(y\\) coordinates of a point in the plane. We can use the <code>FeatureMapConfig</code> class to specify the feature map.</p> <pre><code>from qadence import BasisSet, chain, create_fm_blocks, FeatureMapConfig, ReuploadScaling\n\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs = [\"x\", \"y\"],\n    basis_set=BasisSet.CHEBYSHEV,\n    reupload_scaling=ReuploadScaling.TOWER,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\n\nfm_blocks = create_fm_blocks(register=4, config=fm_config)\nfeature_map = chain(*fm_blocks)\n</code></pre> %3 cluster_489cffe289844df9838585332ff84473 Tower Chebyshev FM cluster_96ff5e30e2064d4cb7fd827f41aa9ba2 Tower Chebyshev FM b2d1a067bbec4ef988378f863593a54b 0 bac4d734102d4dbca03b00be65f9f400 RX(1.0*acos(x)) b2d1a067bbec4ef988378f863593a54b--bac4d734102d4dbca03b00be65f9f400 d1f70317dd2847c399cc5a4aec49b344 1 8347762d0242414fa57ef555feb677b7 bac4d734102d4dbca03b00be65f9f400--8347762d0242414fa57ef555feb677b7 46240487586f424c858c83f0a296bda6 77bcbd74d2e54140b903fc587bb5f083 RX(2.0*acos(x)) d1f70317dd2847c399cc5a4aec49b344--77bcbd74d2e54140b903fc587bb5f083 e751122f8e3848cd87b3bbbae8da450e 2 77bcbd74d2e54140b903fc587bb5f083--46240487586f424c858c83f0a296bda6 36c97d3478a344a091193609cb419383 7c91b12b141d48eeb0403f52067cfcc2 RX(1.0*acos(2.0*y - 1.0)) e751122f8e3848cd87b3bbbae8da450e--7c91b12b141d48eeb0403f52067cfcc2 d4f216cd77cf4eb2a4be9b31df7d6998 3 7c91b12b141d48eeb0403f52067cfcc2--36c97d3478a344a091193609cb419383 7b6ec4499bce44a997840681537ed08b 9f7937ee48e14de789d3b4c68d2f830b RX(2.0*acos(2.0*y - 1.0)) d4f216cd77cf4eb2a4be9b31df7d6998--9f7937ee48e14de789d3b4c68d2f830b 9f7937ee48e14de789d3b4c68d2f830b--7b6ec4499bce44a997840681537ed08b <p>We have specified that the feature map should take two features, and have named the <code>FeatureParameter</code> \"x\" and \"y\" respectively. Both these parameters are encoded using the Chebyshev basis set, and the reupload scaling is set to <code>ReuploadScaling.TOWER</code>. One can optionally add the basis and the reupload scaling for each parameter separately.</p> <p>The <code>feature_range</code> parameter is a dictionary that specifies the range of values that each feature comes from. This is useful for scaling the input data to the range that the encoding function can handle. In default case, this range is mapped to the target range of the Chebyshev basis set which is \\([-1, 1]\\). One can also specify the target range for each feature separately.</p> <p>For full details on the <code>FeatureMapConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-ansatz","title":"Defining the Ansatz","text":"<p>The next part of the QNN is the ansatz. We use <code>AnsatzConfig</code> class to specify the type of ansatz.</p> <p>Let us say, we want to follow this feature map with 2 layers of hardware efficient ansatz.</p> <pre><code>from qadence import AnsatzConfig, AnsatzType, create_ansatz, Strategy\n\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nansatz = create_ansatz(register=4, config=ansatz_config)\n</code></pre> %3 1dc04b6e201d4b268629888ab47b768b 0 c9f0064eeeac43669325d31acbd8a713 RX(theta\u2080) 1dc04b6e201d4b268629888ab47b768b--c9f0064eeeac43669325d31acbd8a713 2de9b4e0963a44e4b0781ab4405f1668 1 92946c66cf6e40c19295df7e21785e2e RY(theta\u2084) c9f0064eeeac43669325d31acbd8a713--92946c66cf6e40c19295df7e21785e2e 1a1b22bea5584f4b92f22812c812608c RX(theta\u2088) 92946c66cf6e40c19295df7e21785e2e--1a1b22bea5584f4b92f22812c812608c 787f157ece2b45cb8f4737fb8e9408b6 1a1b22bea5584f4b92f22812c812608c--787f157ece2b45cb8f4737fb8e9408b6 ab2c8691d7ec46989a240b33b6f5472b 787f157ece2b45cb8f4737fb8e9408b6--ab2c8691d7ec46989a240b33b6f5472b 696fa36b4c0c47c98a31a84550f2c28e RX(theta\u2081\u2082) ab2c8691d7ec46989a240b33b6f5472b--696fa36b4c0c47c98a31a84550f2c28e f2f3b752b8b0404fa6f354b7fa08c94b RY(theta\u2081\u2086) 696fa36b4c0c47c98a31a84550f2c28e--f2f3b752b8b0404fa6f354b7fa08c94b 5f24a030f64a4c24aba35ea43fc88f25 RX(theta\u2082\u2080) f2f3b752b8b0404fa6f354b7fa08c94b--5f24a030f64a4c24aba35ea43fc88f25 61c47c88e78f455c9088d82f2d3b9b23 5f24a030f64a4c24aba35ea43fc88f25--61c47c88e78f455c9088d82f2d3b9b23 e580e904157740c281014c68d10d6a74 61c47c88e78f455c9088d82f2d3b9b23--e580e904157740c281014c68d10d6a74 435a7271cedb4066a7c9880a602d51fc e580e904157740c281014c68d10d6a74--435a7271cedb4066a7c9880a602d51fc 1a074225bbb64a42bab546a53dee83a9 5b5c43fdc0ef4e97baf901b6afb027ae RX(theta\u2081) 2de9b4e0963a44e4b0781ab4405f1668--5b5c43fdc0ef4e97baf901b6afb027ae 89396481168249e79c1932fa5141a38b 2 483865d84ef94655921f103b71723ca9 RY(theta\u2085) 5b5c43fdc0ef4e97baf901b6afb027ae--483865d84ef94655921f103b71723ca9 220e3eb4c0414eea996e0c6454d5641c RX(theta\u2089) 483865d84ef94655921f103b71723ca9--220e3eb4c0414eea996e0c6454d5641c 84bdd70acb454fb9835a65c5e586d8c3 X 220e3eb4c0414eea996e0c6454d5641c--84bdd70acb454fb9835a65c5e586d8c3 84bdd70acb454fb9835a65c5e586d8c3--787f157ece2b45cb8f4737fb8e9408b6 c3a7cabaf0c143db9eb637c413433a0f 84bdd70acb454fb9835a65c5e586d8c3--c3a7cabaf0c143db9eb637c413433a0f fcde6bcbd0f347be9a6fb32f4a2cc5ee RX(theta\u2081\u2083) c3a7cabaf0c143db9eb637c413433a0f--fcde6bcbd0f347be9a6fb32f4a2cc5ee b26d10f61c5a416eb73cbd5a531c2792 RY(theta\u2081\u2087) fcde6bcbd0f347be9a6fb32f4a2cc5ee--b26d10f61c5a416eb73cbd5a531c2792 69388d79c9c14551b9e1f927e0439f49 RX(theta\u2082\u2081) b26d10f61c5a416eb73cbd5a531c2792--69388d79c9c14551b9e1f927e0439f49 a8faa9ff89fd4c769eaf1ea8fb5ba2c2 X 69388d79c9c14551b9e1f927e0439f49--a8faa9ff89fd4c769eaf1ea8fb5ba2c2 a8faa9ff89fd4c769eaf1ea8fb5ba2c2--61c47c88e78f455c9088d82f2d3b9b23 a55724b10aa24bffbd3c264cfcf46eb2 a8faa9ff89fd4c769eaf1ea8fb5ba2c2--a55724b10aa24bffbd3c264cfcf46eb2 a55724b10aa24bffbd3c264cfcf46eb2--1a074225bbb64a42bab546a53dee83a9 46b284955e004addac16736d4d556625 db418da0db2e4341803ef3981f620888 RX(theta\u2082) 89396481168249e79c1932fa5141a38b--db418da0db2e4341803ef3981f620888 15839764a6984deb826a8cef7d09bf18 3 6ce4a00680214354b1495324bf3b2d09 RY(theta\u2086) db418da0db2e4341803ef3981f620888--6ce4a00680214354b1495324bf3b2d09 9d886cddc6584364b1051fe97bf2ebec RX(theta\u2081\u2080) 6ce4a00680214354b1495324bf3b2d09--9d886cddc6584364b1051fe97bf2ebec 23bfa266cd4f4e69a1fcad963d217842 9d886cddc6584364b1051fe97bf2ebec--23bfa266cd4f4e69a1fcad963d217842 421aa17090124a59a017bbfb97a8c70b X 23bfa266cd4f4e69a1fcad963d217842--421aa17090124a59a017bbfb97a8c70b 421aa17090124a59a017bbfb97a8c70b--c3a7cabaf0c143db9eb637c413433a0f 2c73e1d8997040b58b76135ed1fcf263 RX(theta\u2081\u2084) 421aa17090124a59a017bbfb97a8c70b--2c73e1d8997040b58b76135ed1fcf263 07b5a2d210eb472d8d3e7d979b82a41c RY(theta\u2081\u2088) 2c73e1d8997040b58b76135ed1fcf263--07b5a2d210eb472d8d3e7d979b82a41c a71d9dd596b0425282ad047ea6e3d366 RX(theta\u2082\u2082) 07b5a2d210eb472d8d3e7d979b82a41c--a71d9dd596b0425282ad047ea6e3d366 fd827673b0384a06a6be1e4af5b3a752 a71d9dd596b0425282ad047ea6e3d366--fd827673b0384a06a6be1e4af5b3a752 dc2a740181d44279b9c8f6e0cb76e5aa X fd827673b0384a06a6be1e4af5b3a752--dc2a740181d44279b9c8f6e0cb76e5aa dc2a740181d44279b9c8f6e0cb76e5aa--a55724b10aa24bffbd3c264cfcf46eb2 dc2a740181d44279b9c8f6e0cb76e5aa--46b284955e004addac16736d4d556625 d505e8e5d4b046b6ae16b7c6ee4b41a7 5d59c4487f49425cb77ac593b1de48fd RX(theta\u2083) 15839764a6984deb826a8cef7d09bf18--5d59c4487f49425cb77ac593b1de48fd f98c6fb05a9f4570b6fc12bc89fa26fb RY(theta\u2087) 5d59c4487f49425cb77ac593b1de48fd--f98c6fb05a9f4570b6fc12bc89fa26fb 7a2d6d488bf543e8b853e07862c71219 RX(theta\u2081\u2081) f98c6fb05a9f4570b6fc12bc89fa26fb--7a2d6d488bf543e8b853e07862c71219 f93210ddaae5402e8aa1bc62cb0b95fb X 7a2d6d488bf543e8b853e07862c71219--f93210ddaae5402e8aa1bc62cb0b95fb f93210ddaae5402e8aa1bc62cb0b95fb--23bfa266cd4f4e69a1fcad963d217842 861f03bbd726483699b041d7316f0c81 f93210ddaae5402e8aa1bc62cb0b95fb--861f03bbd726483699b041d7316f0c81 10fa39f69efc44c8a100073c6253be0e RX(theta\u2081\u2085) 861f03bbd726483699b041d7316f0c81--10fa39f69efc44c8a100073c6253be0e bf112636fb8949f4b8de6a712752a242 RY(theta\u2081\u2089) 10fa39f69efc44c8a100073c6253be0e--bf112636fb8949f4b8de6a712752a242 d325e7c50d1541aabad9a3f369ec4445 RX(theta\u2082\u2083) bf112636fb8949f4b8de6a712752a242--d325e7c50d1541aabad9a3f369ec4445 fecd5c05ee3f4799be6a2d8ecdc1b8d2 X d325e7c50d1541aabad9a3f369ec4445--fecd5c05ee3f4799be6a2d8ecdc1b8d2 fecd5c05ee3f4799be6a2d8ecdc1b8d2--fd827673b0384a06a6be1e4af5b3a752 eb443f49af8f4123a84c945c0206e592 fecd5c05ee3f4799be6a2d8ecdc1b8d2--eb443f49af8f4123a84c945c0206e592 eb443f49af8f4123a84c945c0206e592--d505e8e5d4b046b6ae16b7c6ee4b41a7 <p>We have specified that the ansatz should have a depth of 2, and the ansatz type is \"hea\" (Hardware Efficient Ansatz). The ansatz strategy is set to \"digital\", which means digital gates are being used. One could alternatively use \"analog\" or \"rydberg\" as the ansatz strategy.</p> <p>For full details on the <code>AnsatzConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-qnn-from-the-configs","title":"Defining the QNN from the Configs","text":"<p>To build the QNN, we can now use the <code>QNN</code> class as a <code>QuantumModel</code> subtype. In addition to the feature map, ansatz and the observable configs, we can also specify options such as the <code>backend</code>, <code>diff_mode</code>, etc. For full details on the <code>QNN</code> class, see the API documentation or the documentation on the config constructor here.</p> <pre><code>from qadence import BackendName, DiffMode, QNN\n\nqnn = QNN.from_configs(\n    register=4,\n    obs_config=observable_config,\n    fm_config=fm_config,\n    ansatz_config=ansatz_config,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.AD,\n)\n</code></pre> %3 cluster_fcd6ca276af84497a59f3e88503dd899 Obs. cluster_68d2cd94bf6c4beca9ac2ee29c489eb7 cluster_a04ef34015ae467f8baa51e4a340082e Tower Chebyshev FM cluster_b479ddb07b6f44b8af7657d4518efa1b Tower Chebyshev FM cluster_15d8a8e2c1024199bb20800dc72dded9 HEA aeb0a61c07774d5babf1b6e1691e2540 0 66f7ce93d88f4dd6a4fbf9a617890829 RX(1.0*acos(x)) aeb0a61c07774d5babf1b6e1691e2540--66f7ce93d88f4dd6a4fbf9a617890829 f2f4da764dbb4394bd4bff362c8d59d6 1 1894101271b042ea9b2a58e2a1336345 RX(theta\u2080) 66f7ce93d88f4dd6a4fbf9a617890829--1894101271b042ea9b2a58e2a1336345 280341718f4947d499ad181ee58a7eb3 RY(theta\u2084) 1894101271b042ea9b2a58e2a1336345--280341718f4947d499ad181ee58a7eb3 454135867f1641b8b0efdb84b3cbc719 RX(theta\u2088) 280341718f4947d499ad181ee58a7eb3--454135867f1641b8b0efdb84b3cbc719 96a4537c80c14760b074605e329b49e3 454135867f1641b8b0efdb84b3cbc719--96a4537c80c14760b074605e329b49e3 f0f011cacbd44c2a92404b75432649fd 96a4537c80c14760b074605e329b49e3--f0f011cacbd44c2a92404b75432649fd f476ba4d1eed45b7881604cd70b0e43b RX(theta\u2081\u2082) f0f011cacbd44c2a92404b75432649fd--f476ba4d1eed45b7881604cd70b0e43b d0a1fa8905e84edcb064f39e1e438078 RY(theta\u2081\u2086) f476ba4d1eed45b7881604cd70b0e43b--d0a1fa8905e84edcb064f39e1e438078 d2c05476603f4b52a0e4772a928bcfe4 RX(theta\u2082\u2080) d0a1fa8905e84edcb064f39e1e438078--d2c05476603f4b52a0e4772a928bcfe4 2cb4242d572c43ea8031b0924ae70ec7 d2c05476603f4b52a0e4772a928bcfe4--2cb4242d572c43ea8031b0924ae70ec7 a97c4ab3eb3945c088e058912a1d0d21 2cb4242d572c43ea8031b0924ae70ec7--a97c4ab3eb3945c088e058912a1d0d21 a865561e042f437085873e25221b27df a97c4ab3eb3945c088e058912a1d0d21--a865561e042f437085873e25221b27df 7d54a5dfae0a48838dd9643a866978a7 a865561e042f437085873e25221b27df--7d54a5dfae0a48838dd9643a866978a7 590869a9698f4a7b8179539d4b53a695 e06477fc16fc4fa9ad238971018215ea RX(2.0*acos(x)) f2f4da764dbb4394bd4bff362c8d59d6--e06477fc16fc4fa9ad238971018215ea c9eb8101359d44a0990378a5a484f8ca 2 5ab91bce19164f96a8eba3bd7e23449a RX(theta\u2081) e06477fc16fc4fa9ad238971018215ea--5ab91bce19164f96a8eba3bd7e23449a 054dc43b08584bfabc477ed6ba73ff97 RY(theta\u2085) 5ab91bce19164f96a8eba3bd7e23449a--054dc43b08584bfabc477ed6ba73ff97 c84da9fdf54b4500a342e400958a9d5d RX(theta\u2089) 054dc43b08584bfabc477ed6ba73ff97--c84da9fdf54b4500a342e400958a9d5d 7519b93e112d40ab80ef50fe9d55b054 X c84da9fdf54b4500a342e400958a9d5d--7519b93e112d40ab80ef50fe9d55b054 7519b93e112d40ab80ef50fe9d55b054--96a4537c80c14760b074605e329b49e3 38f4055ae61440b68aa2e8c789db508c 7519b93e112d40ab80ef50fe9d55b054--38f4055ae61440b68aa2e8c789db508c 653a4b8770b5427fa6c5a11d0a88c080 RX(theta\u2081\u2083) 38f4055ae61440b68aa2e8c789db508c--653a4b8770b5427fa6c5a11d0a88c080 0c80a858cf804c39879cf92bf93bc249 RY(theta\u2081\u2087) 653a4b8770b5427fa6c5a11d0a88c080--0c80a858cf804c39879cf92bf93bc249 c7b8613edf4044aea94d32c4d27d07e5 RX(theta\u2082\u2081) 0c80a858cf804c39879cf92bf93bc249--c7b8613edf4044aea94d32c4d27d07e5 aa49bae4e1cf4670abdda2a02cc5ec43 X c7b8613edf4044aea94d32c4d27d07e5--aa49bae4e1cf4670abdda2a02cc5ec43 aa49bae4e1cf4670abdda2a02cc5ec43--2cb4242d572c43ea8031b0924ae70ec7 3b1bd910de384101b3caf79cead37e8b aa49bae4e1cf4670abdda2a02cc5ec43--3b1bd910de384101b3caf79cead37e8b 9cf81eb51c5740158ad55b5d1c20880d AddBlock 3b1bd910de384101b3caf79cead37e8b--9cf81eb51c5740158ad55b5d1c20880d 9cf81eb51c5740158ad55b5d1c20880d--590869a9698f4a7b8179539d4b53a695 43856d36bf36497285154431dc595abc 95253544957547d68d62bebdaeca62c8 RX(1.0*acos(2.0*y - 1.0)) c9eb8101359d44a0990378a5a484f8ca--95253544957547d68d62bebdaeca62c8 63d2c7e11b1048a0ab44b5be9337db43 3 82586aebbcae4c329a659676a8004101 RX(theta\u2082) 95253544957547d68d62bebdaeca62c8--82586aebbcae4c329a659676a8004101 cc9d783f8aa9417fad4f57cbd842e05a RY(theta\u2086) 82586aebbcae4c329a659676a8004101--cc9d783f8aa9417fad4f57cbd842e05a 0beb65ff1a984ff99bb9fbbbed589e40 RX(theta\u2081\u2080) cc9d783f8aa9417fad4f57cbd842e05a--0beb65ff1a984ff99bb9fbbbed589e40 11167cf1601a430395c4d2b6c75785b8 0beb65ff1a984ff99bb9fbbbed589e40--11167cf1601a430395c4d2b6c75785b8 cd703a0e47a04634af2f9af822a1ceed X 11167cf1601a430395c4d2b6c75785b8--cd703a0e47a04634af2f9af822a1ceed cd703a0e47a04634af2f9af822a1ceed--38f4055ae61440b68aa2e8c789db508c 21f7cce45e1b4cc5847bdca65bec8c06 RX(theta\u2081\u2084) cd703a0e47a04634af2f9af822a1ceed--21f7cce45e1b4cc5847bdca65bec8c06 44c2efb6b115436ea4f1b07af869bae9 RY(theta\u2081\u2088) 21f7cce45e1b4cc5847bdca65bec8c06--44c2efb6b115436ea4f1b07af869bae9 19124612f2b24c08a9fa167af3dc2458 RX(theta\u2082\u2082) 44c2efb6b115436ea4f1b07af869bae9--19124612f2b24c08a9fa167af3dc2458 6568cec8faf24915bdc6f4c8325561f0 19124612f2b24c08a9fa167af3dc2458--6568cec8faf24915bdc6f4c8325561f0 c0eb20908209470e929876ea997c41b0 X 6568cec8faf24915bdc6f4c8325561f0--c0eb20908209470e929876ea997c41b0 c0eb20908209470e929876ea997c41b0--3b1bd910de384101b3caf79cead37e8b dad2caebd8b34c6095ee3c181c3acc30 c0eb20908209470e929876ea997c41b0--dad2caebd8b34c6095ee3c181c3acc30 dad2caebd8b34c6095ee3c181c3acc30--43856d36bf36497285154431dc595abc 58d368f37ac84db2a28d7388f2472caa d45afa8a52cb4ad7a86c8b96b00f8955 RX(2.0*acos(2.0*y - 1.0)) 63d2c7e11b1048a0ab44b5be9337db43--d45afa8a52cb4ad7a86c8b96b00f8955 4cd354aae94043109886a10b017952e0 RX(theta\u2083) d45afa8a52cb4ad7a86c8b96b00f8955--4cd354aae94043109886a10b017952e0 76117c5113d74fd3970abeac66a1a3ce RY(theta\u2087) 4cd354aae94043109886a10b017952e0--76117c5113d74fd3970abeac66a1a3ce 1c7a19ea47154359a259b90ecf39f984 RX(theta\u2081\u2081) 76117c5113d74fd3970abeac66a1a3ce--1c7a19ea47154359a259b90ecf39f984 aaa7a47f1e674dba80ac87e35822e91b X 1c7a19ea47154359a259b90ecf39f984--aaa7a47f1e674dba80ac87e35822e91b aaa7a47f1e674dba80ac87e35822e91b--11167cf1601a430395c4d2b6c75785b8 eef550ba57214f7bb43fa6d27a30ccdd aaa7a47f1e674dba80ac87e35822e91b--eef550ba57214f7bb43fa6d27a30ccdd 546e267ab45c493e9398414fb54b3186 RX(theta\u2081\u2085) eef550ba57214f7bb43fa6d27a30ccdd--546e267ab45c493e9398414fb54b3186 fa43138d3de94d84948cc720b8c238f7 RY(theta\u2081\u2089) 546e267ab45c493e9398414fb54b3186--fa43138d3de94d84948cc720b8c238f7 b834de979a7f49eea5a449e5b235706f RX(theta\u2082\u2083) fa43138d3de94d84948cc720b8c238f7--b834de979a7f49eea5a449e5b235706f 16acfda6032544809522f00a1c50fb5e X b834de979a7f49eea5a449e5b235706f--16acfda6032544809522f00a1c50fb5e 16acfda6032544809522f00a1c50fb5e--6568cec8faf24915bdc6f4c8325561f0 0cd52addb34e4670b187fad2a68c3930 16acfda6032544809522f00a1c50fb5e--0cd52addb34e4670b187fad2a68c3930 a7dd1e3fa718442f82b8e36348d77a84 0cd52addb34e4670b187fad2a68c3930--a7dd1e3fa718442f82b8e36348d77a84 a7dd1e3fa718442f82b8e36348d77a84--58d368f37ac84db2a28d7388f2472caa"},{"location":"tutorials/qml/dqc_1d/","title":"Solving a 1D ODE","text":"<p>In this tutorial we will show how to use Qadence to solve a basic 1D Ordinary Differential Equation (ODE) with a QNN using Differentiable Quantum Circuits (DQC) <sup>1</sup>.</p> <p>Consider the following non-linear ODE and boundary condition:</p> \\[ \\frac{df}{dx}= 5\\times(4x^3+x^2-2x-\\frac12), \\qquad f(0)=0 \\] <p>It admits an exact solution:</p> \\[ f(x)=5\\times(x^4+\\frac13x^3-x^2-\\frac12x) \\] <p>Our goal will be to find this solution for \\(x\\in[-1, 1]\\).</p> <pre><code>import torch\n\ndef dfdx_equation(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Derivative as per the equation.\"\"\"\n    return 5*(4*x**3 + x**2 - 2*x - 0.5)\n</code></pre> <p>For the purpose of this tutorial, we will compute the derivative of the circuit using <code>torch.autograd</code>. The point of the DQC algorithm is to use differentiable circuits with parameter shift rules. In Qadence, PSR is implemented directly as custom overrides of the derivative function in the autograd engine, and thus we can later change the derivative method for the model itself if we wish.</p> <pre><code>def calc_deriv(outputs: torch.Tensor, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute a derivative of model that learns f(x), computes df/dx using torch.autograd.\"\"\"\n    grad = torch.autograd.grad(\n        outputs=outputs,\n        inputs=inputs,\n        grad_outputs = torch.ones_like(inputs),\n        create_graph = True,\n        retain_graph = True,\n    )[0]\n    return grad\n</code></pre>"},{"location":"tutorials/qml/dqc_1d/#defining-the-loss-function","title":"Defining the loss function","text":"<p>The essential part of solving this problem is to define the right loss function to represent our goal. In this case, we want to define a model that has the capacity to learn the target solution, and we want to minimize: - The derivative of this model in comparison with the exact derivative in the equation; - The output of the model at the boundary in comparison with the value for the boundary condition;</p> <p>We can write it like so:</p> <pre><code># Mean-squared error as the comparison criterion\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Loss function encoding the problem to solve.\"\"\"\n    # Equation loss\n    model_output = model(inputs)\n    deriv_model = calc_deriv(model_output, inputs)\n    deriv_exact = dfdx_equation(inputs)\n    ode_loss = criterion(deriv_model, deriv_exact)\n\n    # Boundary loss, f(0) = 0\n    boundary_model = model(torch.tensor([[0.0]]))\n    boundary_exact = torch.tensor([[0.0]])\n    boundary_loss = criterion(boundary_model, boundary_exact)\n\n    return ode_loss + boundary_loss\n</code></pre> <p>Different loss criterions could be considered, and we could also play with the balance between the sum of the two loss terms. For now, let's proceed with the definition above.</p> <p>Note that so far we have not used any quantum specific assumption, and we could in principle use the same loss function with a classical neural network.</p>"},{"location":"tutorials/qml/dqc_1d/#defining-a-qnn-with-qadence","title":"Defining a QNN with Qadence","text":"<p>Now, we can finally use Qadence to write a QNN. We will use a feature map to encode the input values, a trainable ansatz circuit, and an observable to measure as the output.</p> <pre><code>from qadence import feature_map, hea, chain\nfrom qadence import QNN, QuantumCircuit, Z\nfrom qadence.types import BasisSet, ReuploadScaling\n\nn_qubits = 3\ndepth = 3\n\n# Feature map\nfm = feature_map(\n    n_qubits = n_qubits,\n    param = \"x\",\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.TOWER,\n)\n\n# Ansatz\nansatz = hea(n_qubits = n_qubits, depth = depth)\n\n# Observable\nobservable = Z(0)\n\ncircuit = QuantumCircuit(n_qubits, chain(fm, ansatz))\nmodel = QNN(circuit = circuit, observable = observable, inputs = [\"x\"])\n</code></pre> <p>We used a Chebyshev feature map with a tower-like scaling of the input reupload, and a standard hardware-efficient ansatz. You can check the qml constructors tutorial to see how you can customize these components. In the observable, for now we consider the simple case of measuring the magnetization of the first qubit.</p> <pre><code>from qadence.draw import display\n\n# display(circuit)\n</code></pre> %3 cluster_c49e89c3ca1b4626b7f9ec6cb906c6c5 HEA cluster_14272b41a20b40bf9b8250434cde8078 Tower Chebyshev FM 4be1b774312549258f782278efad8499 0 f896549f34e3454981fa39d9ae2c5819 RX(1.0*acos(x)) 4be1b774312549258f782278efad8499--f896549f34e3454981fa39d9ae2c5819 7772f7820056480a8d742175e5bf4148 1 0e9985d3e7744ce58900cc41fc64a9ed RX(theta\u2080) f896549f34e3454981fa39d9ae2c5819--0e9985d3e7744ce58900cc41fc64a9ed 82636c37555b4f09a2307a5a9d0962e0 RY(theta\u2083) 0e9985d3e7744ce58900cc41fc64a9ed--82636c37555b4f09a2307a5a9d0962e0 974a4476a26b4e2c9519fc76f0223f55 RX(theta\u2086) 82636c37555b4f09a2307a5a9d0962e0--974a4476a26b4e2c9519fc76f0223f55 745661565fcc4023b0f239307b1603fb 974a4476a26b4e2c9519fc76f0223f55--745661565fcc4023b0f239307b1603fb f617470ef0c04e1db4ce8176e43c4963 745661565fcc4023b0f239307b1603fb--f617470ef0c04e1db4ce8176e43c4963 2ead0c07e5864dfeab4ce92e088e0702 RX(theta\u2089) f617470ef0c04e1db4ce8176e43c4963--2ead0c07e5864dfeab4ce92e088e0702 de9e384370c74a0d8bb75c4751eab097 RY(theta\u2081\u2082) 2ead0c07e5864dfeab4ce92e088e0702--de9e384370c74a0d8bb75c4751eab097 cd52141563094fb48324ff366c83c1f7 RX(theta\u2081\u2085) de9e384370c74a0d8bb75c4751eab097--cd52141563094fb48324ff366c83c1f7 f2cde84c7574437fa856a1c6c6bdd960 cd52141563094fb48324ff366c83c1f7--f2cde84c7574437fa856a1c6c6bdd960 2b9df055dfba44a98669fd50171f8c4f f2cde84c7574437fa856a1c6c6bdd960--2b9df055dfba44a98669fd50171f8c4f bb07db6a86ad45a5bf511a9824792867 RX(theta\u2081\u2088) 2b9df055dfba44a98669fd50171f8c4f--bb07db6a86ad45a5bf511a9824792867 3ae78aef25b84845b48fbf3da9d54e86 RY(theta\u2082\u2081) bb07db6a86ad45a5bf511a9824792867--3ae78aef25b84845b48fbf3da9d54e86 4dbbbe3efb5c4815a771596e4129faf1 RX(theta\u2082\u2084) 3ae78aef25b84845b48fbf3da9d54e86--4dbbbe3efb5c4815a771596e4129faf1 69a218ab94b34a509c0ae0831fd3a7e2 4dbbbe3efb5c4815a771596e4129faf1--69a218ab94b34a509c0ae0831fd3a7e2 0aab8b68924e4042b0a42fb015fefb03 69a218ab94b34a509c0ae0831fd3a7e2--0aab8b68924e4042b0a42fb015fefb03 14e70923425c418398213ae87516d0b1 0aab8b68924e4042b0a42fb015fefb03--14e70923425c418398213ae87516d0b1 5ad9d3dc90c546f58fbab6b90eedc260 acdc8bb0fbf242138565daea3a5b2713 RX(2.0*acos(x)) 7772f7820056480a8d742175e5bf4148--acdc8bb0fbf242138565daea3a5b2713 14ff83d4c57640989bbacd1786c07415 2 9e577100d14c441db978d62215d5425f RX(theta\u2081) acdc8bb0fbf242138565daea3a5b2713--9e577100d14c441db978d62215d5425f fba954a8629840d79a395c7a819969cb RY(theta\u2084) 9e577100d14c441db978d62215d5425f--fba954a8629840d79a395c7a819969cb 4fd76ed72b314049a759518af931c430 RX(theta\u2087) fba954a8629840d79a395c7a819969cb--4fd76ed72b314049a759518af931c430 758fcdae0e334e50825793c8100fadf3 X 4fd76ed72b314049a759518af931c430--758fcdae0e334e50825793c8100fadf3 758fcdae0e334e50825793c8100fadf3--745661565fcc4023b0f239307b1603fb 548c49d824764771a2882e0231ede823 758fcdae0e334e50825793c8100fadf3--548c49d824764771a2882e0231ede823 00e0c6799ede44aea2e1a3ba1cd99dde RX(theta\u2081\u2080) 548c49d824764771a2882e0231ede823--00e0c6799ede44aea2e1a3ba1cd99dde 0b535180ffad446183fcf4781751c766 RY(theta\u2081\u2083) 00e0c6799ede44aea2e1a3ba1cd99dde--0b535180ffad446183fcf4781751c766 171456ed69f74a5c9d37baf77c506a41 RX(theta\u2081\u2086) 0b535180ffad446183fcf4781751c766--171456ed69f74a5c9d37baf77c506a41 7791ff2818dc465a9b8fbfdfca374cbd X 171456ed69f74a5c9d37baf77c506a41--7791ff2818dc465a9b8fbfdfca374cbd 7791ff2818dc465a9b8fbfdfca374cbd--f2cde84c7574437fa856a1c6c6bdd960 771cf446a394478aac9f4b58d52157a2 7791ff2818dc465a9b8fbfdfca374cbd--771cf446a394478aac9f4b58d52157a2 2f0591e1206a45798b48dd9a1cd7c0eb RX(theta\u2081\u2089) 771cf446a394478aac9f4b58d52157a2--2f0591e1206a45798b48dd9a1cd7c0eb 9fad57ca50614e8e9aabf22da1fa03a0 RY(theta\u2082\u2082) 2f0591e1206a45798b48dd9a1cd7c0eb--9fad57ca50614e8e9aabf22da1fa03a0 c7eb7cfe42fe40489e98c6b925cda571 RX(theta\u2082\u2085) 9fad57ca50614e8e9aabf22da1fa03a0--c7eb7cfe42fe40489e98c6b925cda571 4af818a125b6463d9ce497f830e7f79e X c7eb7cfe42fe40489e98c6b925cda571--4af818a125b6463d9ce497f830e7f79e 4af818a125b6463d9ce497f830e7f79e--69a218ab94b34a509c0ae0831fd3a7e2 5048ed9ba2bb49f18c779e9b8032ea81 4af818a125b6463d9ce497f830e7f79e--5048ed9ba2bb49f18c779e9b8032ea81 5048ed9ba2bb49f18c779e9b8032ea81--5ad9d3dc90c546f58fbab6b90eedc260 7fed9bb60f7e49d091609f5cb8b53f37 72b79e6bff694c8b89c49fa45c590795 RX(3.0*acos(x)) 14ff83d4c57640989bbacd1786c07415--72b79e6bff694c8b89c49fa45c590795 77d26c35494a4c48894c5b1543ccdfba RX(theta\u2082) 72b79e6bff694c8b89c49fa45c590795--77d26c35494a4c48894c5b1543ccdfba 2794ec11991347b598aa7e1a4af664c4 RY(theta\u2085) 77d26c35494a4c48894c5b1543ccdfba--2794ec11991347b598aa7e1a4af664c4 ca89411fa0124de799a3c2076857a3a8 RX(theta\u2088) 2794ec11991347b598aa7e1a4af664c4--ca89411fa0124de799a3c2076857a3a8 d3a8085e320645c581e902898acd96ca ca89411fa0124de799a3c2076857a3a8--d3a8085e320645c581e902898acd96ca 9bfbf4e5b8174bc58082d369abe4f2fc X d3a8085e320645c581e902898acd96ca--9bfbf4e5b8174bc58082d369abe4f2fc 9bfbf4e5b8174bc58082d369abe4f2fc--548c49d824764771a2882e0231ede823 4e4aba30f12d49edab7d12380f3d1925 RX(theta\u2081\u2081) 9bfbf4e5b8174bc58082d369abe4f2fc--4e4aba30f12d49edab7d12380f3d1925 e37510e54c7643d4815f52258d4e68a8 RY(theta\u2081\u2084) 4e4aba30f12d49edab7d12380f3d1925--e37510e54c7643d4815f52258d4e68a8 ebe81a2698544680a104a13eed38a4db RX(theta\u2081\u2087) e37510e54c7643d4815f52258d4e68a8--ebe81a2698544680a104a13eed38a4db 93fbbb725dec4ed691cf0d2785a0fdf8 ebe81a2698544680a104a13eed38a4db--93fbbb725dec4ed691cf0d2785a0fdf8 5d1fe1e06ea141a788a6fe595a09297d X 93fbbb725dec4ed691cf0d2785a0fdf8--5d1fe1e06ea141a788a6fe595a09297d 5d1fe1e06ea141a788a6fe595a09297d--771cf446a394478aac9f4b58d52157a2 98a480f70bd5476589bfabbd34a38de3 RX(theta\u2082\u2080) 5d1fe1e06ea141a788a6fe595a09297d--98a480f70bd5476589bfabbd34a38de3 cd13c8d240c94d6d88af68b0e6eb296a RY(theta\u2082\u2083) 98a480f70bd5476589bfabbd34a38de3--cd13c8d240c94d6d88af68b0e6eb296a 4ccdb7dbb1194f0197ddb2a4ba5435c0 RX(theta\u2082\u2086) cd13c8d240c94d6d88af68b0e6eb296a--4ccdb7dbb1194f0197ddb2a4ba5435c0 f73afab43b1440ab9a2a0b659a5adeb7 4ccdb7dbb1194f0197ddb2a4ba5435c0--f73afab43b1440ab9a2a0b659a5adeb7 a872a8a1dcd14dd5b183ee0d3ae50b69 X f73afab43b1440ab9a2a0b659a5adeb7--a872a8a1dcd14dd5b183ee0d3ae50b69 a872a8a1dcd14dd5b183ee0d3ae50b69--5048ed9ba2bb49f18c779e9b8032ea81 a872a8a1dcd14dd5b183ee0d3ae50b69--7fed9bb60f7e49d091609f5cb8b53f37"},{"location":"tutorials/qml/dqc_1d/#training-the-model","title":"Training the model","text":"<p>Now that the model is defined we can proceed with the training. the <code>QNN</code> class can be used like any other <code>torch.nn.Module</code>. Here we write a simple training loop, but you can also look at the ml tools tutorial to use the convenience training functions that Qadence provides.</p> <p>To train the model, we will select a random set of collocation points uniformly distributed within \\(-1.0&lt; x &lt;1.0\\) and compute the loss function for those points.</p> <pre><code>n_epochs = 200\nn_points = 10\n\nxmin = -0.99\nxmax = 0.99\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Training data. We unsqueeze essentially making each batch have a single x value.\n    x_train = (xmin + (xmax-xmin)*torch.rand(n_points, requires_grad = True)).unsqueeze(1)\n\n    loss = loss_fn(inputs = x_train, model = model)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Note the values of \\(x\\) are only picked from \\(x\\in[-0.99, 0.99]\\) since we are using a Chebyshev feature map, and derivative of \\(\\text{acos}(x)\\) diverges at \\(-1\\) and \\(1\\).</p>"},{"location":"tutorials/qml/dqc_1d/#plotting-the-results","title":"Plotting the results","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef f_exact(x: torch.Tensor) -&gt; torch.Tensor:\n    return 5*(x**4 + (1/3)*x**3 - x**2 - 0.5*x)\n\nx_test = torch.arange(xmin, xmax, step = 0.01).unsqueeze(1)\n\nresult_exact = f_exact(x_test).flatten()\n\nresult_model = model(x_test).flatten().detach()\n\nplt.plot(x_test, result_exact, label = \"Exact solution\")\nplt.plot(x_test, result_model, label = \" Trained model\")\n</code></pre> 2025-01-28T16:51:32.130530 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>Clearly, the result is not optimal.</p>"},{"location":"tutorials/qml/dqc_1d/#improving-the-solution","title":"Improving the solution","text":"<p>One point to consider when defining the QNN is the possible output range, which is bounded by the spectrum of the chosen observable. For the magnetization of a single qubit, this means that the output is bounded between -1 and 1, which we can clearly see in the plot.</p> <p>One option would be to define the observable as the total magnetization over all qubits, which would allow a range of -3 to 3.</p> <pre><code>from qadence import add\n\nobservable = add(Z(i) for i in range(n_qubits))\n\nmodel = QNN(circuit = circuit, observable = observable, inputs = [\"x\"])\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Training data\n    x_train = (xmin + (xmax-xmin)*torch.rand(n_points, requires_grad = True)).unsqueeze(1)\n\n    loss = loss_fn(inputs = x_train, model = model)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>And we again plot the result:</p> <pre><code>x_test = torch.arange(xmin, xmax, step = 0.01).unsqueeze(1)\n\nresult_exact = f_exact(x_test).flatten()\n\nresult_model = model(x_test).flatten().detach()\n\nplt.plot(x_test, result_exact, label = \"Exact solution\")\nplt.plot(x_test, result_model, label = \"Trained model\")\n</code></pre> 2025-01-28T16:51:39.077533 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/dqc_1d/#references","title":"References","text":"<ol> <li> <p>Kyriienko et al., Solving nonlinear differential equations with differentiable quantum circuits. \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/qaoa/","title":"Solving MaxCut with QAOA","text":"<p>This tutorial shows how to solve the maximum cut (MaxCut) combinatorial optimization problem on a graph using the Quantum Approximate Optimization Algorithm (QAOA), first introduced by Farhi et al. in 2014 <sup>1</sup>.</p> <p>Given an arbitrary graph, the MaxCut problem consists in finding a graph cut which partitions the nodes into two disjoint sets, such that the number of edges in the cut is maximized. This is a very common combinatorial optimization problem known to be computationally hard (NP-hard).</p> <p>The graph used for this tutorial is an unweighted graph randomly generated using the <code>networkx</code> library with a certain probability \\(p\\) of having an edge between two arbitrary nodes (known as Erd\u0151s\u2013R\u00e9nyi graph).</p> <pre><code>import numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport random\n\n# ensure reproducibility\nseed = 42\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Create random graph\nn_nodes = 4\nedge_prob = 0.8\ngraph = nx.gnp_random_graph(n_nodes, edge_prob)\n\nnx.draw(graph)\n</code></pre> 2025-01-28T16:51:39.150490 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>The goal of the MaxCut algorithm is to maximize the following cost function:</p> \\[\\mathcal{C}(p) = \\sum_{\\alpha}^m \\mathcal{C}_{\\alpha}(p)\\] <p>where \\(p\\) is a given cut of the graph, \\(\\alpha\\) is an index over the edges and \\(\\mathcal{C}_{\\alpha}(p)\\) is written such that if the nodes connected by the \\(\\alpha\\) edge are in the same set, it returns \\(0\\), otherwise it returns \\(1\\). We will represent a cut \\(p\\) as a bitstring of length \\(N\\), where \\(N\\) is the number of nodes, and where the bit in position \\(i\\) shows to which partition node \\(i\\) belongs. We assign value 0 to one of the partitions defined by the cut and 1 to the other. Since this choice is arbitrary, every cut is represented by two bitstrings, e.g. \"0011\" and \"1100\" are equivalent.</p> <p>Since in this tutorial we are only dealing with small graphs, we can find the maximum cut by brute force to make sure QAOA works as intended. <pre><code># Function to calculate the cost associated with a cut\ndef calculate_cost(cut: str, graph: nx.graph) -&gt; float:\n    \"\"\"Returns the cost of a given cut (represented by a bitstring)\"\"\"\n    cost = 0\n    for edge in graph.edges():\n        (i, j) = edge\n        if cut[i] != cut[j]:\n            cost += 1\n    return cost\n\n\n# Function to get a binary representation of an int\nget_binary = lambda x, n: format(x, \"b\").zfill(n)\n\n# List of all possible cuts\nall_possible_cuts = [bin(k)[2:].rjust(n_nodes, \"0\") for k in range(2**n_nodes)]\n\n# List with the costs associated to each cut\nall_costs = [calculate_cost(cut, graph) for cut in all_possible_cuts]\n\n# Get the maximum cost\nmaxcost = max(all_costs)\n\n# Get all cuts that correspond to the maximum cost\nmaxcuts = [get_binary(i, n_nodes) for i, j in enumerate(all_costs) if j == maxcost]\nprint(f\"The maximum cut is represented by the bitstrings {maxcuts}, with a cost of {maxcost}\")\n</code></pre> <pre><code>The maximum cut is represented by the bitstrings ['0011', '0101', '0110', '1001', '1010', '1100'], with a cost of 4\n</code></pre> </p>"},{"location":"tutorials/qml/qaoa/#the-qaoa-quantum-circuit","title":"The QAOA quantum circuit","text":"<p>The Max-Cut problem can be solved by using the QAOA algorithm. QAOA belongs to the class of Variational Quantum Algorithms (VQAs), which means that its quantum circuit contains a certain number of parametrized quantum gates that need to be optimized with a classical optimizer. The QAOA circuit is composed of two operators:</p> <ul> <li>The cost operator \\(U_c\\): a circuit generated by the cost Hamiltonian which encodes the cost function described above into a quantum circuit. The solution to the optimization problem is encoded in the ground state of the cost Hamiltonian \\(H_c\\). The cost operator  is simply the evolution of the cost Hamiltonian parametrized by a variational parameter \\(\\gamma\\) so that \\(U_c = e^{i\\gamma H_c}.\\)</li> <li>The mixing operator \\(U_b\\): a simple set of single-qubit rotations with adjustable   angles which are tuned during the classical optimization loop to minimize the cost</li> </ul> <p>The cost Hamiltonian of the MaxCut problem can be written as:</p> \\[H_c = \\frac12 \\sum_{\\langle i,j\\rangle} (\\mathbb{1} - Z_iZ_j)\\] <p>where \\(\\langle i,j\\rangle\\) represents the edge between nodes \\(i\\) and \\(j\\). The solution of the MaxCut problem is encoded in the ground state of the above Hamiltonian.</p> <p>The QAOA quantum circuit consists of a number of layers, each layer containing a cost and a mixing operator. Below, the QAOA quantum circuit is defined using <code>qadence</code> operations. First, a layer of Hadamard gates is applied to all qubits to prepare the initial state \\(|+\\rangle ^{\\otimes n}\\). The cost operator of each layer can be built \"manually\", implementing the \\(e^{iZZ\\gamma}\\) terms with CNOTs and a \\(\\rm{RZ}(2\\gamma)\\) rotation, or it can also be automatically decomposed into digital single and two-qubits operations via the <code>.digital_decomposition()</code> method. The decomposition is exact since the Hamiltonian generator is diagonal.</p> <pre><code>from qadence import tag, kron, chain, RX, RZ, Z, H, CNOT, I, add\nfrom qadence import HamEvo, QuantumCircuit, Parameter\n\nn_qubits = graph.number_of_nodes()\nn_edges = graph.number_of_edges()\nn_layers = 6\n\n# Generate the cost Hamiltonian\nzz_ops = add(Z(edge[0]) @ Z(edge[1]) for edge in graph.edges)\ncost_ham = 0.5 * (n_edges * kron(I(i) for i in range(n_qubits)) - zz_ops)\n\n\n# QAOA circuit\ndef build_qaoa_circuit(n_qubits, n_layers, graph):\n    layers = []\n    # Layer of Hadamards\n    initial_layer = kron(H(i) for i in range(n_qubits))\n    layers.append(initial_layer)\n    for layer in range(n_layers):\n\n        # cost layer with digital decomposition\n        # cost_layer = HamEvo(cost_ham, f\"g{layer}\").digital_decomposition(approximation=\"basic\")\n        cost_layer = []\n        for edge in graph.edges():\n            (q0, q1) = edge\n            zz_term = chain(\n                CNOT(q0, q1),\n                RZ(q1, Parameter(f\"g{layer}\")),\n                CNOT(q0, q1),\n            )\n            cost_layer.append(zz_term)\n        cost_layer = chain(*cost_layer)\n        cost_layer = tag(cost_layer, \"cost\")\n\n        # mixing layer with single qubit rotations\n        mixing_layer = kron(RX(i, f\"b{layer}\") for i in range(n_qubits))\n        mixing_layer = tag(mixing_layer, \"mixing\")\n\n        # putting all together in a single ChainBlock\n        layers.append(chain(cost_layer, mixing_layer))\n\n    final_b = chain(*layers)\n    return QuantumCircuit(n_qubits, final_b)\n\n\ncircuit = build_qaoa_circuit(n_qubits, n_layers, graph)\n\n# Print a single layer of the circuit\n</code></pre> %3 cluster_5bc9658999f84645bce59b86c9ab6623 mixing cluster_eadf06f3b2974c45af56c8969c311fb7 cost 77a27556eee8442e889b03646011f3d5 0 f94aa900a318401db636ba66b9162e3a H 77a27556eee8442e889b03646011f3d5--f94aa900a318401db636ba66b9162e3a 7f62f2c2a2ea464a8be5060028acd86a 1 297b9f01b8bf48f585ac0bd828318166 f94aa900a318401db636ba66b9162e3a--297b9f01b8bf48f585ac0bd828318166 81b72c5596e34180992fd52679ba5f25 297b9f01b8bf48f585ac0bd828318166--81b72c5596e34180992fd52679ba5f25 1a0c5f25c7d64851b567014bc8244d79 81b72c5596e34180992fd52679ba5f25--1a0c5f25c7d64851b567014bc8244d79 d41fbab942634f24b769efb0227b1739 1a0c5f25c7d64851b567014bc8244d79--d41fbab942634f24b769efb0227b1739 ce0748cdf432468a89e5fec727853982 d41fbab942634f24b769efb0227b1739--ce0748cdf432468a89e5fec727853982 a3fcdb72fb384ee6898b68c30938e2f2 ce0748cdf432468a89e5fec727853982--a3fcdb72fb384ee6898b68c30938e2f2 712b4fcda81f470bb2cda7b47ae477a7 a3fcdb72fb384ee6898b68c30938e2f2--712b4fcda81f470bb2cda7b47ae477a7 7273c53d77094a109e695598adfd4af3 712b4fcda81f470bb2cda7b47ae477a7--7273c53d77094a109e695598adfd4af3 4b93345264b14daaa09d88654a46f7a3 7273c53d77094a109e695598adfd4af3--4b93345264b14daaa09d88654a46f7a3 dcaab6f32e74471a9faa787c5d538684 4b93345264b14daaa09d88654a46f7a3--dcaab6f32e74471a9faa787c5d538684 d0612864240b495ebdc8068d66d9eade dcaab6f32e74471a9faa787c5d538684--d0612864240b495ebdc8068d66d9eade c639a43c2d7d4a5ebdaba087bc104e48 d0612864240b495ebdc8068d66d9eade--c639a43c2d7d4a5ebdaba087bc104e48 83633dff9f5b44fa9de6d30268767422 c639a43c2d7d4a5ebdaba087bc104e48--83633dff9f5b44fa9de6d30268767422 b1f80d7500814a0890cc3d2b65c8e0f8 83633dff9f5b44fa9de6d30268767422--b1f80d7500814a0890cc3d2b65c8e0f8 4304ba7ce76a4c05a87d17a49b2ee9fb b1f80d7500814a0890cc3d2b65c8e0f8--4304ba7ce76a4c05a87d17a49b2ee9fb 8c4156aceaa9446aa1e4710f4d1a6d2a 4304ba7ce76a4c05a87d17a49b2ee9fb--8c4156aceaa9446aa1e4710f4d1a6d2a 8f831d0a5b2f4660916fd08a5b443153 8c4156aceaa9446aa1e4710f4d1a6d2a--8f831d0a5b2f4660916fd08a5b443153 488713325b1f4cc8b4e85e5ee6c392d0 8f831d0a5b2f4660916fd08a5b443153--488713325b1f4cc8b4e85e5ee6c392d0 f27625d9396a4fb996316aabac43f8c7 RX(b0) 488713325b1f4cc8b4e85e5ee6c392d0--f27625d9396a4fb996316aabac43f8c7 6b4d4d5abef041809612d1102c211d5c f27625d9396a4fb996316aabac43f8c7--6b4d4d5abef041809612d1102c211d5c b84a7d5cd835408880009e9459bb61d8 ab38a7570d8b4ceaae8c4864d5c9b309 H 7f62f2c2a2ea464a8be5060028acd86a--ab38a7570d8b4ceaae8c4864d5c9b309 ed8e8c2f3c4f4e62b6227b71a59473c8 2 0078e6e2a845451baecd6f59351705fa X ab38a7570d8b4ceaae8c4864d5c9b309--0078e6e2a845451baecd6f59351705fa 0078e6e2a845451baecd6f59351705fa--297b9f01b8bf48f585ac0bd828318166 d4f49ea0c22a47c488c5308896bd012a RZ(g0) 0078e6e2a845451baecd6f59351705fa--d4f49ea0c22a47c488c5308896bd012a 9a595e5d1f974d179988777a8e519ead X d4f49ea0c22a47c488c5308896bd012a--9a595e5d1f974d179988777a8e519ead 9a595e5d1f974d179988777a8e519ead--1a0c5f25c7d64851b567014bc8244d79 dc42410d0ad340f28c7340948a651ce3 9a595e5d1f974d179988777a8e519ead--dc42410d0ad340f28c7340948a651ce3 dc1e267d18154d34954b8c5a9332100c dc42410d0ad340f28c7340948a651ce3--dc1e267d18154d34954b8c5a9332100c 8e1ee6ade074406e9f70831545b5f829 dc1e267d18154d34954b8c5a9332100c--8e1ee6ade074406e9f70831545b5f829 ecce38284ae54acfa4265e62cbf0e560 8e1ee6ade074406e9f70831545b5f829--ecce38284ae54acfa4265e62cbf0e560 0699b0bc8e924eef8adfba42983d2811 ecce38284ae54acfa4265e62cbf0e560--0699b0bc8e924eef8adfba42983d2811 e052b08b8ae64cf99f047cade33be40e 0699b0bc8e924eef8adfba42983d2811--e052b08b8ae64cf99f047cade33be40e c32f3ec55c5a444e82259c6a94242f22 e052b08b8ae64cf99f047cade33be40e--c32f3ec55c5a444e82259c6a94242f22 093257c8dba94142b446aeeabac9b4ef c32f3ec55c5a444e82259c6a94242f22--093257c8dba94142b446aeeabac9b4ef d7c3e4bc5fb94185bd2c2d7e84538233 093257c8dba94142b446aeeabac9b4ef--d7c3e4bc5fb94185bd2c2d7e84538233 c5ee4d09218145808053e7ec597e7602 d7c3e4bc5fb94185bd2c2d7e84538233--c5ee4d09218145808053e7ec597e7602 e05958afe94b491486fc298896bdfc4f c5ee4d09218145808053e7ec597e7602--e05958afe94b491486fc298896bdfc4f a524035e65384d72a730bcd4f31b1a37 e05958afe94b491486fc298896bdfc4f--a524035e65384d72a730bcd4f31b1a37 eb4000a4b301428489992430aee0b1c9 a524035e65384d72a730bcd4f31b1a37--eb4000a4b301428489992430aee0b1c9 30b15383d6004a65982369c7ddbb215b eb4000a4b301428489992430aee0b1c9--30b15383d6004a65982369c7ddbb215b 7ab8c22cdc0b465196e4bfc42a04097b 30b15383d6004a65982369c7ddbb215b--7ab8c22cdc0b465196e4bfc42a04097b 658970ab1f6d497d9c069ece589327ec RX(b0) 7ab8c22cdc0b465196e4bfc42a04097b--658970ab1f6d497d9c069ece589327ec 658970ab1f6d497d9c069ece589327ec--b84a7d5cd835408880009e9459bb61d8 af2478cfb3dd49b996d71319e6196e86 d06433f9fa1349f7b496e90e3276b27d H ed8e8c2f3c4f4e62b6227b71a59473c8--d06433f9fa1349f7b496e90e3276b27d fdbef891dbee4f52ac1c6fd03aab5fb8 3 dfc152a8478844d6b7a180b5cfb214be d06433f9fa1349f7b496e90e3276b27d--dfc152a8478844d6b7a180b5cfb214be 3abb92d1cc174408a33676fe48edb205 dfc152a8478844d6b7a180b5cfb214be--3abb92d1cc174408a33676fe48edb205 ff6fd097678a4ba78ef47a7082d7a41e 3abb92d1cc174408a33676fe48edb205--ff6fd097678a4ba78ef47a7082d7a41e 5e69fb783f9644509e210c8aafab0277 X ff6fd097678a4ba78ef47a7082d7a41e--5e69fb783f9644509e210c8aafab0277 5e69fb783f9644509e210c8aafab0277--d41fbab942634f24b769efb0227b1739 d0ad16df86994e3396e80592265e2144 RZ(g0) 5e69fb783f9644509e210c8aafab0277--d0ad16df86994e3396e80592265e2144 d425ea8f9ec84b9b9d0c398aa707211a X d0ad16df86994e3396e80592265e2144--d425ea8f9ec84b9b9d0c398aa707211a d425ea8f9ec84b9b9d0c398aa707211a--a3fcdb72fb384ee6898b68c30938e2f2 6667fcfee2904809bfe18d2e19178e0d d425ea8f9ec84b9b9d0c398aa707211a--6667fcfee2904809bfe18d2e19178e0d 19d932915344448ba81c723c5eb04bf8 6667fcfee2904809bfe18d2e19178e0d--19d932915344448ba81c723c5eb04bf8 322f0245aeee49fb9e03598bc52d4caa 19d932915344448ba81c723c5eb04bf8--322f0245aeee49fb9e03598bc52d4caa ee2db13cd7f14d7eace6cff480482dc6 X 322f0245aeee49fb9e03598bc52d4caa--ee2db13cd7f14d7eace6cff480482dc6 ee2db13cd7f14d7eace6cff480482dc6--c32f3ec55c5a444e82259c6a94242f22 65882b337fff447680cd22f3c54390fd RZ(g0) ee2db13cd7f14d7eace6cff480482dc6--65882b337fff447680cd22f3c54390fd 72e312af8a114a88b1a64d4e461e4cf2 X 65882b337fff447680cd22f3c54390fd--72e312af8a114a88b1a64d4e461e4cf2 72e312af8a114a88b1a64d4e461e4cf2--d7c3e4bc5fb94185bd2c2d7e84538233 362e77e4d6b44b1e90f8ab94fe62ac45 72e312af8a114a88b1a64d4e461e4cf2--362e77e4d6b44b1e90f8ab94fe62ac45 66b7d08fcfc7421ca0a718dea74bfddd 362e77e4d6b44b1e90f8ab94fe62ac45--66b7d08fcfc7421ca0a718dea74bfddd f25d906cedc94fefa185fc3fbe0fee7c 66b7d08fcfc7421ca0a718dea74bfddd--f25d906cedc94fefa185fc3fbe0fee7c 162850045b02446ab4cdb9c24e7a3272 f25d906cedc94fefa185fc3fbe0fee7c--162850045b02446ab4cdb9c24e7a3272 381bd9baae204701a974421d00ea6df8 162850045b02446ab4cdb9c24e7a3272--381bd9baae204701a974421d00ea6df8 b3bb6a554da14807ae31e9d44b1d53fb 381bd9baae204701a974421d00ea6df8--b3bb6a554da14807ae31e9d44b1d53fb d7e9c4502fe7448ab89fd8e6a5405f4f RX(b0) b3bb6a554da14807ae31e9d44b1d53fb--d7e9c4502fe7448ab89fd8e6a5405f4f d7e9c4502fe7448ab89fd8e6a5405f4f--af2478cfb3dd49b996d71319e6196e86 b008e9085b314a70bfc1d330e9a63002 91b241221efb4afd8267fa296ab6d063 H fdbef891dbee4f52ac1c6fd03aab5fb8--91b241221efb4afd8267fa296ab6d063 e9f8758a3bd44aa9acebeda68488c9bd 91b241221efb4afd8267fa296ab6d063--e9f8758a3bd44aa9acebeda68488c9bd a5a6814c7cb74af1bf9ee5d0d764899c e9f8758a3bd44aa9acebeda68488c9bd--a5a6814c7cb74af1bf9ee5d0d764899c 91f3ef445fe14d7fb513e66cfd83d558 a5a6814c7cb74af1bf9ee5d0d764899c--91f3ef445fe14d7fb513e66cfd83d558 3f5356d4fdae485293e44358fe70947f 91f3ef445fe14d7fb513e66cfd83d558--3f5356d4fdae485293e44358fe70947f 2fa880f38eb64eca88e79264bf985d94 3f5356d4fdae485293e44358fe70947f--2fa880f38eb64eca88e79264bf985d94 a620e42cbc7641d4bbc94045cdfdb79d 2fa880f38eb64eca88e79264bf985d94--a620e42cbc7641d4bbc94045cdfdb79d a42329fc3a4f415382cc6d4bf8120f57 X a620e42cbc7641d4bbc94045cdfdb79d--a42329fc3a4f415382cc6d4bf8120f57 a42329fc3a4f415382cc6d4bf8120f57--712b4fcda81f470bb2cda7b47ae477a7 02fc3d1beab441e4b7da43f861207e51 RZ(g0) a42329fc3a4f415382cc6d4bf8120f57--02fc3d1beab441e4b7da43f861207e51 021482dd6fa04f61b13b965beb30a10d X 02fc3d1beab441e4b7da43f861207e51--021482dd6fa04f61b13b965beb30a10d 021482dd6fa04f61b13b965beb30a10d--4b93345264b14daaa09d88654a46f7a3 1fab46fbcea44480a9e40c92b12d6fdd 021482dd6fa04f61b13b965beb30a10d--1fab46fbcea44480a9e40c92b12d6fdd 3cd04dc9a7db4d8abf4b0328e5a3f2f8 1fab46fbcea44480a9e40c92b12d6fdd--3cd04dc9a7db4d8abf4b0328e5a3f2f8 b3bc634139ab4d01b73a4cb6900ad821 3cd04dc9a7db4d8abf4b0328e5a3f2f8--b3bc634139ab4d01b73a4cb6900ad821 8697a60f84bf44c3a55acb323276c196 X b3bc634139ab4d01b73a4cb6900ad821--8697a60f84bf44c3a55acb323276c196 8697a60f84bf44c3a55acb323276c196--c5ee4d09218145808053e7ec597e7602 05cd336cf14d4c77b6cbbe0f7d905d11 RZ(g0) 8697a60f84bf44c3a55acb323276c196--05cd336cf14d4c77b6cbbe0f7d905d11 93ece927e66549d19654b3fa2a9841f3 X 05cd336cf14d4c77b6cbbe0f7d905d11--93ece927e66549d19654b3fa2a9841f3 93ece927e66549d19654b3fa2a9841f3--a524035e65384d72a730bcd4f31b1a37 a6a2a1eca95a4090b6ad9b1e7f45b7a2 X 93ece927e66549d19654b3fa2a9841f3--a6a2a1eca95a4090b6ad9b1e7f45b7a2 a6a2a1eca95a4090b6ad9b1e7f45b7a2--162850045b02446ab4cdb9c24e7a3272 a80f6d0e841a4c59869c4aedfbabc27f RZ(g0) a6a2a1eca95a4090b6ad9b1e7f45b7a2--a80f6d0e841a4c59869c4aedfbabc27f 3f5196110a7846e3959265127890bcca X a80f6d0e841a4c59869c4aedfbabc27f--3f5196110a7846e3959265127890bcca 3f5196110a7846e3959265127890bcca--b3bb6a554da14807ae31e9d44b1d53fb 52abfab8b44b4303a6eff55e3bae275e RX(b0) 3f5196110a7846e3959265127890bcca--52abfab8b44b4303a6eff55e3bae275e 52abfab8b44b4303a6eff55e3bae275e--b008e9085b314a70bfc1d330e9a63002"},{"location":"tutorials/qml/qaoa/#train-the-qaoa-circuit-to-solve-maxcut","title":"Train the QAOA circuit to solve MaxCut","text":"<p>Given the QAOA circuit above, one can construct the associated Qadence <code>QuantumModel</code> and train it using standard gradient based optimization.</p> <p>The loss function to be minimized reads:</p> \\[\\mathcal{L} =-\\langle \\psi | H_c| \\psi \\rangle= -\\frac12 \\sum_{\\langle i,j\\rangle}  \\left(1 - \\langle \\psi | Z_i Z_j | \\psi \\rangle \\right)\\] <p>where \\(|\\psi\\rangle(\\beta, \\gamma)\\) is the wavefunction obtained by running the QAQA quantum circuit and the sum runs over the edges of the graph \\(\\langle i,j\\rangle\\).</p> <pre><code>import torch\nfrom qadence import QuantumModel\n\ntorch.manual_seed(seed)\n\n\ndef loss_function(model: QuantumModel):\n    # The loss corresponds to the expectation\n    # value of the cost Hamiltonian\n    return -1.0 * model.expectation().squeeze()\n\n\n# initialize the parameters to random values\nmodel = QuantumModel(circuit, observable=cost_ham)\nmodel.reset_vparams(torch.rand(model.num_vparams))\ninitial_loss = loss_function(model)\nprint(f\"Initial loss: {initial_loss}\")\n\n# train the model\nn_epochs = 100\nlr = 0.1\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_function(model)\n    loss.backward()\n    optimizer.step()\n    if (i + 1) % (n_epochs // 10) == 0:\n        print(f\"MaxCut cost at iteration {i+1}: {-loss.item()}\")\n</code></pre> <pre><code>Initial loss: -2.1782381363858794\nMaxCut cost at iteration 10: 3.7470706807026417\nMaxCut cost at iteration 20: 3.8378810288930216\nMaxCut cost at iteration 30: 3.9424197899236133\nMaxCut cost at iteration 40: 3.9981256255766002\nMaxCut cost at iteration 50: 3.996470528508214\nMaxCut cost at iteration 60: 3.9991374608876606\nMaxCut cost at iteration 70: 3.9994678542919555\nMaxCut cost at iteration 80: 3.999872558672829\nMaxCut cost at iteration 90: 3.9999475834121063\nMaxCut cost at iteration 100: 3.9999793311641003\n</code></pre> <p>Qadence offers some convenience functions to implement this training loop with advanced logging and metrics track features. You can refer to this tutorial for more details.</p>"},{"location":"tutorials/qml/qaoa/#results","title":"Results","text":"<p>Given the trained quantum model, one needs to sample the resulting quantum state to recover the bitstring with the highest probability which corresponds to the maximum cut of the graph.</p> <pre><code>samples = model.sample(n_shots=100)[0]\nmost_frequent = max(samples, key=samples.get)\n\nprint(f\"Most frequently sampled bitstring corresponding to the maximum cut: {most_frequent}\")\n\n# let's now draw the cut obtained with the QAOA procedure\ncolors = []\nlabels = {}\nfor node, b in zip(graph.nodes(), most_frequent):\n    colors.append(\"green\") if int(b) == 0 else colors.append(\"red\")\n    labels[node] = \"A\" if int(b) == 0 else \"B\"\n\nnx.draw_networkx(graph, node_color=colors, with_labels=True, labels=labels)\n</code></pre>   Most frequently sampled bitstring corresponding to the maximum cut: 1001  2025-01-28T16:51:43.176558 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/qaoa/#references","title":"References","text":"<ol> <li> <p>Farhi et al. - A Quantum Approximate Optimization Algorithm\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/qml/qcl/","title":"Quantum circuit learning","text":"<p>This tutorial shows how to apply <code>qadence</code> for solving a basic quantum machine learning application: fitting a simple function with the quantum circuit learning<sup>1</sup> (QCL) algorithm.</p> <p>QCL is a supervised quantum machine learning algorithm that uses a parametrized quantum neural network to learn the behavior of an arbitrary mathematical function using a set of function values as training data. This tutorial shows how to fit the \\(\\sin(x)\\) function in the \\([-1, 1]\\) domain.</p> <p>In the following, train and test data are defined.</p> <pre><code>import torch\nfrom torch.utils.data import random_split\n\n# make sure all tensors are kept on the same device\n# only available from PyTorch 2.0\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.set_default_device(device)\n\ndef qcl_training_data(\n    domain: tuple = (0, 2*torch.pi), n_points: int = 200\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n\n    start, end = domain\n\n    x_rand, _ = torch.sort(torch.DoubleTensor(n_points).uniform_(start, end))\n    y_rand = torch.sin(x_rand)\n\n    return x_rand, y_rand\n\nx, y = qcl_training_data()\n\n# random train/test split of the dataset\ntrain_subset, test_subset = random_split(x, [0.75, 0.25])\ntrain_ind = sorted(train_subset.indices)\ntest_ind = sorted(test_subset.indices)\n\nx_train, y_train = x[train_ind], y[train_ind]\nx_test, y_test = x[test_ind], y[test_ind]\n</code></pre>"},{"location":"tutorials/qml/qcl/#train-the-qcl-model","title":"Train the QCL model","text":"<p>Qadence provides the <code>QNN</code> convenience constructor to build a quantum neural network. The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>Total qubit magnetization is used as observable:</p> \\[ \\hat{O} = \\sum_i^N \\hat{\\sigma}_i^z \\] <p>In the following the observable, quantum circuit and corresponding QNN model are constructed.</p> <pre><code>import qadence as qd\n\nn_qubits = 4\n\n# create a simple feature map to encode the input data\nfeature_param = qd.FeatureParameter(\"phi\")\nfeature_map = qd.kron(qd.RX(i, feature_param) for i in range(n_qubits))\nfeature_map = qd.tag(feature_map, \"feature_map\")\n\n# create a digital-analog variational ansatz using Qadence convenience constructors\nansatz = qd.hea(n_qubits, depth=n_qubits)\nansatz = qd.tag(ansatz, \"ansatz\")\n\n# total qubit magnetization observable\nobservable = qd.hamiltonian_factory(n_qubits, detuning=qd.Z)\n\ncircuit = qd.QuantumCircuit(n_qubits, feature_map, ansatz)\nmodel = qd.QNN(circuit, [observable])\nexpval = model(values=torch.rand(10))\n</code></pre> <pre><code>tensor([[ 0.0092],\n        [-0.0207],\n        [-0.4616],\n        [-0.1845],\n        [-0.3761],\n        [-0.1463],\n        [-0.4563],\n        [-0.0016],\n        [-0.3436],\n        [-0.4252]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> <p>The QCL algorithm uses the output of the quantum neural network as a tunable universal function approximator. Standard PyTorch code is used for training the QNN using a mean-square error loss, Adam optimizer. Training is performend on the GPU if available:</p> <pre><code>n_epochs = 100\nlr = 0.25\n\ninput_values = {\"phi\": x_train}\nmse_loss = torch.nn.MSELoss()  # standard PyTorch loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)  # standard PyTorch Adam optimizer\n\nprint(f\"Initial loss: {mse_loss(model(values=x_train), y_train)}\")\ny_pred_initial = model(values=x_test)\n\nfor i in range(n_epochs):\n\n    optimizer.zero_grad()\n\n    # given a `n_batch` number of input points and a `n_observables`\n    # number of input observables to measure, the QNN returns\n    # an output of the following shape: [n_batch x n_observables]\n    # given that there is only one observable, a squeeze is applied to get\n    # a 1-dimensional tensor\n    loss = mse_loss(model(values=x_train).squeeze(), y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (i+1) % 20 == 0:\n        print(f\"Epoch {i+1} - Loss: {loss.item()}\")\n\nassert loss.item() &lt; 1e-3\n</code></pre> <pre><code>Initial loss: 0.6272721767455237\nEpoch 20 - Loss: 0.008173087377230498\nEpoch 40 - Loss: 0.0011247726222838813\nEpoch 60 - Loss: 0.0001415308609619855\nEpoch 80 - Loss: 2.3606578815826947e-05\nEpoch 100 - Loss: 2.503287372853267e-06\n</code></pre> <p>Qadence offers some convenience functions to implement this training loop with advanced logging and metrics track features. You can refer to this tutorial for more details.</p> <p>The quantum model is now trained on the training data points. To determine the quality of the results, one can check to see how well it fits the function on the test set.</p> <pre><code>import matplotlib.pyplot as plt\n\ny_pred = model({\"phi\": x_test})\n\n# convert all the results to numpy arrays for plotting\nx_train_np = x_train.cpu().detach().numpy().flatten()\ny_train_np = y_train.cpu().detach().numpy().flatten()\nx_test_np = x_test.cpu().detach().numpy().flatten()\ny_test_np = y_test.cpu().detach().numpy().flatten()\ny_pred_initial_np = y_pred_initial.cpu().detach().numpy().flatten()\ny_pred_np = y_pred.cpu().detach().numpy().flatten()\n\nfig, _ = plt.subplots()\nplt.scatter(x_test_np, y_test_np, label=\"Test points\", marker=\"o\", color=\"orange\")\nplt.plot(x_test_np, y_pred_initial_np, label=\"Initial prediction\", color=\"green\", alpha=0.5)\nplt.plot(x_test_np, y_pred_np, label=\"Final prediction\")\nplt.legend()\n</code></pre> 2025-01-28T16:51:47.269126 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/qcl/#references","title":"References","text":"<ol> <li> <p>Mitarai et al., Quantum Circuit Learning \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/ml_tools/callbacks/","title":"Callbacks for Trainer","text":"<p>Qadence <code>ml_tools</code> provides a powerful callback system for customizing various stages of the training process. With callbacks, you can monitor, log, save, and alter your training workflow efficiently. A <code>CallbackManager</code> is used with <code>Trainer</code> to execute the training process with defined callbacks. Following default callbacks are already provided in the <code>Trainer</code>.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#default-callbacks","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks already implemented in the <code>CallbackManager</code> used with <code>Trainer</code>:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>This guide covers how to define and use callbacks in <code>TrainConfig</code>, integrate them with the <code>Trainer</code> class, and create custom callbacks using hooks.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#1-built-in-callbacks","title":"1. Built-in Callbacks","text":"<p>Qadence ml_tools offers several built-in callbacks for common tasks like saving checkpoints, logging metrics, and tracking models. Below is an overview of each.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#11-printmetrics","title":"1.1. <code>PrintMetrics</code>","text":"<p>Prints metrics at specified intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\nprint_metrics_callback = PrintMetrics(on=\"val_batch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[print_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#12-writemetrics","title":"1.2. <code>WriteMetrics</code>","text":"<p>Writes metrics to a specified logging destination.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import WriteMetrics\n\nwrite_metrics_callback = WriteMetrics(on=\"train_epoch_end\", called_every=50)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[write_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#13-plotmetrics","title":"1.3. <code>PlotMetrics</code>","text":"<p>Plots metrics based on user-defined plotting functions.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PlotMetrics\n\nplot_metrics_callback = PlotMetrics(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[plot_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#14-loghyperparameters","title":"1.4. <code>LogHyperparameters</code>","text":"<p>Logs hyperparameters to keep track of training settings.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogHyperparameters\n\nlog_hyper_callback = LogHyperparameters(on=\"train_start\", called_every=1)\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_hyper_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#15-savecheckpoint","title":"1.5. <code>SaveCheckpoint</code>","text":"<p>Saves model checkpoints at specified intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint\n\nsave_checkpoint_callback = SaveCheckpoint(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#16-savebestcheckpoint","title":"1.6. <code>SaveBestCheckpoint</code>","text":"<p>Saves the best model checkpoint based on a validation criterion.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveBestCheckpoint\n\nsave_best_checkpoint_callback = SaveBestCheckpoint(on=\"val_epoch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_best_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#17-loadcheckpoint","title":"1.7. <code>LoadCheckpoint</code>","text":"<p>Loads a saved model checkpoint at the start of training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LoadCheckpoint\n\nload_checkpoint_callback = LoadCheckpoint(on=\"train_start\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[load_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#18-logmodeltracker","title":"1.8. <code>LogModelTracker</code>","text":"<p>Logs the model structure and parameters.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogModelTracker\n\nlog_model_callback = LogModelTracker(on=\"train_end\")\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_model_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#19-lrschedulerstepdecay","title":"1.9. <code>LRSchedulerStepDecay</code>","text":"<p>Reduces the learning rate by a factor at regular intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerStepDecay\n\nlr_step_decay = LRSchedulerStepDecay(on=\"train_epoch_end\", called_every=100, gamma=0.5)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_step_decay]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#110-lrschedulercyclic","title":"1.10. <code>LRSchedulerCyclic</code>","text":"<p>Applies a cyclic learning rate schedule during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCyclic\n\nlr_cyclic = LRSchedulerCyclic(on=\"train_batch_end\", called_every=1, base_lr=0.001, max_lr=0.01, step_size=2000)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_cyclic]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#111-lrschedulercosineannealing","title":"1.11. <code>LRSchedulerCosineAnnealing</code>","text":"<p>Applies cosine annealing to the learning rate during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCosineAnnealing\n\nlr_cosine = LRSchedulerCosineAnnealing(on=\"train_batch_end\", called_every=1, t_max=5000, min_lr=1e-6)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_cosine]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#112-earlystopping","title":"1.12. <code>EarlyStopping</code>","text":"<p>Stops training when a monitored metric has not improved for a specified number of epochs.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(on=\"val_epoch_end\", called_every=1, monitor=\"val_loss\", patience=5, mode=\"min\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[early_stopping]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#113-gradientmonitoring","title":"1.13. <code>GradientMonitoring</code>","text":"<p>Logs gradient statistics (e.g., mean, standard deviation, max) during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import GradientMonitoring\n\ngradient_monitoring = GradientMonitoring(on=\"train_batch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[gradient_monitoring]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#2-custom-callbacks","title":"2. Custom Callbacks","text":"<p>The base <code>Callback</code> class in Qadence allows defining custom behavior that can be triggered at specified events (e.g., start of training, end of epoch). You can set parameters such as when the callback runs (<code>on</code>), frequency of execution (<code>called_every</code>), and optionally define a <code>callback_condition</code>.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#defining-callbacks","title":"Defining Callbacks","text":"<p>There are two main ways to define a callback: 1. Directly providing a function in the <code>Callback</code> instance. 2. Subclassing the <code>Callback</code> class and implementing custom logic.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-1-providing-a-callback-function-directly","title":"Example 1: Providing a Callback Function Directly","text":"<pre><code>from qadence.ml_tools.callbacks import Callback\n\n# Define a custom callback function\ndef custom_callback_function(trainer, config, writer):\n    print(\"Executing custom callback.\")\n\n# Create the callback instance\ncustom_callback = Callback(\n    on=\"train_end\",\n    callback=custom_callback_function\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-2-subclassing-the-callback","title":"Example 2: Subclassing the Callback","text":"<pre><code>from qadence.ml_tools.callbacks import Callback\n\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in run_callback method.\")\n\n# Create the subclassed callback instance\ncustom_callback = CustomCallback(on=\"train_batch_end\", called_every=10)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#3-adding-callbacks-to-trainconfig","title":"3. Adding Callbacks to <code>TrainConfig</code>","text":"<p>To use callbacks in <code>TrainConfig</code>, add them to the <code>callbacks</code> list when configuring the training process.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint, PrintMetrics\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[\n        SaveCheckpoint(on=\"val_epoch_end\", called_every=50),\n        PrintMetrics(on=\"train_epoch_end\", called_every=100),\n    ]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#4-using-callbacks-with-trainer","title":"4. Using Callbacks with <code>Trainer</code>","text":"<p>The <code>Trainer</code> class in <code>qadence.ml_tools</code> provides built-in support for executing callbacks at various stages in the training process, managed through a callback manager. By default, several callbacks are added to specific hooks to automate common tasks, such as check-pointing, metric logging, and model tracking.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#default-callbacks_1","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks and their assigned hooks:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>These defaults handle common needs, but you can also add custom callbacks to any hook.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-adding-a-custom-callback","title":"Example: Adding a Custom Callback","text":"<p>To create a custom <code>Trainer</code> that includes a <code>PrintMetrics</code> callback executed specifically at the end of each epoch, follow the steps below.</p> <pre><code>from qadence.ml_tools.trainer import Trainer\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.print_metrics_callback = PrintMetrics(on=\"train_epoch_end\", called_every = 10)\n\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        self.print_metrics_callback.run_callback(self)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/data_and_config/","title":"Data and Configurations","text":""},{"location":"tutorials/qml/ml_tools/data_and_config/#1-dataloaders","title":"1. Dataloaders","text":"<p>When using Qadence, you can supply classical data to a quantum machine learning algorithm by using a standard PyTorch <code>DataLoader</code> instance. Qadence also provides the <code>DictDataLoader</code> convenience class which allows to build dictionaries of <code>DataLoader</code>s instances and easily iterate over them.</p> <pre><code>import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom qadence.ml_tools import DictDataLoader, to_dataloader\n\n\ndef dataloader(data_size: int = 25, batch_size: int = 5, infinite: bool = False) -&gt; DataLoader:\n    x = torch.linspace(0, 1, data_size).reshape(-1, 1)\n    y = torch.sin(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=infinite)\n\n\ndef dictdataloader(data_size: int = 25, batch_size: int = 5) -&gt; DictDataLoader:\n    dls = {}\n    for k in [\"y1\", \"y2\"]:\n        x = torch.rand(data_size, 1)\n        y = torch.sin(x)\n        dls[k] = to_dataloader(x, y, batch_size=batch_size, infinite=True)\n    return DictDataLoader(dls)\n\n\n# iterate over standard DataLoader\nfor (x,y) in dataloader(data_size=6, batch_size=2):\n    print(f\"Standard {x = }\")\n\n# construct an infinite dataset which will keep sampling indefinitely\nn_epochs = 5\ndl = iter(dataloader(data_size=6, batch_size=2, infinite=True))\nfor _ in range(n_epochs):\n    (x, y) = next(dl)\n    print(f\"Infinite {x = }\")\n\n# iterate over DictDataLoader\nddl = dictdataloader()\ndata = next(iter(ddl))\nprint(f\"{data = }\")\n</code></pre> <pre><code>Standard x = tensor([[0.0000],\n        [0.2000]])\nStandard x = tensor([[0.4000],\n        [0.6000]])\nStandard x = tensor([[0.8000],\n        [1.0000]])\nInfinite x = tensor([[0.2000],\n        [0.4000]])\nInfinite x = tensor([[1.0000],\n        [0.6000]])\nInfinite x = tensor([[0.0000],\n        [0.8000]])\nInfinite x = tensor([[0.2000],\n        [0.4000]])\nInfinite x = tensor([[1.0000],\n        [0.6000]])\ndata = {'y1': [tensor([[0.4409],\n        [0.0250],\n        [0.3490],\n        [0.3025],\n        [0.4626]]), tensor([[0.4267],\n        [0.0250],\n        [0.3420],\n        [0.2979],\n        [0.4463]])], 'y2': [tensor([[0.4027],\n        [0.4736],\n        [0.4973],\n        [0.2373],\n        [0.9213]]), tensor([[0.3919],\n        [0.4561],\n        [0.4770],\n        [0.2350],\n        [0.7964]])]}\n</code></pre> <p>Note:     In case of <code>infinite</code>=True, the dataloader iterator will provide a random sample from the dataset.</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#2-training-configuration","title":"2. Training Configuration","text":"<p>The <code>TrainConfig</code> class provides a comprehensive configuration setup for training quantam machine learning models in Qadence. This configuration includes settings for batch size, logging, check-pointing, validation, and additional custom callbacks that control the training process's granularity and flexibility.</p> <p>The <code>TrainConfig</code> tells <code>Trainer</code>  what batch_size should be used, how many epochs to train, in which intervals to print/log metrics and how often to store intermediate checkpoints. It is also possible to provide custom callback functions by instantiating a <code>Callback</code> with a function <code>callback</code>.</p> <p>For example of how to use the TrainConfig with <code>Trainer</code>, please see Examples in Trainer</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#21-explanation-of-trainconfig-attributes","title":"2.1 Explanation of <code>TrainConfig</code> Attributes","text":"Attribute Type Default Description <code>max_iter</code> <code>int</code> <code>10000</code> Total number of training epochs. <code>batch_size</code> <code>int</code> <code>1</code> Batch size for training. <code>print_every</code> <code>int</code> <code>0</code> Frequency of console output. Set to <code>0</code> to disable. <code>write_every</code> <code>int</code> <code>0</code> Frequency of logging metrics. Set to <code>0</code> to disable. <code>plot_every</code> <code>int</code> <code>0</code> Frequency of plotting metrics. Set to <code>0</code> to disable. <code>checkpoint_every</code> <code>int</code> <code>0</code> Frequency of saving checkpoints. Set to <code>0</code> to disable. <code>val_every</code> <code>int</code> <code>0</code> Frequency of validation checks. Set to <code>0</code> to disable. <code>val_epsilon</code> <code>float</code> <code>1e-5</code> Threshold for validation improvement. <code>validation_criterion</code> <code>Callable</code> <code>None</code> Function for validating metric improvement. <code>trainstop_criterion</code> <code>Callable</code> <code>None</code> Function to stop training early. <code>callbacks</code> <code>list[Callback]</code> <code>[]</code> List of custom callbacks. <code>root_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Root directory for saving logs and checkpoints. <code>log_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Logging directory for saving logs and checkpoints. <code>log_model</code> <code>bool</code> <code>False</code> Enables model logging. <code>verbose</code> <code>bool</code> <code>True</code> Enables detailed logging. <code>tracking_tool</code> <code>ExperimentTrackingTool</code> <code>TENSORBOARD</code> Tool for tracking training metrics. <code>plotting_functions</code> <code>tuple</code> <code>()</code> Functions for plotting metrics. <pre><code>from qadence.ml_tools import OptimizeResult, TrainConfig\nfrom qadence.ml_tools.callbacks import Callback\n\nbatch_size = 5\nn_epochs = 100\n\nprint_parameters = lambda opt_res: print(opt_res.model.parameters())\ncondition_print = lambda opt_res: opt_res.loss &lt; 1.0e-03\nmodify_extra_opt_res = {\"n_epochs\": n_epochs}\ncustom_callback = Callback(on=\"train_end\", callback = print_parameters, callback_condition=condition_print, modify_optimize_result=modify_extra_opt_res, called_every=10,)\n\nconfig = TrainConfig(\n    root_folder=\"some_path/\",\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n    callbacks = [custom_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/data_and_config/#22-key-configuration-options-in-trainconfig","title":"2.2 Key Configuration Options in <code>TrainConfig</code>","text":""},{"location":"tutorials/qml/ml_tools/data_and_config/#iterations-and-batch-size","title":"Iterations and Batch Size","text":"<ul> <li><code>max_iter</code> (int): Specifies the total number of training iterations (epochs). For an <code>InfiniteTensorDataset</code>, each epoch contains one batch; for a <code>TensorDataset</code>, it contains <code>len(dataloader)</code> batches.</li> <li><code>batch_size</code> (int): Defines the number of samples processed in each training iteration.</li> </ul> <p>Example: <pre><code>config = TrainConfig(max_iter=2000, batch_size=32)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#training-parameters","title":"Training Parameters","text":"<ul> <li><code>print_every</code> (int): Controls how often loss and metrics are printed to the console.</li> <li><code>write_every</code> (int): Determines how frequently metrics are written to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>checkpoint_every</code> (int): Sets the frequency for saving model checkpoints.</li> </ul> <p>Note: Set 0 to diable.</p> <p>Example: <pre><code>config = TrainConfig(print_every=100, write_every=50, checkpoint_every=50)\n</code></pre></p> <p>The user can provide either the <code>root_folder</code> or the <code>log_folder</code> for saving checkpoints and logging. When neither are provided, the default <code>root_folder</code> \"./qml_logs\" is used.</p> <ul> <li><code>root_folder</code> (Path): The root directory for saving checkpoints and logs. All training logs will be saved inside a subfolder in this root directory. (The path to these subfolders can be accessed using config._subfolders, and the current logging folder is config.log_folder)</li> <li><code>create_subfolder_per_run</code> (bool): Creates a unique subfolder for each training run within the specified folder.</li> <li><code>tracking_tool</code> (ExperimentTrackingTool): Specifies the tracking tool to log metrics, e.g., TensorBoard or MLflow.</li> <li><code>log_model</code> (bool): Enables logging of a serialized version of the model, which is useful for model versioning. Thi happens at the end of training.</li> </ul> <p>Note     - The user can also provide <code>log_folder</code> argument - which will only be used when <code>create_subfolder_per_run</code> = False.     -  <code>log_folder</code> (Path): The log folder used for saving checkpoints and logs.</p> <p>Example: <pre><code>config = TrainConfig(root_folder=\"path/to/checkpoints\", tracking_tool=ExperimentTrackingTool.MLFLOW, checkpoint_best_only=True)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#validation-parameters","title":"Validation Parameters","text":"<ul> <li><code>checkpoint_best_only</code> (bool): If set to <code>True</code>, saves checkpoints only when there is an improvement in the validation metric.</li> <li><code>val_every</code> (int): Frequency of validation checks. Setting this to <code>0</code> disables validation.</li> <li><code>val_epsilon</code> (float): A small threshold used to compare the current validation loss with previous best losses.</li> <li><code>validation_criterion</code> (Callable): A custom function to assess if the validation metric meets a specified condition.</li> </ul> <p>Example: <pre><code>config = TrainConfig(val_every=200, checkpoint_best_only = True, validation_criterion=lambda current, best: current &lt; best - 0.001)\n</code></pre></p> <p>If it is desired to only the save the \"best\" checkpoint, the following must be ensured:</p> <pre><code>(a) `checkpoint_best_only = True` is used while creating the configuration through `TrainConfig`,\n(b) `val_every` is set to a valid integer value (for example, `val_every = 10`) which controls the no. of iterations after which the validation data should be used to evaluate the model during training, which can also be set through `TrainConfig`,\n(c) a validation criterion is provided through the `validation_criterion`, set through `TrainConfig` to quantify the definition of \"best\", and\n(d) the validation dataloader passed to `Trainer` is of type `DataLoader`. In this case, it is expected that a validation dataloader is also provided along with the train dataloader since the validation data will be used to decide the \"best\" checkpoint.\n</code></pre> <p>The criterion used to decide the \"best\" checkpoint can be customized by <code>validation_criterion</code>, which should be a function that can take val_loss, best_loss, and val_epsilon arguments and return a boolean value (True or False) indicating whether some validation metric is satisfied or not. An example of a simple <code>validation_criterion</code> is: <pre><code>def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:\n    return val_loss &lt; (best_val_loss - val_epsilon)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#custom-callbacks","title":"Custom Callbacks","text":"<p><code>TrainConfig</code> supports custom callbacks that can be triggered at specific stages of training. The <code>callbacks</code> attribute accepts a list of callback instances, which allow for custom behaviors like early stopping or additional logging. See Callbacks for more details.</p> <ul> <li><code>callbacks</code> (list[Callback]): List of custom callbacks to execute during training.</li> </ul> <p>Example: <pre><code>from qadence.ml_tools.callbacks import Callback\n\ndef callback_fn(trainer, config, writer):\n    if trainer.opt_res.loss &lt; 0.001:\n        print(\"Custom Callback: Loss threshold reached!\")\n\ncustom_callback = Callback(on = \"train_epoch_end\", called_every = 10, callback_function = callback_fn )\n\nconfig = TrainConfig(callbacks=[custom_callback])\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#hyperparameters-and-plotting","title":"Hyperparameters and Plotting","text":"<ul> <li><code>hyperparams</code> (dict): A dictionary of hyperparameters (e.g., learning rate, regularization) to be tracked by the tracking tool.</li> <li><code>plot_every</code> (int): Determines how frequently plots are saved to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>plotting_functions</code> (tuple[LoggablePlotFunction, ...]): Functions for in-training plotting of metrics or model state.</li> </ul> <p>Note: Please ensure that plotting_functions are provided when plot_every &gt; 0</p> <p>Example: <pre><code>config = TrainConfig(\n    plot_every=10,\n    hyperparams={\"learning_rate\": 0.001, \"batch_size\": 32},\n    plotting_functions=(plot_loss_function,)\n)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#3-experiment-tracking-with-mlflow","title":"3. Experiment tracking with mlflow","text":"<p>Qadence allows to track runs and log hyperparameters, models and plots with tensorboard and mlflow. In the following, we demonstrate the integration with mlflow.</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#mlflow-configuration","title":"mlflow configuration","text":"<p>We have control over our tracking configuration by setting environment variables. First, let's look at the tracking URI. For the purpose of this demo we will be working with a local database, in a similar fashion as described here, <pre><code>export MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n</code></pre></p> <p>Qadence can also read the following two environment variables to define the mlflow experiment name and run name <pre><code>export MLFLOW_EXPERIMENT=test_experiment\nexport MLFLOW_RUN_NAME=run_0\n</code></pre></p> <p>If no tracking URI is provided, mlflow stores run information and artifacts in the local <code>./mlflow</code> directory and if no names are defined, the experiment and run will be named with random UUIDs.</p>"},{"location":"tutorials/qml/ml_tools/trainer/","title":"Qadence Trainer Guide","text":"<p>The <code>Trainer</code> class in <code>qadence.ml_tools</code> is a versatile tool designed to streamline the training of quantum machine learning models. It offers flexibility for both gradient-based and gradient-free optimization methods, supports custom loss functions, and integrates seamlessly with tracking tools like TensorBoard and MLflow. Additionally, it provides hooks for implementing custom behaviors during the training process.</p> <p>For training QML models, Qadence offers this out-of-the-box <code>Trainer</code> for optimizing differentiable models, e.g. <code>QNN</code>s and <code>QuantumModel</code>, containing either trainable and/or non-trainable parameters (see the parameters tutorial for detailed information about parameter types):</p>"},{"location":"tutorials/qml/ml_tools/trainer/#1-overview","title":"1. Overview","text":"<p>The <code>Trainer</code> class simplifies the training workflow by managing the training loop, handling data loading, and facilitating model evaluation. It is compatible with various optimization strategies and allows for extensive customization to meet specific training requirements.</p> <p>Example of initializing the <code>Trainer</code>:</p> <pre><code>from qadence.ml_tools import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\n# Initialize Trainer with model, optimizer, and configuration\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\n</code></pre> <p>Notes: <code>qadence</code> versions prior to 1.9.0 provided <code>train_with_grad</code> and <code>train_no_grad</code> functions, which are being replaced with <code>Trainer</code>. The user can transition as following. <pre><code>from qadence.ml_tools import train_with_grad\ntrain_with_grad(model=model, optimizer=optimizer, config=config, data = data)\n</code></pre> to <pre><code>from qadence.ml_tools import Trainer\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\ntrainer.fit(train_dataloader = data)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/trainer/#2-gradient-based-and-gradient-free-optimization","title":"2. Gradient-Based and Gradient-Free Optimization","text":"<p>The <code>Trainer</code> supports both gradient-based and gradient-free optimization methods. Default is gradient-based optimization.</p> <ul> <li>Gradient-Based Optimization: Utilizes optimizers from PyTorch's <code>torch.optim</code> module. This is the default behaviour of the <code>Trainer</code>, thus setting this is not necessary. However, it can be explicity mentioned as follows. Example of using gradient-based optimization:</li> </ul> <pre><code>from qadence.ml_tools import Trainer\n\n# set_use_grad(True) to enable gradient based training. This is the default behaviour of Trainer.\nTrainer.set_use_grad(True)\n</code></pre> <ul> <li>Gradient-Free Optimization: Employs optimization algorithms from the Nevergrad library.</li> </ul> <p>Example of using gradient-free optimization with Nevergrad:</p> <pre><code>from qadence.ml_tools import Trainer\n\n# set_use_grad(False) to disable gradient based training.\nTrainer.set_use_grad(False)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#using-context-managers-for-mixed-optimization","title":"Using Context Managers for Mixed Optimization","text":"<p>For cases requiring both optimization methods in a single training session, the <code>Trainer</code> class provides context managers to enable or disable gradients.</p> <pre><code># Temporarily switch to gradient-based optimization\nwith trainer.enable_grad_opt(optimizer):\n    print(\"Gradient Based Optimization\")\n    # trainer.fit(train_loader)\n\n# Switch to gradient-free optimization for specific steps\nwith trainer.disable_grad_opt(ng_optimizer):\n    print(\"Gradient Free Optimization\")\n    # trainer.fit(train_loader)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#3-custom-loss-functions","title":"3. Custom Loss Functions","text":"<p>Users can define custom loss functions tailored to their specific tasks. The <code>Trainer</code> accepts a <code>loss_fn</code> parameter, which should be a callable that takes the model and data as inputs and returns a tuple containing the loss tensor and a dictionary of metrics.</p> <p>Example of using a custom loss function:</p> <pre><code>import torch\nfrom itertools import count\ncnt = count()\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n</code></pre> <p>This custom loss function can be used in the trainer <pre><code>from qadence.ml_tools import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\ntrainer = Trainer(model=model, optimizer=optimizer, config=config, loss_fn=loss_fn)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/trainer/#4-hooks-for-custom-behavior","title":"4. Hooks for Custom Behavior","text":"<p>The <code>Trainer</code> class provides several hooks that enable users to inject custom behavior at different stages of the training process. These hooks are methods that can be overridden in a subclass to execute custom code. The available hooks include:</p> <ul> <li><code>on_train_start</code>: Called at the beginning of the training process.</li> <li><code>on_train_end</code>: Called at the end of the training process.</li> <li><code>on_train_epoch_start</code>: Called at the start of each training epoch.</li> <li><code>on_train_epoch_end</code>: Called at the end of each training epoch.</li> <li><code>on_train_batch_start</code>: Called at the start of each training batch.</li> <li><code>on_train_batch_end</code>: Called at the end of each training batch.</li> </ul> <p>Each \"start\" and \"end\" hook receives data and loss metrics as arguments. The specific values provided for these arguments depend on the training stage associated with the hook. The context of the training stage (e.g., training, validation, or testing) determines which metrics are relevant and how they are populated. For details of inputs on each hook, please review the documentation of <code>BaseTrainer</code>.</p> <pre><code>- Example of what inputs are provided to training hooks.\n\n    ```\n    def on_train_batch_start(self, batch: Tuple[torch.Tensor, ...] | None) -&gt; None:\n        \"\"\"\n        Called at the start of each training batch.\n\n        Args:\n            batch: A batch of data from the DataLoader. Typically a tuple containing\n                input tensors and corresponding target tensors.\n        \"\"\"\n        pass\n    ```\n    ```\n    def on_train_batch_end(self, train_batch_loss_metrics: Tuple[torch.Tensor, Any]) -&gt; None:\n        \"\"\"\n        Called at the end of each training batch.\n\n        Args:\n            train_batch_loss_metrics: Metrics for the training batch loss.\n                Tuple of (loss, metrics)\n        \"\"\"\n        pass\n    ```\n</code></pre> <p>Example of using a hook to log a message at the end of each epoch:</p> <pre><code>from qadence.ml_tools import Trainer\n\nclass CustomTrainer(Trainer):\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        print(f\"End of epoch - Loss and Metrics: {train_epoch_loss_metrics}\")\n</code></pre> <p>Notes: Trainer offers inbuilt callbacks as well. Callbacks are mainly for logging/tracking purposes, but the above mentioned hooks are generic. The workflow for every train batch looks like: 1. perform on_train_batch_start callbacks, 2. call the on_train_batch_start hook, 3. do the batch training, 4. call the on_train_batch_end hook, and 5. perform on_train_batch_end callbacks.</p> <p>The use of <code>on_</code>{phase}<code>_start</code> and <code>on_</code>{phase}<code>_end</code> hooks is not specifically to add extra callbacks, but for any other generic pre/post processing. For example, reshaping input batch in case of RNNs/LSTMs, post processing loss and adding an extra metric. They could also be used to add more callbacks (which is not recommended - as we provide methods to add extra callbacks in the TrainCofig)</p>"},{"location":"tutorials/qml/ml_tools/trainer/#5-experiment-tracking-with-tensorboard-and-mlflow","title":"5. Experiment Tracking with TensorBoard and MLflow","text":"<p>The <code>Trainer</code> integrates with TensorBoard and MLflow for experiment tracking:</p> <ul> <li> <p>TensorBoard: Logs metrics and visualizations during training, allowing users to monitor the training process.</p> </li> <li> <p>MLflow: Tracks experiments, logs parameters, metrics, and artifacts, and provides a user-friendly interface for comparing different runs.</p> </li> </ul> <p>To utilize these tracking tools, the <code>Trainer</code> can be configured with appropriate writers that handle the logging of metrics and other relevant information during training.</p> <p>Example of using TensorBoard tracking:</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.types import ExperimentTrackingTool\n\n# Set up tracking with TensorBoard\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.TENSORBOARD)\n</code></pre> <p>Example of using MLflow tracking:</p> <pre><code>from qadence.types import ExperimentTrackingTool\n\n# Set up tracking with MLflow\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.MLFLOW)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#6-examples","title":"6. Examples","text":""},{"location":"tutorials/qml/ml_tools/trainer/#61-training-with-trainer-and-trainconfig","title":"6.1. Training with <code>Trainer</code> and <code>TrainConfig</code>","text":""},{"location":"tutorials/qml/ml_tools/trainer/#setup","title":"Setup","text":"<p>Let's do the necessary imports and declare a <code>DataLoader</code>. We can already define some hyperparameters here, including the seed for random number generators. mlflow can log hyperparameters with arbitrary types, for example the observable that we want to monitor (<code>Z</code> in this case, which has a <code>qadence.Operation</code> type).</p> <pre><code>import random\nfrom itertools import count\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom matplotlib.figure import Figure\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\n\nfrom qadence import hea, QuantumCircuit, Z\nfrom qadence.constructors import feature_map, hamiltonian_factory\nfrom qadence.ml_tools import Trainer, TrainConfig\nfrom qadence.ml_tools.data import to_dataloader\nfrom qadence.ml_tools.utils import rand_featureparameters\nfrom qadence.ml_tools.models import QNN, QuantumModel\nfrom qadence.types import ExperimentTrackingTool\n\nhyperparams = {\n    \"seed\": 42,\n    \"batch_size\": 10,\n    \"n_qubits\": 2,\n    \"ansatz_depth\": 1,\n    \"observable\": Z,\n}\n\nnp.random.seed(hyperparams[\"seed\"])\ntorch.manual_seed(hyperparams[\"seed\"])\nrandom.seed(hyperparams[\"seed\"])\n\n\ndef dataloader(batch_size: int = 25) -&gt; DataLoader:\n    x = torch.linspace(0, 1, batch_size).reshape(-1, 1)\n    y = torch.cos(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=True)\n</code></pre> <p>We continue with the regular QNN definition, together with the loss function and optimizer.</p> <pre><code>obs = hamiltonian_factory(register=hyperparams[\"n_qubits\"], detuning=hyperparams[\"observable\"])\n\ndata = dataloader(hyperparams[\"batch_size\"])\nfm = feature_map(hyperparams[\"n_qubits\"], param=\"x\")\n\nmodel = QNN(\n    QuantumCircuit(\n        hyperparams[\"n_qubits\"], fm, hea(hyperparams[\"n_qubits\"], hyperparams[\"ansatz_depth\"])\n    ),\n    observable=obs,\n    inputs=[\"x\"],\n)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ninputs = rand_featureparameters(model, 1)\n\ndef loss_fn(model: QuantumModel, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    out = model.expectation(inputs)\n    loss = criterion(out, torch.rand(1))\n    return loss, {}\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#trainconfig-specifications","title":"<code>TrainConfig</code> specifications","text":"<p>Qadence offers different tracking options via <code>TrainConfig</code>. Here we use the <code>ExperimentTrackingTool</code> type to specify that we want to track the experiment with mlflow. Tracking with tensorboard is also possible. We can then indicate what and how often we want to track or log.</p> <p>For Training <code>write_every</code> controls the number of epochs after which the loss values is logged. Thanks to the <code>plotting_functions</code> and <code>plot_every</code>arguments, we are also able to plot model-related quantities throughout training. Notice that arbitrary plotting functions can be passed, as long as the signature is the same as <code>plot_fn</code> below. Finally, the trained model can be logged by setting <code>log_model=True</code>. Here is an example of plotting function and training configuration</p> <pre><code>def plot_fn(model: Module, iteration: int) -&gt; tuple[str, Figure]:\n    descr = f\"ufa_prediction_epoch_{iteration}.png\"\n    fig, ax = plt.subplots()\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    out = model.expectation(x)\n    ax.plot(x.detach().numpy(), out.detach().numpy())\n    return descr, fig\n\n\nconfig = TrainConfig(\n    root_folder=\"mlflow_demonstration\",\n    max_iter=10,\n    checkpoint_every=1,\n    plot_every=2,\n    write_every=1,\n    log_model=True,\n    tracking_tool=ExperimentTrackingTool.MLFLOW,\n    hyperparams=hyperparams,\n    plotting_functions=(plot_fn,),\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#training-and-inspecting","title":"Training and inspecting","text":"<p>Model training happens as usual <pre><code>trainer = Trainer(model, optimizer, config, loss_fn)\ntrainer.fit(train_dataloader=data)\n</code></pre></p> <p>After training , we can inspect our experiment via the mlflow UI <pre><code>mlflow ui --port 8080 --backend-store-uri sqlite:///mlruns.db\n</code></pre> In this case, since we're running on a local server, we can access the mlflow UI by navigating to http://localhost:8080/.</p>"},{"location":"tutorials/qml/ml_tools/trainer/#62-fitting-a-function-with-a-qnn-using-ml_tools","title":"6.2. Fitting a function with a QNN using <code>ml_tools</code>","text":"<p>In Quantum Machine Learning, the general consensus is to use <code>complex128</code> precision for states and operators and <code>float64</code> precision for parameters. This is also the convention which is used in <code>qadence</code>. However, for specific usecases, lower precision can greatly speed up training and reduce memory consumption. When using the <code>pyqtorch</code> backend, <code>qadence</code> offers the option to move a <code>QuantumModel</code> instance to a specific precision using the torch <code>to</code> syntax.</p> <p>Let's look at a complete example of how to use <code>Trainer</code> now. Here we perform a validation check during training and use a validation criterion that checks whether the validation loss in the current iteration has decreased compared to the lowest validation loss from all previous iterations. For demonstration, the train and the validation data are kept the same here. However, it is beneficial and encouraged to keep them distinct in practice to understand model's generalization capabilities.</p> <pre><code>from pathlib import Path\nimport torch\nfrom functools import reduce\nfrom operator import add\nfrom itertools import count\nimport matplotlib.pyplot as plt\n\nfrom qadence import Parameter, QuantumCircuit, Z\nfrom qadence import hamiltonian_factory, hea, feature_map, chain\nfrom qadence import QNN\nfrom qadence.ml_tools import  TrainConfig, Trainer, to_dataloader\n\nTrainer.set_use_grad(True)\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDTYPE = torch.complex64\nn_qubits = 4\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 100\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data[0], data[1]\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n\ndef validation_criterion(\n    current_validation_loss: float, current_best_validation_loss: float, val_epsilon: float\n) -&gt; bool:\n    return current_validation_loss &lt;= current_best_validation_loss - val_epsilon\n\nn_epochs = 300\n\nconfig = TrainConfig(\n    max_iter=n_epochs,\n    batch_size=batch_size,\n    checkpoint_best_only=True,\n    val_every=10,  # The model will be run on the validation data after every `val_every` epochs.\n    validation_criterion=validation_criterion\n)\n\nfn = lambda x, degree: .05 * reduce(add, (torch.cos(i*x) + torch.sin(i*x) for i in range(degree)), 0.)\nx = torch.linspace(0, 10, batch_size, dtype=torch.float32).reshape(-1, 1)\ny = fn(x, 5)\n\ntrain_dataloader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_dataloader =  to_dataloader(x, y, batch_size=batch_size, infinite=True)\n\ntrainer = Trainer(model, optimizer, config, loss_fn=loss_fn,\n                    train_dataloader = train_dataloader, val_dataloader = val_dataloader,\n                    device=DEVICE, dtype=DTYPE)\ntrainer.fit()\n\nplt.clf()\nplt.plot(x.numpy(), y.numpy(), label='truth')\nplt.plot(x.numpy(), model(x).detach().numpy(), \"--\", label=\"final\", linewidth=3)\nplt.legend()\n</code></pre> 2025-01-28T16:52:09.048918 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/ml_tools/trainer/#63-fitting-a-function-low-level-api","title":"6.3. Fitting a function - Low-level API","text":"<p>For users who want to use the low-level API of <code>qadence</code>, here an example written without <code>Trainer</code>.</p> <pre><code>from pathlib import Path\nimport torch\nfrom itertools import count\nfrom qadence.constructors import hamiltonian_factory, hea, feature_map\nfrom qadence import chain, Parameter, QuantumCircuit, Z\nfrom qadence import QNN\nfrom qadence.ml_tools import TrainConfig\n\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 1\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nn_epochs=50\ncnt = count()\n\ntmp_path = Path(\"/tmp\")\n\nconfig = TrainConfig(\n    root_folder=tmp_path,\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n)\n\nx = torch.linspace(0, 1, batch_size).reshape(-1, 1)\ny = torch.sin(x)\n\nfor i in range(n_epochs):\n    out = model(x)\n    loss = criterion(out, y)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#64-custom-train-loop","title":"6.4. Custom <code>train</code> loop","text":"<p>If you need custom training functionality that goes beyond what is available in <code>qadence.ml_tools.Trainer</code> you can write your own training loop based on the building blocks that are available in Qadence.</p> <p>A simplified version of Qadence's train loop is defined below. Feel free to copy it and modify at will.</p> <p>For logging we can use the <code>get_writer</code> from the <code>Writer Registry</code>. This will set up the default writer based on the experiment tracking tool. All writers from the <code>Writer Registry</code> offer <code>open</code>, <code>close</code>, <code>print_metrics</code>, <code>write_metrics</code>, <code>plot_metrics</code>, etc methods.</p> <pre><code>from typing import Callable, Union\n\nfrom torch.nn import Module\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom qadence.ml_tools.config import TrainConfig\nfrom qadence.ml_tools.data import DictDataLoader, data_to_device\nfrom qadence.ml_tools.optimize_step import optimize_step\nfrom qadence.ml_tools.callbacks import get_writer\nfrom qadence.ml_tools.callbacks.saveload import load_checkpoint, write_checkpoint\n\n\ndef train(\n    model: Module,\n    data: DataLoader,\n    optimizer: Optimizer,\n    config: TrainConfig,\n    loss_fn: Callable,\n    device: str = \"cpu\",\n    optimize_step: Callable = optimize_step,\n    write_tensorboard: Callable = write_tensorboard,\n) -&gt; tuple[Module, Optimizer]:\n\n    # Move model to device before optimizer is loaded\n    model = model.to(device)\n\n    # load available checkpoint\n    init_iter = 0\n    if config.log_folder:\n        model, optimizer, init_iter = load_checkpoint(config.log_folder, model, optimizer)\n\n    # Initialize writer based on the tracking tool specified in the configuration\n    writer = get_writer(config.tracking_tool)  # Uses ExperimentTrackingTool to select writer\n    writer.open(config, iteration=init_iter)\n\n    dl_iter = iter(dataloader)\n\n    # outer epoch loop\n    for iteration in range(init_iter, init_iter + config.max_iter):\n        data = data_to_device(next(dl_iter), device)\n        loss, metrics = optimize_step(model, optimizer, loss_fn, data)\n\n        if iteration % config.print_every == 0 and config.verbose:\n            writer.print_metrics(OptimizeResult(iteration, model, optimizer, loss, metrics))\n\n        if iteration % config.write_every == 0:\n            writer.write(iteration, metrics)\n\n        if config.log_folder:\n            if iteration % config.checkpoint_every == 0:\n                write_checkpoint(config.log_folder, model, optimizer, iteration)\n\n    # Final writing and checkpointing\n    if config.log_folder:\n        write_checkpoint(config.log_folder, model, optimizer, iteration)\n    writer.write(iteration,metrics)\n    writer.close()\n\n    return model, optimizer\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#65-gradient-free-optimization-using-trainer","title":"6.5. Gradient-free optimization using <code>Trainer</code>","text":"<p>We can achieve gradient free optimization with <code>Trainer.set_use_grad(False)</code> or <code>trainer.disable_grad_opt(ng_optimizer)</code>. An example solving a QUBO using gradient free optimization based on <code>Nevergrad</code> optimizers and <code>Trainer</code> is shown in the analog QUBO Tutorial.</p>"},{"location":"tutorials/realistic_sims/","title":"Realistic simulations","text":"<p>This section describes how to perform realistic simulations in Qadence.</p>"},{"location":"tutorials/realistic_sims/measurements/","title":"Measurement protocols","text":"<p>Sample-based measurement protocols are fundamental tools for the prediction and estimation of a quantum state as the result of NISQ programs executions. Their resource efficient implementation is a current and active research field. Qadence offers two main measurement protocols: quantum state tomography and classical shadows.</p>"},{"location":"tutorials/realistic_sims/measurements/#quantum-state-tomography","title":"Quantum state tomography","text":"<p>The fundamental task of quantum state tomography is to learn an approximate classical description of an output quantum state described by a density matrix \\(\\rho\\), from repeated measurements of copies on a chosen basis. To do so, \\(\\rho\\) is expanded in a basis of observables (the tomography step) and for a given observable \\(\\hat{\\mathcal{O}}\\), the expectation value is calculated with \\(\\langle \\hat{\\mathcal{O}} \\rangle=\\textrm{Tr}(\\hat{\\mathcal{O}}\\rho)\\). A number of measurement repetitions in a suitable basis is then required to estimate \\(\\langle \\hat{\\mathcal{O}} \\rangle\\).</p> <p>The main drawback is the scaling in measurements for the retrieval of the classical expression for a \\(n\\)-qubit quantum state as \\(2^n \\times 2^n\\), together with a large amount of classical post-processing.</p> <p>For an observable expressed as a Pauli string \\(\\hat{\\mathcal{P}}\\), the expectation value for a state \\(|\\psi \\rangle\\) can be derived as:</p> \\[ \\langle \\hat{\\mathcal{P}} \\rangle=\\langle \\psi | \\hat{\\mathcal{P}} |\\psi \\rangle=\\langle \\psi | \\hat{\\mathcal{R}}^\\dagger \\hat{\\mathcal{D}} \\hat{\\mathcal{R}} |\\psi \\rangle \\] <p>The operator \\(\\hat{\\mathcal{R}}\\) diagonalizes \\(\\hat{\\mathcal{P}}\\) and rotates the state into an eigenstate in the computational basis. Therefore, \\(\\hat{\\mathcal{R}}|\\psi \\rangle=\\sum\\limits_{z}a_z|z\\rangle\\) and the expectation value can finally be expressed as:</p> \\[ \\langle \\hat{\\mathcal{P}} \\rangle=\\sum_{z,z'}\\langle z |\\bar{a}_z\\hat{\\mathcal{D}}a_{z'}|z'\\rangle = \\sum_{z}|a_z|^2(-1)^{\\phi_z(\\hat{\\mathcal{P}})} \\] <p>In Qadence, running a tomographical experiment is made simple by defining a <code>Measurements</code> object that captures all options for execution:</p> <pre><code>from torch import tensor\nfrom qadence import hamiltonian_factory, BackendName, DiffMode\nfrom qadence import Parameter, chain, kron, RX, RY, Z, QuantumCircuit, QuantumModel\nfrom qadence.measurements import Measurements\n\n# Define parameters for a circuit.\ntheta1 = Parameter(\"theta1\", trainable=False)\ntheta2 = Parameter(\"theta2\", trainable=False)\ntheta3 = Parameter(\"theta3\", trainable=False)\ntheta4 = Parameter(\"theta4\", trainable=False)\n\nblocks = chain(\n    kron(RX(0, theta1), RY(1, theta2)),\n    kron(RX(0, theta3), RY(1, theta4)),\n)\n\nvalues = {\n    \"theta1\": tensor([0.5]),\n    \"theta2\": tensor([1.5]),\n    \"theta3\": tensor([2.0]),\n    \"theta4\": tensor([2.5]),\n}\n\n# Create a circuit and an observable.\ncircuit = QuantumCircuit(2, blocks)\nobservable = hamiltonian_factory(2, detuning=Z)\n\n# Create a model.\nmodel = QuantumModel(\n    circuit=circuit,\n    observable=observable,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.GPSR,\n)\n\n# Define a measurement protocol by passing the shot budget as an option.\ntomo_options = {\"n_shots\": 100000}\ntomo_measurement = Measurements(protocol=Measurements.TOMOGRAPHY, options=tomo_options)\n\n# Get the exact expectation value.\nexact_values = model.expectation(\n    values=values,\n)\n\n# Run the tomography experiment.\nestimated_values_tomo = model.expectation(\n    values=values,\n    measurement=tomo_measurement,\n)\n</code></pre> <pre><code>Exact expectation value = tensor([[-1.4548]])\nEstimated expectation value tomo = tensor([[-1.4566]])\n</code></pre>"},{"location":"tutorials/realistic_sims/measurements/#classical-shadows","title":"Classical shadows","text":"<p>Recently, a much less resource demanding protocol based on classical shadows has been proposed<sup>1</sup>. It combines ideas from shadow tomography<sup>2</sup> and randomized measurement protocols capable of learning a classical shadow of an unknown quantum state \\(\\rho\\). It relies on deliberately discarding the full classical characterization of the quantum state, and instead focuses on accurately predicting a restricted set of properties that provide efficient protocols for the study of the system.</p> <p>A random measurement consists of applying random unitary rotations before a fixed measurement on each copy of a state. Appropriately averaging over these measurements produces an efficient estimator for the expectation value of an observable. This protocol therefore creates a robust classical representation of the quantum state or classical shadow. The captured measurement information is then reuseable for multiple purposes, i.e. any observable expected value and available for noise mitigation postprocessing.</p> <p>A classical shadow is therefore an unbiased estimator of a quantum state \\(\\rho\\). Such an estimator is obtained with the following procedure<sup>1</sup>: first, apply a random unitary gate \\(U\\) to rotate the state: \\(\\rho \\rightarrow U \\rho U^\\dagger\\) and then perform a basis measurement to obtain a \\(n\\)-bit measurement \\(|\\hat{b}\\rangle \\in \\{0, 1\\}^n\\). Both unitary gates \\(U\\) and the measurement outcomes \\(|\\hat{b}\\rangle\\) are stored on a classical computer for postprocessing v \\(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U\\), a classical snapshot of the state \\(\\rho\\). The whole procedure can be seen as a quantum channel \\(\\mathcal{M}\\) that maps the initial unknown quantum state \\(\\rho\\) to the average result of the measurement protocol:</p> \\[ \\mathbb{E}[U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U] = \\mathcal{M}(\\rho) \\Rightarrow \\rho = \\mathbb{E}[\\mathcal{M}^{-1}(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U)] \\] <p>It is worth noting that the single classical snapshot \\(\\hat{\\rho}=\\mathcal{M}^{-1}(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U)\\) equals \\(\\rho\\) in expectation: \\(\\mathbb{E}[\\hat{\\rho}]=\\rho\\) despite \\(\\mathcal{M}^{-1}\\) not being a completely positive map. Repeating this procedure \\(N\\) times results in an array of \\(N\\) independent, classical snapshots of \\(\\rho\\) called the classical shadow:</p> \\[ S(\\rho, N) = \\{ \\hat{\\rho}_1=\\mathcal{M}^{-1}(U_1^\\dagger |\\hat{b}_1\\rangle\\langle \\hat{b}_1|U_1),\\cdots,\\hat{\\rho}_N=\\mathcal{M}^{-1}(U_N^\\dagger |\\hat{b}_N\\rangle\\langle \\hat{b}_N|U_N)\\} \\] <p>Along the same lines as the example before, estimating the expectation value using classical shadows in Qadence only requires to pass the right set of parameters to the <code>Measurements</code> object:</p> <pre><code># Classical shadows are defined up to some accuracy and confidence.\nshadow_options = {\"accuracy\": 0.1, \"confidence\": 0.1}  # Shadow size N=54400.\nshadow_measurement = Measurements(protocol=Measurements.SHADOW, options=shadow_options)\n\n# Run the experiment with classical shadows.\nestimated_values_shadow = model.expectation(\n    values=values,\n    measurement=shadow_measurement,\n)\n</code></pre> <pre><code>Estimated expectation value shadow = tensor([[-1.4572]])\n</code></pre>"},{"location":"tutorials/realistic_sims/measurements/#references","title":"References","text":"<ol> <li> <p>Hsin-Yuan Huang, Richard Kueng and John Preskill, Predicting Many Properties of a Quantum System from Very Few Measurements (2020) \u21a9\u21a9</p> </li> <li> <p>S. Aaronson. Shadow tomography of quantum states. In Proceedings of the 50th Annual A ACM SIGACT Symposium on Theory of Computing, STOC 2018, pages 325\u2013338, New York, NY, USA, 2018. ACM\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/realistic_sims/mitigation/","title":"Error mitigation","text":"<p>Beyond running noisy simulations, Qadence offers a number of noise mitigation techniques to achieve better accuracy of simulation outputs. Currently, mitigation addresses readout errors and depolarizing and dephasing noise for analog blocks.</p>"},{"location":"tutorials/realistic_sims/mitigation/#readout-error-mitigation","title":"Readout error mitigation","text":"<p>The complete implementation of the mitigation technique is to measure \\(T\\) and classically apply \\(T^{\u22121}\\) to measured probability distributions. However there are several limitations of this approach:</p> <ul> <li>The complete implementation requires \\(2^n\\) characterization experiments (probability measurements), which is not scalable. The classical processing of the calibration data is also inefficient.</li> <li>The matrix \\(T\\) may become singular for large \\(n\\), preventing direct inversion.</li> <li>The inverse \\(T^{\u22121}\\) might not be a stochastic matrix, meaning that it can produce negative corrected probabilities.</li> <li>The correction is not rigorously justified, so we cannot be sure that we are only removing SPAM errors and not otherwise corrupting an estimated probability distribution.</li> </ul> <p>Qadence relies on the assumption of uncorrelated readout errors:</p> \\[ T=T_1\\otimes T_2\\otimes \\dots \\otimes T_n \\] <p>for which the inversion is straightforward:</p> \\[ T^{-1}=T_1^{-1}\\otimes T_2^{-1}\\otimes \\dots \\otimes T_n^{-1} \\] <p>However, even for a reduced \\(n\\) the third limitation holds. This can be avoided by reformulating into a minimization problem<sup>1</sup>:</p> \\[ \\lVert Tp_{\\textrm{corr}}-p_{\\textrm{raw}}\\rVert_{2}^{2} \\] <p>subjected to physicality constraints \\(0 \\leq p_{corr}(x) \\leq 1\\) and \\(\\lVert p_{corr} \\rVert = 1\\). At this point, two methods are implemented to solve this problem. The first one relies on solving using standard optimization tools, the second on Maximum-Likelihood Estimation<sup>2</sup>. In Qadence, this can be user defined using the mitigation protocol:</p> <pre><code>from qadence import QuantumModel, QuantumCircuit, kron, H, Z\nfrom qadence import hamiltonian_factory\nfrom qadence.noise import NoiseHandler\nfrom qadence.mitigations import Mitigations\nfrom qadence.types import ReadOutOptimization, NoiseProtocol\n\n# Simple circuit and observable construction.\nblock = kron(H(0), Z(1))\ncircuit = QuantumCircuit(2, block)\nobservable = hamiltonian_factory(circuit.n_qubits, detuning=Z)\n\n# Construct a quantum model.\nmodel = QuantumModel(circuit=circuit, observable=observable)\n\n# Define a noise model to use:\nnoise = NoiseHandler(NoiseProtocol.READOUT.INDEPENDENT)\n# Define the mitigation method solving the minimization problem:\noptions={\"optimization_type\": ReadOutOptimization.CONSTRAINED}  # ReadOutOptimization.MLE for the alternative method.\nmitigation = Mitigations(protocol=Mitigations.READOUT, options=options)\n\n# Run noiseless, noisy and mitigated simulations.\nn_shots = 100\nnoiseless_samples = model.sample(n_shots=n_shots)\nnoisy_samples = model.sample(noise=noise, n_shots=n_shots)\nmitigated_samples = model.sample(\n    noise=noise, mitigation=mitigation, n_shots=n_shots\n)\n\nprint(f\"noiseless {noiseless_samples}\")\nprint(f\"noisy {noisy_samples}\")\nprint(f\"mitigated {mitigated_samples}\")\n</code></pre> <pre><code>noiseless [OrderedCounter({'10': 53, '00': 47})]\nnoisy [OrderedCounter({'10': 60, '00': 39, '01': 1})]\nmitigated [Counter({'11': 61, '10': 31, '01': 8})]\n</code></pre>"},{"location":"tutorials/realistic_sims/mitigation/#wip-zero-noise-extrapolation-for-analog-blocks","title":"[WIP] Zero-noise extrapolation for analog blocks","text":"<p>Zero-noise extrapolation (ZNE) is an error mitigation technique in which an expectation value is computed at different noise levels and, as a second step, the ideal expectation value is inferred by extrapolating the measured results to the zero-noise limit. In digital computing, this is typically implemented by \"folding\" the circuit and its dagger to artificially increase the noise through sequences of identities<sup>3</sup>. In the analog ZNE variation, analog blocks are time stretched to again artificially increase noise<sup>3</sup>.</p>"},{"location":"tutorials/realistic_sims/mitigation/#references","title":"References","text":"<ol> <li> <p>Michael R. Geller and Mingyu Sun, Efficient correction of multiqubit measurement errors, (2020) \u21a9</p> </li> <li> <p>Smolin et al., Maximum Likelihood, Minimum Effort, (2011) \u21a9</p> </li> <li> <p>Mitiq: What's the theory behind ZNE? \u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/realistic_sims/noise/","title":"Simulated errors","text":"<p>Running programs on NISQ devices often leads to partially useful results due to the presence of noise. In order to perform realistic simulations, a number of noise models (for digital operations, analog operations and simulated readout errors) are supported in Qadence through their implementation in backends and corresponding error mitigation techniques whenever possible.</p>"},{"location":"tutorials/realistic_sims/noise/#noisehandler","title":"NoiseHandler","text":"<p>Noise models can be defined via the <code>NoiseHandler</code>. It is a container of several noise instances which require to specify a <code>protocols</code> and a dictionary of <code>options</code> (or lists). The <code>protocol</code> field is to be instantiated from <code>NoiseProtocol</code>.</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\nanalog_noise = NoiseHandler(protocol=NoiseProtocol.ANALOG.DEPOLARIZING, options={\"noise_probs\": 0.1})\ndigital_noise = NoiseHandler(protocol=NoiseProtocol.DIGITAL.DEPOLARIZING, options={\"error_probability\": 0.1})\nreadout_noise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options={\"error_probability\": 0.1, \"seed\": 0})\n</code></pre> <pre><code>\n</code></pre> <p>One can also define a <code>NoiseHandler</code> passing a list of protocols and a list of options (careful with the order):</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\nprotocols = [NoiseProtocol.DIGITAL.DEPOLARIZING, NoiseProtocol.READOUT]\noptions = [{\"error_probability\": 0.1}, {\"error_probability\": 0.1, \"seed\": 0}]\n\nnoise_combination = NoiseHandler(protocols, options)\nprint(noise_combination)\n</code></pre> <pre><code>Noise(Depolarizing, {'error_probability': 0.1})\nNoise(&lt;enum 'ReadoutNoise'&gt;, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>One can also append to a <code>NoiseHandler</code> other <code>NoiseHandler</code> instances:</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\ndepo_noise = NoiseHandler(protocol=NoiseProtocol.DIGITAL.DEPOLARIZING, options={\"error_probability\": 0.1})\nreadout_noise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options={\"error_probability\": 0.1, \"seed\": 0})\n\nnoise_combination = NoiseHandler(protocol=NoiseProtocol.DIGITAL.BITFLIP, options={\"error_probability\": 0.1})\nnoise_combination.append([depo_noise, readout_noise])\nprint(noise_combination)\n</code></pre> <pre><code>Noise(BitFlip, {'error_probability': 0.1})\nNoise(Depolarizing, {'error_probability': 0.1})\nNoise(Independent Readout, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>Finally, one can add directly a few pre-defined types using several <code>NoiseHandler</code> methods:</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\nnoise_combination = NoiseHandler(protocol=NoiseProtocol.DIGITAL.BITFLIP, options={\"error_probability\": 0.1})\nnoise_combination.digital_depolarizing({\"error_probability\": 0.1}).readout_independent({\"error_probability\": 0.1, \"seed\": 0})\nprint(noise_combination)\n</code></pre> <pre><code>Noise(BitFlip, {'error_probability': 0.1})\nNoise(Depolarizing, {'error_probability': 0.1})\nNoise(Independent Readout, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>NoiseHandler scope</p> <p>Note it is not possible to define a <code>NoiseHandler</code> instances with both digital and analog noises, both readout and analog noises, several analog noises, several readout noises, or a readout noise that is not the last defined protocol within <code>NoiseHandler</code>.</p>"},{"location":"tutorials/realistic_sims/noise/#readout-errors","title":"Readout errors","text":"<p>State Preparation and Measurement (SPAM) in the hardware is a major source of noise in the execution of quantum programs. They are typically described using confusion matrices of the form:</p> \\[ T(x|x')=\\delta_{xx'} \\] <p>Two types of readout protocols are available: - <code>NoiseProtocol.READOUT.INDEPENDENT</code> where each bit can be corrupted independently of each other. - <code>NoiseProtocol.READOUT.CORRELATED</code> where we can define of confusion matrix of corruption between each possible bitstrings.</p> <p>Qadence offers to simulate readout errors with the <code>NoiseHandler</code> to corrupt the output samples of a simulation, through execution via a <code>QuantumModel</code>:</p> <pre><code>from qadence import QuantumModel, QuantumCircuit, kron, H, Z\nfrom qadence import hamiltonian_factory\n\n# Simple circuit and observable construction.\nblock = kron(H(0), Z(1))\ncircuit = QuantumCircuit(2, block)\nobservable = hamiltonian_factory(circuit.n_qubits, detuning=Z)\n\n# Construct a quantum model.\nmodel = QuantumModel(circuit=circuit, observable=observable)\n\n# Define a noise model to use.\nnoise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT)\n\n# Run noiseless and noisy simulations.\nnoiseless_samples = model.sample(n_shots=100)\nnoisy_samples = model.sample(noise=noise, n_shots=100)\n</code></pre> <pre><code>noiseless = [OrderedCounter({'10': 53, '00': 47})]\nnoisy = [OrderedCounter({'10': 52, '00': 40, '01': 5, '11': 3})]\n</code></pre> <p>It is possible to pass options to the noise model. In the previous example, a noise matrix is implicitly computed from a uniform distribution.</p> <p>For <code>NoiseProtocol.READOUT.INDEPENDENT</code>, the <code>option</code> dictionary argument accepts the following options:</p> <ul> <li><code>seed</code>: defaulted to <code>None</code>, for reproducibility purposes</li> <li><code>error_probability</code>: If float, the same probability is applied to every bit. By default, this is 0.1.     If a 1D tensor with the number of elements equal to the number of qubits, a different probability can be set for each qubit. If a tensor of shape (n_qubits, 2, 2) is passed, that is a confusion matrix obtained from experiments, we extract the error_probability.     and do not compute internally the confusion matrix as in the other cases.</li> <li><code>noise_distribution</code>: defaulted to <code>WhiteNoise.UNIFORM</code>, for non-uniform noise distributions</li> </ul> <p>For <code>NoiseProtocol.READOUT.CORRELATED</code>, the <code>option</code> dictionary argument accepts the following options: - <code>confusion_matrix</code>: The square matrix representing \\(T(x|x')\\) for each possible bitstring of length <code>n</code> qubits. Should be of size (2n, 2n). - <code>seed</code>: defaulted to <code>None</code>, for reproducibility purposes</p> <p>Noisy simulations go hand-in-hand with measurement protocols discussed in the previous section, to assess the impact of noise on expectation values. In this case, both measurement and noise protocols have to be defined appropriately. Please note that a noise protocol without a measurement protocol will be ignored for expectation values computations.</p> <pre><code>from qadence.measurements import Measurements\n\n# Define a noise model with options.\noptions = {\"error_probability\": 0.01}\nnoise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options=options)\n\n# Define a tomographical measurement protocol with options.\noptions = {\"n_shots\": 10000}\nmeasurement = Measurements(protocol=Measurements.TOMOGRAPHY, options=options)\n\n# Run noiseless and noisy simulations.\nnoiseless_exp = model.expectation(measurement=measurement)\nnoisy_exp = model.expectation(measurement=measurement, noise=noise)\n</code></pre> <pre><code>noiseless = tensor([[0.9998]], grad_fn=&lt;TransposeBackward0&gt;)\nnoisy = tensor([[0.9972]], grad_fn=&lt;TransposeBackward0&gt;)\n</code></pre>"},{"location":"tutorials/realistic_sims/noise/#analog-noisy-simulation","title":"Analog noisy simulation","text":"<p>At the moment, analog noisy simulations are only compatable with the Pulser backend. <pre><code>from qadence import DiffMode, NoiseHandler, QuantumModel\nfrom qadence.blocks import chain, kron\nfrom qadence.circuit import QuantumCircuit\nfrom qadence.operations import AnalogRX, AnalogRZ, Z\nfrom qadence.types import PI, BackendName, NoiseProtocol\n\n\nanalog_block = chain(AnalogRX(PI / 2.0), AnalogRZ(PI))\nobservable = Z(0) + Z(1)\ncircuit = QuantumCircuit(2, analog_block)\n\noptions = {\"noise_probs\": 0.1}\nnoise = NoiseHandler(protocol=NoiseProtocol.ANALOG.DEPOLARIZING, options=options)\nmodel_noisy = QuantumModel(\n    circuit=circuit,\n    observable=observable,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR,\n    noise=noise,\n)\nnoisy_expectation = model_noisy.expectation()\n</code></pre> <pre><code>noisy = tensor([[0.3597]])\n</code></pre> </p>"},{"location":"tutorials/realistic_sims/noise/#digital-noisy-simulation","title":"Digital noisy simulation","text":"<p>When dealing with programs involving only digital operations, several options are made available from PyQTorch via the <code>NoiseProtocol.DIGITAL</code>. One can define noisy digital operations as follows:</p> <pre><code>from qadence import NoiseProtocol, RX, run\nimport torch\n\nnoise = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, {\"error_probability\": 0.2})\nop = RX(0, torch.pi, noise = noise)\n\nprint(run(op))\n</code></pre> <pre><code>DensityMatrix([[[0.2000+0.0000e+00j, 0.0000+3.6739e-17j],\n                [0.0000-3.6739e-17j, 0.8000+0.0000e+00j]]])\n</code></pre> <p>It is also possible to set a noise configuration to gates within a composite block or circuit as follows:</p> <pre><code>from qadence import set_noise, chain\n\nn_qubits = 2\n\nblock = chain(RX(i, f\"theta_{i}\") for i in range(n_qubits))\n\nnoise = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, {\"error_probability\": 0.1})\n\n# The function changes the block in place:\nset_noise(block, noise)\nprint(run(block))\n</code></pre> <pre><code>DensityMatrix([[[ 0.6710+0.0000j,  0.0000+0.1450j,  0.0000+0.2399j,\n                 -0.0518+0.0000j],\n                [ 0.0000-0.1450j,  0.1140+0.0000j,  0.0518+0.0000j,\n                  0.0000+0.0407j],\n                [ 0.0000-0.2399j,  0.0518+0.0000j,  0.1838+0.0000j,\n                  0.0000+0.0397j],\n                [-0.0518+0.0000j,  0.0000-0.0407j,  0.0000-0.0397j,\n                  0.0312+0.0000j]]])\n</code></pre> <p>There is an extra optional argument to specify the type of block we want to apply noise to. E.g., let's say we want to apply noise only to <code>X</code> gates, a <code>target_class</code> argument can be passed with the corresponding block:</p> <pre><code>from qadence import X\nblock = chain(RX(0, \"theta\"), X(0))\nset_noise(block, noise, target_class = X)\n\nfor block in block.blocks:\n    print(block.noise)\n</code></pre> <pre><code>None\nNoise(BitFlip, {'error_probability': 0.1})\n</code></pre>"}]}