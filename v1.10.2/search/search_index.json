{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/blocks/","title":"Block system","text":"<p><code>qadence</code> offers a block-based system to construct quantum circuits in a flexible manner.</p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock","title":"<code>AbstractBlock(tag=None, __array_priority__=1000)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for both primitive and composite blocks.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>A human-readable name attached to the block type. Notice, this is the same for all the class instances so it cannot be used for identifying different blocks</p> <p> TYPE: <code>str</code> </p> <code>qubit_support</code> <p>The qubit support of the block expressed as a tuple of integers</p> <p> TYPE: <code>tuple[int, ...]</code> </p> <code>tag</code> <p>A tag identifying a particular instance of the block which can be used for identification and pretty printing</p> <p> TYPE: <code>str | None</code> </p> <code>eigenvalues</code> <p>The eigenvalues of the matrix representing the block. This is used mainly for primitive blocks and it's needed for generalized parameter shift rule computations. Currently unused.</p> <p> TYPE: <code>list[float] | None</code> </p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.is_identity","title":"<code>is_identity</code>  <code>property</code>","text":"<p>Identity predicate for blocks.</p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.n_qubits","title":"<code>n_qubits()</code>","text":"<p>The number of qubits in the whole system.</p> <p>A block acting on qubit N would has at least n_qubits &gt;= N + 1.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef n_qubits(self) -&gt; int:\n    \"\"\"The number of qubits in the whole system.\n\n    A block acting on qubit N would has at least n_qubits &gt;= N + 1.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.n_supports","title":"<code>n_supports()</code>","text":"<p>The number of qubits the block is acting on.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef n_supports(self) -&gt; int:\n    \"\"\"The number of qubits the block is acting on.\"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.qubit_support","title":"<code>qubit_support()</code>","text":"<p>The indices of the qubit(s) the block is acting on.</p> <p>Qadence uses the ordering [0..,N-1] for qubits.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef qubit_support(self) -&gt; Tuple[int, ...]:\n    \"\"\"The indices of the qubit(s) the block is acting on.\n\n    Qadence uses the ordering [0..,N-1] for qubits.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#primitive-blocks","title":"Primitive blocks","text":""},{"location":"api/blocks/#qadence.blocks.primitive.ControlBlock","title":"<code>ControlBlock(control, target_block, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The abstract ControlBlock.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: tuple[int, ...],\n    target_block: PrimitiveBlock,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.control = control\n    self.blocks = (target_block,)\n    self.target = target_block.qubit_support\n\n    # using tuple expansion because some control operations could\n    # have multiple targets, e.g. CSWAP\n    super().__init__((*control, *self.target), noise=noise)  # target_block.qubit_support[0]))\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricBlock","title":"<code>ParametricBlock(qubit_support, noise=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>Parameterized primitive blocks.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricBlock.num_parameters","title":"<code>num_parameters()</code>  <code>abstractmethod</code>","text":"<p>The number of parameters required by the block.</p> <p>This is a class property since the number of parameters is defined automatically before instantiating the operation. Also, this could correspond to a larger number of actual user-facing parameters since any parameter expression is allowed</p> <p>Examples: - RX operation has 1 parameter - U operation has 3 parameters - HamEvo has 2 parameters (generator and time evolution)</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>@abstractmethod\ndef num_parameters(cls) -&gt; int:\n    \"\"\"The number of parameters required by the block.\n\n    This is a class property since the number of parameters is defined\n    automatically before instantiating the operation. Also, this could\n    correspond to a larger number of actual user-facing parameters\n    since any parameter expression is allowed\n\n    Examples:\n    - RX operation has 1 parameter\n    - U operation has 3 parameters\n    - HamEvo has 2 parameters (generator and time evolution)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricControlBlock","title":"<code>ParametricControlBlock(control, target_block, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The abstract parametrized ControlBlock.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: tuple[int, ...],\n    target_block: ParametricBlock,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.blocks = (target_block,)\n    self.control = control\n    self.parameters = target_block.parameters\n    super().__init__((*control, *target_block.qubit_support), noise=noise)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.PrimitiveBlock","title":"<code>PrimitiveBlock(qubit_support, noise=None)</code>","text":"<p>               Bases: <code>AbstractBlock</code></p> <p>Primitive blocks represent elementary unitary operations.</p> <p>Examples are single/multi-qubit gates or Hamiltonian evolution. See <code>qadence.operations</code> for a full list of primitive blocks.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.PrimitiveBlock.digital_decomposition","title":"<code>digital_decomposition()</code>","text":"<p>Decomposition into purely digital gates.</p> <p>This method returns a decomposition of the Block in a combination of purely digital single-qubit and two-qubit 'gates', by manual/custom knowledge of how this can be done efficiently. :return:</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def digital_decomposition(self) -&gt; AbstractBlock:\n    \"\"\"Decomposition into purely digital gates.\n\n    This method returns a decomposition of the Block in a\n    combination of purely digital single-qubit and two-qubit\n    'gates', by manual/custom knowledge of how this can be done efficiently.\n    :return:\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ProjectorBlock","title":"<code>ProjectorBlock(ket, bra, qubit_support, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The abstract ProjectorBlock.</p> <p>Arguments:</p> <pre><code>ket (str): The ket given as a bitstring.\nbra (str): The bra given as a bitstring.\nqubit_support (int | tuple[int]): The qubit_support of the block.\n</code></pre> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    ket: str,\n    bra: str,\n    qubit_support: int | tuple[int, ...],\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    \"\"\"\n    Arguments:\n\n        ket (str): The ket given as a bitstring.\n        bra (str): The bra given as a bitstring.\n        qubit_support (int | tuple[int]): The qubit_support of the block.\n    \"\"\"\n    if isinstance(qubit_support, int):\n        qubit_support = (qubit_support,)\n    if len(bra) != len(ket):\n        raise ValueError(\n            \"Bra and ket must be bitstrings of same length in the 'Projector' definition.\"\n        )\n    elif len(bra) != len(qubit_support):\n        raise ValueError(\"Bra or ket must be of same length as the 'qubit_support'\")\n    for wf in [bra, ket]:\n        if not all(int(item) == 0 or int(item) == 1 for item in wf):\n            raise ValueError(\n                \"All qubits must be either in the '0' or '1' state\"\n                \" in the 'ProjectorBlock' definition.\"\n            )\n\n    self.ket = ket\n    self.bra = bra\n    super().__init__(qubit_support, noise=noise)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ScaleBlock","title":"<code>ScaleBlock(block, parameter)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>Scale blocks are created when multiplying a block by a number or parameter.</p> <p>Example: <pre><code>from qadence import X\n\nprint(X(0) * 2)\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 X(0)\n</code></pre> </p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(self, block: AbstractBlock, parameter: Any):\n    self.block = block\n    # TODO: more meaningful name like `scale`?\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    super().__init__(block.qubit_support)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.TimeEvolutionBlock","title":"<code>TimeEvolutionBlock(qubit_support, noise=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>Simple time evolution block with time-independent Hamiltonian.</p> <p>This class is just a convenience class which is used to label blocks which contains simple time evolution with time-independent Hamiltonian operators</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#analog-blocks","title":"Analog blocks","text":"<p>To learn how to use analog blocks and how to mix digital &amp; analog blocks, check out the digital-analog section of the documentation.</p> <p>Examples on how to use digital-analog blocks can be found in the *examples folder of the qadence repo:</p> <ul> <li>Fit a simple sinus: <code>examples/digital-analog/fit-sin.py</code></li> <li>Solve a QUBO: <code>examples/digital-analog/qubo.py</code></li> </ul>"},{"location":"api/blocks/#qadence.blocks.analog.AnalogChain","title":"<code>AnalogChain(blocks)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogComposite</code></p> <p>A chain of analog blocks.</p> <p>Needed because analog blocks require stricter validation than the general <code>ChainBlock</code>.</p> <p><code>AnalogChain</code>s can only be constructed from <code>AnalogKron</code> blocks or globally supported, primitive, analog blocks (like <code>InteractionBlock</code>s and <code>ConstantAnalogRotation</code>s).</p> <p>Automatically constructed by the <code>chain</code> function if only analog blocks are given.</p> <p>Example: <pre><code>from qadence import X, chain, AnalogInteraction\n\nb = chain(AnalogInteraction(200), AnalogInteraction(200))\nprint(type(b))  # this is an `AnalogChain`\n\nb = chain(X(0), AnalogInteraction(200))\nprint(type(b))  # this is a general `ChainBlock`\n</code></pre> <pre><code>&lt;class 'qadence.blocks.analog.AnalogChain'&gt;\n&lt;class 'qadence.blocks.composite.ChainBlock'&gt;\n</code></pre> </p> Source code in <code>qadence/blocks/analog.py</code> <pre><code>def __init__(self, blocks: Tuple[AnalogBlock, ...]):\n    \"\"\"A chain of analog blocks.\n\n    Needed because analog blocks require\n    stricter validation than the general `ChainBlock`.\n\n    `AnalogChain`s can only be constructed from `AnalogKron` blocks or\n    _**globally supported**_, primitive, analog blocks (like `InteractionBlock`s and\n    `ConstantAnalogRotation`s).\n\n    Automatically constructed by the [`chain`][qadence.blocks.utils.chain]\n    function if only analog blocks are given.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, chain, AnalogInteraction\n\n    b = chain(AnalogInteraction(200), AnalogInteraction(200))\n    print(type(b))  # this is an `AnalogChain`\n\n    b = chain(X(0), AnalogInteraction(200))\n    print(type(b))  # this is a general `ChainBlock`\n    ```\n    \"\"\"\n    for b in blocks:\n        if not (isinstance(b, AnalogKron) or b.qubit_support.is_global):\n            raise ValueError(\"Only KronBlocks or global blocks can be chain'ed.\")\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.analog.AnalogKron","title":"<code>AnalogKron(blocks, interaction=Interaction.NN)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogComposite</code></p> <p>Stack analog blocks vertically (i.e. in time).</p> <p>Needed because analog require stricter validation than the general <code>KronBlock</code>.</p> <p><code>AnalogKron</code>s can only be constructed from non-global, analog blocks with the same duration.</p> Source code in <code>qadence/blocks/analog.py</code> <pre><code>def __init__(self, blocks: Tuple[AnalogBlock, ...], interaction: Interaction = Interaction.NN):\n    \"\"\"Stack analog blocks vertically (i.e. in time).\n\n    Needed because analog require\n    stricter validation than the general `KronBlock`.\n\n    `AnalogKron`s can only be constructed from _**non-global**_, analog blocks\n    with the _**same duration**_.\n    \"\"\"\n    if len(blocks) == 0:\n        raise NotImplementedError(\"Empty KronBlocks not supported\")\n\n    self.blocks = blocks\n    self.interaction = interaction\n\n    qubit_support = QubitSupport()\n    duration = blocks[0].duration\n    for b in blocks:\n        if not isinstance(b, AnalogBlock):\n            raise ValueError(\"Can only kron `AnalgoBlock`s with other `AnalgoBlock`s.\")\n\n        if b.qubit_support == QubitSupport(\"global\"):\n            raise ValueError(\"Blocks with global support cannot be kron'ed.\")\n\n        if not qubit_support.is_disjoint(b.qubit_support):\n            raise ValueError(\"Make sure blocks act on distinct qubits!\")\n\n        if not np.isclose(evaluate(duration), evaluate(b.duration)):\n            raise ValueError(\"Kron'ed blocks have to have same duration.\")\n\n        qubit_support += b.qubit_support\n\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.analog.ConstantAnalogRotation","title":"<code>ConstantAnalogRotation(tag=None, __array_priority__=1000, _eigenvalues_generator=None, parameters=ParamMap(alpha=0.0, duration=1000.0, omega=0.0, delta=0.0, phase=0.0), qubit_support=QubitSupport('global'), add_pattern=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogBlock</code></p> <p>Implements a constant analog rotation with interaction dictated by the chosen Hamiltonian.</p> <pre><code>H/h = \u2211\u1d62(\u03a9/2 cos(\u03c6)*X\u1d62 - sin(\u03c6)*Y\u1d62 - \u03b4n\u1d62) + H\u1d62\u2099\u209c.\n</code></pre> <p>To construct this block you can use of the following convenience wrappers: - The general rotation operation <code>AnalogRot</code> - Shorthands for rotatins around an axis:   <code>AnalogRX</code>,   <code>AnalogRY</code>,   <code>AnalogRZ</code></p> <p>WARNING: do not use <code>ConstantAnalogRotation</code> with <code>alpha</code> as differentiable parameter - use the convenience wrappers mentioned above.</p>"},{"location":"api/blocks/#qadence.blocks.analog.InteractionBlock","title":"<code>InteractionBlock(tag=None, __array_priority__=1000, _eigenvalues_generator=None, parameters=ParamMap(duration=1000.0), qubit_support=QubitSupport('global'), add_pattern=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogBlock</code></p> <p>Free-evolution for the Hamiltonian interaction term of a register of qubits.</p> <p>In real interacting quantum devices, it means letting the system evolve freely according to the time-dependent Schrodinger equation. With emulators, this block is translated to an appropriate interaction Hamiltonian, for example, an Ising interaction</p> <pre><code>H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c\n</code></pre> <p>or an XY-interaction</p> <pre><code>H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2083/r\u2c7c\u2c7c\u00b3 (X\u1d62X\u2c7c + Z\u1d62Z\u2c7c)\n</code></pre> <p>with <code>n\u1d62 = (1-Z\u1d62)/2</code>.</p> <p>To construct, use the <code>AnalogInteraction</code> function.</p>"},{"location":"api/blocks/#composite-blocks","title":"Composite blocks","text":""},{"location":"api/blocks/#qadence.blocks.utils.chain","title":"<code>chain(*args)</code>","text":"<p>Chain blocks sequentially.</p> <p>On digital backends this can be interpreted loosely as a matrix mutliplication of blocks. In the analog case it chains blocks in time.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to chain. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator, List[AbstractBlock]]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>ChainBlock</p> <p>Example: <pre><code>from qadence import X, Y, chain\n\nb = chain(X(0), Y(0))\n\n# or use a generator\nb = chain(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>ChainBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def chain(*args: Union[AbstractBlock, Generator, List[AbstractBlock]]) -&gt; ChainBlock:\n    \"\"\"Chain blocks sequentially.\n\n    On digital backends this can be interpreted\n    loosely as a matrix mutliplication of blocks. In the analog case it chains\n    blocks in time.\n\n    Arguments:\n        *args: Blocks to chain. Can also be a generator.\n\n    Returns:\n        ChainBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, chain\n\n    b = chain(X(0), Y(0))\n\n    # or use a generator\n    b = chain(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    # ugly hack to use `AnalogChain` if we are dealing only with analog blocks\n    if len(args) and all(\n        isinstance(a, AnalogBlock) or isinstance(a, AnalogComposite) for a in args\n    ):\n        return analog_chain(*args)  # type: ignore[return-value,arg-type]\n    return _construct(ChainBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.utils.kron","title":"<code>kron(*args)</code>","text":"<p>Stack blocks vertically.</p> <p>On digital backends this can be intepreted loosely as a kronecker product of blocks. In the analog case it executes blocks parallel in time.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to kron. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>KronBlock</p> <p>Example: <pre><code>from qadence import X, Y, kron\n\nb = kron(X(0), Y(1))\n\n# or use a generator\nb = kron(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def kron(*args: Union[AbstractBlock, Generator]) -&gt; KronBlock:\n    \"\"\"Stack blocks vertically.\n\n    On digital backends this can be intepreted\n    loosely as a kronecker product of blocks. In the analog case it executes\n    blocks parallel in time.\n\n    Arguments:\n        *args: Blocks to kron. Can also be a generator.\n\n    Returns:\n        KronBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, kron\n\n    b = kron(X(0), Y(1))\n\n    # or use a generator\n    b = kron(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    # ugly hack to use `AnalogKron` if we are dealing only with analog blocks\n    if len(args) and all(\n        isinstance(a, AnalogBlock) or isinstance(a, AnalogComposite) for a in args\n    ):\n        return analog_kron(*args)  # type: ignore[return-value,arg-type]\n    return _construct(KronBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.utils.add","title":"<code>add(*args)</code>","text":"<p>Sums blocks.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to add. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>AddBlock</code> <p>AddBlock</p> <p>Example: <pre><code>from qadence import X, Y, add\n\nb = add(X(0), Y(0))\n\n# or use a generator\nb = add(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def add(*args: Union[AbstractBlock, Generator]) -&gt; AddBlock:\n    \"\"\"Sums blocks.\n\n    Arguments:\n        *args: Blocks to add. Can also be a generator.\n\n    Returns:\n        AddBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, add\n\n    b = add(X(0), Y(0))\n\n    # or use a generator\n    b = add(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    return _construct(AddBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.AddBlock","title":"<code>AddBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Adds blocks.</p> <p>Constructed via <code>add</code>.</p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.ChainBlock","title":"<code>ChainBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Chains blocks sequentially.</p> <p>Constructed via <code>chain</code></p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.CompositeBlock","title":"<code>CompositeBlock(tag=None, __array_priority__=1000)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractBlock</code></p> <p>Block which composes multiple blocks into one larger block (which can again be composed).</p> <p>Composite blocks are constructed via <code>chain</code>, <code>kron</code>, and <code>add</code>.</p>"},{"location":"api/blocks/#qadence.blocks.composite.KronBlock","title":"<code>KronBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Stacks blocks horizontally.</p> <p>Constructed via <code>kron</code>.</p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    if len(blocks) == 0:\n        raise NotImplementedError(\"Empty KronBlocks not supported\")\n\n    qubit_support = QubitSupport()\n    for b in blocks:\n        assert (\n            QubitSupportType.GLOBAL,\n        ) != b.qubit_support, \"Blocks with global support cannot be kron'ed.\"\n        assert qubit_support.is_disjoint(\n            b.qubit_support\n        ), \"Make sure blocks act on distinct qubits!\"\n        qubit_support += b.qubit_support\n\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#converting-blocks-to-matrices","title":"Converting blocks to matrices","text":""},{"location":"api/blocks/#qadence.blocks.block_to_tensor._controlled_block_with_params","title":"<code>_controlled_block_with_params(block)</code>","text":"<p>Redefines parameterized/non-parameterized controlled block.</p> PARAMETER DESCRIPTION <code>block</code> <p>original controlled rotation block</p> <p> TYPE: <code>ParametricControlBlock</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>redefined controlled rotation block</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>dict[str, Tensor]</code> <p>dict with new parameters which are added</p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def _controlled_block_with_params(\n    block: ParametricControlBlock | ControlBlock,\n) -&gt; tuple[AbstractBlock, dict[str, torch.Tensor]]:\n    \"\"\"Redefines parameterized/non-parameterized controlled block.\n\n    Args:\n        block (ParametricControlBlock): original controlled rotation block\n\n    Returns:\n        AbstractBlock: redefined controlled rotation block\n        dict with new parameters which are added\n    \"\"\"\n    from qadence.operations import I\n    from qadence.utils import P1\n\n    # redefine controlled rotation block in a way suitable for matrix evaluation\n    control = block.qubit_support[:-1]\n    target = block.qubit_support[-1]\n    p1 = kron(P1(qubit) for qubit in control)\n    p0 = I(control[0]) - p1\n    c_block = kron(p0, I(target)) + kron(p1, block.blocks[0])\n\n    uuid_expr = uuid_to_expression(c_block)\n    newparams = {\n        stringify(expr): evaluate(expr, {}, as_torch=True)\n        for uuid, expr in uuid_expr.items()\n        if expr.is_number\n    }\n\n    return c_block, newparams\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.block_to_tensor._fill_identities","title":"<code>_fill_identities(block_mat, qubit_support, full_qubit_support, diag_only=False, endianness=Endianness.BIG, device=None)</code>","text":"<p>Returns a Kronecker product of a block matrix with identities.</p> <p>The block matrix can defined on a subset of qubits and the full matrix is filled with identities acting on the unused qubits.</p> PARAMETER DESCRIPTION <code>block_mat</code> <p>matrix of an arbitrary gate</p> <p> TYPE: <code>Tensor</code> </p> <code>qubit_support</code> <p>qubit support of <code>block_mat</code></p> <p> TYPE: <code>tuple</code> </p> <code>full_qubit_support</code> <p>full qubit support of the circuit</p> <p> TYPE: <code>tuple</code> </p> <code>diag_only</code> <p>Use diagonals only</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>torch.Tensor: augmented matrix with dimensions (2nqubits, 2nqubits)</p> <code>Tensor</code> <p>or a tensor (2**n_qubits) if diag_only</p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def _fill_identities(\n    block_mat: torch.Tensor,\n    qubit_support: tuple,\n    full_qubit_support: tuple | list,\n    diag_only: bool = False,\n    endianness: Endianness = Endianness.BIG,\n    device: torch.device | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Returns a Kronecker product of a block matrix with identities.\n\n    The block matrix can defined on a subset of qubits and the full matrix is\n    filled with identities acting on the unused qubits.\n\n    Args:\n        block_mat (torch.Tensor): matrix of an arbitrary gate\n        qubit_support (tuple): qubit support of `block_mat`\n        full_qubit_support (tuple): full qubit support of the circuit\n        diag_only (bool): Use diagonals only\n\n    Returns:\n        torch.Tensor: augmented matrix with dimensions (2**nqubits, 2**nqubits)\n        or a tensor (2**n_qubits) if diag_only\n    \"\"\"\n    qubit_support = tuple(sorted(qubit_support))\n    block_mat = block_mat.to(device)\n    mat = IMAT.to(device) if qubit_support[0] != full_qubit_support[0] else block_mat\n    if diag_only:\n        mat = torch.diag(mat.squeeze(0))\n    for i in full_qubit_support[1:]:\n        if i == qubit_support[0]:\n            other = torch.diag(block_mat.squeeze(0)) if diag_only else block_mat\n            if endianness == Endianness.LITTLE:\n                mat = torch.kron(other, mat)\n            else:\n                mat = torch.kron(mat.contiguous(), other.contiguous())\n        elif i not in qubit_support:\n            other = torch.diag(IMAT.squeeze(0).to(device)) if diag_only else IMAT.to(device)\n            if endianness == Endianness.LITTLE:\n                mat = torch.kron(other.contiguous(), mat.contiguous())\n            else:\n                mat = torch.kron(mat.contiguous(), other.contiguous())\n\n    return mat\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.block_to_tensor._phase_matrix","title":"<code>_phase_matrix(theta)</code>","text":"<p>Args:</p> <pre><code>theta(torch.Tensor): input parameter\n</code></pre> <p>Returns:     torch.Tensor: a batch of gates after applying theta</p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def _phase_matrix(theta: torch.Tensor | TNumber) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n\n        theta(torch.Tensor): input parameter\n    Returns:\n        torch.Tensor: a batch of gates after applying theta\n    \"\"\"\n    exp_t = torch.exp(1j * theta).unsqueeze(1).unsqueeze(2)\n    exp_t = exp_t.repeat((1, 2, 2))\n    return 0.5 * (IMAT + ZMAT) + exp_t * 0.5 * (IMAT - ZMAT)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.block_to_tensor._rot_matrices","title":"<code>_rot_matrices(theta, generator)</code>","text":"<p>Args:</p> <pre><code>theta(torch.Tensor): input parameter\ngenerator(torch.Tensor): the tensor of the generator\n</code></pre> <p>Returns:     torch.Tensor: a batch of gates after applying theta</p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def _rot_matrices(theta: torch.Tensor, generator: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n\n        theta(torch.Tensor): input parameter\n        generator(torch.Tensor): the tensor of the generator\n    Returns:\n        torch.Tensor: a batch of gates after applying theta\n    \"\"\"\n    batch_size = theta.size(0)\n\n    cos_t = torch.cos(theta / 2).unsqueeze(1).unsqueeze(2)\n    cos_t = cos_t.repeat((1, 2, 2))\n    sin_t = torch.sin(theta / 2).unsqueeze(1).unsqueeze(2)\n    sin_t = sin_t.repeat((1, 2, 2))\n\n    batch_imat = IMAT.repeat(batch_size, 1, 1)\n    batch_generator = generator.repeat(batch_size, 1, 1)\n\n    return cos_t * batch_imat - 1j * sin_t * batch_generator\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.block_to_tensor._swap_block","title":"<code>_swap_block(block)</code>","text":"<p>Redefines SWAP block.</p> PARAMETER DESCRIPTION <code>block</code> <p>original SWAP block</p> <p> TYPE: <code>AbstractBlock</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>redefined SWAP block</p> <p> TYPE: <code>AbstractBlock</code> </p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def _swap_block(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Redefines SWAP block.\n\n    Args:\n        block (AbstractBlock): original SWAP block\n\n    Returns:\n        AbstractBlock: redefined SWAP block\n    \"\"\"\n    from qadence.operations import CNOT\n\n    # redefine controlled rotation block in a way suitable for matrix evaluation\n    control = block.qubit_support[0]\n    target = block.qubit_support[1]\n    swap_block = chain(CNOT(control, target), CNOT(target, control), CNOT(control, target))\n\n    return swap_block\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.block_to_tensor._u_matrix","title":"<code>_u_matrix(theta)</code>","text":"<p>Args:</p> <pre><code>theta(tuple[torch.Tensor]): tuple of torch Tensor with 3 elements\n    per each parameter of the arbitrary rotation\n</code></pre> <p>Returns:     torch.Tensor: matrix corresponding to the U gate after applying theta</p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def _u_matrix(theta: tuple[torch.Tensor, ...]) -&gt; torch.Tensor:\n    \"\"\"\n    Args:\n\n        theta(tuple[torch.Tensor]): tuple of torch Tensor with 3 elements\n            per each parameter of the arbitrary rotation\n    Returns:\n        torch.Tensor: matrix corresponding to the U gate after applying theta\n    \"\"\"\n    z_phi = _rot_matrices(theta[0], OPERATIONS_DICT[\"Z\"])\n    y_theta = _rot_matrices(theta[1], OPERATIONS_DICT[\"Y\"])\n    z_omega = _rot_matrices(theta[2], OPERATIONS_DICT[\"Z\"])\n\n    res = torch.matmul(y_theta, z_phi)\n    res = torch.matmul(z_omega, res)\n    return res\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.block_to_tensor.block_to_tensor","title":"<code>block_to_tensor(block, values={}, qubit_support=None, use_full_support=True, tensor_type=TensorType.DENSE, endianness=Endianness.BIG, device=None)</code>","text":"<p>Convert a block into a torch tensor.</p> PARAMETER DESCRIPTION <code>block</code> <p>The block to convert.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>values</code> <p>A optional dict with values for parameters.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>qubit_support</code> <p>The qubit_support of the block.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>use_full_support</code> <p>True infers the total number of qubits.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>tensor_type</code> <p>the target tensor type.</p> <p> TYPE: <code>TensorType</code> DEFAULT: <code>DENSE</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence import hea, hamiltonian_factory, Z, block_to_tensor\n\nblock = hea(2,2)\nprint(block_to_tensor(block))\n\n# In case you have a diagonal observable, you can use\nobs = hamiltonian_factory(2, detuning = Z)\nprint(block_to_tensor(obs, tensor_type=\"SparseDiagonal\"))\n</code></pre> <pre><code>tensor([[[ 0.3052+0.2510j, -0.6465-0.4229j, -0.3594+0.0514j, -0.1261-0.3151j],\n         [ 0.4255-0.3326j,  0.3136-0.0212j, -0.3918-0.5166j, -0.4298+0.0669j],\n         [-0.1877-0.6846j, -0.1549-0.2858j,  0.4008-0.0887j, -0.0860-0.4631j],\n         [ 0.0622-0.2108j, -0.4116+0.1713j, -0.0676-0.5241j,  0.6449+0.2404j]]],\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\ntensor(indices=tensor([[0, 3],\n                       [0, 3]]),\n       values=tensor([ 2.+0.j, -2.+0.j]),\n       size=(4, 4), nnz=2, layout=torch.sparse_coo)\n</code></pre> </p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def block_to_tensor(\n    block: AbstractBlock,\n    values: dict[str, TNumber | torch.Tensor] = {},\n    qubit_support: tuple | None = None,\n    use_full_support: bool = True,\n    tensor_type: TensorType = TensorType.DENSE,\n    endianness: Endianness = Endianness.BIG,\n    device: torch.device = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a block into a torch tensor.\n\n    Arguments:\n        block (AbstractBlock): The block to convert.\n        values (dict): A optional dict with values for parameters.\n        qubit_support (tuple): The qubit_support of the block.\n        use_full_support (bool): True infers the total number of qubits.\n        tensor_type (TensorType): the target tensor type.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import hea, hamiltonian_factory, Z, block_to_tensor\n\n    block = hea(2,2)\n    print(block_to_tensor(block))\n\n    # In case you have a diagonal observable, you can use\n    obs = hamiltonian_factory(2, detuning = Z)\n    print(block_to_tensor(obs, tensor_type=\"SparseDiagonal\"))\n    ```\n    \"\"\"\n\n    # FIXME: default use_full_support to False. In general, it would\n    # be more efficient to do that, and make sure that computations such\n    # as observables only do the matmul of the size of the qubit support.\n\n    if tensor_type == TensorType.DENSE:\n        from qadence.blocks import embedding\n\n        (ps, embed) = embedding(block)\n        return _block_to_tensor_embedded(\n            block,\n            embed(ps, values),\n            qubit_support,\n            use_full_support,\n            endianness=endianness,\n            device=device,\n        )\n\n    elif tensor_type == TensorType.SPARSEDIAGONAL:\n        t = block_to_diagonal(block, endianness=endianness)\n        indices, values, size = torch.nonzero(t), t[t != 0], len(t)\n        indices = torch.stack((indices.flatten(), indices.flatten()))\n        return torch.sparse_coo_tensor(indices, values, (size, size))\n</code></pre>"},{"location":"api/constructors/","title":"Constructors for common quantum circuits","text":""},{"location":"api/constructors/#qadence.constructors.feature_maps.exp_fourier_feature_map","title":"<code>exp_fourier_feature_map(n_qubits, support=None, param='x', feature_range=None)</code>","text":"<p>Exponential fourier feature map.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the feature</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>qubit support</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>param</code> <p>name of feature <code>Parameter</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'x'</code> </p> <code>feature_range</code> <p>min and max value of the feature, as floats in a Tuple</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/constructors/feature_maps.py</code> <pre><code>def exp_fourier_feature_map(\n    n_qubits: int,\n    support: tuple[int, ...] = None,\n    param: str = \"x\",\n    feature_range: tuple[float, float] = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Exponential fourier feature map.\n\n    Args:\n        n_qubits: number of qubits in the feature\n        support: qubit support\n        param: name of feature `Parameter`\n        feature_range: min and max value of the feature, as floats in a Tuple\n    \"\"\"\n\n    if feature_range is None:\n        feature_range = (0.0, 2.0**n_qubits)\n\n    support = tuple(range(n_qubits)) if support is None else support\n    hlayer = kron(H(qubit) for qubit in support)\n    rlayer = feature_map(\n        n_qubits,\n        support=support,\n        param=param,\n        op=RZ,\n        fm_type=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.EXP,\n        feature_range=feature_range,\n        target_range=(0.0, 2 * PI),\n    )\n    rlayer.tag = None\n    return tag(chain(hlayer, rlayer), f\"ExpFourierFM({param})\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.feature_maps.feature_map","title":"<code>feature_map(n_qubits, support=None, param='phi', op=RX, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT, feature_range=None, target_range=None, multiplier=None, param_prefix=None)</code>","text":"<p>Construct a feature map of a given type.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Number of qubits the feature map covers. Results in <code>support=range(n_qubits)</code>.</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>Puts one feature-encoding rotation gate on every qubit in <code>support</code>. n_qubits in this case specifies the total overall qubits of the circuit, which may be wider than the support itself, but not narrower.</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>param</code> <p>Parameter of the feature map; you can pass a string or Parameter; it will be set as non-trainable (FeatureParameter) regardless.</p> <p> TYPE: <code>Parameter | str</code> DEFAULT: <code>'phi'</code> </p> <code>op</code> <p>Rotation operation of the feature map; choose from RX, RY, RZ or PHASE.</p> <p> TYPE: <code>RotationTypes</code> DEFAULT: <code>RX</code> </p> <code>fm_type</code> <p>Basis set for data encoding; choose from <code>BasisSet.FOURIER</code> for Fourier encoding, or <code>BasisSet.CHEBYSHEV</code> for Chebyshev polynomials of the first kind.</p> <p> TYPE: <code>BasisSet | Callable | str</code> DEFAULT: <code>FOURIER</code> </p> <code>reupload_scaling</code> <p>how the feature map scales the data that is re-uploaded for each qubit. choose from <code>ReuploadScaling</code> enumeration or provide your own function with a single int as input and int or float as output.</p> <p> TYPE: <code>ReuploadScaling | Callable | str</code> DEFAULT: <code>CONSTANT</code> </p> <code>feature_range</code> <p>range of data that the input data provided comes from. Used to map input data to the correct domain of the feature-encoding function.</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> <code>target_range</code> <p>range of data the data encoder assumes as the natural range. For example, in Chebyshev polynomials it is (-1, 1), while for Fourier it may be chosen as (0, 2*PI). Used to map data to the correct domain of the feature-encoding function.</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> <code>multiplier</code> <p>overall multiplier; this is useful for reuploading the feature map serially with different scalings; can be a number or parameter/expression.</p> <p> TYPE: <code>Parameter | TParameter | None</code> DEFAULT: <code>None</code> </p> <code>param_prefix</code> <p>string prefix to create trainable parameters multiplying the feature parameter inside the feature-encoding function. Note that currently this does not take into account the domain of the feature-encoding function.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <p>Example: <pre><code>from qadence import feature_map, BasisSet, ReuploadScaling\n\nfm = feature_map(3, fm_type=BasisSet.FOURIER)\nprint(f\"{fm = }\")\n\nfm = feature_map(3, fm_type=BasisSet.CHEBYSHEV)\nprint(f\"{fm = }\")\n\nfm = feature_map(3, fm_type=BasisSet.FOURIER, reupload_scaling = ReuploadScaling.TOWER)\nprint(f\"{fm = }\")\n</code></pre> <pre><code>fm = KronBlock(0,1,2) [tag: Constant Fourier FM]\n\u251c\u2500\u2500 RX(0) [params: ['phi']]\n\u251c\u2500\u2500 RX(1) [params: ['phi']]\n\u2514\u2500\u2500 RX(2) [params: ['phi']]\nfm = KronBlock(0,1,2) [tag: Constant Chebyshev FM]\n\u251c\u2500\u2500 RX(0) [params: ['acos(phi)']]\n\u251c\u2500\u2500 RX(1) [params: ['acos(phi)']]\n\u2514\u2500\u2500 RX(2) [params: ['acos(phi)']]\nfm = KronBlock(0,1,2) [tag: Tower Fourier FM]\n\u251c\u2500\u2500 RX(0) [params: ['1_0*phi']]\n\u251c\u2500\u2500 RX(1) [params: ['2_0*phi']]\n\u2514\u2500\u2500 RX(2) [params: ['3_0*phi']]\n</code></pre> </p> Source code in <code>qadence/constructors/feature_maps.py</code> <pre><code>def feature_map(\n    n_qubits: int,\n    support: tuple[int, ...] | None = None,\n    param: Parameter | str = \"phi\",\n    op: RotationTypes = RX,\n    fm_type: BasisSet | Callable | str = BasisSet.FOURIER,\n    reupload_scaling: ReuploadScaling | Callable | str = ReuploadScaling.CONSTANT,\n    feature_range: tuple[float, float] | None = None,\n    target_range: tuple[float, float] | None = None,\n    multiplier: Parameter | TParameter | None = None,\n    param_prefix: str | None = None,\n) -&gt; KronBlock:\n    \"\"\"Construct a feature map of a given type.\n\n    Arguments:\n        n_qubits: Number of qubits the feature map covers. Results in `support=range(n_qubits)`.\n        support: Puts one feature-encoding rotation gate on every qubit in `support`. n_qubits in\n            this case specifies the total overall qubits of the circuit, which may be wider than the\n            support itself, but not narrower.\n        param: Parameter of the feature map; you can pass a string or Parameter;\n            it will be set as non-trainable (FeatureParameter) regardless.\n        op: Rotation operation of the feature map; choose from RX, RY, RZ or PHASE.\n        fm_type: Basis set for data encoding; choose from `BasisSet.FOURIER` for Fourier\n            encoding, or `BasisSet.CHEBYSHEV` for Chebyshev polynomials of the first kind.\n        reupload_scaling: how the feature map scales the data that is re-uploaded for each qubit.\n            choose from `ReuploadScaling` enumeration or provide your own function with a single\n            int as input and int or float as output.\n        feature_range: range of data that the input data provided comes from. Used to map input data\n            to the correct domain of the feature-encoding function.\n        target_range: range of data the data encoder assumes as the natural range. For example,\n            in Chebyshev polynomials it is (-1, 1), while for Fourier it may be chosen as (0, 2*PI).\n            Used to map data to the correct domain of the feature-encoding function.\n        multiplier: overall multiplier; this is useful for reuploading the feature map serially with\n            different scalings; can be a number or parameter/expression.\n        param_prefix: string prefix to create trainable parameters multiplying the feature parameter\n            inside the feature-encoding function. Note that currently this does not take into\n            account the domain of the feature-encoding function.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import feature_map, BasisSet, ReuploadScaling\n\n    fm = feature_map(3, fm_type=BasisSet.FOURIER)\n    print(f\"{fm = }\")\n\n    fm = feature_map(3, fm_type=BasisSet.CHEBYSHEV)\n    print(f\"{fm = }\")\n\n    fm = feature_map(3, fm_type=BasisSet.FOURIER, reupload_scaling = ReuploadScaling.TOWER)\n    print(f\"{fm = }\")\n    ```\n    \"\"\"\n\n    # Process input\n    if support is None:\n        support = tuple(range(n_qubits))\n    elif len(support) != n_qubits:\n        raise ValueError(\"Wrong qubit support supplied\")\n\n    if op not in ROTATIONS:\n        raise ValueError(\n            f\"Operation {op} not supported. \"\n            f\"Please provide one from {[rot.__name__ for rot in ROTATIONS]}.\"\n        )\n\n    scaled_fparam = fm_parameter_scaling(\n        fm_type, param, feature_range=feature_range, target_range=target_range\n    )\n\n    transform_func = fm_parameter_func(fm_type)\n\n    basis_tag = fm_type.value if isinstance(fm_type, BasisSet) else str(fm_type)\n    rs_func, rs_tag = fm_reupload_scaling_fn(reupload_scaling)\n\n    # Set overall multiplier\n    multiplier = 1 if multiplier is None else Parameter(multiplier)\n\n    # Build feature map\n    op_list = []\n    fparam = scaled_fparam\n    for i, qubit in enumerate(support):\n        if param_prefix is not None:\n            train_param = VariationalParameter(param_prefix + f\"_{i}\")\n            fparam = train_param * scaled_fparam\n        op_list.append(op(qubit, multiplier * rs_func(i) * transform_func(fparam)))\n    fm = kron(*op_list)\n\n    fm.tag = rs_tag + \" \" + basis_tag + \" FM\"\n\n    return fm\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea._entangler","title":"<code>_entangler(control, target, param_str, op=CNOT)</code>","text":"<p>Create a 2-qubit operation for the digital HEA.</p> PARAMETER DESCRIPTION <code>control</code> <p>control qubit index</p> <p> TYPE: <code>int</code> </p> <code>target</code> <p>target qubit index</p> <p> TYPE: <code>int</code> </p> <code>param_str</code> <p>base for naming the variational parameter if parametric block</p> <p> TYPE: <code>str</code> </p> <code>op</code> <p>2-qubit operation (CNOT, CZ, CRX, CRY, CRZ or CPHASE)</p> <p> TYPE: <code>Type[DigitalEntanglers]</code> DEFAULT: <code>CNOT</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The 2-qubit digital entangler for the HEA.</p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def _entangler(\n    control: int,\n    target: int,\n    param_str: str,\n    op: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Create a 2-qubit operation for the digital HEA.\n\n    Args:\n        control (int): control qubit index\n        target (int): target qubit index\n        param_str (str): base for naming the variational parameter if parametric block\n        op (Type[DigitalEntanglers]): 2-qubit operation (CNOT, CZ, CRX, CRY, CRZ or CPHASE)\n\n    Returns:\n        The 2-qubit digital entangler for the HEA.\n    \"\"\"\n    if op in [CNOT, CZ]:\n        return op(control, target)  # type: ignore\n    elif op in [CRZ, CRY, CRX, CPHASE]:\n        return op(control, target, param_str)  # type: ignore\n    else:\n        raise ValueError(\"Provided entangler not accepted for digital ansatz\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea._entanglers_analog","title":"<code>_entanglers_analog(depth, param_prefix='theta', entangler=None)</code>","text":"<p>Creates the entanglers for the sDAQC.</p> PARAMETER DESCRIPTION <code>depth</code> <p>The number of layers of entanglers.</p> <p> TYPE: <code>int</code> </p> <code>param_prefix</code> <p>The prefix for the parameter names.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>entangler</code> <p>The entangler to use.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>A list of analog entanglers for sDAQC HEA.</p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def _entanglers_analog(\n    depth: int,\n    param_prefix: str = \"theta\",\n    entangler: AbstractBlock | None = None,\n) -&gt; list[AbstractBlock]:\n    \"\"\"\n    Creates the entanglers for the sDAQC.\n\n    Args:\n        depth: The number of layers of entanglers.\n        param_prefix: The prefix for the parameter names.\n        entangler: The entangler to use.\n\n    Returns:\n        A list of analog entanglers for sDAQC HEA.\n    \"\"\"\n    return [HamEvo(entangler, param_prefix + f\"_t_{d}\") for d in range(depth)]  # type: ignore\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea._entanglers_digital","title":"<code>_entanglers_digital(n_qubits, depth, param_prefix='theta', support=None, periodic=False, entangler=CNOT)</code>","text":"<p>Creates the layers of digital entangling operations in an HEA.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>The entanglers for the digital Hardware Efficient Ansatz (HEA).</p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def _entanglers_digital(\n    n_qubits: int,\n    depth: int,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    periodic: bool = False,\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; list[AbstractBlock]:\n    \"\"\"Creates the layers of digital entangling operations in an HEA.\n\n    Args:\n        n_qubits (int): number of qubits in the block.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        support (tuple): qubit indices where the HEA is applied.\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n\n    Returns:\n        The entanglers for the digital Hardware Efficient Ansatz (HEA).\n    \"\"\"\n    if support is None:\n        support = tuple(range(n_qubits))\n    iterator = itertools.count()\n    ent_list: list[AbstractBlock] = []\n    for d in range(depth):\n        ents = []\n        ents.append(\n            kron(\n                _entangler(\n                    control=support[n],\n                    target=support[n + 1],\n                    param_str=param_prefix + f\"_ent_{next(iterator)}\",\n                    op=entangler,\n                )\n                for n in range(n_qubits)\n                if not n % 2 and n &lt; n_qubits - 1\n            )\n        )\n        if n_qubits &gt; 2:\n            ents.append(\n                kron(\n                    _entangler(\n                        control=support[n],\n                        target=support[(n + 1) % n_qubits],\n                        param_str=param_prefix + f\"_ent_{next(iterator)}\",\n                        op=entangler,\n                    )\n                    for n in range(n_qubits - (not periodic))\n                    if n % 2\n                )\n            )\n        ent_list.append(chain(*ents))\n    return ent_list\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea._rotations_digital","title":"<code>_rotations_digital(n_qubits, depth, param_prefix='theta', support=None, operations=[RX, RY, RX])</code>","text":"<p>Creates the layers of single qubit rotations in an HEA.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits in the HEA.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>The number of layers of rotations.</p> <p> TYPE: <code>int</code> </p> <code>param_prefix</code> <p>The prefix for the parameter names.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>The qubits to apply the rotations to.</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>operations</code> <p>The operations to apply the rotations with.</p> <p> TYPE: <code>list[Type[AbstractBlock]]</code> DEFAULT: <code>[RX, RY, RX]</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>A list of digital rotation layers in the HEA.</p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def _rotations_digital(\n    n_qubits: int,\n    depth: int,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    operations: list[Type[AbstractBlock]] = [RX, RY, RX],\n) -&gt; list[AbstractBlock]:\n    \"\"\"Creates the layers of single qubit rotations in an HEA.\n\n    Args:\n        n_qubits: The number of qubits in the HEA.\n        depth: The number of layers of rotations.\n        param_prefix: The prefix for the parameter names.\n        support: The qubits to apply the rotations to.\n        operations: The operations to apply the rotations with.\n\n    Returns:\n        A list of digital rotation layers in the HEA.\n    \"\"\"\n    if support is None:\n        support = tuple(range(n_qubits))\n    iterator = itertools.count()\n    rot_list: list[AbstractBlock] = []\n    for d in range(depth):\n        rots = [\n            kron(\n                gate(support[n], param_prefix + f\"_{next(iterator)}\")  # type: ignore [arg-type]\n                for n in range(n_qubits)\n            )\n            for gate in operations\n        ]\n        rot_list.append(chain(*rots))\n    return rot_list\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea.hea","title":"<code>hea(n_qubits, depth=1, param_prefix='theta', support=None, strategy=Strategy.DIGITAL, **strategy_args)</code>","text":"<p>Factory function for the Hardware Efficient Ansatz (HEA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the circuit</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the HEA is applied</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>strategy</code> <p>Strategy for the ansatz. One of the Strategy variants.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>**strategy_args</code> <p>see below</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> PARAMETER DESCRIPTION <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer. Valid for Digital and DigitalAnalog HEA.</p> <p> TYPE: <code>list</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c. Valid for only for Digital HEA.</p> <p> TYPE: <code>bool</code> </p> <code>entangler</code> <ul> <li>Digital: 2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational parameters on the rotation angles.</li> <li>SDAQC | Analog: Hamiltonian generator for the analog entangling layer. Defaults to global ZZ Hamiltonian. Time parameter is considered variational.</li> </ul> <p> TYPE: <code>AbstractBlock</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The Hardware Efficient Ansatz (HEA) circuit.</p> <p>Examples: <pre><code>from qadence import RZ, RX\nfrom qadence import hea\n\n# create the circuit\nn_qubits, depth = 2, 4\nansatz = hea(\n    n_qubits=n_qubits,\n    depth=depth,\n    strategy=\"sDAQC\",\n    operations=[RZ,RX,RZ]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def hea(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    strategy: Strategy = Strategy.DIGITAL,\n    **strategy_args: Any,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Factory function for the Hardware Efficient Ansatz (HEA).\n\n    Args:\n        n_qubits: number of qubits in the circuit\n        depth: number of layers of the HEA\n        param_prefix: the base name of the variational parameters\n        support: qubit indices where the HEA is applied\n        strategy: Strategy for the ansatz. One of the Strategy variants.\n        **strategy_args: see below\n\n    Keyword Arguments:\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer. Valid for\n            Digital and DigitalAnalog HEA.\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c. Valid for only\n            for Digital HEA.\n        entangler (AbstractBlock):\n            - Digital: 2-qubit entangling operation. Supports CNOT, CZ,\n            CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational\n            parameters on the rotation angles.\n            - SDAQC | Analog: Hamiltonian generator for the\n            analog entangling layer. Defaults to global ZZ Hamiltonian.\n            Time parameter is considered variational.\n\n    Returns:\n        The Hardware Efficient Ansatz (HEA) circuit.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import RZ, RX\n    from qadence import hea\n\n    # create the circuit\n    n_qubits, depth = 2, 4\n    ansatz = hea(\n        n_qubits=n_qubits,\n        depth=depth,\n        strategy=\"sDAQC\",\n        operations=[RZ,RX,RZ]\n    )\n    ```\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    hea_func_dict = {\n        Strategy.DIGITAL: hea_digital,\n        Strategy.SDAQC: hea_sDAQC,\n        Strategy.BDAQC: hea_bDAQC,\n        Strategy.ANALOG: hea_analog,\n    }\n\n    try:\n        hea_func = hea_func_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    hea_block: AbstractBlock = hea_func(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        **strategy_args,\n    )  # type: ignore\n\n    return hea_block\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea.hea_digital","title":"<code>hea_digital(n_qubits, depth=1, param_prefix='theta', support=None, periodic=False, operations=[RX, RY, RX], entangler=CNOT)</code>","text":"<p>Construct the Digital Hardware Efficient Ansatz (HEA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the cricuit.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY, RX]</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The digital Hardware Efficient Ansatz (HEA) circuit.</p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def hea_digital(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    periodic: bool = False,\n    operations: list[type[AbstractBlock]] = [RX, RY, RX],\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the Digital Hardware Efficient Ansatz (HEA).\n\n    Args:\n        n_qubits (int): number of qubits in the cricuit.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        support (tuple): qubit indices where the HEA is applied.\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c.\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n\n    Returns:\n        The digital Hardware Efficient Ansatz (HEA) circuit.\n    \"\"\"\n    try:\n        if entangler not in [CNOT, CZ, CRX, CRY, CRZ, CPHASE]:\n            raise ValueError(\n                \"Please provide a valid two-qubit entangler operation for digital HEA.\"\n            )\n    except TypeError:\n        raise ValueError(\"Please provide a valid two-qubit entangler operation for digital HEA.\")\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        periodic=periodic,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n    return tag(chain(*layers), \"HEA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hea.hea_sDAQC","title":"<code>hea_sDAQC(n_qubits, depth=1, param_prefix='theta', support=None, operations=[RX, RY, RX], entangler=None)</code>","text":"<p>Construct the Hardware Efficient Ansatz (HEA) with analog entangling layers.</p> <p>It uses step-wise digital-analog computation.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the circuit.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY, RX]</code> </p> <code>entangler</code> <p>Hamiltonian generator for the analog entangling layer. Defaults to global ZZ Hamiltonian. Time parameter is considered variational.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The step-wise digital-analog Hardware Efficient Ansatz (sDA HEA) circuit.</p> Source code in <code>qadence/constructors/hea.py</code> <pre><code>def hea_sDAQC(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    operations: list[type[AbstractBlock]] = [RX, RY, RX],\n    entangler: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the Hardware Efficient Ansatz (HEA) with analog entangling layers.\n\n    It uses step-wise digital-analog computation.\n\n    Args:\n        n_qubits (int): number of qubits in the circuit.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        support (tuple): qubit indices where the HEA is applied.\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        entangler (AbstractBlock): Hamiltonian generator for the\n            analog entangling layer. Defaults to global ZZ Hamiltonian.\n            Time parameter is considered variational.\n\n    Returns:\n        The step-wise digital-analog Hardware Efficient Ansatz (sDA HEA) circuit.\n    \"\"\"\n\n    # TODO: Add qubit support\n    if entangler is None:\n        entangler = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n    try:\n        if not block_is_qubit_hamiltonian(entangler):\n            raise ValueError(\n                \"Please provide a valid Pauli Hamiltonian generator for digital-analog HEA.\"\n            )\n    except NotImplementedError:\n        raise ValueError(\n            \"Please provide a valid Pauli Hamiltonian generator for digital-analog HEA.\"\n        )\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_analog(\n        depth=depth,\n        param_prefix=param_prefix,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n    return tag(chain(*layers), \"HEA-sDA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.iia._entangler_analog","title":"<code>_entangler_analog(param_str, generator=None)</code>","text":"<p>Creates the analog entangler for identity initialized ansatz.</p> PARAMETER DESCRIPTION <code>param_str</code> <p>The parameter string.</p> <p> TYPE: <code>str</code> </p> <code>generator</code> <p>The Hamiltonian generator for the analog entangler.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The analog entangler for the identity initialized ansatz.</p> Source code in <code>qadence/constructors/iia.py</code> <pre><code>def _entangler_analog(\n    param_str: str,\n    generator: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Creates the analog entangler for identity initialized ansatz.\n\n    Args:\n        param_str: The parameter string.\n        generator: The Hamiltonian generator for the analog entangler.\n\n    Returns:\n        The analog entangler for the identity initialized ansatz.\n    \"\"\"\n    param = Parameter(name=param_str, value=0.0, trainable=True)\n    return HamEvo(generator=generator, parameter=param)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.iia._rotations","title":"<code>_rotations(n_qubits, layer, side, param_str, values, ops=[RX, RY])</code>","text":"<p>Creates the digital rotation layers for the identity initialized ansatz.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits in the identity initialized ansatz.</p> <p> TYPE: <code>int</code> </p> <code>layer</code> <p>The layer number.</p> <p> TYPE: <code>int</code> </p> <code>side</code> <p>The side of the a single layer the rotations are applied to. Either 'left' or 'right'.</p> <p> TYPE: <code>str</code> </p> <code>param_str</code> <p>The parameter string.</p> <p> TYPE: <code>str</code> </p> <code>values</code> <p>The values of the rotation angles.</p> <p> TYPE: <code>list[float | Tensor]</code> </p> <code>ops</code> <p>The operations to apply the rotations with.</p> <p> TYPE: <code>list[type[AbstractBlock]]</code> DEFAULT: <code>[RX, RY]</code> </p> RETURNS DESCRIPTION <code>list[KronBlock]</code> <p>The digital rotation layers for the identity initialized ansatz.</p> Source code in <code>qadence/constructors/iia.py</code> <pre><code>def _rotations(\n    n_qubits: int,\n    layer: int,\n    side: str,\n    param_str: str,\n    values: list[float | torch.Tensor],\n    ops: list[type[AbstractBlock]] = [RX, RY],\n) -&gt; list[KronBlock]:\n    \"\"\"\n    Creates the digital rotation layers for the identity initialized ansatz.\n\n    Args:\n        n_qubits: The number of qubits in the identity initialized ansatz.\n        layer: The layer number.\n        side: The side of the a single layer the rotations are applied to.\n            Either 'left' or 'right'.\n        param_str: The parameter string.\n        values: The values of the rotation angles.\n        ops: The operations to apply the rotations with.\n\n    Returns:\n        The digital rotation layers for the identity initialized ansatz.\n    \"\"\"\n    if side == \"left\":\n        idx = lambda x: x  # noqa: E731\n    elif side == \"right\":\n        idx = lambda x: len(ops) - x - 1  # noqa: E731\n        ops = list(reversed(ops))\n    else:\n        raise ValueError(\"Please provide either 'left' or 'right'\")\n\n    rot_list = []\n    for i, gate in enumerate(ops):\n        rot_list.append(\n            kron(\n                gate(\n                    target=n,  # type: ignore [call-arg]\n                    parameter=Parameter(  # type: ignore [call-arg]\n                        name=param_str + f\"_{layer}{n + n_qubits * idx(i)}\",\n                        value=values[n + n_qubits * idx(i)],\n                        trainable=True,\n                    ),\n                )\n                for n in range(n_qubits)\n            )\n        )\n\n    return rot_list\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.iia.identity_initialized_ansatz","title":"<code>identity_initialized_ansatz(n_qubits, depth=1, param_prefix='iia', strategy=Strategy.DIGITAL, rotations=[RX, RY], entangler=None, periodic=False)</code>","text":"<p>Identity block for barren plateau mitigation.</p> <p>The initial configuration of this block is equal to an identity unitary but can be trained in the same fashion as other ansatzes, reaching same level of expressivity.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>The base name of the variational parameter. Defaults to \"iia\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'iia'</code> </p> <code>strategy</code> <p>(Strategy) Strategy.DIGITAL for fully digital or Strategy.SDAQC for digital-analog.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>rotations</code> <p>single-qubit rotations with trainable parameters</p> <p> TYPE: <code>list of AbstractBlocks</code> DEFAULT: <code>[RX, RY]</code> </p> <code>entangler</code> <p>For Digital:     2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE.     Controlled rotations will have variational parameters on the rotation angles.     Defaults to CNOT. For Digital-analog:     Hamiltonian generator for the analog entangling layer.     Time parameter is considered variational.     Defaults to a global NN Hamiltonain.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>None</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. Valid only for digital.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The identity initialized ansatz circuit.</p> Source code in <code>qadence/constructors/iia.py</code> <pre><code>def identity_initialized_ansatz(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"iia\",\n    strategy: Strategy = Strategy.DIGITAL,\n    rotations: Any = [RX, RY],\n    entangler: Any = None,\n    periodic: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Identity block for barren plateau mitigation.\n\n    The initial configuration of this block is equal to an identity unitary\n    but can be trained in the same fashion as other ansatzes, reaching same level\n    of expressivity.\n\n    Args:\n        n_qubits: number of qubits in the block\n        depth: number of layers of the HEA\n        param_prefix (str):\n            The base name of the variational parameter. Defaults to \"iia\".\n        strategy: (Strategy)\n            Strategy.DIGITAL for fully digital or Strategy.SDAQC for digital-analog.\n        rotations (list of AbstractBlocks):\n            single-qubit rotations with trainable parameters\n        entangler (AbstractBlock):\n            For Digital:\n                2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE.\n                Controlled rotations will have variational parameters on the rotation angles.\n                Defaults to CNOT.\n            For Digital-analog:\n                Hamiltonian generator for the analog entangling layer.\n                Time parameter is considered variational.\n                Defaults to a global NN Hamiltonain.\n        periodic (bool): if the qubits should be linked periodically. Valid only for digital.\n\n    Returns:\n        The identity initialized ansatz circuit.\n    \"\"\"\n    initialized_layers = []\n    for layer in range(depth):\n        alpha = 2 * PI * torch.rand(n_qubits * len(rotations))\n        gamma = torch.zeros(n_qubits)\n        beta = -alpha\n\n        left_rotations = _rotations(\n            n_qubits=n_qubits,\n            layer=layer,\n            side=\"left\",\n            param_str=f\"{param_prefix}_\u03b1\",\n            values=alpha,\n            ops=rotations,\n        )\n\n        if strategy == Strategy.DIGITAL:\n            if entangler is None:\n                entangler = CNOT\n\n            if entangler not in [CNOT, CZ, CRZ, CRY, CRX, CPHASE]:\n                raise ValueError(\n                    \"Please provide a valid two-qubit entangler operation for digital IIA.\"\n                )\n\n            ent_param_prefix = f\"{param_prefix}_\u03b8_ent_\"\n            if not periodic:\n                left_entanglers = [\n                    chain(\n                        _entangler(\n                            control=n,\n                            target=n + 1,\n                            param_str=ent_param_prefix + f\"_{layer}{n}\",\n                            entangler=entangler,\n                        )\n                        for n in range(n_qubits - 1)\n                    )\n                ]\n            else:\n                left_entanglers = [\n                    chain(\n                        _entangler(\n                            control=n,\n                            target=(n + 1) % n_qubits,\n                            param_str=ent_param_prefix + f\"_{layer}{n}\",\n                            entangler=entangler,\n                        )\n                        for n in range(n_qubits)\n                    )\n                ]\n\n        elif strategy == Strategy.SDAQC:\n            if entangler is None:\n                entangler = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n\n            if not block_is_qubit_hamiltonian(entangler):\n                raise ValueError(\n                    \"Please provide a valid Pauli Hamiltonian generator for digital-analog IIA.\"\n                )\n\n            ent_param_prefix = f\"{param_prefix}_ent_t\"\n\n            left_entanglers = [\n                chain(\n                    _entangler_analog(\n                        param_str=f\"{ent_param_prefix}_{layer}\",\n                        generator=entangler,\n                    )\n                )\n            ]\n\n        else:\n            raise NotImplementedError\n\n        centre_rotations = [\n            kron(\n                RX(\n                    target=n,\n                    parameter=Parameter(name=f\"{param_prefix}_\u03b3\" + f\"_{layer}{n}\", value=gamma[n]),\n                )\n                for n in range(n_qubits)\n            )\n        ]\n\n        right_entanglers = reversed(*left_entanglers)\n\n        right_rotations = _rotations(\n            n_qubits=n_qubits,\n            layer=layer,\n            side=\"right\",\n            param_str=f\"{param_prefix}_\u03b2\",\n            values=beta,\n            ops=rotations,\n        )\n\n        krons = [\n            *left_rotations,\n            *left_entanglers,\n            *centre_rotations,\n            *right_entanglers,\n            *right_rotations,\n        ]\n\n        initialized_layers.append(tag(chain(*krons), tag=f\"BPMA-{layer}\"))\n\n    return chain(*initialized_layers)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ala._entangler","title":"<code>_entangler(control, target, param_str, op=CNOT)</code>","text":"<p>Creates the entangler for a single qubit in an Alternating Layer Ansatz.</p> PARAMETER DESCRIPTION <code>control</code> <p>The control qubit.</p> <p> TYPE: <code>int</code> </p> <code>target</code> <p>The target qubit.</p> <p> TYPE: <code>int</code> </p> <code>param_str</code> <p>The parameter string.</p> <p> TYPE: <code>str</code> </p> <code>op</code> <p>The entangler to use.</p> <p> TYPE: <code>Type[DigitalEntanglers]</code> DEFAULT: <code>CNOT</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The 2-qubit digital entangler for the Alternating Layer Ansatz.</p> Source code in <code>qadence/constructors/ala.py</code> <pre><code>def _entangler(\n    control: int,\n    target: int,\n    param_str: str,\n    op: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Creates the entangler for a single qubit in an Alternating Layer Ansatz.\n\n    Args:\n        control: The control qubit.\n        target: The target qubit.\n        param_str: The parameter string.\n        op: The entangler to use.\n\n    Returns:\n        The 2-qubit digital entangler for the Alternating Layer Ansatz.\n    \"\"\"\n    if op in [CNOT, CZ]:\n        return op(control, target)  # type: ignore\n    elif op in [CRZ, CRY, CRX, CPHASE]:\n        return op(control, target, param_str)  # type: ignore\n    else:\n        raise ValueError(\"Provided entangler not accepted for digital alternating layer ansatz\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ala._entanglers_ala_block_digital","title":"<code>_entanglers_ala_block_digital(n_qubits, m_block_qubits, depth, param_prefix='theta', support=None, entangler=CNOT)</code>","text":"<p>Creates the entanglers for an Alternating Layer Ansatz.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits in the Alternating Layer Ansatz.</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>The number of qubits in each block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>The number of layers of entanglers.</p> <p> TYPE: <code>int</code> </p> <code>param_prefix</code> <p>The prefix for the parameter names.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>entangler</code> <p>The entangler to use.</p> <p> TYPE: <code>Type[DigitalEntanglers]</code> DEFAULT: <code>CNOT</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>The entanglers for the Alternating Layer Ansatz.</p> Source code in <code>qadence/constructors/ala.py</code> <pre><code>def _entanglers_ala_block_digital(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; list[AbstractBlock]:\n    \"\"\"\n    Creates the entanglers for an Alternating Layer Ansatz.\n\n    Args:\n        n_qubits: The number of qubits in the Alternating Layer Ansatz.\n        m_block_qubits: The number of qubits in each block.\n        depth: The number of layers of entanglers.\n        param_prefix: The prefix for the parameter names.\n        support (tuple): qubit indices where the HEA is applied.\n        entangler: The entangler to use.\n\n    Returns:\n        The entanglers for the Alternating Layer Ansatz.\n    \"\"\"\n    if support is None:\n        support = tuple(range(n_qubits))\n    iterator = itertools.count()\n    ent_list: list[AbstractBlock] = []\n\n    for d in range(depth):\n        start_i = 0 if not d % 2 else -m_block_qubits // 2\n        ents = [\n            kron(\n                _entangler(\n                    control=support[i + j],\n                    target=support[i + j + 1],\n                    param_str=param_prefix + f\"_ent_{next(iterator)}\",\n                    op=entangler,\n                )\n                for j in range(start_j, m_block_qubits, 2)\n                for i in range(start_i, n_qubits, m_block_qubits)\n                if i + j + 1 &lt; n_qubits and j + 1 &lt; m_block_qubits and i + j &gt;= 0\n            )\n            for start_j in [i for i in range(2) if m_block_qubits &gt; 2 or i == 0]\n        ]\n\n        ent_list.append(chain(*ents))\n    return ent_list\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ala._rotations_digital","title":"<code>_rotations_digital(n_qubits, depth, param_prefix='theta', support=None, operations=[RX, RY, RX])</code>","text":"<p>Creates the layers of single qubit rotations in an Alternating Layer Ansatz.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits in the Alternating Layer Ansatz.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>The number of layers of rotations.</p> <p> TYPE: <code>int</code> </p> <code>param_prefix</code> <p>The prefix for the parameter names.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>The qubits to apply the rotations to.</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>operations</code> <p>The operations to apply the rotations with.</p> <p> TYPE: <code>list[Type[AbstractBlock]]</code> DEFAULT: <code>[RX, RY, RX]</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>A list of digital rotation layers for the Alternating Layer Ansatz.</p> Source code in <code>qadence/constructors/ala.py</code> <pre><code>def _rotations_digital(\n    n_qubits: int,\n    depth: int,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    operations: list[Type[AbstractBlock]] = [RX, RY, RX],\n) -&gt; list[AbstractBlock]:\n    \"\"\"Creates the layers of single qubit rotations in an Alternating Layer Ansatz.\n\n    Args:\n        n_qubits: The number of qubits in the Alternating Layer Ansatz.\n        depth: The number of layers of rotations.\n        param_prefix: The prefix for the parameter names.\n        support: The qubits to apply the rotations to.\n        operations: The operations to apply the rotations with.\n\n    Returns:\n        A list of digital rotation layers for the Alternating Layer Ansatz.\n    \"\"\"\n    if support is None:\n        support = tuple(range(n_qubits))\n    iterator = itertools.count()\n    rot_list: list[AbstractBlock] = []\n    for d in range(depth):\n        rots = [\n            kron(\n                gate(support[n], param_prefix + f\"_{next(iterator)}\")  # type: ignore [arg-type]\n                for n in range(n_qubits)\n            )\n            for gate in operations\n        ]\n        rot_list.append(chain(*rots))\n    return rot_list\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ala.ala","title":"<code>ala(n_qubits, m_block_qubits, depth=1, param_prefix='theta', support=None, strategy=Strategy.DIGITAL, **strategy_args)</code>","text":"<p>Factory function for the alternating layer ansatz (ala).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the circuit</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>number of qubits in the local entangling block</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the alternating layer ansatz</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the ala is applied</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>strategy</code> <p>Strategy for the ansatz. One of the Strategy variants.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>**strategy_args</code> <p>see below</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> PARAMETER DESCRIPTION <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer. Valid for Digital .</p> <p> TYPE: <code>list</code> </p> <code>entangler</code> <ul> <li>Digital: 2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational parameters on the rotation angles.</li> <li>SDAQC | BDAQC: Hamiltonian generator for the analog entangling     layer. Must be an m-qubit operator where m is the size of the     local entangling block. Defaults to a ZZ interaction.</li> </ul> <p> TYPE: <code>AbstractBlock</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The Alternating Layer Ansatz (ALA) circuit.</p> Source code in <code>qadence/constructors/ala.py</code> <pre><code>def ala(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    strategy: Strategy = Strategy.DIGITAL,\n    **strategy_args: Any,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Factory function for the alternating layer ansatz (ala).\n\n    Args:\n        n_qubits: number of qubits in the circuit\n        m_block_qubits: number of qubits in the local entangling block\n        depth: number of layers of the alternating layer ansatz\n        param_prefix: the base name of the variational parameters\n        support: qubit indices where the ala is applied\n        strategy: Strategy for the ansatz. One of the Strategy variants.\n        **strategy_args: see below\n\n    Keyword Arguments:\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer. Valid for\n            Digital .\n        entangler (AbstractBlock):\n            - Digital: 2-qubit entangling operation. Supports CNOT, CZ,\n            CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational\n            parameters on the rotation angles.\n            - SDAQC | BDAQC: Hamiltonian generator for the analog entangling\n                layer. Must be an m-qubit operator where m is the size of the\n                local entangling block. Defaults to a ZZ interaction.\n\n    Returns:\n        The Alternating Layer Ansatz (ALA) circuit.\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    ala_func_dict = {\n        Strategy.DIGITAL: ala_digital,\n        Strategy.SDAQC: ala_sDAQC,\n        Strategy.BDAQC: ala_bDAQC,\n        Strategy.ANALOG: ala_analog,\n    }\n\n    try:\n        ala_func = ala_func_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    ala_block: AbstractBlock = ala_func(\n        n_qubits=n_qubits,\n        m_block_qubits=m_block_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        **strategy_args,\n    )  # type: ignore\n\n    return ala_block\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ala.ala_digital","title":"<code>ala_digital(n_qubits, m_block_qubits, depth=1, param_prefix='theta', support=None, operations=[RX, RY], entangler=CNOT)</code>","text":"<p>Construct the digital alternating layer ansatz (ALA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the circuit.</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>number of qubits in the local entangling block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the ALA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indices where the ALA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY]</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>The digital Alternating Layer Ansatz (ALA) circuit.</p> Source code in <code>qadence/constructors/ala.py</code> <pre><code>def ala_digital(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] | None = None,\n    operations: list[type[AbstractBlock]] = [RX, RY],\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the digital alternating layer ansatz (ALA).\n\n    Args:\n        n_qubits (int): number of qubits in the circuit.\n        m_block_qubits (int): number of qubits in the local entangling block.\n        depth (int): number of layers of the ALA.\n        param_prefix (str): the base name of the variational parameters\n        support (tuple): qubit indices where the ALA is applied.\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n\n    Returns:\n        The digital Alternating Layer Ansatz (ALA) circuit.\n    \"\"\"\n\n    try:\n        if entangler not in [CNOT, CZ, CRX, CRY, CRZ, CPHASE]:\n            raise ValueError(\n                \"Please provide a valid two-qubit entangler operation for digital ALA.\"\n            )\n    except TypeError:\n        raise ValueError(\"Please provide a valid two-qubit entangler operation for digital ALA.\")\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        support=support,\n        param_prefix=param_prefix,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_ala_block_digital(\n        n_qubits,\n        m_block_qubits,\n        param_prefix=param_prefix + \"_ent\",\n        depth=depth,\n        support=support,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n\n    return tag(chain(*layers), \"ALA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig","title":"<code>ObservableConfig(detuning, scale=1.0, shift=0.0, transformation_type=ObservableTransform.NONE, trainable_transform=None)</code>  <code>dataclass</code>","text":""},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.detuning","title":"<code>detuning</code>  <code>instance-attribute</code>","text":"<p>Single qubit detuning of the observable Hamiltonian.</p> <p>Accepts single-qubit operator N, X, Y, or Z.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.scale","title":"<code>scale = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The scale by which to multiply the output of the observable.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.shift","title":"<code>shift = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The shift to add to the output of the observable.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.trainable_transform","title":"<code>trainable_transform = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to have a trainable transformation on the output of the observable.</p> <p>If None, the scale and shift are numbers. If True, the scale and shift are VariationalParameter. If False, the scale and shift are FeatureParameter.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.transformation_type","title":"<code>transformation_type = ObservableTransform.NONE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The type of transformation.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.hamiltonian_factory","title":"<code>hamiltonian_factory(register, interaction=None, detuning=None, interaction_strength=None, detuning_strength=None, random_strength=False, use_all_node_pairs=False)</code>","text":"<p>General Hamiltonian creation function.</p> <p>Can be used to create Hamiltonians with 2-qubit interactions and single-qubit detunings, both with arbitrary strength or parameterized.</p> PARAMETER DESCRIPTION <code>register</code> <p>register of qubits with a specific graph topology, or number of qubits. When passing a number of qubits a register with all-to-all connectivity is created.</p> <p> TYPE: <code>Register | int</code> </p> <code>interaction</code> <p>Interaction.ZZ, Interaction.NN, Interaction.XY, or Interacton.XYZ.</p> <p> TYPE: <code>Interaction | Callable | None</code> DEFAULT: <code>None</code> </p> <code>detuning</code> <p>single-qubit operator N, X, Y, or Z.</p> <p> TYPE: <code>TDetuning | None</code> DEFAULT: <code>None</code> </p> <code>interaction_strength</code> <p>list of values to be used as the interaction strength for each pair of qubits. Should be ordered following the order of <code>Register(n_qubits).edges</code>. Alternatively, some string \"x\" can be passed, which will create a parameterized interactions for each pair of qubits, each labelled as <code>\"x_ij\"</code>.</p> <p> TYPE: <code>TArray | str | None</code> DEFAULT: <code>None</code> </p> <code>detuning_strength</code> <p>list of values to be used as the detuning strength for each qubit. Alternatively, some string \"x\" can be passed, which will create a parameterized detuning for each qubit, each labelled as <code>\"x_i\"</code>.</p> <p> TYPE: <code>TArray | str | None</code> DEFAULT: <code>None</code> </p> <code>random_strength</code> <p>set random interaction and detuning strengths between -1 and 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_all_node_pairs</code> <p>computes an interaction term for every pair of nodes in the graph, independent of the edge topology in the register. Useful for defining Hamiltonians where the interaction strength decays with the distance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>from qadence import hamiltonian_factory, Interaction, Register, Z\n\nn_qubits = 3\n\n# Constant total magnetization observable:\nobservable = hamiltonian_factory(n_qubits, detuning = Z)\n\n# Parameterized total magnetization observable:\nobservable = hamiltonian_factory(n_qubits, detuning = Z, detuning_strength = \"z\")\n\n# Random all-to-all XY Hamiltonian generator:\ngenerator = hamiltonian_factory(\n    n_qubits,\n    interaction = Interaction.XY,\n    random_strength = True,\n    )\n\n# Parameterized NN Hamiltonian generator with a square grid interaction topology:\nregister = Register.square(qubits_side = n_qubits)\ngenerator = hamiltonian_factory(\n    register,\n    interaction = Interaction.NN,\n    interaction_strength = \"theta\"\n    )\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def hamiltonian_factory(\n    register: Register | int,\n    interaction: Interaction | Callable | None = None,\n    detuning: TDetuning | None = None,\n    interaction_strength: TArray | str | None = None,\n    detuning_strength: TArray | str | None = None,\n    random_strength: bool = False,\n    use_all_node_pairs: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    General Hamiltonian creation function.\n\n    Can be used to create Hamiltonians with 2-qubit\n    interactions and single-qubit detunings, both with arbitrary strength or parameterized.\n\n    Arguments:\n        register: register of qubits with a specific graph topology, or number of qubits.\n            When passing a number of qubits a register with all-to-all connectivity\n            is created.\n        interaction: Interaction.ZZ, Interaction.NN, Interaction.XY, or Interacton.XYZ.\n        detuning: single-qubit operator N, X, Y, or Z.\n        interaction_strength: list of values to be used as the interaction strength for each\n            pair of qubits. Should be ordered following the order of `Register(n_qubits).edges`.\n            Alternatively, some string \"x\" can be passed, which will create a parameterized\n            interactions for each pair of qubits, each labelled as `\"x_ij\"`.\n        detuning_strength: list of values to be used as the detuning strength for each qubit.\n            Alternatively, some string \"x\" can be passed, which will create a parameterized\n            detuning for each qubit, each labelled as `\"x_i\"`.\n        random_strength: set random interaction and detuning strengths between -1 and 1.\n        use_all_node_pairs: computes an interaction term for every pair of nodes in the graph,\n            independent of the edge topology in the register. Useful for defining Hamiltonians\n            where the interaction strength decays with the distance.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import hamiltonian_factory, Interaction, Register, Z\n\n        n_qubits = 3\n\n        # Constant total magnetization observable:\n        observable = hamiltonian_factory(n_qubits, detuning = Z)\n\n        # Parameterized total magnetization observable:\n        observable = hamiltonian_factory(n_qubits, detuning = Z, detuning_strength = \"z\")\n\n        # Random all-to-all XY Hamiltonian generator:\n        generator = hamiltonian_factory(\n            n_qubits,\n            interaction = Interaction.XY,\n            random_strength = True,\n            )\n\n        # Parameterized NN Hamiltonian generator with a square grid interaction topology:\n        register = Register.square(qubits_side = n_qubits)\n        generator = hamiltonian_factory(\n            register,\n            interaction = Interaction.NN,\n            interaction_strength = \"theta\"\n            )\n        ```\n    \"\"\"\n\n    if interaction is None and detuning is None:\n        raise ValueError(\"Please provide an interaction and/or detuning for the Hamiltonian.\")\n\n    # If number of qubits is given, creates all-to-all register\n    register = Register(register) if isinstance(register, int) else register\n\n    # Get interaction function\n    if interaction is not None:\n        if callable(interaction):\n            int_fn = interaction\n            try:\n                if not block_is_qubit_hamiltonian(interaction(0, 1)):\n                    raise ValueError(\"Custom interactions must be composed of Pauli operators.\")\n            except TypeError:\n                raise TypeError(\n                    \"Please use a custom interaction function signed with two integer parameters.\"\n                )\n        else:\n            int_fn = INTERACTION_DICT.get(interaction, None)  # type: ignore [arg-type]\n            if int_fn is None:\n                raise KeyError(f\"Interaction {interaction} not supported.\")\n\n    # Check single-qubit detuning\n    if (detuning is not None) and (detuning not in DETUNINGS):\n        raise TypeError(f\"Detuning of type {type(detuning)} not supported.\")\n\n    # Pre-process detuning and interaction strengths and update register\n    detuning_strength_array = _preprocess_strengths(\n        register, detuning_strength, \"nodes\", random_strength\n    )\n\n    edge_str = \"all_node_pairs\" if use_all_node_pairs else \"edges\"\n    interaction_strength_array = _preprocess_strengths(\n        register, interaction_strength, edge_str, random_strength\n    )\n\n    # Create single-qubit detunings:\n    single_qubit_terms: List[AbstractBlock] = []\n    if detuning is not None:\n        for strength, node in zip(detuning_strength_array, register.nodes):\n            single_qubit_terms.append(strength * detuning(node))\n\n    # Create two-qubit interactions:\n    two_qubit_terms: List[AbstractBlock] = []\n    edge_data = register.all_node_pairs if use_all_node_pairs else register.edges\n    if interaction is not None and int_fn is not None:\n        for strength, edge in zip(interaction_strength_array, edge_data):\n            two_qubit_terms.append(strength * int_fn(*edge))\n\n    return add(*single_qubit_terms, *two_qubit_terms)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_nn","title":"<code>interaction_nn(i, j)</code>","text":"<p>Ising NN interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_nn(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Ising NN interaction.\"\"\"\n    return N(i) @ N(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_xy","title":"<code>interaction_xy(i, j)</code>","text":"<p>XY interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_xy(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"XY interaction.\"\"\"\n    return X(i) @ X(j) + Y(i) @ Y(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_xyz","title":"<code>interaction_xyz(i, j)</code>","text":"<p>Heisenberg XYZ interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_xyz(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Heisenberg XYZ interaction.\"\"\"\n    return X(i) @ X(j) + Y(i) @ Y(j) + Z(i) @ Z(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_zz","title":"<code>interaction_zz(i, j)</code>","text":"<p>Ising ZZ interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_zz(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Ising ZZ interaction.\"\"\"\n    return Z(i) @ Z(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft._alpha","title":"<code>_alpha(c, m, k)</code>","text":"<p>Equation (16) from [1].</p> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def _alpha(c: int, m: int, k: int) -&gt; float:\n    \"\"\"Equation (16) from [1].\"\"\"\n    if c == m:\n        return float(PI / (2 ** (k - m + 2)))\n    else:\n        return 0.0\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft._qft_layer_digital","title":"<code>_qft_layer_digital(n_qubits, support, layer, inverse, gen_build=None)</code>","text":"<p>Apply the Hadamard gate followed by CPHASE gates.</p> <p>This corresponds to one layer of the QFT.</p> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def _qft_layer_digital(\n    n_qubits: int,\n    support: tuple[int, ...],\n    layer: int,\n    inverse: bool,\n    gen_build: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Apply the Hadamard gate followed by CPHASE gates.\n\n    This corresponds to one layer of the QFT.\n    \"\"\"\n    qubit_range_layer = (\n        reversed(range(layer + 1, n_qubits)) if inverse else range(layer + 1, n_qubits)\n    )\n    rots = []\n    for j in qubit_range_layer:\n        angle = torch.tensor(\n            ((-1) ** inverse) * 2 * PI / (2 ** (j - layer + 1)), dtype=torch.cdouble\n        )\n        rots.append(CPHASE(support[j], support[layer], angle))  # type: ignore\n    if inverse:\n        return chain(*rots, H(support[layer]))  # type: ignore\n    return chain(H(support[layer]), *rots)  # type: ignore\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft._qft_layer_sDAQC","title":"<code>_qft_layer_sDAQC(n_qubits, support, layer, inverse, gen_build)</code>","text":"<p>QFT Layer using the sDAQC technique.</p> <p>Following the paper:</p> <p>-- [1] https://arxiv.org/abs/1906.07635</p> <p>4 - qubit edge case is not implemented.</p> <p>Note: the paper follows an index convention of running from 1 to N. A few functions here also use that convention to be consistent with the paper. However, for qadence related things the indices are converted to [0, N-1].</p> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def _qft_layer_sDAQC(\n    n_qubits: int,\n    support: tuple[int, ...],\n    layer: int,\n    inverse: bool,\n    gen_build: AbstractBlock | None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    QFT Layer using the sDAQC technique.\n\n    Following the paper:\n\n    -- [1] https://arxiv.org/abs/1906.07635\n\n    4 - qubit edge case is not implemented.\n\n    Note: the paper follows an index convention of running from 1 to N. A few functions\n    here also use that convention to be consistent with the paper. However, for qadence\n    related things the indices are converted to [0, N-1].\n    \"\"\"\n\n    # TODO: Properly check and include support for changing qubit support\n    allowed_support = tuple(range(n_qubits))\n    if support != allowed_support and support != allowed_support[::-1]:\n        raise NotImplementedError(\"Changing support for DigitalAnalog QFT not yet supported.\")\n\n    if gen_build is None:\n        gen_build = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n\n    m = layer + 1  # Paper index convention\n\n    # Generator for the single-qubit rotations contributing to the CPHASE gate\n    sqg_gen_list = _sqg_gen(n_qubits=n_qubits, support=support, m=m, inverse=inverse)\n\n    # Ising model representing the CPHASE gates two-qubit interactions\n    tqg_gen_list = _tqg_gen(n_qubits=n_qubits, support=support, m=m, inverse=inverse)\n\n    if len(sqg_gen_list) &gt; 0:\n        # Single-qubit rotations (leaving the Hadamard explicit)\n        sq_gate = chain(H(support[m - 1]), HamEvo(add(*sqg_gen_list), -1.0))\n\n        # Two-qubit interaction in the CPHASE converted with sDAQC\n        gen_cphases = add(*tqg_gen_list)\n        transformed_daqc_circuit = daqc_transform(\n            n_qubits=gen_build.n_qubits,\n            gen_target=gen_cphases,\n            t_f=-1.0,\n            gen_build=gen_build,\n        )\n\n        layer_circ = chain(\n            sq_gate,\n            transformed_daqc_circuit,\n        )\n        if inverse:\n            return layer_circ.dagger()\n        return layer_circ  # type: ignore\n    else:\n        return chain(H(support[m - 1]))  # type: ignore\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft._sqg_gen","title":"<code>_sqg_gen(n_qubits, support, m, inverse)</code>","text":"<p>Equation (13) from [1].</p> <p>Creates the generator corresponding to single-qubit rotations coming out of the CPHASE decomposition. The paper also includes the generator for the Hadamard of each layer here, but we left it explicit at the start of each layer.</p> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def _sqg_gen(n_qubits: int, support: tuple[int, ...], m: int, inverse: bool) -&gt; list[AbstractBlock]:\n    \"\"\"Equation (13) from [1].\n\n    Creates the generator corresponding to single-qubit rotations coming\n    out of the CPHASE decomposition. The paper also includes the generator\n    for the Hadamard of each layer here, but we left it explicit at\n    the start of each layer.\n    \"\"\"\n    k_sqg_list = reversed(range(2, n_qubits - m + 2)) if inverse else range(2, n_qubits - m + 2)\n\n    sqg_gen_list = []\n    for k in k_sqg_list:\n        sqg_gen = (\n            kron(I(support[j]) for j in range(n_qubits)) - Z(support[k + m - 2]) - Z(support[m - 1])\n        )\n        sqg_gen_list.append(_theta(k) * sqg_gen)\n\n    return sqg_gen_list\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft._theta","title":"<code>_theta(k)</code>","text":"<p>Equation (16) from [1].</p> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def _theta(k: int) -&gt; float:\n    \"\"\"Equation (16) from [1].\"\"\"\n    return float(PI / (2 ** (k + 1)))\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft._tqg_gen","title":"<code>_tqg_gen(n_qubits, support, m, inverse)</code>","text":"<p>Equation (14) from [1].</p> <p>Creates the generator corresponding to the two-qubit ZZ interactions coming out of the CPHASE decomposition.</p> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def _tqg_gen(n_qubits: int, support: tuple[int, ...], m: int, inverse: bool) -&gt; list[AbstractBlock]:\n    \"\"\"Equation (14) from [1].\n\n    Creates the generator corresponding to the two-qubit ZZ\n    interactions coming out of the CPHASE decomposition.\n    \"\"\"\n    k_tqg_list = reversed(range(2, n_qubits + 1)) if inverse else range(2, n_qubits + 1)\n\n    tqg_gen_list = []\n    for k in k_tqg_list:\n        for c in range(1, k):\n            tqg_gen = kron(Z(support[c - 1]), Z(support[k - 1]))\n            tqg_gen_list.append(_alpha(c, m, k) * tqg_gen)\n\n    return tqg_gen_list\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft.qft","title":"<code>qft(n_qubits, support=None, inverse=False, reverse_in=False, swaps_out=False, strategy=Strategy.DIGITAL, gen_build=None)</code>","text":"<p>The Quantum Fourier Transform.</p> <p>Depending on the application, user should be careful with qubit ordering in the input and output. This can be controlled with reverse_in and swaps_out arguments.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the QFT</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>qubit support to use</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>inverse</code> <p>True performs the inverse QFT</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>reverse_in</code> <p>Reverses the input qubits to account for endianness</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>swaps_out</code> <p>Performs swaps on the output qubits to match the \"textbook\" QFT.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>strategy</code> <p>Strategy.Digital or Strategy.sDAQC</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>gen_build</code> <p>building block Ising Hamiltonian for the DAQC transform. Defaults to constant all-to-all Ising.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>from qadence import qft\n\nn_qubits = 3\n\nqft_circuit = qft(n_qubits, strategy = \"sDAQC\")\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def qft(\n    n_qubits: int,\n    support: tuple[int, ...] = None,\n    inverse: bool = False,\n    reverse_in: bool = False,\n    swaps_out: bool = False,\n    strategy: Strategy = Strategy.DIGITAL,\n    gen_build: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    The Quantum Fourier Transform.\n\n    Depending on the application, user should be careful with qubit ordering\n    in the input and output. This can be controlled with reverse_in and swaps_out\n    arguments.\n\n    Args:\n        n_qubits: number of qubits in the QFT\n        support: qubit support to use\n        inverse: True performs the inverse QFT\n        reverse_in: Reverses the input qubits to account for endianness\n        swaps_out: Performs swaps on the output qubits to match the \"textbook\" QFT.\n        strategy: Strategy.Digital or Strategy.sDAQC\n        gen_build: building block Ising Hamiltonian for the DAQC transform.\n            Defaults to constant all-to-all Ising.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import qft\n\n        n_qubits = 3\n\n        qft_circuit = qft(n_qubits, strategy = \"sDAQC\")\n        ```\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    assert len(support) &lt;= n_qubits, \"Wrong qubit support supplied\"\n\n    if reverse_in:\n        support = support[::-1]\n\n    qft_layer_dict = {\n        Strategy.DIGITAL: _qft_layer_digital,\n        Strategy.SDAQC: _qft_layer_sDAQC,\n        Strategy.BDAQC: _qft_layer_bDAQC,\n        Strategy.ANALOG: _qft_layer_analog,\n    }\n\n    try:\n        layer_func = qft_layer_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    qft_layers = reversed(range(n_qubits)) if inverse else range(n_qubits)\n\n    qft_circ = chain(\n        layer_func(\n            n_qubits=n_qubits, support=support, layer=layer, inverse=inverse, gen_build=gen_build\n        )  # type: ignore\n        for layer in qft_layers\n    )\n\n    if swaps_out:\n        swap_ops = [SWAP(support[i], support[n_qubits - i - 1]) for i in range(n_qubits // 2)]\n        qft_circ = chain(*swap_ops, qft_circ) if inverse else chain(qft_circ, *swap_ops)\n\n    return tag(qft_circ, tag=\"iQFT\") if inverse else tag(qft_circ, tag=\"QFT\")\n</code></pre>"},{"location":"api/constructors/#hardware-efficient-ansatz-for-rydberg-atom-arrays","title":"Hardware efficient ansatz for Rydberg atom arrays","text":""},{"location":"api/constructors/#qadence.constructors.rydberg_hea._amplitude_map","title":"<code>_amplitude_map(n_qubits, pauli_op, weights=None)</code>","text":"<p>Create an generator equivalent to a laser amplitude mapping on the device.</p> <p>Basically, given a certain quantum operation <code>pauli_op</code>, this routine constructs the following generator:</p> <pre><code>H = sum_i^N w_i OP(i)\n</code></pre> <p>where the weights are variational parameters</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits</p> <p> TYPE: <code>int</code> </p> <code>pauli_op</code> <p>type of Pauli operation to use when creating the generator</p> <p> TYPE: <code>TPauliOp</code> </p> <code>weights</code> <p>list of variational paramters with the weights</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AddBlock</code> <p>A block with the Hamiltonian generator</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def _amplitude_map(\n    n_qubits: int,\n    pauli_op: TPauliOp,\n    weights: list[Parameter] | list[float] | None = None,\n) -&gt; AddBlock:\n    \"\"\"Create an generator equivalent to a laser amplitude mapping on the device.\n\n    Basically, given a certain quantum operation `pauli_op`, this routine constructs\n    the following generator:\n\n        H = sum_i^N w_i OP(i)\n\n    where the weights are variational parameters\n\n    Args:\n        n_qubits: number of qubits\n        pauli_op: type of Pauli operation to use when creating the generator\n        weights: list of variational paramters with the weights\n\n    Returns:\n        A block with the Hamiltonian generator\n    \"\"\"\n    if weights is None:\n        return add(pauli_op(j) for j in range(n_qubits))\n    else:\n        assert len(weights) &lt;= n_qubits, \"Wrong weights supplied\"\n        return add(w * pauli_op(j) for j, w in enumerate(weights))  # type:ignore [operator]\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.rydberg_hea.rydberg_hea","title":"<code>rydberg_hea(register, n_layers=1, addressable_detuning=True, addressable_drive=False, tunable_phase=False, additional_prefix=None)</code>","text":"<p>Hardware efficient ansatz for neutral atom (Rydberg) platforms.</p> <p>This constructor implements a variational ansatz which is very close to what is implementable on 2nd generation PASQAL quantum devices. In particular, it implements evolution over a specific Hamiltonian which can be realized on the device. This Hamiltonian contains:</p> <ul> <li> <p>an interaction term given by the standard NN interaction and determined starting     from the positions in the input register: H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c</p> </li> <li> <p>a detuning term which corresponding to a n_i = (1+sigma_i^z)/2 applied to     all the qubits. If the <code>addressable_detuning</code> flag is set to True, the routine     effectively a local n_i = (1+sigma_i^z)/2 term in the     evolved Hamiltonian with a different coefficient for each atom. These     coefficients determine a local addressing pattern for the detuning on a subset     of the qubits. In this routine, the coefficients are variational parameters     and they will therefore be optimized at each optimizer step</p> </li> <li> <p>a drive term which corresponding to a sigma^x evolution operation applied to     all the qubits. If the <code>addressable_drive</code> flag is set to True, the routine     effectively a local sigma_i^x term in the evolved Hamiltonian with a different     coefficient for each atom. These coefficients determine a local addressing pattern     for the drive on a subset of the qubits. In this routine, the coefficients are     variational parameters and they will therefore be optimized at each optimizer step</p> </li> <li> <p>if the <code>tunable_phase</code> flag is set to True, the drive term is modified in the following     way: drive = cos(phi) * sigma^x - sin(phi) * sigma^y     The addressable pattern above is maintained and the phase is considered just as an     additional variational parameter which is optimized with the rest</p> </li> </ul> <p>Notice that, on real devices, the coefficients assigned to each qubit in both the detuning and drive patterns should be non-negative and they should always sum to 1. This is not the case for the implementation in this routine since the coefficients (weights) do not have any constraint. Therefore, this HEA is not completely realizable on neutral atom devices.</p> PARAMETER DESCRIPTION <code>register</code> <p>the input atomic register with Cartesian coordinates.</p> <p> TYPE: <code>Register</code> </p> <code>n_layers</code> <p>number layers in the HEA, each layer includes a drive, detuning and pure interaction pulses whose is a variational parameter</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>addressable_detuning</code> <p>whether to turn on the trainable semi-local addressing pattern on the detuning (n_i terms in the Hamiltonian)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>addressable_drive</code> <p>whether to turn on the trainable semi-local addressing pattern on the drive (sigma_i^x terms in the Hamiltonian)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tunable_phase</code> <p>whether to have a tunable phase to get both sigma^x and sigma^y rotations in the drive term. If False, only a sigma^x term will be included in the drive part of the Hamiltonian generator</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>additional_prefix</code> <p>an additional prefix to attach to the parameter names</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>The Rydberg HEA block</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def rydberg_hea(\n    register: qd.Register,\n    n_layers: int = 1,\n    addressable_detuning: bool = True,\n    addressable_drive: bool = False,\n    tunable_phase: bool = False,\n    additional_prefix: str = None,\n) -&gt; qd.blocks.ChainBlock:\n    \"\"\"Hardware efficient ansatz for neutral atom (Rydberg) platforms.\n\n    This constructor implements a variational ansatz which is very close to\n    what is implementable on 2nd generation PASQAL quantum devices. In particular,\n    it implements evolution over a specific Hamiltonian which can be realized on\n    the device. This Hamiltonian contains:\n\n    * an interaction term given by the standard NN interaction and determined starting\n        from the positions in the input register: H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c\n\n    * a detuning term which corresponding to a n_i = (1+sigma_i^z)/2 applied to\n        all the qubits. If the `addressable_detuning` flag is set to True, the routine\n        effectively a local n_i = (1+sigma_i^z)/2 term in the\n        evolved Hamiltonian with a different coefficient for each atom. These\n        coefficients determine a local addressing pattern for the detuning on a subset\n        of the qubits. In this routine, the coefficients are variational parameters\n        and they will therefore be optimized at each optimizer step\n\n    * a drive term which corresponding to a sigma^x evolution operation applied to\n        all the qubits. If the `addressable_drive` flag is set to True, the routine\n        effectively a local sigma_i^x term in the evolved Hamiltonian with a different\n        coefficient for each atom. These coefficients determine a local addressing pattern\n        for the drive on a subset of the qubits. In this routine, the coefficients are\n        variational parameters and they will therefore be optimized at each optimizer step\n\n    * if the `tunable_phase` flag is set to True, the drive term is modified in the following\n        way: drive = cos(phi) * sigma^x - sin(phi) * sigma^y\n        The addressable pattern above is maintained and the phase is considered just as an\n        additional variational parameter which is optimized with the rest\n\n    Notice that, on real devices, the coefficients assigned to each qubit in both the detuning\n    and drive patterns should be non-negative and they should always sum to 1. This is not the\n    case for the implementation in this routine since the coefficients (weights) do not have any\n    constraint. Therefore, this HEA is not completely realizable on neutral atom devices.\n\n    Args:\n        register: the input atomic register with Cartesian coordinates.\n        n_layers: number layers in the HEA, each layer includes a drive, detuning and\n            pure interaction pulses whose is a variational parameter\n        addressable_detuning: whether to turn on the trainable semi-local addressing pattern\n            on the detuning (n_i terms in the Hamiltonian)\n        addressable_drive: whether to turn on the trainable semi-local addressing pattern\n            on the drive (sigma_i^x terms in the Hamiltonian)\n        tunable_phase: whether to have a tunable phase to get both sigma^x and sigma^y rotations\n            in the drive term. If False, only a sigma^x term will be included in the drive part\n            of the Hamiltonian generator\n        additional_prefix: an additional prefix to attach to the parameter names\n\n    Returns:\n        The Rydberg HEA block\n    \"\"\"\n    n_qubits = register.n_qubits\n    prefix = \"\" if additional_prefix is None else \"_\" + additional_prefix\n\n    detunings = None\n    # add a detuning pattern locally addressing the atoms\n    if addressable_detuning:\n        detunings = [qd.VariationalParameter(f\"detmap_{j}\") for j in range(n_qubits)]\n\n    drives = None\n    # add a drive pattern locally addressing the atoms\n    if addressable_drive:\n        drives = [qd.VariationalParameter(f\"drivemap_{j}\") for j in range(n_qubits)]\n\n    phase = None\n    if tunable_phase:\n        phase = qd.VariationalParameter(\"phase\")\n\n    return chain(\n        rydberg_hea_layer(\n            register,\n            VariationalParameter(f\"At{prefix}_{layer}\"),\n            VariationalParameter(f\"Omega{prefix}_{layer}\"),\n            VariationalParameter(f\"wait{prefix}_{layer}\"),\n            detunings=detunings,\n            drives=drives,\n            phase=phase,\n        )\n        for layer in range(n_layers)\n    )\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.rydberg_hea.rydberg_hea_layer","title":"<code>rydberg_hea_layer(register, tevo_drive, tevo_det, tevo_wait, phase=None, detunings=None, drives=None, drive_scaling=1.0)</code>","text":"<p>A single layer of the Rydberg hardware efficient ansatz.</p> PARAMETER DESCRIPTION <code>register</code> <p>the input register with atomic coordinates needed to build the interaction.</p> <p> TYPE: <code>Register</code> </p> <code>tevo_drive</code> <p>a variational parameter for the duration of the drive term of the Hamiltonian generator, including optional semi-local addressing</p> <p> TYPE: <code>Parameter | float</code> </p> <code>tevo_det</code> <p>a variational parameter for the duration of the detuning term of the Hamiltonian generator, including optional semi-local addressing</p> <p> TYPE: <code>Parameter | float</code> </p> <code>tevo_wait</code> <p>a variational parameter for the duration of the waiting time with interaction only</p> <p> TYPE: <code>Parameter | float</code> </p> <code>phase</code> <p>a variational parameter representing the global phase. If None, the global phase is set to 0 which results in a drive term in sigma^x only. Otherwise both sigma^x and sigma^y terms will be present</p> <p> TYPE: <code>Parameter | float | None</code> DEFAULT: <code>None</code> </p> <code>detunings</code> <p>a list of parameters with the weights of the locally addressed detuning terms. These are variational parameters which are tuned by the optimizer</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> <code>drives</code> <p>a list of parameters with the weights of the locally addressed drive terms. These are variational parameters which are tuned by the optimizer</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> <code>drive_scaling</code> <p>a scaling term to be added to the drive Hamiltonian generator</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>A block with a single layer of Rydberg HEA</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def rydberg_hea_layer(\n    register: qd.Register,\n    tevo_drive: Parameter | float,\n    tevo_det: Parameter | float,\n    tevo_wait: Parameter | float,\n    phase: Parameter | float | None = None,\n    detunings: list[Parameter] | list[float] | None = None,\n    drives: list[Parameter] | list[float] | None = None,\n    drive_scaling: float = 1.0,\n) -&gt; ChainBlock:\n    \"\"\"A single layer of the Rydberg hardware efficient ansatz.\n\n    Args:\n        register: the input register with atomic coordinates needed to build the interaction.\n        tevo_drive: a variational parameter for the duration of the drive term of\n            the Hamiltonian generator, including optional semi-local addressing\n        tevo_det: a variational parameter for the duration of the detuning term of the\n            Hamiltonian generator, including optional semi-local addressing\n        tevo_wait: a variational parameter for the duration of the waiting\n            time with interaction only\n        phase: a variational parameter representing the global phase. If None, the\n            global phase is set to 0 which results in a drive term in sigma^x only. Otherwise\n            both sigma^x and sigma^y terms will be present\n        detunings: a list of parameters with the weights of the locally addressed\n            detuning terms. These are variational parameters which are tuned by the optimizer\n        drives: a list of parameters with the weights of the locally addressed\n            drive terms. These are variational parameters which are tuned by the optimizer\n        drive_scaling: a scaling term to be added to the drive Hamiltonian generator\n\n    Returns:\n        A block with a single layer of Rydberg HEA\n    \"\"\"\n    n_qubits = register.n_qubits\n\n    drive_x = _amplitude_map(n_qubits, qd.X, weights=drives)\n    drive_y = _amplitude_map(n_qubits, qd.Y, weights=drives)\n    detuning = _amplitude_map(n_qubits, qd.N, weights=detunings)\n    interaction = hamiltonian_factory(register, qd.Interaction.NN)\n\n    # drive and interaction are not commuting thus they need to be\n    # added directly into the final Hamiltonian generator\n    if phase is not None:\n        generator = (\n            drive_scaling * sympy.cos(phase) * drive_x\n            - drive_scaling * sympy.sin(phase) * drive_y\n            + interaction\n        )\n    else:\n        generator = drive_scaling * drive_x + interaction\n\n    return chain(\n        qd.HamEvo(generator, tevo_drive),\n        # detuning and interaction are commuting, so they\n        # can be ordered arbitrarily and treated separately\n        qd.HamEvo(interaction, tevo_wait),\n        qd.HamEvo(detuning, tevo_det),\n    )\n</code></pre>"},{"location":"api/constructors/#the-daqc-transform","title":"The DAQC Transform","text":""},{"location":"api/constructors/#qadence.constructors.daqc.daqc.daqc_transform","title":"<code>daqc_transform(n_qubits, gen_target, t_f, gen_build=None, zero_tol=1e-08, strategy=Strategy.SDAQC, ignore_global_phases=False)</code>","text":"<p>Implements the DAQC transform for representing an arbitrary 2-body Hamiltonian.</p> <p>The result is another fixed 2-body Hamiltonian.</p> <p>Reference for universality of 2-body Hamiltonians:</p> <p>-- https://arxiv.org/abs/quant-ph/0106064</p> <p>Based on the transformation for Ising (ZZ) interactions, as described in the paper</p> <p>-- https://arxiv.org/abs/1812.03637</p> <p>The transform translates a target weighted generator of the type:</p> <pre><code>`gen_target = add(g_jk * kron(op(j), op(k)) for j &lt; k)`\n</code></pre> <p>To a circuit using analog evolutions with a fixed building block generator:</p> <pre><code>`gen_build = add(f_jk * kron(op(j), op(k)) for j &lt; k)`\n</code></pre> <p>where <code>op = Z</code> or <code>op = N</code>.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>total number of qubits to use.</p> <p> TYPE: <code>int</code> </p> <code>gen_target</code> <p>target generator built with the structure above. The type of the generator will be automatically evaluated when parsing.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>t_f</code> <p>total time for the gen_target evolution.</p> <p> TYPE: <code>float</code> </p> <code>gen_build</code> <p>fixed generator to act as a building block. Defaults to constant NN: add(1.0 * kron(N(j), N(k)) for j &lt; k). The type of the generator will be automatically evaluated when parsing.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <code>zero_tol</code> <p>default \"zero\" for a missing interaction. Included for numerical reasons, see notes below.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>strategy</code> <p>sDAQC or bDAQC, following definitions in the reference paper.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>SDAQC</code> </p> <code>ignore_global_phases</code> <p>if <code>True</code> the transform does not correct the global phases coming from the mapping between ZZ and NN interactions.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Notes:</p> <pre><code>The paper follows an index convention of running from 1 to N. A few functions\nhere also use that convention to be consistent with the paper. However, for qadence\nrelated things the indices are converted to [0, N-1].\n\nThe case for `n_qubits = 4` is an edge case where the sign matrix is not invertible.\nThere is a workaround for this described in the paper, but it is currently not implemented.\n\nThe current implementation may result in evolution times that are both positive or\nnegative. In practice, both can be represented by simply changing the signs of the\ninteractions. However, for a real implementation where the interactions should remain\nfixed, the paper discusses a workaround that is not currently implemented.\n\nThe transformation works by representing each interaction in the target hamiltonian by\na set of evolutions using the build hamiltonian. As a consequence, some care must be\ntaken when choosing the build hamiltonian. Some cases:\n\n- The target hamiltonian can have any interaction, as long as it is sufficiently\nrepresented in the build hamiltonian. E.g., if the interaction `g_01 * kron(Z(0), Z(1))`\nis in the target hamiltonian, the corresponding interaction `f_01 * kron(Z(0), Z(1))`\nneeds to be in the build hamiltonian. This is checked when the generators are parsed.\n\n- The build hamiltonian can have any interaction, irrespectively of it being needed\nfor the target hamiltonian. This is especially useful for designing local operations\nthrough the repeated evolution of a \"global\" hamiltonian.\n\n- The parameter `zero_tol` controls what it means for an interaction to be \"missing\".\nAny interaction strength smaller than `zero_tol` in the build hamiltonian will not be\nconsidered, and thus that interaction is missing.\n\n- The various ratios `g_jk / f_jk` will influence the time parameter for the various\nevolution slices, meaning that if there is a big discrepancy in the interaction strength\nfor a given qubit pair (j, k), the output circuit may require the usage of hamiltonian\nevolutions with very large times.\n\n- A warning will be issued for evolution times larger than `1/sqrt(zero_tol)`. Evolution\ntimes smaller than `zero_tol` will not be represented.\n</code></pre> <p>Examples:</p> <pre><code>from qadence import Z, N, daqc_transform\n\nn_qubits = 3\n\ngen_build = 0.5 * (N(0)@N(1)) + 0.7 * (N(1)@N(2)) + 0.2 * (N(0)@N(2))\n\ngen_target = 0.1 * (Z(1)@Z(2))\n\nt_f = 2.0\n\ntransformed_circuit = daqc_transform(\n    n_qubits = n_qubits,\n    gen_target = gen_target,\n    t_f = t_f,\n    gen_build = gen_build,\n)\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/daqc/daqc.py</code> <pre><code>def daqc_transform(\n    n_qubits: int,\n    gen_target: AbstractBlock,\n    t_f: float,\n    gen_build: AbstractBlock | None = None,\n    zero_tol: float = 1e-08,\n    strategy: Strategy = Strategy.SDAQC,\n    ignore_global_phases: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Implements the DAQC transform for representing an arbitrary 2-body Hamiltonian.\n\n    The result is another fixed 2-body Hamiltonian.\n\n    Reference for universality of 2-body Hamiltonians:\n\n    -- https://arxiv.org/abs/quant-ph/0106064\n\n    Based on the transformation for Ising (ZZ) interactions, as described in the paper\n\n    -- https://arxiv.org/abs/1812.03637\n\n    The transform translates a target weighted generator of the type:\n\n        `gen_target = add(g_jk * kron(op(j), op(k)) for j &lt; k)`\n\n    To a circuit using analog evolutions with a fixed building block generator:\n\n        `gen_build = add(f_jk * kron(op(j), op(k)) for j &lt; k)`\n\n    where `op = Z` or `op = N`.\n\n    Args:\n        n_qubits: total number of qubits to use.\n        gen_target: target generator built with the structure above. The type\n            of the generator will be automatically evaluated when parsing.\n        t_f (float): total time for the gen_target evolution.\n        gen_build: fixed generator to act as a building block. Defaults to\n            constant NN: add(1.0 * kron(N(j), N(k)) for j &lt; k). The type\n            of the generator will be automatically evaluated when parsing.\n        zero_tol: default \"zero\" for a missing interaction. Included for\n            numerical reasons, see notes below.\n        strategy: sDAQC or bDAQC, following definitions in the reference paper.\n        ignore_global_phases: if `True` the transform does not correct the global\n            phases coming from the mapping between ZZ and NN interactions.\n\n    Notes:\n\n        The paper follows an index convention of running from 1 to N. A few functions\n        here also use that convention to be consistent with the paper. However, for qadence\n        related things the indices are converted to [0, N-1].\n\n        The case for `n_qubits = 4` is an edge case where the sign matrix is not invertible.\n        There is a workaround for this described in the paper, but it is currently not implemented.\n\n        The current implementation may result in evolution times that are both positive or\n        negative. In practice, both can be represented by simply changing the signs of the\n        interactions. However, for a real implementation where the interactions should remain\n        fixed, the paper discusses a workaround that is not currently implemented.\n\n        The transformation works by representing each interaction in the target hamiltonian by\n        a set of evolutions using the build hamiltonian. As a consequence, some care must be\n        taken when choosing the build hamiltonian. Some cases:\n\n        - The target hamiltonian can have any interaction, as long as it is sufficiently\n        represented in the build hamiltonian. E.g., if the interaction `g_01 * kron(Z(0), Z(1))`\n        is in the target hamiltonian, the corresponding interaction `f_01 * kron(Z(0), Z(1))`\n        needs to be in the build hamiltonian. This is checked when the generators are parsed.\n\n        - The build hamiltonian can have any interaction, irrespectively of it being needed\n        for the target hamiltonian. This is especially useful for designing local operations\n        through the repeated evolution of a \"global\" hamiltonian.\n\n        - The parameter `zero_tol` controls what it means for an interaction to be \"missing\".\n        Any interaction strength smaller than `zero_tol` in the build hamiltonian will not be\n        considered, and thus that interaction is missing.\n\n        - The various ratios `g_jk / f_jk` will influence the time parameter for the various\n        evolution slices, meaning that if there is a big discrepancy in the interaction strength\n        for a given qubit pair (j, k), the output circuit may require the usage of hamiltonian\n        evolutions with very large times.\n\n        - A warning will be issued for evolution times larger than `1/sqrt(zero_tol)`. Evolution\n        times smaller than `zero_tol` will not be represented.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import Z, N, daqc_transform\n\n        n_qubits = 3\n\n        gen_build = 0.5 * (N(0)@N(1)) + 0.7 * (N(1)@N(2)) + 0.2 * (N(0)@N(2))\n\n        gen_target = 0.1 * (Z(1)@Z(2))\n\n        t_f = 2.0\n\n        transformed_circuit = daqc_transform(\n            n_qubits = n_qubits,\n            gen_target = gen_target,\n            t_f = t_f,\n            gen_build = gen_build,\n        )\n        ```\n    \"\"\"\n\n    ##################\n    # Input controls #\n    ##################\n\n    if strategy != Strategy.SDAQC:\n        raise NotImplementedError(\"Currently only the sDAQC transform is implemented.\")\n\n    if n_qubits == 4:\n        raise NotImplementedError(\"DAQC transform 4-qubit edge case not implemented.\")\n\n    if gen_build is None:\n        gen_build = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n\n    try:\n        if (not block_is_qubit_hamiltonian(gen_target)) or (\n            not block_is_qubit_hamiltonian(gen_build)\n        ):\n            raise ValueError(\n                \"Generator block is not a qubit Hamiltonian. Only ZZ or NN interactions allowed.\"\n            )\n    except NotImplementedError:\n        # Happens when block_is_qubit_hamiltonian is called on something that is not a block.\n        raise TypeError(\n            \"Generator block is not a qubit Hamiltonian. Only ZZ or NN interactions allowed.\"\n        )\n\n    #####################\n    # Generator parsing #\n    #####################\n\n    g_jk_target, mat_jk_target, target_type = _parse_generator(n_qubits, gen_target, 0.0)\n    g_jk_build, mat_jk_build, build_type = _parse_generator(n_qubits, gen_build, zero_tol)\n\n    # Get the global phase hamiltonian and single-qubit detuning hamiltonian\n    if build_type == GenDAQC.NN:\n        h_phase_build, h_sq_build = _nn_phase_and_detunings(n_qubits, mat_jk_build)\n\n    if target_type == GenDAQC.NN:\n        h_phase_target, h_sq_target = _nn_phase_and_detunings(n_qubits, mat_jk_target)\n\n    # Time re-scalings\n    if build_type == GenDAQC.ZZ and target_type == GenDAQC.NN:\n        t_star = t_f / 4.0\n    elif build_type == GenDAQC.NN and target_type == GenDAQC.ZZ:\n        t_star = 4.0 * t_f\n    else:\n        t_star = t_f\n\n    # Check if target Hamiltonian can be mapped with the build Hamiltonian\n    assert _check_compatibility(g_jk_target, g_jk_build, zero_tol)\n\n    ##################\n    # DAQC Transform #\n    ##################\n\n    # Section III A of https://arxiv.org/abs/1812.03637:\n\n    # Matrix M for the linear system, exemplified in Table I:\n    matrix_M = _build_matrix_M(n_qubits)\n\n    # Linear system mapping interaction ratios -&gt; evolution times.\n    t_slices = torch.linalg.solve(matrix_M, g_jk_target / g_jk_build) * t_star\n\n    # ZZ-DAQC with ZZ or NN build Hamiltonian\n    daqc_slices = []\n    for m in range(2, n_qubits + 1):\n        for n in range(1, m):\n            alpha = _ix_map(n_qubits, n, m)\n            t = t_slices[alpha - 1]\n            if abs(t) &gt; zero_tol:\n                if abs(t) &gt; (1 / (zero_tol**0.5)):\n                    logger.warning(\n                        \"\"\"\nTransformed circuit with very long evolution time.\nMake sure your target interactions are sufficiently\nrepresented in the build Hamiltonian.\"\"\"\n                    )\n                x_gates = kron(X(n - 1), X(m - 1))\n                analog_evo = HamEvo(gen_build, t)\n                # TODO: Fix repeated X-gates\n                if build_type == GenDAQC.NN:\n                    # Local detuning at each DAQC layer for NN build Hamiltonian\n                    sq_detuning_build = HamEvo(h_sq_build, t)\n                    daqc_slices.append(chain(x_gates, sq_detuning_build, analog_evo, x_gates))\n                elif build_type == GenDAQC.ZZ:\n                    daqc_slices.append(chain(x_gates, analog_evo, x_gates))\n\n    daqc_circuit = chain(*daqc_slices)\n\n    ########################\n    # Phases and Detunings #\n    ########################\n\n    if target_type == GenDAQC.NN:\n        # Local detuning given a NN target Hamiltonian\n        sq_detuning_target = HamEvo(h_sq_target, t_f).dagger()\n        daqc_circuit = chain(sq_detuning_target, daqc_circuit)\n\n    if not ignore_global_phases:\n        if build_type == GenDAQC.NN:\n            # Constant global phase given a NN build Hamiltonian\n            global_phase_build = HamEvo(h_phase_build, t_slices.sum())\n            daqc_circuit = chain(global_phase_build, daqc_circuit)\n\n        if target_type == GenDAQC.NN:\n            # Constant global phase and given a NN target Hamiltonian\n            global_phase_target = HamEvo(h_phase_target, t_f).dagger()\n            daqc_circuit = chain(global_phase_target, daqc_circuit)\n\n    return daqc_circuit\n</code></pre>"},{"location":"api/constructors/#some-utility-functions","title":"Some utility functions","text":""},{"location":"api/constructors/#qadence.constructors.utils.build_idx_fms","title":"<code>build_idx_fms(basis, fm_pauli, multivariate_strategy, n_features, n_qubits, reupload_scaling)</code>","text":"<p>Builds the index feature maps based on the given parameters.</p> PARAMETER DESCRIPTION <code>basis</code> <p>Type of basis chosen for the feature map.</p> <p> TYPE: <code>BasisSet</code> </p> <code>fm_pauli</code> <p>The chosen Pauli rotation type.</p> <p> TYPE: <code>PrimitiveBlock type</code> </p> <code>multivariate_strategy</code> <p>The strategy used for encoding the multivariate feature map.</p> <p> TYPE: <code>MultivariateStrategy</code> </p> <code>n_features</code> <p>The number of features.</p> <p> TYPE: <code>int</code> </p> <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>reupload_scaling</code> <p>The chosen scaling for the reupload.</p> <p> TYPE: <code>ReuploadScaling</code> </p> RETURNS DESCRIPTION <code>list[KronBlock]</code> <p>List[KronBlock]: The list of index feature maps.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def build_idx_fms(\n    basis: BasisSet,\n    fm_pauli: Type[RY],\n    multivariate_strategy: MultivariateStrategy,\n    n_features: int,\n    n_qubits: int,\n    reupload_scaling: ReuploadScaling,\n) -&gt; list[KronBlock]:\n    \"\"\"Builds the index feature maps based on the given parameters.\n\n    Args:\n        basis (BasisSet): Type of basis chosen for the feature map.\n        fm_pauli (PrimitiveBlock type): The chosen Pauli rotation type.\n        multivariate_strategy (MultivariateStrategy): The strategy used for encoding\n            the multivariate feature map.\n        n_features (int): The number of features.\n        n_qubits (int): The number of qubits.\n        reupload_scaling (ReuploadScaling): The chosen scaling for the reupload.\n\n    Returns:\n        List[KronBlock]: The list of index feature maps.\n    \"\"\"\n    idx_fms = []\n    for i in range(n_features):\n        target_qubits = get_fm_qubits(multivariate_strategy, i, n_qubits, n_features)\n        param = FeatureParameter(f\"x{i}\")\n        block = kron(\n            *[\n                fm_pauli(qubit, generator_prefactor(reupload_scaling, j) * basis_func(basis, param))\n                for j, qubit in enumerate(target_qubits)\n            ]\n        )\n        idx_fm = block\n        idx_fms.append(idx_fm)\n    return idx_fms\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.utils.generator_prefactor","title":"<code>generator_prefactor(reupload_scaling, qubit_index)</code>","text":"<p>Converts a spectrum string, e.g. tower or exponential.</p> <p>The result is the correct generator prefactor.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def generator_prefactor(reupload_scaling: ReuploadScaling, qubit_index: int) -&gt; float | int:\n    \"\"\"Converts a spectrum string, e.g. tower or exponential.\n\n    The result is the correct generator prefactor.\n    \"\"\"\n    conversion_dict: dict[str, float | int] = {\n        ReuploadScaling.CONSTANT: 1,\n        ReuploadScaling.TOWER: qubit_index + 1,\n        ReuploadScaling.EXP: 2 * PI / (2 ** (qubit_index + 1)),\n    }\n    return conversion_dict[reupload_scaling]\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.utils.get_fm_qubits","title":"<code>get_fm_qubits(multivariate_strategy, i, n_qubits, n_features)</code>","text":"<p>Returns the list of target qubits for the given feature map strategy and feature index.</p> PARAMETER DESCRIPTION <code>multivariate_strategy</code> <p>The strategy used for encoding the multivariate feature map.</p> <p> TYPE: <code>MultivariateStrategy</code> </p> <code>i</code> <p>The feature index.</p> <p> TYPE: <code>int</code> </p> <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>The number of features.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Iterable</code> <p>List[int]: The list of target qubits.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the feature map strategy is not implemented.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def get_fm_qubits(\n    multivariate_strategy: MultivariateStrategy, i: int, n_qubits: int, n_features: int\n) -&gt; Iterable:\n    \"\"\"Returns the list of target qubits for the given feature map strategy and feature index.\n\n    Args:\n        multivariate_strategy (MultivariateStrategy): The strategy used for encoding\n            the multivariate feature map.\n        i (int): The feature index.\n        n_qubits (int): The number of qubits.\n        n_features (int): The number of features.\n\n    Returns:\n        List[int]: The list of target qubits.\n\n    Raises:\n        ValueError: If the feature map strategy is not implemented.\n    \"\"\"\n    if multivariate_strategy == MultivariateStrategy.PARALLEL:\n        n_qubits_per_feature = int(n_qubits / n_features)\n        target_qubits = range(i * n_qubits_per_feature, (i + 1) * n_qubits_per_feature)\n    elif multivariate_strategy == MultivariateStrategy.SERIES:\n        target_qubits = range(0, n_qubits)\n    else:\n        raise ValueError(f\"Multivariate strategy {multivariate_strategy} not implemented.\")\n    return target_qubits\n</code></pre>"},{"location":"api/draw/","title":"Drawing","text":""},{"location":"api/draw/#drawing","title":"Drawing","text":""},{"location":"api/draw/#qadence.draw.display","title":"<code>display(x, qcd=None, layout='LR', theme='light', fill=True, **kwargs)</code>","text":"<p>Display a block, circuit, or quantum model.</p> <p>The <code>kwargs</code> are forwarded to the underlying <code>nx.Graph</code>, so you can e.g. specify the size of the resulting plot via <code>size=\"2,2\"</code> (see examples)</p> PARAMETER DESCRIPTION <code>x</code> <p><code>AbstractBlock</code>, <code>QuantumCircuit</code>, or <code>QuantumModel</code>.</p> <p> TYPE: <code>Any</code> </p> <code>qcd</code> <p>Circuit diagram to plot the block into.</p> <p> TYPE: <code>QuantumCircuitDiagram | Cluster | None</code> DEFAULT: <code>None</code> </p> <code>layout</code> <p>Can be either \"LR\" (left-right), or \"TB\" (top-bottom).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'LR'</code> </p> <code>theme</code> <p>Available themes are: [\"light\", \"dark\", \"black\", \"white\"].</p> <p> TYPE: <code>str</code> DEFAULT: <code>'light'</code> </p> <code>fill</code> <p>Whether to fill the passed <code>x</code> with identities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>Passed on to <code>nx.Graph</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Examples: <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\ndisplay(b, size=\"1,1\", theme=\"dark\")\n</code></pre> </p> Source code in <code>qadence/draw/__init__.py</code> <pre><code>def display(\n    x: Any,\n    qcd: QuantumCircuitDiagram | Cluster | None = None,\n    layout: str = \"LR\",\n    theme: str = \"light\",\n    fill: bool = True,\n    **kwargs: Any,\n) -&gt; Graph:\n    \"\"\"Display a block, circuit, or quantum model.\n\n    The `kwargs` are forwarded to\n    the underlying `nx.Graph`, so you can e.g. specify the size of the resulting plot via\n    `size=\"2,2\"` (see examples)\n\n    Arguments:\n        x: `AbstractBlock`, `QuantumCircuit`, or `QuantumModel`.\n        qcd: Circuit diagram to plot the block into.\n        layout: Can be either \"LR\" (left-right), or \"TB\" (top-bottom).\n        theme: Available themes are: [\"light\", \"dark\", \"black\", \"white\"].\n        fill: Whether to fill the passed `x` with identities.\n        kwargs: Passed on to `nx.Graph`\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" html=\"1\"\n    from qadence import X, Y, kron\n    from qadence.draw import display\n\n    b = kron(X(0), Y(1))\n    def display(*args, **kwargs): return args # markdown-exec: hide\n    display(b, size=\"1,1\", theme=\"dark\")\n    ```\n    \"\"\"\n    return make_diagram(x, **kwargs).show()\n</code></pre>"},{"location":"api/draw/#qadence.draw.savefig","title":"<code>savefig(x, filename, *args, **kwargs)</code>","text":"<p>Save a block, circuit, or quantum model to file. Accepts the same args/kwargs as <code>display</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>AbstractBlock</code>, <code>QuantumCircuit</code>, or <code>QuantumModel</code>.</p> <p> TYPE: <code>Any</code> </p> <code>filename</code> <p>Should end in svg/png.</p> <p> TYPE: <code>str</code> </p> <code>args</code> <p>Same as in <code>display</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>Same as in <code>display</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Examples: <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\nsavefig(b, \"test.svg\", size=\"1,1\", theme=\"dark\")\n</code></pre> </p> Source code in <code>qadence/draw/__init__.py</code> <pre><code>def savefig(x: Any, filename: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Save a block, circuit, or quantum model to file. Accepts the same args/kwargs as `display`.\n\n    Arguments:\n        x: `AbstractBlock`, `QuantumCircuit`, or `QuantumModel`.\n        filename: Should end in svg/png.\n        args: Same as in `display`.\n        kwargs: Same as in `display`.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" html=\"1\"\n    from qadence import X, Y, kron\n    from qadence.draw import display\n\n    b = kron(X(0), Y(1))\n    def savefig(*args, **kwargs): return args # markdown-exec: hide\n    savefig(b, \"test.svg\", size=\"1,1\", theme=\"dark\")\n    ```\n    \"\"\"\n    make_diagram(x, *args, **kwargs).savefig(filename)\n</code></pre>"},{"location":"api/execution/","title":"Execution","text":""},{"location":"api/execution/#qadence.execution.expectation","title":"<code>expectation(x, observable, values=None, state=None, backend=BackendName.PYQTORCH, diff_mode=None, noise=None, endianness=Endianness.BIG, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.expectation</code> method.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>observable</code> <p>Observable(s) w.r.t. which the expectation is computed.</p> <p> TYPE: <code>Union[list[AbstractBlock], AbstractBlock]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>Which differentiation mode to use.</p> <p> TYPE: <code>Union[DiffMode, str, None]</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A wavefunction</p> <pre><code>from qadence import RX, Z, Register, QuantumCircuit, expectation\n\nreg = Register(1)\nblock = RX(0, 0.5)\nobservable = Z(0)\ncirc = QuantumCircuit(reg, block)\n\n# You can compute the expectation for a\n# QuantumCircuit with a given observable.\nexpectation(circ, observable)\n\n# You can also use only a block.\n# In this case the register is constructed automatically to\n# Register.line(block.n_qubits)\nexpectation(block, observable)\n\n# Or a register and block\nexpectation(reg, block, observable)\n</code></pre> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef expectation(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    observable: Union[list[AbstractBlock], AbstractBlock],\n    values: Union[dict, None] = None,\n    state: Tensor = None,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: Union[DiffMode, str, None] = None,\n    noise: Union[NoiseHandler, None] = None,\n    endianness: Endianness = Endianness.BIG,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; Tensor:\n    \"\"\"Convenience wrapper for the `QuantumModel.expectation` method.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        observable: Observable(s) w.r.t. which the expectation is computed.\n        values: User-facing parameter dict.\n        state: Initial state.\n        backend: Name of the backend to run on.\n        diff_mode: Which differentiation mode to use.\n        endianness: The target device endianness.\n        configuration: The backend configuration.\n\n    Returns:\n        A wavefunction\n\n    ```python exec=\"on\" source=\"material-block\"\n    from qadence import RX, Z, Register, QuantumCircuit, expectation\n\n    reg = Register(1)\n    block = RX(0, 0.5)\n    observable = Z(0)\n    circ = QuantumCircuit(reg, block)\n\n    # You can compute the expectation for a\n    # QuantumCircuit with a given observable.\n    expectation(circ, observable)\n\n    # You can also use only a block.\n    # In this case the register is constructed automatically to\n    # Register.line(block.n_qubits)\n    expectation(block, observable)\n\n    # Or a register and block\n    expectation(reg, block, observable)\n    ```\n    \"\"\"\n\n    raise ValueError(f\"Cannot execute {type(x)}\")\n</code></pre>"},{"location":"api/execution/#qadence.execution.run","title":"<code>run(x, *args, values=None, state=None, backend=BackendName.PYQTORCH, endianness=Endianness.BIG, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.run</code> method.</p> <p>This is a <code>functools.singledispatch</code>ed function so it can be called with a number of different arguments. See the examples of the <code>expectation</code> function. This function works exactly the same.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A wavefunction</p> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef run(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    *args: Any,\n    values: Union[dict, None] = None,\n    state: Tensor = None,\n    backend: BackendName = BackendName.PYQTORCH,\n    endianness: Endianness = Endianness.BIG,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; Tensor:\n    \"\"\"Convenience wrapper for the `QuantumModel.run` method.\n\n     This is a\n    `functools.singledispatch`ed function so it can be called with a number of different arguments.\n    See the examples of the [`expectation`][qadence.execution.expectation] function. This function\n    works exactly the same.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        values: User-facing parameter dict.\n        state: Initial state.\n        backend: Name of the backend to run on.\n        endianness: The target device endianness.\n        configuration: The backend configuration.\n\n    Returns:\n        A wavefunction\n    \"\"\"\n    raise ValueError(f\"Cannot run {type(x)}\")\n</code></pre>"},{"location":"api/execution/#qadence.execution.sample","title":"<code>sample(x, *args, values=None, state=None, n_shots=100, backend=BackendName.PYQTORCH, endianness=Endianness.BIG, noise=None, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.sample</code> method.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Union[Tensor, None]</code> DEFAULT: <code>None</code> </p> <code>n_shots</code> <p>Number of shots per element in the batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>noise</code> <p>The noise model to use if any.</p> <p> TYPE: <code>Union[NoiseHandler, None]</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Counter]</code> <p>A list of Counter instances with the sample results</p> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef sample(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    *args: Any,\n    values: Union[dict, None] = None,\n    state: Union[Tensor, None] = None,\n    n_shots: int = 100,\n    backend: BackendName = BackendName.PYQTORCH,\n    endianness: Endianness = Endianness.BIG,\n    noise: Union[NoiseHandler, None] = None,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; list[Counter]:\n    \"\"\"Convenience wrapper for the `QuantumModel.sample` method.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        values: User-facing parameter dict.\n        state: Initial state.\n        n_shots: Number of shots per element in the batch.\n        backend: Name of the backend to run on.\n        endianness: The target device endianness.\n        noise: The noise model to use if any.\n        configuration: The backend configuration.\n\n    Returns:\n        A list of Counter instances with the sample results\n    \"\"\"\n    raise ValueError(f\"Cannot sample from {type(x)}\")\n</code></pre>"},{"location":"api/ml_tools/","title":"QML tools","text":""},{"location":"api/ml_tools/#ml-tools","title":"ML Tools","text":"<p>This module implements a <code>Trainer</code> class for torch <code>Modules</code> and <code>QuantumModel</code>. It also implements the <code>QNN</code> class and callbacks that can be used with the trainer module.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer","title":"<code>Trainer(model, optimizer, config, loss_fn='mse', train_dataloader=None, val_dataloader=None, test_dataloader=None, optimize_step=optimize_step, device=None, dtype=None, max_batches=None)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class to manage and execute training, validation, and testing loops for a model (eg.</p> <p>QNN).</p> <p>This class handles the overall training process, including: - Managing epochs and steps - Handling data loading and batching - Computing and updating gradients - Logging and monitoring training metrics</p> ATTRIBUTE DESCRIPTION <code>current_epoch</code> <p>The current epoch number.</p> <p> TYPE: <code>int</code> </p> <code>global_step</code> <p>The global step across all epochs.</p> <p> TYPE: <code>int</code> </p> <code>log_device</code> <p>Device for logging, default is \"cpu\".</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device used for computation.</p> <p> TYPE: <code>device</code> </p> <code>dtype</code> <p>Data type used for computation.</p> <p> TYPE: <code>dtype | None</code> </p> <code>data_dtype</code> <p>Data type for data. Depends on the model's data type.</p> <p> TYPE: <code>dtype | None</code> </p> Inherited Attributes <p>use_grad (bool): Indicates if gradients are used for optimization. Default is True.</p> <p>model (nn.Module): The neural network model. optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training. config (TrainConfig): The configuration settings for training. train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data. val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data. test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.</p> <p>optimize_step (Callable): Function for performing an optimization step. loss_fn (Callable): loss function to use.</p> <p>num_training_batches (int): Number of training batches. num_validation_batches (int): Number of validation batches. num_test_batches (int): Number of test batches.</p> <p>state (str): Current state in the training process</p> <p>Default training routine <pre><code>for epoch in max_iter + 1:\n    # Training\n    for batch in train_batches:\n        train model\n    # Validation\n    if val_every % epoch == 0:\n        for batch in val_batches:\n            train model\n</code></pre></p> Notes <ul> <li>In case of InfiniteTensorDataset, number of batches = 1.</li> <li>In case of TensorDataset, number of batches are default.</li> <li>Training is run for max_iter + 1 epochs. Epoch 0 logs untrained model.</li> <li>Please look at the CallbackManager initialize_callbacks method to review the default     logging behavior.</li> </ul> <p>Examples:</p> <pre><code>import torch\nfrom torch.optim import SGD\nfrom qadence import (\n    feature_map,\n    hamiltonian_factory,\n    hea,\n    QNN,\n    QuantumCircuit,\n    TrainConfig,\n    Z,\n)\nfrom qadence.ml_tools.trainer import Trainer\nfrom qadence.ml_tools.optimize_step import optimize_step\nfrom qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.data import to_dataloader\n\n# Initialize the model\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=2)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\n\n# Set up the optimizer\noptimizer = SGD(model.parameters(), lr=0.001)\n\n# Use TrainConfig for configuring the training process\nconfig = TrainConfig(\n    max_iter=100,\n    print_every=10,\n    write_every=10,\n    checkpoint_every=10,\n    val_every=10\n)\n\n# Create the Trainer instance with TrainConfig\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\",\n    optimize_step=optimize_step\n)\n\nbatch_size = 25\nx = torch.linspace(0, 1, 32).reshape(-1, 1)\ny = torch.sin(x)\ntrain_loader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_loader = to_dataloader(x, y, batch_size=batch_size, infinite=False)\n\n# Train the model\nmodel, optimizer = trainer.fit(train_loader, val_loader)\n</code></pre> <p>This also supports both gradient based and gradient free optimization. The default support is for gradient based optimization.</p> <p>Notes:</p> <ul> <li>set_use_grad() (class level):This method is used to set the global <code>use_grad</code> flag,     controlling whether the trainer uses gradient-based optimization. <pre><code># gradient based\nTrainer.set_use_grad(True)\n\n# gradient free\nTrainer.set_use_grad(False)\n</code></pre></li> <li>Context Managers (instance level):  <code>enable_grad_opt()</code> and <code>disable_grad_opt()</code> are     context managers that temporarily switch the optimization mode for specific code blocks.     This is useful when you want to mix gradient-based and gradient-free optimization     in the same training process. <pre><code># gradient based\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit()\n\n# gradient free\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit()\n</code></pre></li> </ul> <p>Examples</p> <p>Gradient based optimization example Usage: <pre><code>from torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nTrainer.set_use_grad(True)\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>trainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Gradient free optimization example Usage: <pre><code>import nevergrad as ng\nfrom qadence.ml_tools.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n                budget=config.max_iter, parametrization= num_parameters(model)\n                )\n\nTrainer.set_use_grad(False)\ntrainer = Trainer(\n    model=model,\n    optimizer=ng_optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>import nevergrad as ng\nfrom qadence.ml_tools.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n        budget=config.max_iter, parametrization= num_parameters(model)\n        )\n\ntrainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Initializes the Trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>Training configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>Loss function used for training. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>optimize_step</code> <p>Function to execute an optimization step.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>optimize_step</code> </p> <code>device</code> <p>Device to use for computation.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for computation.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    optimize_step: Callable = optimize_step,\n    device: torch_device | None = None,\n    dtype: torch_dtype | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the Trainer class.\n\n    Args:\n        model (nn.Module): The PyTorch model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training.\n        config (TrainConfig): Training configuration object.\n        loss_fn (str | Callable ): Loss function used for training.\n            If not specified, default mse loss will be used.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for test data.\n        optimize_step (Callable): Function to execute an optimization step.\n        device (torch_device): Device to use for computation.\n        dtype (torch_dtype): Data type for computation.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    super().__init__(\n        model=model,\n        optimizer=optimizer,\n        config=config,\n        loss_fn=loss_fn,\n        optimize_step=optimize_step,\n        train_dataloader=train_dataloader,\n        val_dataloader=val_dataloader,\n        test_dataloader=test_dataloader,\n        max_batches=max_batches,\n    )\n    self.current_epoch: int = 0\n    self.global_step: int = 0\n    self.log_device: str = \"cpu\" if device is None else device\n    self.device: torch_device | None = device\n    self.dtype: torch_dtype | None = dtype\n    self.data_dtype: torch_dtype | None = None\n    self.stop_training: bool = False\n    if self.dtype:\n        self.data_dtype = float64 if (self.dtype == complex128) else float32\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer._batch_iter","title":"<code>_batch_iter(dataloader, num_batches)</code>","text":"<p>Yields batches from the provided dataloader.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>The dataloader to iterate over.</p> <p> TYPE: <code>[DataLoader]</code> </p> <code>num_batches</code> <p>The maximum number of batches to yield.</p> <p> TYPE: <code>int</code> </p> YIELDS DESCRIPTION <code>Iterable[tuple[Tensor, ...] | None]</code> <p>Iterable[tuple[torch.Tensor, ...] | None]: A batch from the dataloader moved to the specified device and dtype.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def _batch_iter(\n    self,\n    dataloader: DataLoader | DictDataLoader,\n    num_batches: int,\n) -&gt; Iterable[tuple[torch.Tensor, ...] | None]:\n    \"\"\"\n    Yields batches from the provided dataloader.\n\n    Args:\n        dataloader ([DataLoader]): The dataloader to iterate over.\n        num_batches (int): The maximum number of batches to yield.\n\n    Yields:\n        Iterable[tuple[torch.Tensor, ...] | None]: A batch from the dataloader moved to the\n            specified device and dtype.\n    \"\"\"\n    if dataloader is None:\n        for _ in range(num_batches):\n            yield None\n    else:\n        for batch in islice(dataloader, num_batches):\n            # batch is moved to device inside optimize step\n            # batch = data_to_device(batch, device=self.device, dtype=self.data_dtype)\n            yield batch\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer._fit_end","title":"<code>_fit_end()</code>","text":"<p>Finalizes the training and closes the writer.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def _fit_end(self) -&gt; None:\n    \"\"\"Finalizes the training and closes the writer.\"\"\"\n    self.callback_manager.end_training(trainer=self)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer._fit_setup","title":"<code>_fit_setup()</code>","text":"<p>Sets up the training environment, initializes configurations,.</p> <p>and moves the model to the specified device and data type. The callback_manager.start_training takes care of loading checkpoint, and setting up the writer.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def _fit_setup(self) -&gt; None:\n    \"\"\"\n    Sets up the training environment, initializes configurations,.\n\n    and moves the model to the specified device and data type.\n    The callback_manager.start_training takes care of loading checkpoint,\n    and setting up the writer.\n    \"\"\"\n    self.stop_training = False\n    self.config_manager.initialize_config()\n    self.callback_manager.start_training(trainer=self)\n\n    # Move model to device\n    if isinstance(self.model, nn.DataParallel):\n        self.model = self.model.module.to(device=self.device, dtype=self.dtype)\n    else:\n        self.model = self.model.to(device=self.device, dtype=self.dtype)\n\n    # Progress bar for training visualization\n    self.progress: Progress = Progress(\n        TextColumn(\"[progress.description]{task.description}\"),\n        BarColumn(),\n        TaskProgressColumn(),\n        TimeRemainingColumn(elapsed_when_finished=True),\n    )\n\n    # Quick Fix for iteration 0\n    self._reset_model_and_opt()\n\n    # Run validation at the start if specified in the configuration\n    self.perform_val = self.config.val_every &gt; 0\n    if self.perform_val:\n        self.run_validation(self.val_dataloader)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer._modify_batch_end_loss_metrics","title":"<code>_modify_batch_end_loss_metrics(loss_metrics)</code>","text":"<p>Modifies the loss and metrics at the end of batch for proper logging.</p> <p>All metrics are prefixed with the proper state of the training process  - \"train_\" or \"val_\" or \"test_\" A \"{state}_loss\" is added to metrics.</p> PARAMETER DESCRIPTION <code>loss_metrics</code> <p>Original loss and metrics.</p> <p> TYPE: <code>tuple[Tensor, dict[str, Any]]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[None | torch.Tensor, dict[str, Any]]: Modified loss and metrics.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def _modify_batch_end_loss_metrics(\n    self, loss_metrics: tuple[torch.Tensor, dict[str, Any]]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Modifies the loss and metrics at the end of batch for proper logging.\n\n    All metrics are prefixed with the proper state of the training process\n     - \"train_\" or \"val_\" or \"test_\"\n    A \"{state}_loss\" is added to metrics.\n\n    Args:\n        loss_metrics (tuple[torch.Tensor, dict[str, Any]]): Original loss and metrics.\n\n    Returns:\n        tuple[None | torch.Tensor, dict[str, Any]]: Modified loss and metrics.\n    \"\"\"\n    for phase in [\"train\", \"val\", \"test\"]:\n        if phase in self.training_stage:\n            loss, metrics = loss_metrics\n            updated_metrics = {f\"{phase}_{key}\": value for key, value in metrics.items()}\n            updated_metrics[f\"{phase}_loss\"] = loss\n            return loss, updated_metrics\n    return loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer._reset_model_and_opt","title":"<code>_reset_model_and_opt()</code>","text":"<p>Save model_old and optimizer_old for epoch 0.</p> <p>This allows us to create a copy of model and optimizer before running the optimization.</p> <p>We do this because optimize step provides loss, metrics before step of optimization To align them with model/optimizer correctly, we checkpoint the older copy of the model.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def _reset_model_and_opt(self) -&gt; None:\n    \"\"\"\n    Save model_old and optimizer_old for epoch 0.\n\n    This allows us to create a copy of model\n    and optimizer before running the optimization.\n\n    We do this because optimize step provides loss, metrics\n    before step of optimization\n    To align them with model/optimizer correctly, we checkpoint\n    the older copy of the model.\n    \"\"\"\n\n    # TODO: review optimize_step to provide iteration aligned model and loss.\n    try:\n        # Deep copy model and optimizer to maintain checkpoints\n        self.model_old = copy.deepcopy(self.model)\n        self.optimizer_old = copy.deepcopy(self.optimizer)\n    except Exception:\n        self.model_old = self.model\n        self.optimizer_old = self.optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer._train","title":"<code>_train()</code>","text":"<p>Runs the main training loop, iterating over epochs.</p> RETURNS DESCRIPTION <code>list[list[tuple[Tensor, dict[str, Any]]]]</code> <p>list[list[tuple[torch.Tensor, dict[str, Any]]]]: Training loss</p> <code>list[list[tuple[Tensor, dict[str, Any]]]]</code> <p>metrics for all epochs. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Training Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train\")\ndef _train(self) -&gt; list[list[tuple[torch.Tensor, dict[str, Any]]]]:\n    \"\"\"\n    Runs the main training loop, iterating over epochs.\n\n    Returns:\n        list[list[tuple[torch.Tensor, dict[str, Any]]]]: Training loss\n        metrics for all epochs.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.on_train_start()\n    train_losses = []\n    val_losses = []\n\n    with self.progress:\n        train_task = self.progress.add_task(\n            \"Training\", total=self.config_manager.config.max_iter\n        )\n        if self.perform_val:\n            val_task = self.progress.add_task(\n                \"Validation\",\n                total=(self.config_manager.config.max_iter + 1) / self.config.val_every,\n            )\n        for epoch in range(\n            self.global_step, self.global_step + self.config_manager.config.max_iter + 1\n        ):\n            if not self.stop_training:\n                try:\n                    self.current_epoch = epoch\n                    self.on_train_epoch_start()\n                    train_epoch_loss_metrics = self.run_training(self.train_dataloader)\n                    train_losses.append(train_epoch_loss_metrics)\n                    self.on_train_epoch_end(train_epoch_loss_metrics)\n\n                    # Run validation periodically if specified\n                    if self.perform_val and self.current_epoch % self.config.val_every == 0:\n                        self.on_val_epoch_start()\n                        val_epoch_loss_metrics = self.run_validation(self.val_dataloader)\n                        val_losses.append(val_epoch_loss_metrics)\n                        self.on_val_epoch_end(val_epoch_loss_metrics)\n                        self.progress.update(val_task, advance=1)\n\n                    self.progress.update(train_task, advance=1)\n                except KeyboardInterrupt:\n                    logger.info(\"Terminating training gracefully after the current iteration.\")\n                    break\n\n    self.on_train_end(train_losses, val_losses)\n    return train_losses\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.build_optimize_result","title":"<code>build_optimize_result(result)</code>","text":"<p>Builds and stores the optimization result by calculating the average loss and metrics.</p> <p>Result (or loss_metrics) can have multiple formats: - <code>None</code> Indicates no loss or metrics data is provided. - <code>tuple[torch.Tensor, dict[str, Any]]</code> A single tuple containing the loss tensor     and metrics dictionary - at the end of batch. - <code>list[tuple[torch.Tensor, dict[str, Any]]]</code> A list of tuples for     multiple batches. - <code>list[list[tuple[torch.Tensor, dict[str, Any]]]]</code> A list of lists of tuples, where each inner list represents metrics across multiple batches within an epoch.</p> PARAMETER DESCRIPTION <code>result</code> <p>(None |     tuple[torch.Tensor, dict[Any, Any]] |     list[tuple[torch.Tensor, dict[Any, Any]]] |     list[list[tuple[torch.Tensor, dict[Any, Any]]]])         The loss and metrics data, which can have multiple formats</p> <p> TYPE: <code>None | tuple[Tensor, dict[Any, Any]] | list[tuple[Tensor, dict[Any, Any]]] | list[list[tuple[Tensor, dict[Any, Any]]]]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>This method does not return anything. It sets <code>self.opt_result</code> with</p> <p> TYPE: <code>None</code> </p> <code>None</code> <p>the computed average loss and metrics.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def build_optimize_result(\n    self,\n    result: (\n        None\n        | tuple[torch.Tensor, dict[Any, Any]]\n        | list[tuple[torch.Tensor, dict[Any, Any]]]\n        | list[list[tuple[torch.Tensor, dict[Any, Any]]]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Builds and stores the optimization result by calculating the average loss and metrics.\n\n    Result (or loss_metrics) can have multiple formats:\n    - `None` Indicates no loss or metrics data is provided.\n    - `tuple[torch.Tensor, dict[str, Any]]` A single tuple containing the loss tensor\n        and metrics dictionary - at the end of batch.\n    - `list[tuple[torch.Tensor, dict[str, Any]]]` A list of tuples for\n        multiple batches.\n    - `list[list[tuple[torch.Tensor, dict[str, Any]]]]` A list of lists of tuples,\n    where each inner list represents metrics across multiple batches within an epoch.\n\n    Args:\n        result: (None |\n                tuple[torch.Tensor, dict[Any, Any]] |\n                list[tuple[torch.Tensor, dict[Any, Any]]] |\n                list[list[tuple[torch.Tensor, dict[Any, Any]]]])\n                    The loss and metrics data, which can have multiple formats\n\n    Returns:\n        None: This method does not return anything. It sets `self.opt_result` with\n        the computed average loss and metrics.\n    \"\"\"\n    loss_metrics = result\n    if loss_metrics is None:\n        loss = None\n        metrics: dict[Any, Any] = {}\n    elif isinstance(loss_metrics, tuple):\n        # Single tuple case\n        loss, metrics = loss_metrics\n    else:\n        last_epoch: list[tuple[torch.Tensor, dict[Any, Any]]] = []\n        if isinstance(loss_metrics, list):\n            # Check if it's a list of tuples\n            if all(isinstance(item, tuple) for item in loss_metrics):\n                last_epoch = cast(list[tuple[torch.Tensor, dict[Any, Any]]], loss_metrics)\n            # Check if it's a list of lists of tuples\n            elif all(isinstance(item, list) for item in loss_metrics):\n                last_epoch = cast(\n                    list[tuple[torch.Tensor, dict[Any, Any]]],\n                    loss_metrics[-1] if loss_metrics else [],\n                )\n            else:\n                raise ValueError(\n                    \"Invalid format for result: Expected None, tuple, list of tuples,\"\n                    \" or list of lists of tuples.\"\n                )\n\n        if not last_epoch:\n            loss, metrics = None, {}\n        else:\n            # Compute the average loss over the batches\n            loss_tensor = torch.stack([loss_batch for loss_batch, _ in last_epoch])\n            avg_loss = loss_tensor.mean()\n\n            # Collect and average metrics for all batches\n            metric_keys = last_epoch[0][1].keys()\n            metrics_stacked: dict = {key: [] for key in metric_keys}\n\n            for _, metrics_batch in last_epoch:\n                for key in metric_keys:\n                    value = metrics_batch[key]\n                    metrics_stacked[key].append(value)\n\n            avg_metrics = {key: torch.stack(metrics_stacked[key]).mean() for key in metric_keys}\n\n            loss, metrics = avg_loss, avg_metrics\n\n    # Store the optimization result\n    self.opt_result = OptimizeResult(\n        self.current_epoch, self.model_old, self.optimizer_old, loss, metrics\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.fit","title":"<code>fit(train_dataloader=None, val_dataloader=None)</code>","text":"<p>Fits the model using the specified training configuration.</p> <p>The dataloaders can be provided to train on new datasets, or the default dataloaders provided in the trainer will be used.</p> PARAMETER DESCRIPTION <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Module, Optimizer]</code> <p>tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def fit(\n    self,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; tuple[nn.Module, optim.Optimizer]:\n    \"\"\"\n    Fits the model using the specified training configuration.\n\n    The dataloaders can be provided to train on new datasets, or the default dataloaders\n    provided in the trainer will be used.\n\n    Args:\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n\n    Returns:\n        tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.\n    \"\"\"\n    if train_dataloader is not None:\n        self.train_dataloader = train_dataloader\n    if val_dataloader is not None:\n        self.val_dataloader = val_dataloader\n\n    self._fit_setup()\n    self._train()\n    self._fit_end()\n    self.training_stage = TrainingStage(\"idle\")\n    return self.model, self.optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_test_batch","title":"<code>run_test_batch(batch)</code>","text":"<p>Runs a single test batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"test_batch\")\ndef run_test_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single test batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_train_batch","title":"<code>run_train_batch(batch)</code>","text":"<p>Runs a single training batch, performing optimization.</p> <p>We use the step function to optimize the model based on use_grad.     use_grad = True entails gradient based optimization, for which we use     optimize_step function.     use_grad = False entails gradient free optimization, for which we use     update_ng_parameters function.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch. tuple of (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_batch\")\ndef run_train_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single training batch, performing optimization.\n\n    We use the step function to optimize the model based on use_grad.\n        use_grad = True entails gradient based optimization, for which we use\n        optimize_step function.\n        use_grad = False entails gradient free optimization, for which we use\n        update_ng_parameters function.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n            tuple of (loss, metrics)\n    \"\"\"\n\n    if self.use_grad:\n        # Perform gradient-based optimization\n        loss_metrics = self.optimize_step(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            xs=batch,\n            device=self.device,\n            dtype=self.data_dtype,\n        )\n    else:\n        # Perform optimization using Nevergrad\n        loss, metrics, ng_params = update_ng_parameters(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            data=batch,\n            ng_params=self.ng_params,  # type: ignore[arg-type]\n        )\n        self.ng_params = ng_params\n        loss_metrics = loss, metrics\n\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_training","title":"<code>run_training(dataloader)</code>","text":"<p>Runs the training for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_epoch\")\ndef run_training(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the training for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for training data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.train()\n    train_epoch_loss_metrics = []\n    # Quick Fix for iteration 0\n    self._reset_model_and_opt()\n\n    for batch in self._batch_iter(dataloader, self.num_training_batches):\n        self.on_train_batch_start(batch)\n        train_batch_loss_metrics = self.run_train_batch(batch)\n        train_epoch_loss_metrics.append(train_batch_loss_metrics)\n        self.on_train_batch_end(train_batch_loss_metrics)\n\n    return train_epoch_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_val_batch","title":"<code>run_val_batch(batch)</code>","text":"<p>Runs a single validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_batch\")\ndef run_val_batch(self, batch: tuple[torch.Tensor, ...]) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single validation batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_validation","title":"<code>run_validation(dataloader)</code>","text":"<p>Runs the validation loop for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_epoch\")\ndef run_validation(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the validation loop for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for validation data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.eval()\n    val_epoch_loss_metrics = []\n\n    for batch in self._batch_iter(dataloader, self.num_validation_batches):\n        self.on_val_batch_start(batch)\n        val_batch_loss_metrics = self.run_val_batch(batch)\n        val_epoch_loss_metrics.append(val_batch_loss_metrics)\n        self.on_val_batch_end(val_batch_loss_metrics)\n\n    return val_epoch_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.test","title":"<code>test(test_dataloader=None)</code>","text":"<p>Runs the testing loop if a test DataLoader is provided.</p> <p>if the test_dataloader is not provided, default test_dataloader defined in the Trainer class is used.</p> PARAMETER DESCRIPTION <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                    -&gt; tuples Test Batches            -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def test(self, test_dataloader: DataLoader = None) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the testing loop if a test DataLoader is provided.\n\n    if the test_dataloader is not provided, default test_dataloader defined\n    in the Trainer class is used.\n\n    Args:\n        test_dataloader (DataLoader): DataLoader for test data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                    -&gt; tuples\n            Test Batches            -&gt; (loss, metrics)\n    \"\"\"\n    if test_dataloader is not None:\n        self.test_dataloader = test_dataloader\n\n    self.model.eval()\n    test_loss_metrics = []\n\n    for batch in self._batch_iter(test_dataloader, self.num_training_batches):\n        self.on_test_batch_start(batch)\n        loss_metrics = self.run_test_batch(batch)\n        test_loss_metrics.append(loss_metrics)\n        self.on_test_batch_end(loss_metrics)\n\n    return test_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig","title":"<code>AnsatzConfig(depth=1, ansatz_type=AnsatzType.HEA, ansatz_strategy=Strategy.DIGITAL, strategy_args=dict(), m_block_qubits=None, param_prefix='theta', tag=None)</code>  <code>dataclass</code>","text":""},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.ansatz_strategy","title":"<code>ansatz_strategy = Strategy.DIGITAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Ansatz strategy.</p> <p><code>Strategy.DIGITAL</code> for fully digital ansatz. Required if <code>ansatz_type</code> is <code>AnsatzType.ALA</code>. <code>Strategy.SDAQC</code> for analog entangling block. Only available for <code>AnsatzType.HEA</code> or <code>AnsatzType.ALA</code>. <code>Strategy.RYDBERG</code> for fully rydberg hea ansatz. Only available for <code>AnsatzType.HEA</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.ansatz_type","title":"<code>ansatz_type = AnsatzType.HEA</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>What type of ansatz.</p> <p><code>AnsatzType.HEA</code> for Hardware Efficient Ansatz. <code>AnsatzType.IIA</code> for Identity Intialized Ansatz. <code>AnsatzType.ALA</code> for Alternating Layer Ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.depth","title":"<code>depth = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of layers of the ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.m_block_qubits","title":"<code>m_block_qubits = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The number of qubits in the local entangling block of an Alternating Layer Ansatz (ALA).</p> <p>Only used when <code>ansatz_type</code> is <code>AnsatzType.ALA</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.param_prefix","title":"<code>param_prefix = 'theta'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The base bame of the variational parameter.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.strategy_args","title":"<code>strategy_args = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary containing keyword arguments to the function creating the ansatz.</p> <p>Details about each below.</p> <p>For <code>Strategy.DIGITAL</code> strategy, accepts the following:     periodic (bool): if the qubits should be linked periodically.         periodic=False is not supported in emu-c.     operations (list): list of operations to cycle through in the         digital single-qubit rotations of each layer.         Defaults to  [RX, RY, RX] for hea and [RX, RY] for iia.     entangler (AbstractBlock): 2-qubit entangling operation.         Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlld rotations         will have variational parameters on the rotation angles.         Defaults to CNOT</p> <p>For <code>Strategy.SDAQC</code> strategy, accepts the following:     operations (list): list of operations to cycle through in the         digital single-qubit rotations of each layer.         Defaults to  [RX, RY, RX] for hea and [RX, RY] for iia.     entangler (AbstractBlock): Hamiltonian generator for the         analog entangling layer. Time parameter is considered variational.         Defaults to NN interaction.</p> <p>For <code>Strategy.RYDBERG</code> strategy, accepts the following:     addressable_detuning: whether to turn on the trainable semi-local addressing pattern         on the detuning (n_i terms in the Hamiltonian).         Defaults to True.     addressable_drive: whether to turn on the trainable semi-local addressing pattern         on the drive (sigma_i^x terms in the Hamiltonian).         Defaults to False.     tunable_phase: whether to have a tunable phase to get both sigma^x and sigma^y rotations         in the drive term. If False, only a sigma^x term will be included in the drive part         of the Hamiltonian generator.         Defaults to False.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.tag","title":"<code>tag = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to indicate the name tag of the ansatz.</p> <p>Defaults to None, in which case no tag will be applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig","title":"<code>FeatureMapConfig(num_features=0, basis_set=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT, feature_range=None, target_range=None, multivariate_strategy=MultivariateStrategy.PARALLEL, feature_map_strategy=Strategy.DIGITAL, param_prefix=None, num_repeats=0, operation=None, inputs=None, tag=None)</code>  <code>dataclass</code>","text":""},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.basis_set","title":"<code>basis_set = BasisSet.FOURIER</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basis set for feature encoding.</p> <p>Takes qadence.BasisSet. Give a single BasisSet to use the same for all features. Give a dict of (str, BasisSet) where the key is the name of the variable and the value is the BasisSet to use for encoding that feature. BasisSet.FOURIER for Fourier encoding. BasisSet.CHEBYSHEV for Chebyshev encoding.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.feature_map_strategy","title":"<code>feature_map_strategy = Strategy.DIGITAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Strategy for feature map.</p> <p>Accepts DIGITAL, ANALOG or RYDBERG. Defaults to DIGITAL. If the strategy is incompatible with the <code>operation</code> chosen, then <code>operation</code> gets preference and the given strategy is ignored.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.feature_range","title":"<code>feature_range = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Range of data that the input data is assumed to come from.</p> <p>Give a single tuple to use the same range for all features. Give a dict of (str, tuple) where the key is the name of the variable and the value is the feature range to use for that feature.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.inputs","title":"<code>inputs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List that indicates the order of variables of the tensors that are passed.</p> <p>Optional if a single feature is being encoded, required otherwise. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.multivariate_strategy","title":"<code>multivariate_strategy = MultivariateStrategy.PARALLEL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The encoding strategy in case of multi-variate function.</p> <p>Takes qadence.MultivariateStrategy. If PARALLEL, the features are encoded in one block of rotation gates with the register being split in sub-registers for each feature. If SERIES, the features are encoded sequentially using the full register for each feature, with an ansatz block between them. PARALLEL is allowed only for DIGITAL <code>feature_map_strategy</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.num_features","title":"<code>num_features = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of feature parameters to be encoded.</p> <p>Defaults to 0. Thus, no feature parameters are encoded.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.num_repeats","title":"<code>num_repeats = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of feature map layers repeated in the data reuploading step.</p> <p>If all features are to be repeated the same number of times, then can give a single <code>int</code>. For different number of repetitions for each feature, provide a dict of (str, int) where the key is the name of the variable and the value is the number of repetitions for that feature. This amounts to the number of additional reuploads. So if <code>num_repeats</code> is N, the data gets uploaded N+1 times. Defaults to no repetition.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.operation","title":"<code>operation = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of operation.</p> <p>Choose among the analog or digital rotations or a custom callable function returning an AnalogBlock instance. If the type of operation is incompatible with the <code>strategy</code> chosen, then <code>operation</code> gets preference and the given strategy is ignored.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.param_prefix","title":"<code>param_prefix = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String prefix to create trainable parameters in Feature Map.</p> <p>A string prefix to create trainable parameters multiplying the feature parameter inside the feature-encoding function. Note that currently this does not take into account the domain of the feature-encoding function. Defaults to <code>None</code> and thus, the feature map is not trainable. Note that this is separate from the name of the parameter. The user can provide a single prefix for all features, and it will be appended by appropriate feature name automatically.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.reupload_scaling","title":"<code>reupload_scaling = ReuploadScaling.CONSTANT</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Scaling for encoding the same feature on different qubits.</p> <p>Scaling used to encode the same feature on different qubits in the same layer of the feature maps. Takes qadence.ReuploadScaling. Give a single ReuploadScaling to use the same for all features. Give a dict of (str, ReuploadScaling) where the key is the name of the variable and the value is the ReuploadScaling to use for encoding that feature. ReuploadScaling.CONSTANT for constant scaling. ReuploadScaling.TOWER for linearly increasing scaling. ReuploadScaling.EXP for exponentially increasing scaling.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.tag","title":"<code>tag = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to indicate the name tag of the feature map.</p> <p>Defaults to None, in which case no tag will be applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.target_range","title":"<code>target_range = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Range of data the data encoder assumes as natural range.</p> <p>Give a single tuple to use the same range for all features. Give a dict of (str, tuple) where the key is the name of the variable and the value is the target range to use for that feature.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig","title":"<code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=lambda: list()(), log_model=False, root_folder=Path('./qml_logs'), create_subfolder_per_run=False, log_folder=Path('./'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=ExperimentTrackingTool.TENSORBOARD, hyperparams=dict(), plotting_functions=tuple(), _subfolders=list())</code>  <code>dataclass</code>","text":"<p>Default configuration for the training process.</p> <p>This class provides default settings for various aspects of the training loop, such as logging, checkpointing, and validation. The default values for these fields can be customized when an instance of <code>TrainConfig</code> is created.</p> <p>Example: <pre><code>from qadence.ml_tools import TrainConfig\nc = TrainConfig(root_folder=\"/tmp/train\")\n</code></pre> <pre><code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=[], log_model=False, root_folder='/tmp/train', create_subfolder_per_run=False, log_folder=PosixPath('.'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=&lt;ExperimentTrackingTool.TENSORBOARD: 'tensorboard'&gt;, hyperparams={}, plotting_functions=(), _subfolders=[])\n</code></pre> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig._subfolders","title":"<code>_subfolders = field(default_factory=list)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of subfolders used for logging different runs using the same config inside the.</p> <p>root folder.</p> <p>Each subfolder is of structure <code>&lt;id&gt;_&lt;timestamp&gt;_&lt;PID&gt;</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.batch_size","title":"<code>batch_size = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The batch size to use when processing a list or tuple of torch.Tensors.</p> <p>This specifies how many samples are processed in each training iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.callbacks","title":"<code>callbacks = field(default_factory=lambda: list())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of callbacks to execute during training.</p> <p>Callbacks can be used for custom behaviors, such as early stopping, custom logging, or other actions triggered at specific events.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.checkpoint_best_only","title":"<code>checkpoint_best_only = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>True</code>, checkpoints are only saved if there is an improvement in the.</p> <p>validation metric. This conserves storage by only keeping the best models.</p> <p>validation_criterion is required when this is set to True.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.checkpoint_every","title":"<code>checkpoint_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for saving model and optimizer checkpoints during training.</p> <p>Set to 0 to disable checkpointing. This helps in resuming training or recovering models. Note that setting checkpoint_best_only = True will disable this and only best checkpoints will be saved.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.create_subfolder_per_run","title":"<code>create_subfolder_per_run = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to create a subfolder for each run, named <code>&lt;id&gt;_&lt;timestamp&gt;_&lt;PID&gt;</code>.</p> <p>This ensures logs and checkpoints from different runs do not overwrite each other, which is helpful for rapid prototyping. If <code>False</code>, training will resume from the latest checkpoint if one exists in the specified log folder.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.hyperparams","title":"<code>hyperparams = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of hyperparameters to be tracked.</p> <p>This can include learning rates, regularization parameters, or any other training-related configurations.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.log_folder","title":"<code>log_folder = Path('./')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The log folder for saving checkpoints and tensorboard logs.</p> <p>This stores the path where all logs and checkpoints are being saved for this training session. <code>log_folder</code> takes precedence over <code>root_folder</code> and <code>create_subfolder_per_run</code> arguments. If the user specifies a log_folder, all checkpoints will be saved in this folder and <code>root_folder</code> argument will not be used.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.log_model","title":"<code>log_model = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to log a serialized version of the model.</p> <p>When set to <code>True</code>, the model's state will be logged, useful for model versioning and reproducibility.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.max_iter","title":"<code>max_iter = 10000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of training iterations (epochs) to perform.</p> <p>This defines the total number of times the model will be updated.</p> <p>In case of InfiniteTensorDataset, each epoch will have 1 batch. In case of TensorDataset, each epoch will have len(dataloader) batches.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.plot_every","title":"<code>plot_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for generating and saving figures during training.</p> <p>Set to 0 to disable plotting.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.plotting_functions","title":"<code>plotting_functions = field(default_factory=tuple)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Functions used for in-training plotting.</p> <p>These are called to generate plots that are logged or saved at specified intervals.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.print_every","title":"<code>print_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for printing loss and metrics to the console during training.</p> <p>Set to 0 to disable this output, meaning that metrics and loss will not be printed during training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.root_folder","title":"<code>root_folder = Path('./qml_logs')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The root folder for saving checkpoints and tensorboard logs.</p> <p>The default path is \"./qml_logs\"</p> <p>This can be set to a specific directory where training artifacts are to be stored. Checkpoints will be saved inside a subfolder in this directory. Subfolders will be created based on <code>create_subfolder_per_run</code> argument.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.tracking_tool","title":"<code>tracking_tool = ExperimentTrackingTool.TENSORBOARD</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The tool used for tracking training progress and logging metrics.</p> <p>Options include tools like TensorBoard, which help visualize and monitor model training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.trainstop_criterion","title":"<code>trainstop_criterion = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to determine if the training process should stop based on a.</p> <p>specific stopping metric. If <code>None</code>, training continues until <code>max_iter</code> is reached.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.val_epsilon","title":"<code>val_epsilon = 1e-05</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A small safety margin used to compare the current validation loss with the.</p> <p>best previous validation loss. This is used to determine improvements in metrics.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.val_every","title":"<code>val_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for performing validation.</p> <p>If set to 0, validation is not performed. Note that metrics from validation are always written, regardless of the <code>write_every</code> setting. Note that initial validation happens at the start of training (when val_every &gt; 0)     For initial validation  - initial metrics are written.                             - checkpoint is saved (when checkpoint_best_only = False)</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.validation_criterion","title":"<code>validation_criterion = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to evaluate whether a given validation metric meets a desired condition.</p> <p>The validation_criterion has the following format: def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:     # process</p> <p>If <code>None</code>, no custom validation criterion is applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.verbose","title":"<code>verbose = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to print metrics and status messages during training.</p> <p>If <code>True</code>, detailed metrics and status updates will be displayed in the console.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.write_every","title":"<code>write_every = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for writing loss and metrics using the tracking tool during training.</p> <p>Set to 0 to disable this logging, which prevents metrics from being logged to the tracking tool. Note that the metrics will always be written at the end of training regardless of this setting.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.get_parameters","title":"<code>get_parameters(model)</code>","text":"<p>Retrieve all trainable model parameters in a single vector.</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a 1-dimensional tensor with the parameters</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def get_parameters(model: Module) -&gt; Tensor:\n    \"\"\"Retrieve all trainable model parameters in a single vector.\n\n    Args:\n        model (Module): the input PyTorch model\n\n    Returns:\n        Tensor: a 1-dimensional tensor with the parameters\n    \"\"\"\n    ps = [p.reshape(-1) for p in model.parameters() if p.requires_grad]\n    return torch.concat(ps)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.num_parameters","title":"<code>num_parameters(model)</code>","text":"<p>Return the total number of parameters of the given model.</p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def num_parameters(model: Module) -&gt; int:\n    \"\"\"Return the total number of parameters of the given model.\"\"\"\n    return len(get_parameters(model))\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.set_parameters","title":"<code>set_parameters(model, theta)</code>","text":"<p>Set all trainable parameters of a model from a single vector.</p> <p>Notice that this function assumes prior knowledge of right number of parameters in the model</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> <code>theta</code> <p>the parameters to assign</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def set_parameters(model: Module, theta: Tensor) -&gt; None:\n    \"\"\"Set all trainable parameters of a model from a single vector.\n\n    Notice that this function assumes prior knowledge of right number\n    of parameters in the model\n\n    Args:\n        model (Module): the input PyTorch model\n        theta (Tensor): the parameters to assign\n    \"\"\"\n\n    with torch.no_grad():\n        idx = 0\n        for ps in model.parameters():\n            if ps.requires_grad:\n                n = torch.numel(ps)\n                if ps.ndim == 0:\n                    ps[()] = theta[idx : idx + n]\n                else:\n                    ps[:] = theta[idx : idx + n].reshape(ps.size())\n                idx += n\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.optimize_step.optimize_step","title":"<code>optimize_step(model, optimizer, loss_fn, xs, device=None, dtype=None)</code>","text":"<p>Default Torch optimize step with closure.</p> <p>This is the default optimization step.</p> PARAMETER DESCRIPTION <code>model</code> <p>The input model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The chosen Torch optimizer.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable</code> </p> <code>xs</code> <p>The input data. If None, it means the given model does not require any input data.</p> <p> TYPE: <code>dict | list | Tensor | None</code> </p> <code>device</code> <p>A target device to run computations on.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for <code>xs</code> conversion.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor | float, dict | None]</code> <p>tuple[Tensor | float, dict | None]: A tuple containing the computed loss value and a dictionary with collected metrics.</p> Source code in <code>qadence/ml_tools/optimize_step.py</code> <pre><code>def optimize_step(\n    model: Module,\n    optimizer: Optimizer,\n    loss_fn: Callable,\n    xs: dict | list | torch.Tensor | None,\n    device: torch.device = None,\n    dtype: torch.dtype = None,\n) -&gt; tuple[torch.Tensor | float, dict | None]:\n    \"\"\"Default Torch optimize step with closure.\n\n    This is the default optimization step.\n\n    Args:\n        model (Module): The input model to be optimized.\n        optimizer (Optimizer): The chosen Torch optimizer.\n        loss_fn (Callable): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        xs (dict | list | Tensor | None): The input data. If None, it means\n            the given model does not require any input data.\n        device (torch.device): A target device to run computations on.\n        dtype (torch.dtype): Data type for `xs` conversion.\n\n    Returns:\n        tuple[Tensor | float, dict | None]: A tuple containing the computed loss value\n            and a dictionary with collected metrics.\n    \"\"\"\n\n    loss, metrics = None, {}\n    xs_to_device = data_to_device(xs, device=device, dtype=dtype)\n\n    def closure() -&gt; Any:\n        # NOTE: We need the nonlocal as we can't return a metric dict and\n        # because e.g. LBFGS calls this closure multiple times but for some\n        # reason the returned loss is always the first one...\n        nonlocal metrics, loss\n        optimizer.zero_grad()\n        loss, metrics = loss_fn(model, xs_to_device)\n        loss.backward(retain_graph=True)\n        return loss.item()\n\n    optimizer.step(closure)\n    # return the loss/metrics that are being mutated inside the closure...\n    return loss, metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.optimize_step.update_ng_parameters","title":"<code>update_ng_parameters(model, optimizer, loss_fn, data, ng_params)</code>","text":"<p>Update the model parameters using Nevergrad.</p> <p>This function integrates Nevergrad for derivative-free optimization.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>A Nevergrad optimizer instance.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable[[Module, Tensor | None], tuple[float, dict]]</code> </p> <code>data</code> <p>Input data for the model. If None, it means the model does not require input data.</p> <p> TYPE: <code>Tensor | None</code> </p> <code>ng_params</code> <p>The current set of parameters managed by Nevergrad.</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>tuple[float, dict, Array]</code> <p>tuple[float, dict, ng.p.Array]: A tuple containing the computed loss value, a dictionary of metrics, and the updated Nevergrad parameters.</p> Source code in <code>qadence/ml_tools/optimize_step.py</code> <pre><code>def update_ng_parameters(\n    model: Module,\n    optimizer: ng.optimizers.Optimizer,\n    loss_fn: Callable[[Module, torch.Tensor | None], tuple[float, dict]],\n    data: torch.Tensor | None,\n    ng_params: ng.p.Array,\n) -&gt; tuple[float, dict, ng.p.Array]:\n    \"\"\"Update the model parameters using Nevergrad.\n\n    This function integrates Nevergrad for derivative-free optimization.\n\n    Args:\n        model (Module): The PyTorch model to be optimized.\n        optimizer (ng.optimizers.Optimizer): A Nevergrad optimizer instance.\n        loss_fn (Callable[[Module, Tensor | None], tuple[float, dict]]): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        data (Tensor | None): Input data for the model. If None, it means the model does\n            not require input data.\n        ng_params (ng.p.Array): The current set of parameters managed by Nevergrad.\n\n    Returns:\n        tuple[float, dict, ng.p.Array]: A tuple containing the computed loss value,\n            a dictionary of metrics, and the updated Nevergrad parameters.\n    \"\"\"\n    loss, metrics = loss_fn(model, data)  # type: ignore[misc]\n    optimizer.tell(ng_params, float(loss))\n    ng_params = optimizer.ask()  # type: ignore[assignment]\n    params = promote_to_tensor(ng_params.value, requires_grad=False)\n    set_parameters(model, params)\n    return loss, metrics, ng_params\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.DictDataLoader","title":"<code>DictDataLoader(dataloaders)</code>  <code>dataclass</code>","text":"<p>This class only holds a dictionary of <code>DataLoader</code>s and samples from them.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.InfiniteTensorDataset","title":"<code>InfiniteTensorDataset(*tensors)</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Randomly sample points from the first dimension of the given tensors.</p> <p>Behaves like a normal torch <code>Dataset</code> just that we can sample from it as many times as we want.</p> <p>Examples: <pre><code>import torch\nfrom qadence.ml_tools.data import InfiniteTensorDataset\n\nx_data, y_data = torch.rand(5,2), torch.ones(5,1)\n# The dataset accepts any number of tensors with the same batch dimension\nds = InfiniteTensorDataset(x_data, y_data)\n\n# call `next` to get one sample from each tensor:\nxs = next(iter(ds))\n</code></pre> <pre><code>(tensor([0.2015, 0.3942]), tensor([1.]))\n</code></pre></p> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>def __init__(self, *tensors: Tensor):\n    \"\"\"Randomly sample points from the first dimension of the given tensors.\n\n    Behaves like a normal torch `Dataset` just that we can sample from it as\n    many times as we want.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from qadence.ml_tools.data import InfiniteTensorDataset\n\n    x_data, y_data = torch.rand(5,2), torch.ones(5,1)\n    # The dataset accepts any number of tensors with the same batch dimension\n    ds = InfiniteTensorDataset(x_data, y_data)\n\n    # call `next` to get one sample from each tensor:\n    xs = next(iter(ds))\n    print(str(xs)) # markdown-exec: hide\n    ```\n    \"\"\"\n    self.tensors = tensors\n    self.indices = list(range(self.tensors[0].size(0)))\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult","title":"<code>OptimizeResult(iteration, model, optimizer, loss=None, metrics=lambda: dict()(), extra=lambda: dict()())</code>  <code>dataclass</code>","text":"<p>OptimizeResult stores many optimization intermediate values.</p> <p>We store at a current iteration, the model, optimizer, loss values, metrics. An extra dict can be used for saving other information to be used for callbacks.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.extra","title":"<code>extra = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Extra dict for saving anything else to be used in callbacks.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.iteration","title":"<code>iteration</code>  <code>instance-attribute</code>","text":"<p>Current iteration number.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.loss","title":"<code>loss = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Loss value.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.metrics","title":"<code>metrics = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that can be saved during training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model at iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.optimizer","title":"<code>optimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer at iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.data_to_device","title":"<code>data_to_device(xs, *args, **kwargs)</code>","text":"<p>Utility method to move arbitrary data to 'device'.</p> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>@singledispatch\ndef data_to_device(xs: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Utility method to move arbitrary data to 'device'.\"\"\"\n    raise ValueError(f\"Unable to move {type(xs)} with input args: {args} and kwargs: {kwargs}.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.to_dataloader","title":"<code>to_dataloader(*tensors, batch_size=1, infinite=False)</code>","text":"<p>Convert torch tensors an (infinite) Dataloader.</p> PARAMETER DESCRIPTION <code>*tensors</code> <p>Torch tensors to use in the dataloader.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>()</code> </p> <code>batch_size</code> <p>batch size of sampled tensors</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>infinite</code> <p>if <code>True</code>, the dataloader will keep sampling indefinitely even after the whole dataset was sampled once</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>import torch\nfrom qadence.ml_tools import to_dataloader\n\n(x, y, z) = [torch.rand(10) for _ in range(3)]\nloader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\nprint(next(loader))\nprint(next(loader))\nprint(next(loader))\n</code></pre> <pre><code>[tensor([0.7278, 0.8139, 0.3191, 0.3749, 0.3190]), tensor([0.8628, 0.3784, 0.1783, 0.4360, 0.3089]), tensor([0.1129, 0.9516, 0.2054, 0.3806, 0.9519])]\n[tensor([0.2871, 0.9413, 0.2439, 0.6154, 0.4216]), tensor([0.2169, 0.3042, 0.6463, 0.4242, 0.1694]), tensor([0.2239, 0.3965, 0.0184, 0.1661, 0.6278])]\n[tensor([0.7278, 0.8139, 0.3191, 0.3749, 0.3190]), tensor([0.8628, 0.3784, 0.1783, 0.4360, 0.3089]), tensor([0.1129, 0.9516, 0.2054, 0.3806, 0.9519])]\n</code></pre> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>def to_dataloader(*tensors: Tensor, batch_size: int = 1, infinite: bool = False) -&gt; DataLoader:\n    \"\"\"Convert torch tensors an (infinite) Dataloader.\n\n    Arguments:\n        *tensors: Torch tensors to use in the dataloader.\n        batch_size: batch size of sampled tensors\n        infinite: if `True`, the dataloader will keep sampling indefinitely even after the whole\n            dataset was sampled once\n\n    Examples:\n\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from qadence.ml_tools import to_dataloader\n\n    (x, y, z) = [torch.rand(10) for _ in range(3)]\n    loader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\n    print(next(loader))\n    print(next(loader))\n    print(next(loader))\n    ```\n    \"\"\"\n    ds = InfiniteTensorDataset(*tensors) if infinite else TensorDataset(*tensors)\n    return DataLoader(ds, batch_size=batch_size)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN","title":"<code>QNN(circuit, observable, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, inputs=None, input_diff_mode=InputDiffMode.AD)</code>","text":"<p>               Bases: <code>QuantumModel</code></p> <p>Quantum neural network model for n-dimensional inputs.</p> <p>Examples: <pre><code>import torch\nfrom qadence import QuantumCircuit, QNN, Z\nfrom qadence import hea, feature_map, hamiltonian_factory, kron\n\n# create the circuit\nn_qubits, depth = 2, 4\nfm = kron(\n    feature_map(1, support=(0,), param=\"x\"),\n    feature_map(1, support=(1,), param=\"y\")\n)\nansatz = hea(n_qubits=n_qubits, depth=depth)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nobs_base = hamiltonian_factory(n_qubits, detuning=Z)\n\n# the QNN will yield two outputs\nobs = [2.0 * obs_base, 4.0 * obs_base]\n\n# initialize and use the model\nqnn = QNN(circuit, obs, inputs=[\"x\", \"y\"])\ny = qnn(torch.rand(3, 2))\n</code></pre> <pre><code>tensor([[0.6646, 1.3292],\n        [0.3881, 0.7763],\n        [0.4052, 0.8104]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> <p>Initialize the QNN.</p> <p>The number of inputs is determined by the feature parameters in the input quantum circuit while the number of outputs is determined by how many observables are provided as input</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The quantum circuit to use for the QNN.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>The observable.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>optional configuration for the backend</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>inputs</code> <p>List that indicates the order of variables of the tensors that are passed to the model. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p> <p> TYPE: <code>list[Basic | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode | str</code> DEFAULT: <code>AD</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    inputs: list[sympy.Basic | str] | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n):\n    \"\"\"Initialize the QNN.\n\n    The number of inputs is determined by the feature parameters in the input\n    quantum circuit while the number of outputs is determined by how many\n    observables are provided as input\n\n    Args:\n        circuit: The quantum circuit to use for the QNN.\n        observable: The observable.\n        backend: The chosen quantum backend.\n        diff_mode: The differentiation engine to use. Choices 'gpsr' or 'ad'.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        configuration: optional configuration for the backend\n        inputs: List that indicates the order of variables of the tensors that are passed\n            to the model. Given input tensors `xs = torch.rand(batch_size, input_size:=2)` a QNN\n            with `inputs=[\"t\", \"x\"]` will assign `t, x = xs[:,0], xs[:,1]`.\n        input_diff_mode: The differentiation mode for the input tensor.\n    \"\"\"\n    super().__init__(\n        circuit,\n        observable=observable,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        configuration=configuration,\n        noise=noise,\n    )\n    if self._observable is None:\n        raise ValueError(\"You need to provide at least one observable in the QNN constructor\")\n    if (inputs is not None) and (len(self.inputs) == len(inputs)):\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in inputs]  # type: ignore[union-attr]\n    elif (inputs is None) and len(self.inputs) &lt;= 1:\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in self.inputs]  # type: ignore[union-attr]\n    else:\n        raise ValueError(\n            \"\"\"\n            Your QNN has more than one input. Please provide a list of inputs in the order of\n            your tensor domain. For example, if you want to pass\n            `xs = torch.rand(batch_size, input_size:=3)` to you QNN, where\n            ```\n            t = x[:,0]\n            x = x[:,1]\n            y = x[:,2]\n            ```\n            you have to specify\n            ```\n            QNN(circuit, observable, inputs=[\"t\", \"x\", \"y\"])\n            ```\n            You can also pass a list of sympy symbols.\n        \"\"\"\n        )\n    self.format_to_dict = format_to_dict_fn(self.inputs)  # type: ignore[arg-type]\n    self.input_diff_mode = InputDiffMode(input_diff_mode)\n    if self.input_diff_mode == InputDiffMode.FD:\n        from qadence.backends.utils import finitediff\n\n        self.__derivative = finitediff\n    elif self.input_diff_mode == InputDiffMode.AD:\n        self.__derivative = _torch_derivative  # type: ignore[assignment]\n    else:\n        raise ValueError(f\"Unkown forward diff mode: {self.input_diff_mode}\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN.forward","title":"<code>forward(values=None, state=None, measurement=None, noise=None, endianness=Endianness.BIG)</code>","text":"<p>Forward pass of the model.</p> <p>This returns the (differentiable) expectation value of the given observable operator defined in the constructor. Differently from the base QuantumModel class, the QNN accepts also a tensor as input for the forward pass. The tensor is expected to have shape: <code>n_batches x in_features</code> where <code>n_batches</code> is the number of data points and <code>in_features</code> is the dimensionality of the problem</p> <p>The output of the forward pass is the expectation value of the input observable(s). If a single observable is given, the output shape is <code>n_batches</code> while if multiple observables are given the output shape is instead <code>n_batches x n_observables</code></p> PARAMETER DESCRIPTION <code>values</code> <p>the values of the feature parameters</p> <p> TYPE: <code>dict[str, Tensor] | Tensor</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a tensor with the expectation value of the observables passed in the constructor of the model</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def forward(\n    self,\n    values: dict[str, Tensor] | Tensor = None,\n    state: Tensor | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the model.\n\n    This returns the (differentiable) expectation value of the given observable\n    operator defined in the constructor. Differently from the base QuantumModel\n    class, the QNN accepts also a tensor as input for the forward pass. The\n    tensor is expected to have shape: `n_batches x in_features` where `n_batches`\n    is the number of data points and `in_features` is the dimensionality of the problem\n\n    The output of the forward pass is the expectation value of the input\n    observable(s). If a single observable is given, the output shape is\n    `n_batches` while if multiple observables are given the output shape\n    is instead `n_batches x n_observables`\n\n    Args:\n        values: the values of the feature parameters\n        state: Initial state.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n\n    Returns:\n        Tensor: a tensor with the expectation value of the observables passed\n            in the constructor of the model\n    \"\"\"\n    return self.expectation(\n        values, state=state, measurement=measurement, noise=noise, endianness=endianness\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN.from_configs","title":"<code>from_configs(register, obs_config, fm_config=FeatureMapConfig(), ansatz_config=AnsatzConfig(), backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, input_diff_mode=InputDiffMode.AD)</code>  <code>classmethod</code>","text":"<p>Create a QNN from a set of configurations.</p> PARAMETER DESCRIPTION <code>register</code> <p>The number of qubits or a register object.</p> <p> TYPE: <code>int | Register</code> </p> <code>obs_config</code> <p>The configuration(s) for the observable(s).</p> <p> TYPE: <code>list[ObservableConfig] | ObservableConfig</code> </p> <code>fm_config</code> <p>The configuration for the feature map. Defaults to no feature encoding block.</p> <p> TYPE: <code>FeatureMapConfig</code> DEFAULT: <code>FeatureMapConfig()</code> </p> <code>ansatz_config</code> <p>The configuration for the ansatz. Defaults to a single layer of hardware efficient ansatz.</p> <p> TYPE: <code>AnsatzConfig</code> DEFAULT: <code>AnsatzConfig()</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices are 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>Noise</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Optional backend configuration.</p> <p> TYPE: <code>BackendConfiguration | dict</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode</code> DEFAULT: <code>AD</code> </p> RETURNS DESCRIPTION <code>QNN</code> <p>A QNN object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the observable configuration is not provided.</p> <p>Example: <pre><code>import torch\nfrom qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\nfrom qadence.ml_tools import QNN\nfrom qadence.constructors import ObservableConfig\nfrom qadence.operations import Z\nfrom qadence.types import (\n    AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n)\n\nregister = 4\nobs_config = ObservableConfig(\n    detuning=Z,\n    scale=5.0,\n    shift=0.0,\n    transformation_type=ObservableTransform.SCALE,\n    trainable_transform=None,\n)\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs=[\"x\", \"y\"],\n    basis_set=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.CONSTANT,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nqnn = QNN.from_configs(\n    register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n)\n\nx = torch.rand(2, 2)\ny = qnn(x)\n</code></pre> <pre><code>tensor([[-2.4324],\n        [-1.5849]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>@classmethod\ndef from_configs(\n    cls,\n    register: int | Register,\n    obs_config: Any,\n    fm_config: Any = FeatureMapConfig(),\n    ansatz_config: Any = AnsatzConfig(),\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n) -&gt; QNN:\n    \"\"\"Create a QNN from a set of configurations.\n\n    Args:\n        register (int | Register): The number of qubits or a register object.\n        obs_config (list[ObservableConfig] | ObservableConfig): The configuration(s)\n            for the observable(s).\n        fm_config (FeatureMapConfig): The configuration for the feature map.\n            Defaults to no feature encoding block.\n        ansatz_config (AnsatzConfig): The configuration for the ansatz.\n            Defaults to a single layer of hardware efficient ansatz.\n        backend (BackendName): The chosen quantum backend.\n        diff_mode (DiffMode): The differentiation engine to use. Choices are\n            'gpsr' or 'ad'.\n        measurement (Measurements): Optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator.\n        noise (Noise): A noise model to use.\n        configuration (BackendConfiguration | dict): Optional backend configuration.\n        input_diff_mode (InputDiffMode): The differentiation mode for the input tensor.\n\n    Returns:\n        A QNN object.\n\n    Raises:\n        ValueError: If the observable configuration is not provided.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\n    from qadence.ml_tools import QNN\n    from qadence.constructors import ObservableConfig\n    from qadence.operations import Z\n    from qadence.types import (\n        AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n    )\n\n    register = 4\n    obs_config = ObservableConfig(\n        detuning=Z,\n        scale=5.0,\n        shift=0.0,\n        transformation_type=ObservableTransform.SCALE,\n        trainable_transform=None,\n    )\n    fm_config = FeatureMapConfig(\n        num_features=2,\n        inputs=[\"x\", \"y\"],\n        basis_set=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.CONSTANT,\n        feature_range={\n            \"x\": (-1.0, 1.0),\n            \"y\": (0.0, 1.0),\n        },\n    )\n    ansatz_config = AnsatzConfig(\n        depth=2,\n        ansatz_type=AnsatzType.HEA,\n        ansatz_strategy=Strategy.DIGITAL,\n    )\n\n    qnn = QNN.from_configs(\n        register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n    )\n\n    x = torch.rand(2, 2)\n    y = qnn(x)\n    print(str(y)) # markdown-exec: hide\n    ```\n    \"\"\"\n    from .constructors import build_qnn_from_configs\n\n    return build_qnn_from_configs(\n        register=register,\n        observable_config=obs_config,\n        fm_config=fm_config,\n        ansatz_config=ansatz_config,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        noise=noise,\n        configuration=configuration,\n        input_diff_mode=input_diff_mode,\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.derivative","title":"<code>derivative(ufa, x, derivative_indices)</code>","text":"<p>Compute derivatives w.r.t.</p> <p>inputs of a UFA with a single output. The <code>derivative_indices</code> specify which derivative(s) are computed.  E.g. <code>derivative_indices=(1,2)</code> would compute the a second order derivative w.r.t to the indices <code>1</code> and <code>2</code> of the input tensor.</p> PARAMETER DESCRIPTION <code>ufa</code> <p>The model for which we want to compute the derivative.</p> <p> TYPE: <code>Module</code> </p> <code>x</code> <p>(batch_size, input_size) input tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>derivative_indices</code> <p>Define which derivatives to compute.</p> <p> TYPE: <code>tuple</code> </p> <p>Examples: If we create a UFA with three inputs and denote the first, second, and third input with <code>x</code>, <code>y</code>, and <code>z</code> we can compute the following derivatives w.r.t to those inputs: <pre><code>import torch\nfrom qadence.ml_tools.models import derivative, QNN\nfrom qadence.ml_tools.config import FeatureMapConfig, AnsatzConfig\nfrom qadence.constructors.hamiltonians import ObservableConfig\nfrom qadence.operations import Z\n\nfm_config = FeatureMapConfig(num_features=3, inputs=[\"x\", \"y\", \"z\"])\nansatz_config = AnsatzConfig()\nobs_config = ObservableConfig(detuning=Z)\n\nf = QNN.from_configs(\n    register=3, obs_config=obs_config, fm_config=fm_config, ansatz_config=ansatz_config,\n)\ninputs = torch.rand(5,3,requires_grad=True)\n\n# df_dx\nderivative(f, inputs, (0,))\n\n# d2f_dydz\nderivative(f, inputs, (1,2))\n\n# d3fdy2dx\nderivative(f, inputs, (1,1,0))\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def derivative(ufa: torch.nn.Module, x: Tensor, derivative_indices: tuple[int, ...]) -&gt; Tensor:\n    \"\"\"Compute derivatives w.r.t.\n\n    inputs of a UFA with a single output. The\n    `derivative_indices` specify which derivative(s) are computed.  E.g.\n    `derivative_indices=(1,2)` would compute the a second order derivative w.r.t\n    to the indices `1` and `2` of the input tensor.\n\n    Arguments:\n        ufa: The model for which we want to compute the derivative.\n        x (Tensor): (batch_size, input_size) input tensor.\n        derivative_indices (tuple): Define which derivatives to compute.\n\n    Examples:\n    If we create a UFA with three inputs and denote the first, second, and third\n    input with `x`, `y`, and `z` we can compute the following derivatives w.r.t\n    to those inputs:\n    ```py exec=\"on\" source=\"material-block\"\n    import torch\n    from qadence.ml_tools.models import derivative, QNN\n    from qadence.ml_tools.config import FeatureMapConfig, AnsatzConfig\n    from qadence.constructors.hamiltonians import ObservableConfig\n    from qadence.operations import Z\n\n    fm_config = FeatureMapConfig(num_features=3, inputs=[\"x\", \"y\", \"z\"])\n    ansatz_config = AnsatzConfig()\n    obs_config = ObservableConfig(detuning=Z)\n\n    f = QNN.from_configs(\n        register=3, obs_config=obs_config, fm_config=fm_config, ansatz_config=ansatz_config,\n    )\n    inputs = torch.rand(5,3,requires_grad=True)\n\n    # df_dx\n    derivative(f, inputs, (0,))\n\n    # d2f_dydz\n    derivative(f, inputs, (1,2))\n\n    # d3fdy2dx\n    derivative(f, inputs, (1,1,0))\n    ```\n    \"\"\"\n    assert ufa.out_features == 1, \"Can only call `derivative` on models with 1D output.\"\n    return ufa._derivative(x, derivative_indices)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.format_to_dict_fn","title":"<code>format_to_dict_fn(inputs=[])</code>","text":"<p>Format an input tensor into the format required by the forward pass.</p> <p>The tensor is assumed to have dimensions: n_batches x in_features where in_features corresponds to the number of input features of the QNN</p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def format_to_dict_fn(\n    inputs: list[sympy.Symbol | str] = [],\n) -&gt; Callable[[Tensor | ParamDictType], ParamDictType]:\n    \"\"\"Format an input tensor into the format required by the forward pass.\n\n    The tensor is assumed to have dimensions: n_batches x in_features where in_features\n    corresponds to the number of input features of the QNN\n    \"\"\"\n    in_features = len(inputs)\n\n    def tensor_to_dict(values: Tensor | ParamDictType) -&gt; ParamDictType:\n        if isinstance(values, Tensor):\n            values = values.reshape(-1, 1) if len(values.size()) == 1 else values\n            if not values.shape[1] == in_features:\n                raise ValueError(\n                    f\"Model expects in_features={in_features} but got {values.shape[1]}.\"\n                )\n            values = {fparam.name: values[:, inputs.index(fparam)] for fparam in inputs}  # type: ignore[union-attr]\n        return values\n\n    return tensor_to_dict\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback","title":"<code>Callback(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>Base class for defining various training callbacks.</p> ATTRIBUTE DESCRIPTION <code>on</code> <p>The event on which to trigger the callback. Must be a valid on value from: [\"train_start\", \"train_end\",     \"train_epoch_start\", \"train_epoch_end\", \"train_batch_start\",     \"train_batch_end\",\"val_epoch_start\", \"val_epoch_end\",     \"val_batch_start\", \"val_batch_end\", \"test_batch_start\",     \"test_batch_end\"]</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>callback</code> <p>The function to call if the condition is met.</p> <p> TYPE: <code>CallbackFunction | None</code> </p> <code>callback_condition</code> <p>Condition to check before calling.</p> <p> TYPE: <code>CallbackConditionFunction | None</code> </p> <code>modify_optimize_result</code> <p>Function to modify <code>OptimizeResult</code>.</p> <p> TYPE: <code>CallbackFunction | dict[str, Any] | None</code> </p> <p>A callback can be defined in two ways:</p> <ol> <li>By providing a callback function directly in the base class:    This is useful for simple callbacks that don't require subclassing.</li> </ol> <p>Example:    <pre><code>from qadence.ml_tools.callbacks import Callback\n\ndef custom_callback_function(trainer, config, writer):\n    print(\"Custom callback executed.\")\n\ncustom_callback = Callback(\n    on=\"train_end\",\n    called_every=5,\n    callback=custom_callback_function\n)\n</code></pre> <pre><code>\n</code></pre> </p> <ol> <li>By inheriting and implementing the <code>run_callback</code> method:    This is suitable for more complex callbacks that require customization.</li> </ol> <p>Example:    <pre><code>from qadence.ml_tools.callbacks import Callback\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in the inherited run_callback method.\")\n\ncustom_callback = CustomCallback(on=\"train_end\", called_every=10)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.on","title":"<code>on</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the TrainingStage.</p> RETURNS DESCRIPTION <code>TrainingStage</code> <p>TrainingStage for the callback</p> <p> TYPE: <code>TrainingStage | str</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.__call__","title":"<code>__call__(when, trainer, config, writer)</code>","text":"<p>Executes the callback if conditions are met.</p> PARAMETER DESCRIPTION <code>when</code> <p>The event when the callback is triggered.</p> <p> TYPE: <code>str</code> </p> <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback function if executed.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __call__(\n    self, when: TrainingStage, trainer: Any, config: TrainConfig, writer: BaseWriter\n) -&gt; Any:\n    \"\"\"Executes the callback if conditions are met.\n\n    Args:\n        when (str): The event when the callback is triggered.\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback function if executed.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if self.on == when:\n        if opt_result:\n            opt_result = self.modify_optimize_result(opt_result)\n        if self._should_call(when, opt_result):\n            return self.run_callback(trainer, config, writer)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback._should_call","title":"<code>_should_call(when, opt_result)</code>","text":"<p>Checks if the callback should be called.</p> PARAMETER DESCRIPTION <code>when</code> <p>The event when the callback is considered for execution.</p> <p> TYPE: <code>str</code> </p> <code>opt_result</code> <p>The current optimization results.</p> <p> TYPE: <code>OptimizeResult</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Whether the callback should be called.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def _should_call(self, when: str, opt_result: OptimizeResult) -&gt; bool:\n    \"\"\"Checks if the callback should be called.\n\n    Args:\n        when (str): The event when the callback is considered for execution.\n        opt_result (OptimizeResult): The current optimization results.\n\n    Returns:\n        bool: Whether the callback should be called.\n    \"\"\"\n    if when in [TrainingStage(\"train_start\"), TrainingStage(\"train_end\")]:\n        return True\n    if self.called_every == 0 or opt_result.iteration == 0:\n        return False\n    if opt_result.iteration % self.called_every == 0 and self.callback_condition(opt_result):\n        return True\n    return False\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Executes the defined callback.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback execution.</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If not implemented in subclasses.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Executes the defined callback.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback execution.\n\n    Raises:\n        NotImplementedError: If not implemented in subclasses.\n    \"\"\"\n    if self.callback is not None:\n        return self.callback(trainer, config, writer)\n    raise NotImplementedError(\"Subclasses should override the run_callback method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.EarlyStopping","title":"<code>EarlyStopping(on, called_every, monitor, patience=5, mode='min')</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Stops training when a monitored metric has not improved for a specified number of epochs.</p> <p>This callback monitors a specified metric (e.g., validation loss or accuracy). If the metric does not improve for a given patience period, training is stopped.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>EarlyStopping</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import EarlyStopping\n\n# Create an instance of the EarlyStopping callback\nearly_stopping = EarlyStopping(on=\"val_epoch_end\",\n                               called_every=1,\n                               monitor=\"val_loss\",\n                               patience=5,\n                               mode=\"min\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    print_every=1000,\n    callbacks=[early_stopping]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the EarlyStopping callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback (e.g., \"val_epoch_end\").</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>monitor</code> <p>The metric to monitor (e.g., \"val_loss\" or \"train_loss\"). All metrics returned by optimize step are available to monitor. Please add \"val_\" and \"train_\" strings at the start of the metric name.</p> <p> TYPE: <code>str</code> </p> <code>patience</code> <p>Number of iterations to wait for improvement. Default is 5.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>mode</code> <p>Whether to minimize (\"min\") or maximize (\"max\") the metric. Default is \"min\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'min'</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self, on: str, called_every: int, monitor: str, patience: int = 5, mode: str = \"min\"\n):\n    \"\"\"Initializes the EarlyStopping callback.\n\n    Args:\n        on (str): The event to trigger the callback (e.g., \"val_epoch_end\").\n        called_every (int): Frequency of callback calls in terms of iterations.\n        monitor (str): The metric to monitor (e.g., \"val_loss\" or \"train_loss\").\n            All metrics returned by optimize step are available to monitor.\n            Please add \"val_\" and \"train_\" strings at the start of the metric name.\n        patience (int, optional): Number of iterations to wait for improvement. Default is 5.\n        mode (str, optional): Whether to minimize (\"min\") or maximize (\"max\") the metric.\n            Default is \"min\".\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.monitor = monitor\n    self.patience = patience\n    self.mode = mode\n    self.best_value = float(\"inf\") if mode == \"min\" else -float(\"inf\")\n    self.counter = 0\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.EarlyStopping.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Monitors the metric and stops training if no improvement is observed.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Monitors the metric and stops training if no improvement is observed.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    current_value = trainer.opt_result.metrics.get(self.monitor)\n    if current_value is None:\n        raise ValueError(f\"Metric '{self.monitor}' is not available in the trainer's metrics.\")\n\n    if (self.mode == \"min\" and current_value &lt; self.best_value) or (\n        self.mode == \"max\" and current_value &gt; self.best_value\n    ):\n        self.best_value = current_value\n        self.counter = 0\n    else:\n        self.counter += 1\n\n    if self.counter &gt;= self.patience:\n        logger.info(\n            f\"EarlyStopping: No improvement in '{self.monitor}' for {self.patience} epochs. \"\n            \"Stopping training.\"\n        )\n        trainer.stop_training = True\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.GradientMonitoring","title":"<code>GradientMonitoring(on, called_every=1)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Logs gradient statistics (e.g., mean, standard deviation, max) during training.</p> <p>This callback monitors and logs statistics about the gradients of the model parameters to help debug or optimize the training process.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>GradientMonitoring</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import GradientMonitoring\n\n# Create an instance of the GradientMonitoring callback\ngradient_monitoring = GradientMonitoring(on=\"train_batch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    print_every=1000,\n    callbacks=[gradient_monitoring]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the GradientMonitoring callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback (e.g., \"train_batch_end\").</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int = 1):\n    \"\"\"Initializes the GradientMonitoring callback.\n\n    Args:\n        on (str): The event to trigger the callback (e.g., \"train_batch_end\").\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.GradientMonitoring.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs gradient statistics.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Logs gradient statistics.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    gradient_stats = {}\n    for name, param in trainer.model.named_parameters():\n        if param.grad is not None:\n            grad = param.grad\n            gradient_stats.update(\n                {\n                    name + \"_mean\": grad.mean().item(),\n                    name + \"_std\": grad.std().item(),\n                    name + \"_max\": grad.max().item(),\n                    name + \"_min\": grad.min().item(),\n                }\n            )\n\n    writer.write(trainer.opt_result.iteration, gradient_stats)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCosineAnnealing","title":"<code>LRSchedulerCosineAnnealing(on, called_every, t_max, min_lr=0.0)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Applies cosine annealing to the learning rate during training.</p> <p>This callback decreases the learning rate following a cosine curve, starting from the initial learning rate and annealing to a minimum (min_lr).</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerCosineAnnealing</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCosineAnnealing\n\n# Create an instance of the LRSchedulerCosineAnnealing callback\nlr_cosine = LRSchedulerCosineAnnealing(on=\"train_batch_end\",\n                                       called_every=1,\n                                       t_max=5000,\n                                       min_lr=1e-6)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_cosine]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerCosineAnnealing callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>t_max</code> <p>The total number of iterations for one annealing cycle.</p> <p> TYPE: <code>int</code> </p> <code>min_lr</code> <p>The minimum learning rate. Default is 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, t_max: int, min_lr: float = 0.0):\n    \"\"\"Initializes the LRSchedulerCosineAnnealing callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        t_max (int): The total number of iterations for one annealing cycle.\n        min_lr (float, optional): The minimum learning rate. Default is 0.0.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.t_max = t_max\n    self.min_lr = min_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCosineAnnealing.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Adjusts the learning rate using cosine annealing.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Adjusts the learning rate using cosine annealing.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    for param_group in trainer.optimizer.param_groups:\n        max_lr = param_group[\"lr\"]\n        new_lr = (\n            self.min_lr\n            + (max_lr - self.min_lr)\n            * (1 + math.cos(math.pi * trainer.opt_result.iteration / self.t_max))\n            / 2\n        )\n        param_group[\"lr\"] = new_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCyclic","title":"<code>LRSchedulerCyclic(on, called_every, base_lr, max_lr, step_size)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Applies a cyclic learning rate schedule during training.</p> <p>This callback oscillates the learning rate between a minimum (base_lr) and a maximum (max_lr) over a defined cycle length (step_size). The learning rate follows a triangular wave pattern.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerCyclic</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCyclic\n\n# Create an instance of the LRSchedulerCyclic callback\nlr_cyclic = LRSchedulerCyclic(on=\"train_batch_end\",\n                              called_every=1,\n                              base_lr=0.001,\n                              max_lr=0.01,\n                              step_size=2000)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_cyclic]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerCyclic callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>base_lr</code> <p>The minimum learning rate.</p> <p> TYPE: <code>float</code> </p> <code>max_lr</code> <p>The maximum learning rate.</p> <p> TYPE: <code>float</code> </p> <code>step_size</code> <p>Number of iterations for half a cycle.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, base_lr: float, max_lr: float, step_size: int):\n    \"\"\"Initializes the LRSchedulerCyclic callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        base_lr (float): The minimum learning rate.\n        max_lr (float): The maximum learning rate.\n        step_size (int): Number of iterations for half a cycle.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.base_lr = base_lr\n    self.max_lr = max_lr\n    self.step_size = step_size\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCyclic.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Adjusts the learning rate cyclically.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Adjusts the learning rate cyclically.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    cycle = trainer.opt_result.iteration // (2 * self.step_size)\n    x = abs(trainer.opt_result.iteration / self.step_size - 2 * cycle - 1)\n    scale = max(0, (1 - x))\n    new_lr = self.base_lr + (self.max_lr - self.base_lr) * scale\n    for param_group in trainer.optimizer.param_groups:\n        param_group[\"lr\"] = new_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerStepDecay","title":"<code>LRSchedulerStepDecay(on, called_every, gamma=0.5)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Reduces the learning rate by a factor at regular intervals.</p> <p>This callback adjusts the learning rate by multiplying it with a decay factor after a specified number of iterations. The learning rate is updated as:     lr = lr * gamma</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerStepDecay</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerStepDecay\n\n# Create an instance of the LRSchedulerStepDecay callback\nlr_step_decay = LRSchedulerStepDecay(on=\"train_epoch_end\",\n                                     called_every=100,\n                                     gamma=0.5)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_step_decay]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerStepDecay callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>gamma</code> <p>The decay factor applied to the learning rate. A value &lt; 1 reduces the learning rate over time. Default is 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, gamma: float = 0.5):\n    \"\"\"Initializes the LRSchedulerStepDecay callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        gamma (float, optional): The decay factor applied to the learning rate.\n            A value &lt; 1 reduces the learning rate over time. Default is 0.5.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.gamma = gamma\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerStepDecay.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Runs the callback to apply step decay to the learning rate.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Runs the callback to apply step decay to the learning rate.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    for param_group in trainer.optimizer.param_groups:\n        param_group[\"lr\"] *= self.gamma\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LoadCheckpoint","title":"<code>LoadCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to load a model checkpoint.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LoadCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Loads a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The result of loading the checkpoint.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Loads a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: The result of loading the checkpoint.\n    \"\"\"\n    folder = config.log_folder\n    model = trainer.model\n    optimizer = trainer.optimizer\n    device = trainer.log_device\n    return load_checkpoint(folder, model, optimizer, device=device)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogHyperparameters","title":"<code>LogHyperparameters(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log hyperparameters using the writer.</p> <p>The <code>LogHyperparameters</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LogHyperparameters</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogHyperparameters\n\n# Create an instance of the LogHyperparameters callback\nlog_hyper_callback = LogHyperparameters(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[log_hyper_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogHyperparameters.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs hyperparameters using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs hyperparameters using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    hyperparams = config.hyperparams\n    writer.log_hyperparams(hyperparams)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogModelTracker","title":"<code>LogModelTracker(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log the model using the writer.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogModelTracker.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs the model using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs the model using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    model = trainer.model\n    writer.log_model(\n        model, trainer.train_dataloader, trainer.val_dataloader, trainer.test_dataloader\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PlotMetrics","title":"<code>PlotMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to plot metrics using the writer.</p> <p>The <code>PlotMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PlotMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PlotMetrics\n\n# Create an instance of the PlotMetrics callback\nplot_metrics_callback = PlotMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[plot_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PlotMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Plots metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Plots metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    plotting_functions = config.plotting_functions\n    writer.plot(trainer.model, opt_result.iteration, plotting_functions)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PrintMetrics","title":"<code>PrintMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to print metrics using the writer.</p> <p>The <code>PrintMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PrintMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\n# Create an instance of the PrintMetrics callback\nprint_metrics_callback = PrintMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[print_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PrintMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Prints metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Prints metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.print_metrics(opt_result)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveBestCheckpoint","title":"<code>SaveBestCheckpoint(on, called_every)</code>","text":"<p>               Bases: <code>SaveCheckpoint</code></p> <p>Callback to save the best model checkpoint based on a validation criterion.</p> <p>Initializes the SaveBestCheckpoint callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int):\n    \"\"\"Initializes the SaveBestCheckpoint callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.best_loss = float(\"inf\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveBestCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves the checkpoint if the current loss is better than the best loss.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves the checkpoint if the current loss is better than the best loss.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if config.validation_criterion and config.validation_criterion(\n        opt_result.loss, self.best_loss, config.val_epsilon\n    ):\n        self.best_loss = opt_result.loss\n\n        folder = config.log_folder\n        model = trainer.model\n        optimizer = trainer.optimizer\n        opt_result = trainer.opt_result\n        write_checkpoint(folder, model, optimizer, \"best\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveCheckpoint","title":"<code>SaveCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to save a model checkpoint.</p> <p>The <code>SaveCheckpoint</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>SaveCheckpoint</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint\n\n# Create an instance of the SaveCheckpoint callback\nsave_checkpoint_callback = SaveCheckpoint(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    folder = config.log_folder\n    model = trainer.model\n    optimizer = trainer.optimizer\n    opt_result = trainer.opt_result\n    write_checkpoint(folder, model, optimizer, opt_result.iteration)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.WriteMetrics","title":"<code>WriteMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to write metrics using the writer.</p> <p>The <code>WriteMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>WriteMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import WriteMetrics\n\n# Create an instance of the WriteMetrics callback\nwrite_metrics_callback = WriteMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[write_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.WriteMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Writes metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Writes metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.write(opt_result.iteration, opt_result.metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer","title":"<code>BaseTrainer(model, optimizer, config, loss_fn='mse', optimize_step=optimize_step, train_dataloader=None, val_dataloader=None, test_dataloader=None, max_batches=None)</code>","text":"<p>Base class for training machine learning models using a given optimizer.</p> <p>The base class implements contextmanager for gradient based/free optimization, properties, property setters, input validations, callback decorator generator, and empty hooks for different training steps.</p> This class provides <ul> <li>Context managers for enabling/disabling gradient-based optimization</li> <li>Properties for managing models, optimizers, and dataloaders</li> <li>Input validations and a callback decorator generator</li> <li>Config and callback managers using the provided <code>TrainConfig</code></li> </ul> ATTRIBUTE DESCRIPTION <code>use_grad</code> <p>Indicates if gradients are used for optimization. Default is True.</p> <p> TYPE: <code>bool</code> </p> <code>model</code> <p>The neural network model.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The configuration settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>optimize_step</code> <p>Function for performing an optimization step.</p> <p> TYPE: <code>Callable</code> </p> <code>loss_fn</code> <p>loss function to use. Default loss function used is 'mse'</p> <p> TYPE: <code>Callable | str ]</code> </p> <code>num_training_batches</code> <p>Number of training batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_validation_batches</code> <p>Number of validation batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_test_batches</code> <p>Number of test batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>state</code> <p>Current state in the training process</p> <p> TYPE: <code>str</code> </p> <p>Initializes the BaseTrainer.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The TrainConfig settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>The loss function to use. str input to be specified to use a default loss function. currently supported loss functions: 'mse', 'cross_entropy'. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data. If the model does not need data to evaluate loss, no dataset should be provided.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    optimize_step: Callable = optimize_step,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the BaseTrainer.\n\n    Args:\n        model (nn.Module): The model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer\n            for training.\n        config (TrainConfig): The TrainConfig settings for training.\n        loss_fn (str | Callable): The loss function to use.\n            str input to be specified to use a default loss function.\n            currently supported loss functions: 'mse', 'cross_entropy'.\n            If not specified, default mse loss will be used.\n        train_dataloader (Dataloader | DictDataLoader | None): DataLoader for training data.\n            If the model does not need data to evaluate loss, no dataset\n            should be provided.\n        val_dataloader (Dataloader | DictDataLoader | None): DataLoader for validation data.\n        test_dataloader (Dataloader | DictDataLoader | None): DataLoader for testing data.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    self._model: nn.Module\n    self._optimizer: optim.Optimizer | NGOptimizer | None\n    self._config: TrainConfig\n    self._train_dataloader: DataLoader | DictDataLoader | None = None\n    self._val_dataloader: DataLoader | DictDataLoader | None = None\n    self._test_dataloader: DataLoader | DictDataLoader | None = None\n\n    self.config = config\n    self.model = model\n    self.optimizer = optimizer\n    self.max_batches = max_batches\n\n    self.num_training_batches: int\n    self.num_validation_batches: int\n    self.num_test_batches: int\n\n    self.train_dataloader = train_dataloader\n    self.val_dataloader = val_dataloader\n    self.test_dataloader = test_dataloader\n\n    self.loss_fn: Callable = get_loss_fn(loss_fn)\n    self.optimize_step: Callable = optimize_step\n    self.ng_params: ng.p.Array\n    self.training_stage: TrainingStage = TrainingStage(\"idle\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.config","title":"<code>config</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training configuration.</p> RETURNS DESCRIPTION <code>TrainConfig</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.model","title":"<code>model</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the model if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Module</code> <p>nn.Module: The model.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.optimizer","title":"<code>optimizer</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimizer if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Optimizer | Optimizer | None</code> <p>optim.Optimizer | NGOptimizer | None: The optimizer.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.test_dataloader","title":"<code>test_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the test DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for testing data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.train_dataloader","title":"<code>train_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.use_grad","title":"<code>use_grad</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimization framework for the trainer.</p> <p>use_grad = True : Gradient based optimization use_grad = False : Gradient free optimization</p> RETURNS DESCRIPTION <code>bool</code> <p>Bool value for using gradient.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.val_dataloader","title":"<code>val_dataloader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the validation DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer._compute_num_batches","title":"<code>_compute_num_batches(dataloader)</code>","text":"<p>Computes the number of batches for the given DataLoader.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>The DataLoader for which to compute the number of batches.</p> <p> TYPE: <code>DataLoader</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def _compute_num_batches(self, dataloader: DataLoader | DictDataLoader) -&gt; int:\n    \"\"\"\n    Computes the number of batches for the given DataLoader.\n\n    Args:\n        dataloader (DataLoader): The DataLoader for which to compute\n            the number of batches.\n    \"\"\"\n    if dataloader is None:\n        return 1\n    if isinstance(dataloader, DictDataLoader):\n        dataloader_name, dataloader_value = list(dataloader.dataloaders.items())[0]\n        dataset = dataloader_value.dataset\n        batch_size = dataloader_value.batch_size\n    else:\n        dataset = dataloader.dataset\n        batch_size = dataloader.batch_size\n\n    if isinstance(dataset, TensorDataset):\n        n_batches = int((dataset.tensors[0].size(0) + batch_size - 1) // batch_size)\n        return min(self.max_batches, n_batches) if self.max_batches is not None else n_batches\n    else:\n        return 1\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer._validate_dataloader","title":"<code>_validate_dataloader(dataloader, dataloader_type)</code>","text":"<p>Validates the type of the DataLoader and raises errors for unsupported types.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>The DataLoader to validate.</p> <p> TYPE: <code>DataLoader | DictDataLoader</code> </p> <code>dataloader_type</code> <p>The type of DataLoader (\"train\", \"val\", or \"test\").</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def _validate_dataloader(\n    self, dataloader: DataLoader | DictDataLoader, dataloader_type: str\n) -&gt; None:\n    \"\"\"\n    Validates the type of the DataLoader and raises errors for unsupported types.\n\n    Args:\n        dataloader (DataLoader | DictDataLoader): The DataLoader to validate.\n        dataloader_type (str): The type of DataLoader (\"train\", \"val\", or \"test\").\n    \"\"\"\n    if dataloader is not None:\n        if not isinstance(dataloader, (DataLoader, DictDataLoader)):\n            raise NotImplementedError(\n                f\"Unsupported dataloader type: {type(dataloader)}.\"\n                \"The dataloader must be an instance of DataLoader.\"\n            )\n    if dataloader_type == \"val\" and self.config.val_every &gt; 0:\n        if not isinstance(dataloader, (DataLoader, DictDataLoader)):\n            raise ValueError(\n                \"If `config.val_every` is provided as an integer &gt; 0, validation_dataloader\"\n                \"must be an instance of `DataLoader` or `DictDataLoader`.\"\n            )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.callback","title":"<code>callback(phase)</code>  <code>staticmethod</code>","text":"<p>Decorator for executing callbacks before and after a phase.</p> <p>Phase are different hooks during the training. list of valid phases is defined in Callbacks. We also update the current state of the training process in the callback decorator.</p> PARAMETER DESCRIPTION <code>phase</code> <p>The phase for which the callback is executed (e.g., \"train\", \"train_epoch\", \"train_batch\").</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>The decorated function.</p> <p> TYPE: <code>Callable</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@staticmethod\ndef callback(phase: str) -&gt; Callable:\n    \"\"\"\n    Decorator for executing callbacks before and after a phase.\n\n    Phase are different hooks during the training. list of valid\n    phases is defined in Callbacks.\n    We also update the current state of the training process in\n    the callback decorator.\n\n    Args:\n        phase (str): The phase for which the callback is executed (e.g., \"train\",\n            \"train_epoch\", \"train_batch\").\n\n    Returns:\n        Callable: The decorated function.\n    \"\"\"\n\n    def decorator(method: Callable) -&gt; Callable:\n        def wrapper(self: Any, *args: Any, **kwargs: Any) -&gt; Any:\n            start_event = f\"{phase}_start\"\n            end_event = f\"{phase}_end\"\n\n            self.training_stage = TrainingStage(start_event)\n            self.callback_manager.run_callbacks(trainer=self)\n            result = method(self, *args, **kwargs)\n\n            self.training_stage = TrainingStage(end_event)\n            # build_optimize_result method is defined in the trainer.\n            self.build_optimize_result(result)\n            self.callback_manager.run_callbacks(trainer=self)\n\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.disable_grad_opt","title":"<code>disable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily disable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The Nevergrad optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef disable_grad_opt(self, optimizer: NGOptimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily disable gradient-based optimization.\n\n    Args:\n        optimizer (NGOptimizer): The Nevergrad optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = False\n        self.callback_manager.use_grad = False\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.enable_grad_opt","title":"<code>enable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily enable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The PyTorch optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef enable_grad_opt(self, optimizer: optim.Optimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily enable gradient-based optimization.\n\n    Args:\n        optimizer (optim.Optimizer): The PyTorch optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = True\n        self.callback_manager.use_grad = True\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_test_batch_end","title":"<code>on_test_batch_end(test_batch_loss_metrics)</code>","text":"<p>Called at the end of each testing batch.</p> PARAMETER DESCRIPTION <code>test_batch_loss_metrics</code> <p>Metrics for the testing batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_end(self, test_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each testing batch.\n\n    Args:\n        test_batch_loss_metrics: Metrics for the testing batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_test_batch_start","title":"<code>on_test_batch_start(batch)</code>","text":"<p>Called at the start of each testing batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each testing batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_batch_end","title":"<code>on_train_batch_end(train_batch_loss_metrics)</code>","text":"<p>Called at the end of each training batch.</p> PARAMETER DESCRIPTION <code>train_batch_loss_metrics</code> <p>Metrics for the training batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_end(self, train_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each training batch.\n\n    Args:\n        train_batch_loss_metrics: Metrics for the training batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_batch_start","title":"<code>on_train_batch_start(batch)</code>","text":"<p>Called at the start of each training batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each training batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_end","title":"<code>on_train_end(train_losses, val_losses=None)</code>","text":"<p>Called at the end of training.</p> PARAMETER DESCRIPTION <code>train_losses</code> <p>Metrics for the training losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]]</code> </p> <code>val_losses</code> <p>Metrics for the validation losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_end(\n    self,\n    train_losses: list[list[tuple[torch.Tensor, Any]]],\n    val_losses: list[list[tuple[torch.Tensor, Any]]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Called at the end of training.\n\n    Args:\n        train_losses (list[list[tuple[torch.Tensor, Any]]]):\n            Metrics for the training losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Training Batches      -&gt; (loss, metrics)\n        val_losses (list[list[tuple[torch.Tensor, Any]]] | None):\n            Metrics for the validation losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_epoch_end","title":"<code>on_train_epoch_end(train_epoch_loss_metrics)</code>","text":"<p>Called at the end of each training epoch.</p> PARAMETER DESCRIPTION <code>train_epoch_loss_metrics</code> <p>Metrics for the training epoch losses. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_end(self, train_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each training epoch.\n\n    Args:\n        train_epoch_loss_metrics: Metrics for the training epoch losses.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>Called at the start of each training epoch.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each training epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_start","title":"<code>on_train_start()</code>","text":"<p>Called at the start of training.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_start(self) -&gt; None:\n    \"\"\"Called at the start of training.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_batch_end","title":"<code>on_val_batch_end(val_batch_loss_metrics)</code>","text":"<p>Called at the end of each validation batch.</p> PARAMETER DESCRIPTION <code>val_batch_loss_metrics</code> <p>Metrics for the validation batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_end(self, val_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation batch.\n\n    Args:\n        val_batch_loss_metrics: Metrics for the validation batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_batch_start","title":"<code>on_val_batch_start(batch)</code>","text":"<p>Called at the start of each validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each validation batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_epoch_end","title":"<code>on_val_epoch_end(val_epoch_loss_metrics)</code>","text":"<p>Called at the end of each validation epoch.</p> PARAMETER DESCRIPTION <code>val_epoch_loss_metrics</code> <p>Metrics for the validation epoch loss. list                    -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_end(self, val_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation epoch.\n\n    Args:\n        val_epoch_loss_metrics: Metrics for the validation epoch loss.\n            list                    -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_epoch_start","title":"<code>on_val_epoch_start()</code>","text":"<p>Called at the start of each validation epoch.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each validation epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.set_use_grad","title":"<code>set_use_grad(value)</code>  <code>classmethod</code>","text":"<p>Sets the global use_grad flag.</p> PARAMETER DESCRIPTION <code>value</code> <p>Whether to use gradient-based optimization.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@classmethod\ndef set_use_grad(cls, value: bool) -&gt; None:\n    \"\"\"\n    Sets the global use_grad flag.\n\n    Args:\n        value (bool): Whether to use gradient-based optimization.\n    \"\"\"\n    if not isinstance(value, bool):\n        raise TypeError(\"use_grad must be a boolean value.\")\n    cls._use_grad = value\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for experiment tracking writers.</p> METHOD DESCRIPTION <code>open</code> <p>Opens the writer and sets up the logging environment.</p> <code>close</code> <p>Closes the writer and finalizes any ongoing logging processes.</p> <code>print_metrics</code> <p>Prints metrics and loss in a formatted manner.</p> <code>write</code> <p>Writes the optimization results to the tracking tool.</p> <code>log_hyperparams</code> <p>Logs the hyperparameters to the tracking tool.</p> <code>plot</code> <p>Logs model plots using provided plotting functions.</p> <code>log_model</code> <p>Logs the model and any relevant information.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Closes the writer and finalizes logging.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Closes the writer and finalizes logging.\"\"\"\n    raise NotImplementedError(\"Writers must implement a close method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Logs hyperparameters.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_hyperparams method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>  <code>abstractmethod</code>","text":"<p>Logs the model and associated data.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and associated data.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_model method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.open","title":"<code>open(config, iteration=None)</code>  <code>abstractmethod</code>","text":"<p>Opens the writer and prepares it for logging.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef open(self, config: TrainConfig, iteration: int | None = None) -&gt; Any:\n    \"\"\"\n    Opens the writer and prepares it for logging.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement an open method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>  <code>abstractmethod</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used to\n            generate plots.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a plot method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.print_metrics","title":"<code>print_metrics(result)</code>","text":"<p>Prints the metrics and loss in a readable format.</p> PARAMETER DESCRIPTION <code>result</code> <p>The optimization results to display.</p> <p> TYPE: <code>OptimizeResult</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def print_metrics(self, result: OptimizeResult) -&gt; None:\n    \"\"\"Prints the metrics and loss in a readable format.\n\n    Args:\n        result (OptimizeResult): The optimization results to display.\n    \"\"\"\n\n    # Find the key in result.metrics that contains \"loss\" (case-insensitive)\n    loss_key = next((k for k in result.metrics if \"loss\" in k.lower()), None)\n    if loss_key:\n        loss_value = result.metrics[loss_key]\n        msg = f\"Iteration {result.iteration: &gt;7} | {loss_key.title()}: {loss_value:.7f} -\"\n    else:\n        msg = f\"Iteration {result.iteration: &gt;7} | Loss: None -\"\n    msg += \" \".join([f\"{k}: {v:.7f}\" for k, v in result.metrics.items() if k != loss_key])\n    print(msg)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.write","title":"<code>write(iteration, metrics)</code>  <code>abstractmethod</code>","text":"<p>Logs the results of the current iteration.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a write method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter","title":"<code>MLFlowWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to MLflow.</p> ATTRIBUTE DESCRIPTION <code>run</code> <p>The active MLflow run.</p> <p> TYPE: <code>Run</code> </p> <code>mlflow</code> <p>The MLflow module.</p> <p> TYPE: <code>ModuleType</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    try:\n        from mlflow.entities import Run\n    except ImportError:\n        raise ImportError(\n            \"mlflow is not installed. Please install qadence with the mlflow feature: \"\n            \"`pip install qadence[mlflow]`.\"\n        )\n\n    self.run: Run\n    self.mlflow: ModuleType\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.close","title":"<code>close()</code>","text":"<p>Closes the MLflow run.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the MLflow run.\"\"\"\n    if self.run:\n        self.mlflow.end_run()\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.get_signature_from_dataloader","title":"<code>get_signature_from_dataloader(model, dataloader)</code>","text":"<p>Infers the signature of the model based on the input data from the dataloader.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to use for inference.</p> <p> TYPE: <code>Module</code> </p> <code>dataloader</code> <p>DataLoader for model inputs.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Optional[Any]: The inferred signature, if available.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def get_signature_from_dataloader(\n    self, model: Module, dataloader: DataLoader | DictDataLoader | None\n) -&gt; Any:\n    \"\"\"\n    Infers the signature of the model based on the input data from the dataloader.\n\n    Args:\n        model (Module): The model to use for inference.\n        dataloader (DataLoader | DictDataLoader |  None): DataLoader for model inputs.\n\n    Returns:\n        Optional[Any]: The inferred signature, if available.\n    \"\"\"\n    from mlflow.models import infer_signature\n\n    if dataloader is None:\n        return None\n\n    xs: InputData\n    xs, *_ = next(iter(dataloader))\n    preds = model(xs)\n\n    if isinstance(xs, Tensor):\n        xs = xs.detach().cpu().numpy()\n        preds = preds.detach().cpu().numpy()\n        return infer_signature(xs, preds)\n\n    return None\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to MLflow.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to MLflow.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_params(hyperparams)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model and its signature to MLflow using the provided data loaders.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and its signature to MLflow using the provided data loaders.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    if not self.mlflow:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n\n    signatures = self.get_signature_from_dataloader(model, train_dataloader)\n    self.mlflow.pytorch.log_model(model, artifact_path=\"model\", signature=signatures)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the MLflow writer and initializes an MLflow run.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>mlflow</code> <p>The MLflow module instance.</p> <p> TYPE: <code>ModuleType | None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; ModuleType | None:\n    \"\"\"\n    Opens the MLflow writer and initializes an MLflow run.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        mlflow: The MLflow module instance.\n    \"\"\"\n    import mlflow\n\n    self.mlflow = mlflow\n    tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"\")\n    experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", str(uuid4()))\n    run_name = os.getenv(\"MLFLOW_RUN_NAME\", str(uuid4()))\n\n    if self.mlflow:\n        self.mlflow.set_tracking_uri(tracking_uri)\n\n        # Create or get the experiment\n        exp_filter_string = f\"name = '{experiment_name}'\"\n        experiments = self.mlflow.search_experiments(filter_string=exp_filter_string)\n        if not experiments:\n            self.mlflow.create_experiment(name=experiment_name)\n\n        self.mlflow.set_experiment(experiment_name)\n        self.run = self.mlflow.start_run(run_name=run_name, nested=False)\n\n    return self.mlflow\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.mlflow:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.mlflow.log_figure(fig, descr)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.write","title":"<code>write(iteration, metrics)</code>","text":"<p>Logs the results of the current iteration to MLflow.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to MLflow.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_metrics(metrics, step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing.\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter","title":"<code>TensorBoardWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to TensorBoard.</p> ATTRIBUTE DESCRIPTION <code>writer</code> <p>The TensorBoard SummaryWriter instance.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.writer = None\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.close","title":"<code>close()</code>","text":"<p>Closes the TensorBoard writer.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the TensorBoard writer.\"\"\"\n    if self.writer:\n        self.writer.close()\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to TensorBoard.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to TensorBoard.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.writer:\n        self.writer.add_hparams(hyperparams, {})\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model.</p> <p>Currently not supported by TensorBoard.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model.\n\n    Currently not supported by TensorBoard.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    logger.warning(\"Model logging is not supported by tensorboard. No model will be logged.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the TensorBoard writer.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SummaryWriter</code> <p>The initialized TensorBoard writer.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; SummaryWriter:\n    \"\"\"\n    Opens the TensorBoard writer.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        SummaryWriter: The initialized TensorBoard writer.\n    \"\"\"\n    log_dir = str(config.log_folder)\n    purge_step = iteration if isinstance(iteration, int) else None\n    self.writer = SummaryWriter(log_dir=log_dir, purge_step=purge_step)\n    return self.writer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.writer:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.writer.add_figure(descr, fig, global_step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.write","title":"<code>write(iteration, metrics)</code>","text":"<p>Logs the results of the current iteration to TensorBoard.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to TensorBoard.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    if self.writer:\n        for key, value in metrics.items():\n            self.writer.add_scalar(key, value, iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing.\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.get_writer","title":"<code>get_writer(tracking_tool)</code>","text":"<p>Factory method to get the appropriate writer based on the tracking tool.</p> PARAMETER DESCRIPTION <code>tracking_tool</code> <p>The experiment tracking tool to use.</p> <p> TYPE: <code>ExperimentTrackingTool</code> </p> RETURNS DESCRIPTION <code>BaseWriter</code> <p>An instance of the appropriate writer.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def get_writer(tracking_tool: ExperimentTrackingTool) -&gt; BaseWriter:\n    \"\"\"Factory method to get the appropriate writer based on the tracking tool.\n\n    Args:\n        tracking_tool (ExperimentTrackingTool): The experiment tracking tool to use.\n\n    Returns:\n        BaseWriter: An instance of the appropriate writer.\n    \"\"\"\n    writer_class = WRITER_REGISTRY.get(tracking_tool)\n    if writer_class:\n        return writer_class()\n    else:\n        raise ValueError(f\"Unsupported tracking tool: {tracking_tool}\")\n</code></pre>"},{"location":"api/models/","title":"Quantum models","text":""},{"location":"api/models/#qadence.model.QuantumModel","title":"<code>QuantumModel(circuit, observable=None, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, mitigation=None, configuration=None)</code>","text":"<p>               Bases: <code>Module</code></p> <p>The central class of qadence that executes <code>QuantumCircuit</code>s and make them differentiable.</p> <p>This class should be used as base class for any new quantum model supported in the qadence framework for information on the implementation of custom models see here.</p> <p>Example: <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit, RX, RY, Z, PI, chain, kron\nfrom qadence import FeatureParameter, VariationalParameter\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, theta), RY(1, theta)),\n    kron(RX(0, phi), RY(1, phi)),\n)\n\ncircuit = QuantumCircuit(2, block)\n\nobservable = Z(0) + Z(1)\n\nmodel = QuantumModel(circuit, observable)\nvalues = {\"phi\": torch.tensor([PI, PI/2]), \"theta\": torch.tensor([PI, PI/2])}\n\nwf = model.run(values)\nxs = model.sample(values, n_shots=100)\nex = model.expectation(values)\nprint(wf)\nprint(xs)\nprint(ex)\n</code></pre> <pre><code>tensor([[ 1.0000e+00+0.0000e+00j, -1.2246e-16+0.0000e+00j,\n          0.0000e+00+1.2246e-16j,  0.0000e+00-1.4998e-32j],\n        [ 4.9304e-32+0.0000e+00j,  2.2204e-16+0.0000e+00j,\n          0.0000e+00-2.2204e-16j,  0.0000e+00-1.0000e+00j]])\n[OrderedCounter({'00': 100}), OrderedCounter({'11': 100})]\ntensor([[ 2.],\n        [-2.]], requires_grad=True)\n</code></pre>  ```</p> <p>Initialize a generic QuantumModel instance.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The circuit that is executed.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>Optional observable(s) that are used only in the <code>expectation</code> method. You can also provide observables on the fly to the expectation call directly.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>A backend for circuit execution.</p> <p> TYPE: <code>BackendName | str</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>A differentiability mode. Parameter shift based modes work on all backends. AD based modes only on PyTorch based backends.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Configuration for the backend.</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the <code>diff_mode</code> argument is set to None</p> Source code in <code>qadence/model.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock | None = None,\n    backend: BackendName | str = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n):\n    \"\"\"Initialize a generic QuantumModel instance.\n\n    Arguments:\n        circuit: The circuit that is executed.\n        observable: Optional observable(s) that are used only in the `expectation` method. You\n            can also provide observables on the fly to the expectation call directly.\n        backend: A backend for circuit execution.\n        diff_mode: A differentiability mode. Parameter shift based modes work on all backends.\n            AD based modes only on PyTorch based backends.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        configuration: Configuration for the backend.\n        noise: A noise model to use.\n\n    Raises:\n        ValueError: if the `diff_mode` argument is set to None\n    \"\"\"\n    super().__init__()\n\n    if not isinstance(circuit, QuantumCircuit):\n        TypeError(\n            f\"The circuit should be of type '&lt;class QuantumCircuit&gt;'. Got {type(circuit)}.\"\n        )\n\n    if diff_mode is None:\n        raise ValueError(\"`diff_mode` cannot be `None` in a `QuantumModel`.\")\n\n    self.backend = backend_factory(\n        backend=backend, diff_mode=diff_mode, configuration=configuration\n    )\n\n    if isinstance(observable, list) or observable is None:\n        observable = observable\n    else:\n        observable = [observable]\n\n    def _is_feature_param(p: Parameter) -&gt; bool:\n        return not p.trainable and not p.is_number\n\n    if observable is None:\n        self.inputs = list(filter(_is_feature_param, circuit.unique_parameters))\n    else:\n        uparams = unique_parameters(chain(circuit.block, *observable))\n        self.inputs = list(filter(_is_feature_param, uparams))\n\n    conv = self.backend.convert(circuit, observable)\n    self.embedding_fn = conv.embedding_fn\n    self._circuit = conv.circuit\n    self._observable = conv.observable\n    self._backend_name = backend\n    self._diff_mode = diff_mode\n    self._measurement = measurement\n    self._noise = noise\n    self._mitigation = mitigation\n    self._params = nn.ParameterDict(\n        {\n            str(key): nn.Parameter(val, requires_grad=val.requires_grad)\n            for key, val in conv.params.items()\n        }\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.device","title":"<code>device</code>  <code>property</code>","text":"<p>Get device.</p> RETURNS DESCRIPTION <code>device</code> <p>torch.device</p>"},{"location":"api/models/#qadence.model.QuantumModel.in_features","title":"<code>in_features</code>  <code>property</code>","text":"<p>Number of inputs.</p>"},{"location":"api/models/#qadence.model.QuantumModel.num_vparams","title":"<code>num_vparams</code>  <code>property</code>","text":"<p>The number of variational parameters.</p>"},{"location":"api/models/#qadence.model.QuantumModel.out_features","title":"<code>out_features</code>  <code>property</code>","text":"<p>Number of outputs.</p>"},{"location":"api/models/#qadence.model.QuantumModel.vals_vparams","title":"<code>vals_vparams</code>  <code>property</code>","text":"<p>Dictionary with parameters which are actually updated during optimization.</p>"},{"location":"api/models/#qadence.model.QuantumModel.vparams","title":"<code>vparams</code>  <code>property</code>","text":"<p>Variational parameters.</p>"},{"location":"api/models/#qadence.model.QuantumModel._from_dict","title":"<code>_from_dict(d, as_torch=False)</code>  <code>classmethod</code>","text":"<p>Initialize instance of QuantumModel from dictionary.</p> PARAMETER DESCRIPTION <code>d</code> <p>Dictionary.</p> <p> TYPE: <code>dict</code> </p> <code>as_torch</code> <p>Load parameters as torch tensors. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel instance</p> Source code in <code>qadence/model.py</code> <pre><code>@classmethod\ndef _from_dict(cls, d: dict, as_torch: bool = False) -&gt; QuantumModel:\n    \"\"\"Initialize instance of QuantumModel from dictionary.\n\n    Args:\n        d: Dictionary.\n        as_torch: Load parameters as torch tensors. Defaults to False.\n\n    Returns:\n        QuantumModel instance\n    \"\"\"\n    from qadence.serialization import deserialize\n\n    qm: QuantumModel\n    try:\n        qm_dict = d[cls.__name__]\n        qm = cls(\n            circuit=QuantumCircuit._from_dict(qm_dict[\"circuit\"]),\n            observable=(\n                None\n                if not isinstance(qm_dict[\"observable\"], list)\n                else [deserialize(q_obs) for q_obs in qm_dict[\"observable\"]]  # type: ignore[misc]\n            ),\n            backend=qm_dict[\"backend\"],\n            diff_mode=qm_dict[\"diff_mode\"],\n            measurement=Measurements._from_dict(qm_dict[\"measurement\"]),\n            noise=NoiseHandler._from_dict(qm_dict[\"noise\"]),\n            configuration=config_factory(qm_dict[\"backend\"], qm_dict[\"backend_configuration\"]),\n        )\n\n        if as_torch:\n            conv_pd = torch.nn.ParameterDict()\n            param_dict = d[\"param_dict\"]\n            for n, param in param_dict.items():\n                conv_pd[n] = torch.nn.Parameter(param)\n            qm._params = conv_pd\n        logger.debug(f\"Initialized {cls.__name__} from {d}.\")\n\n    except Exception as e:\n        logger.warning(f\"Unable to deserialize object {d} to {cls.__name__} due to {e}.\")\n\n    return qm\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel._to_dict","title":"<code>_to_dict(save_params=True)</code>","text":"<p>Convert QuantumModel to a dictionary for serialization.</p> PARAMETER DESCRIPTION <code>save_params</code> <p>Save parameters. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>dict[str, Any]</code> <p>The dictionary</p> Source code in <code>qadence/model.py</code> <pre><code>def _to_dict(self, save_params: bool = True) -&gt; dict[str, Any]:\n    \"\"\"Convert QuantumModel to a dictionary for serialization.\n\n    Arguments:\n        save_params: Save parameters. Defaults to True.\n\n    Returns:\n        The dictionary\n    \"\"\"\n    d = dict()\n    try:\n        if isinstance(self._observable, list):\n            abs_obs = [obs.abstract._to_dict() for obs in self._observable]\n        else:\n            abs_obs = [dict()]\n\n        d = {\n            \"circuit\": self._circuit.abstract._to_dict(),\n            \"observable\": abs_obs,\n            \"backend\": self._backend_name,\n            \"diff_mode\": self._diff_mode,\n            \"measurement\": (\n                self._measurement._to_dict() if self._measurement is not None else dict()\n            ),\n            \"noise\": self._noise._to_dict() if self._noise is not None else dict(),\n            \"backend_configuration\": asdict(self.backend.backend.config),  # type: ignore\n        }\n        param_dict_conv = {}\n        if save_params:\n            param_dict_conv = {name: param for name, param in self._params.items()}\n        d = {self.__class__.__name__: d, \"param_dict\": param_dict_conv}\n        logger.debug(f\"{self.__class__.__name__} serialized to {d}.\")\n    except Exception as e:\n        logger.warning(f\"Unable to serialize {self.__class__.__name__} due to {e}.\")\n    return d\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.assign_parameters","title":"<code>assign_parameters(values)</code>","text":"<p>Return the final, assigned circuit that is used in e.g. <code>backend.run</code>.</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Final, assigned circuit that is used in e.g. <code>backend.run</code></p> Source code in <code>qadence/model.py</code> <pre><code>def assign_parameters(self, values: dict[str, Tensor]) -&gt; Any:\n    \"\"\"Return the final, assigned circuit that is used in e.g. `backend.run`.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n\n    Returns:\n        Final, assigned circuit that is used in e.g. `backend.run`\n    \"\"\"\n    params = self.embedding_fn(self._params, values)\n    return self.backend.assign_parameters(self._circuit, params)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.circuit","title":"<code>circuit(circuit)</code>","text":"<p>Get backend-converted circuit.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>QuantumCircuit instance.</p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>Backend circuit.</p> Source code in <code>qadence/model.py</code> <pre><code>def circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Get backend-converted circuit.\n\n    Args:\n        circuit: QuantumCircuit instance.\n\n    Returns:\n        Backend circuit.\n    \"\"\"\n    return self.backend.circuit(circuit)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.expectation","title":"<code>expectation(values={}, observable=None, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute expectation using the given backend.</p> <p>Given an input state \\(|\\psi_0 \\rangle\\), a set of variational parameters \\(\\vec{\\theta}\\) and the unitary representation of the model \\(U(\\vec{\\theta})\\) we return \\(\\langle \\psi_0 | U(\\vec{\\theta}) | \\psi_0 \\rangle\\).</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>observable</code> <p>Observable part of the expectation.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable | None</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Optional input state.</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>A mitigation protocol to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>when no observable is set.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor of shape n_batches x n_obs</p> Source code in <code>qadence/model.py</code> <pre><code>def expectation(\n    self,\n    values: dict[str, Tensor] = {},\n    observable: list[ConvertedObservable] | ConvertedObservable | None = None,\n    state: Optional[Tensor] = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    r\"\"\"Compute expectation using the given backend.\n\n\n\n    Given an input state $|\\psi_0 \\rangle$,\n    a set of variational parameters $\\vec{\\theta}$\n    and the unitary representation of the model $U(\\vec{\\theta})$\n    we return $\\langle \\psi_0 | U(\\vec{\\theta}) | \\psi_0 \\rangle$.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        observable: Observable part of the expectation.\n        state: Optional input state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: A mitigation protocol to use.\n        endianness: Storage convention for binary information.\n\n    Raises:\n        ValueError: when no observable is set.\n\n    Returns:\n        A torch.Tensor of shape n_batches x n_obs\n    \"\"\"\n    if observable is None:\n        if self._observable is None:\n            raise ValueError(\n                \"Provide an AbstractBlock as the observable to compute expectation.\"\n                \"Either pass a 'native_observable' directly to 'QuantumModel.expectation'\"\n                \"or pass a (non-native) '&lt;class AbstractBlock&gt;' to the 'QuantumModel.__init__'.\"\n            )\n        observable = self._observable\n\n    params = self.embedding_fn(self._params, values)\n    if measurement is None:\n        measurement = self._measurement\n    if noise is None:\n        noise = self._noise\n    else:\n        self._noise = noise\n    if mitigation is None:\n        mitigation = self._mitigation\n    return self.backend.expectation(\n        circuit=self._circuit,\n        observable=observable,\n        param_values=params,\n        state=state,\n        measurement=measurement,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Calls run method with arguments.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor representing output.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/model.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Calls run method with arguments.\n\n    Returns:\n        Tensor: A torch.Tensor representing output.\n    \"\"\"\n    return self.run(*args, **kwargs)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.load","title":"<code>load(file_path, as_torch=False, map_location='cpu')</code>  <code>classmethod</code>","text":"<p>Load QuantumModel.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>File path to load model from.</p> <p> TYPE: <code>str | Path</code> </p> <code>as_torch</code> <p>Load parameters as torch tensor. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>map_location</code> <p>Location for loading. Defaults to \"cpu\".</p> <p> TYPE: <code>str | device</code> DEFAULT: <code>'cpu'</code> </p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel from file_path.</p> Source code in <code>qadence/model.py</code> <pre><code>@classmethod\ndef load(\n    cls, file_path: str | Path, as_torch: bool = False, map_location: str | torch.device = \"cpu\"\n) -&gt; QuantumModel:\n    \"\"\"Load QuantumModel.\n\n    Arguments:\n        file_path: File path to load model from.\n        as_torch: Load parameters as torch tensor. Defaults to False.\n        map_location (str | torch.device, optional): Location for loading. Defaults to \"cpu\".\n\n    Returns:\n        QuantumModel from file_path.\n    \"\"\"\n    qm_pt = {}\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if os.path.isdir(file_path):\n        from qadence.ml_tools.callbacks.saveload import get_latest_checkpoint_name\n\n        file_path = file_path / get_latest_checkpoint_name(file_path, \"model\")\n\n    try:\n        qm_pt = torch.load(file_path, map_location=map_location)\n    except Exception as e:\n        logger.error(f\"Unable to load QuantumModel due to {e}\")\n    return cls._from_dict(qm_pt, as_torch)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.load_params_from_dict","title":"<code>load_params_from_dict(d, strict=True)</code>","text":"<p>Copy parameters from dictionary into this QuantumModel.</p> <p>Unlike :meth:<code>~qadence.QuantumModel.from_dict</code>, this method does not create a new QuantumModel instance, but rather loads the parameters into the same QuantumModel. The behaviour of this method is similar to :meth:<code>~torch.nn.Module.load_state_dict</code>.</p> <p>The dictionary is assumed to have the format as saved via :meth:<code>~qadence.QuantumModel.to_dict</code></p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary</p> <p> TYPE: <code>dict</code> </p> <code>strict</code> <p>Whether to strictly enforce that the parameter keys in the dictionary and in the model match exactly. Default: <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>qadence/model.py</code> <pre><code>def load_params_from_dict(self, d: dict, strict: bool = True) -&gt; None:\n    \"\"\"Copy parameters from dictionary into this QuantumModel.\n\n    Unlike :meth:`~qadence.QuantumModel.from_dict`, this method does not create a new\n    QuantumModel instance, but rather loads the parameters into the same QuantumModel.\n    The behaviour of this method is similar to :meth:`~torch.nn.Module.load_state_dict`.\n\n    The dictionary is assumed to have the format as saved via\n    :meth:`~qadence.QuantumModel.to_dict`\n\n    Args:\n        d (dict): The dictionary\n        strict (bool, optional):\n            Whether to strictly enforce that the parameter keys in the dictionary and\n            in the model match exactly. Default: ``True``.\n    \"\"\"\n    param_dict = d[\"param_dict\"]\n    missing_keys = set(self._params.keys()) - set(param_dict.keys())\n    unexpected_keys = set(param_dict.keys()) - set(self._params.keys())\n\n    if strict:\n        error_msgs = []\n        if len(unexpected_keys) &gt; 0:\n            error_msgs.append(f\"Unexpected key(s) in dictionary: {unexpected_keys}\")\n        if len(missing_keys) &gt; 0:\n            error_msgs.append(f\"Missing key(s) in dictionary: {missing_keys}\")\n        if len(error_msgs) &gt; 0:\n            errors_string = \"\\n\\t\".join(error_msgs)\n            raise RuntimeError(\n                f\"Error(s) loading the parameter dictionary due to: \\n\\t{errors_string}\\n\"\n                \"This error was thrown because the `strict` argument is set `True`.\"\n                \"If you don't need the parameter keys of the dictionary to exactly match \"\n                \"the model parameters, set `strict=False`.\"\n            )\n\n    for n, param in param_dict.items():\n        try:\n            with torch.no_grad():\n                self._params[n].copy_(\n                    torch.nn.Parameter(param, requires_grad=param.requires_grad)\n                )\n        except Exception as e:\n            logger.warning(f\"Unable to load parameter {n} from dictionary due to {e}.\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.observable","title":"<code>observable(observable, n_qubits)</code>","text":"<p>Get backend observable.</p> PARAMETER DESCRIPTION <code>observable</code> <p>Observable block.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Backend observable.</p> Source code in <code>qadence/model.py</code> <pre><code>def observable(self, observable: AbstractBlock, n_qubits: int) -&gt; Any:\n    \"\"\"Get backend observable.\n\n    Args:\n        observable: Observable block.\n        n_qubits: Number of qubits\n\n    Returns:\n        Backend observable.\n    \"\"\"\n    return self.backend.observable(observable, n_qubits)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.overlap","title":"<code>overlap()</code>","text":"<p>Overlap of model.</p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>The overlap method is not implemented for this model.</p> Source code in <code>qadence/model.py</code> <pre><code>def overlap(self) -&gt; Tensor:\n    \"\"\"Overlap of model.\n\n    Raises:\n        NotImplementedError: The overlap method is not implemented for this model.\n    \"\"\"\n    raise NotImplementedError(\"The overlap method is not implemented for this model.\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.reset_vparams","title":"<code>reset_vparams(values)</code>","text":"<p>Reset all the variational parameters with a given list of values.</p> Source code in <code>qadence/model.py</code> <pre><code>def reset_vparams(self, values: Sequence) -&gt; None:\n    \"\"\"Reset all the variational parameters with a given list of values.\"\"\"\n    current_vparams = OrderedDict({k: v for k, v in self._params.items() if v.requires_grad})\n\n    assert (\n        len(values) == self.num_vparams\n    ), \"Pass an iterable with the values of all variational parameters\"\n    for i, k in enumerate(current_vparams.keys()):\n        current_vparams[k].data = torch.tensor([values[i]])\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.run","title":"<code>run(values=None, state=None, endianness=Endianness.BIG)</code>","text":"<p>Run model.</p> <p>Given an input state \\(| \\psi_0 \\rangle\\), a set of variational parameters \\(\\vec{\\theta}\\) and the unitary representation of the model \\(U(\\vec{\\theta})\\) we return \\(U(\\vec{\\theta}) | \\psi_0 \\rangle\\).</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Optional input state to apply model on.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor representing output.</p> Source code in <code>qadence/model.py</code> <pre><code>def run(\n    self,\n    values: dict[str, Tensor] = None,\n    state: Tensor | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    r\"\"\"Run model.\n\n    Given an input state $| \\psi_0 \\rangle$,\n    a set of variational parameters $\\vec{\\theta}$\n    and the unitary representation of the model $U(\\vec{\\theta})$\n    we return $U(\\vec{\\theta}) | \\psi_0 \\rangle$.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        state: Optional input state to apply model on.\n        endianness: Storage convention for binary information.\n\n    Returns:\n        A torch.Tensor representing output.\n    \"\"\"\n    if values is None:\n        values = {}\n\n    params = self.embedding_fn(self._params, values)\n\n    return self.backend.run(self._circuit, params, state=state, endianness=endianness)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.sample","title":"<code>sample(values={}, n_shots=1000, state=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Obtain samples from model.</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>n_shots</code> <p>Observable part of the expectation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>state</code> <p>Optional input state to apply model on.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>A mitigation protocol to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>list[Counter]</code> <p>A list of Counter instances with the sample results.</p> Source code in <code>qadence/model.py</code> <pre><code>def sample(\n    self,\n    values: dict[str, torch.Tensor] = {},\n    n_shots: int = 1000,\n    state: torch.Tensor | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; list[Counter]:\n    \"\"\"Obtain samples from model.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        n_shots: Observable part of the expectation.\n        state: Optional input state to apply model on.\n        noise: A noise model to use.\n        mitigation: A mitigation protocol to use.\n        endianness: Storage convention for binary information.\n\n    Returns:\n        A list of Counter instances with the sample results.\n    \"\"\"\n    params = self.embedding_fn(self._params, values)\n    if noise is None:\n        noise = self._noise\n    if mitigation is None:\n        mitigation = self._mitigation\n    return self.backend.sample(\n        self._circuit,\n        params,\n        n_shots=n_shots,\n        state=state,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.save","title":"<code>save(folder, file_name='quantum_model.pt', save_params=True)</code>","text":"<p>Save model.</p> PARAMETER DESCRIPTION <code>folder</code> <p>Folder where model is saved.</p> <p> TYPE: <code>str | Path</code> </p> <code>file_name</code> <p>File name for saving model. Defaults to \"quantum_model.pt\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'quantum_model.pt'</code> </p> <code>save_params</code> <p>Save parameters if True. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If folder is not a directory.</p> Source code in <code>qadence/model.py</code> <pre><code>def save(\n    self, folder: str | Path, file_name: str = \"quantum_model.pt\", save_params: bool = True\n) -&gt; None:\n    \"\"\"Save model.\n\n    Arguments:\n        folder: Folder where model is saved.\n        file_name: File name for saving model. Defaults to \"quantum_model.pt\".\n        save_params: Save parameters if True. Defaults to True.\n\n    Raises:\n        FileNotFoundError: If folder is not a directory.\n    \"\"\"\n    if not os.path.isdir(folder):\n        raise FileNotFoundError\n    try:\n        torch.save(self._to_dict(save_params), folder / Path(file_name))\n    except Exception as e:\n        logger.error(f\"Unable to write QuantumModel to disk due to {e}\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.to","title":"<code>to(*args, **kwargs)</code>","text":"<p>Conversion method for device or types.</p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel with conversions.</p> Source code in <code>qadence/model.py</code> <pre><code>def to(self, *args: Any, **kwargs: Any) -&gt; QuantumModel:\n    \"\"\"Conversion method for device or types.\n\n    Returns:\n        QuantumModel with conversions.\n    \"\"\"\n    from pyqtorch import QuantumCircuit as PyQCircuit\n\n    try:\n        if isinstance(self._circuit.native, PyQCircuit):\n            self._circuit.native = self._circuit.native.to(*args, **kwargs)\n            if self._observable is not None:\n                if isinstance(self._observable, ConvertedObservable):\n                    self._observable.native = self._observable.native.to(*args, **kwargs)\n                elif isinstance(self._observable, list):\n                    for obs in self._observable:\n                        obs.native = obs.native.to(*args, **kwargs)\n            self._params = self._params.to(\n                device=self._circuit.native.device,\n                dtype=(\n                    torch.float64\n                    if self._circuit.native.dtype == torch.cdouble\n                    else torch.float32\n                ),\n            )\n            logger.debug(f\"Moved {self} to {args}, {kwargs}.\")\n        else:\n            logger.debug(\"QuantumModel.to only supports pyqtorch.QuantumCircuits.\")\n    except Exception as e:\n        logger.warning(f\"Unable to move {self} to {args}, {kwargs} due to {e}.\")\n    return self\n</code></pre>"},{"location":"api/noise/","title":"Noise","text":""},{"location":"api/noise/#noise-for-simulations","title":"Noise for simulations","text":""},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler","title":"<code>NoiseHandler(protocol, options=dict())</code>","text":"<p>A container for multiple sources of noise.</p> <p>Note <code>NoiseProtocol.ANALOG</code> and <code>NoiseProtocol.DIGITAL</code> sources cannot be both present. Also <code>NoiseProtocol.READOUT</code> can only be present once as the last noise sources, and only exclusively with <code>NoiseProtocol.DIGITAL</code> sources.</p> PARAMETER DESCRIPTION <code>protocol</code> <p>The protocol(s) applied. To be defined from <code>NoiseProtocol</code>.</p> <p> TYPE: <code>NoiseEnum | list[NoiseEnum]</code> </p> <code>options</code> <p>A list of options defining the protocol. For <code>NoiseProtocol.ANALOG</code>, options should contain a field <code>noise_probs</code>. For <code>NoiseProtocol.DIGITAL</code>, options should contain a field <code>error_probability</code>.</p> <p> TYPE: <code>dict | list[dict]</code> DEFAULT: <code>dict()</code> </p> <p>Examples:</p> <pre><code>    from qadence import NoiseProtocol, NoiseHandler\n\n    analog_options = {\"noise_probs\": 0.1}\n    digital_options = {\"error_probability\": 0.1}\n    readout_options = {\"error_probability\": 0.1, \"seed\": 0}\n\n    # single noise sources\n    analog_noise = NoiseHandler(NoiseProtocol.ANALOG.DEPOLARIZING, analog_options)\n    digital_depo_noise = NoiseHandler(NoiseProtocol.DIGITAL.DEPOLARIZING, digital_options)\n    readout_noise = NoiseHandler(NoiseProtocol.READOUT, readout_options)\n\n    # init from multiple sources\n    protocols: list = [NoiseProtocol.DIGITAL.DEPOLARIZING, NoiseProtocol.READOUT]\n    options: list = [digital_options, readout_noise]\n    noise_combination = NoiseHandler(protocols, options)\n\n    # Appending noise sources\n    noise_combination = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, digital_options)\n    noise_combination.append([digital_depo_noise, readout_noise])\n</code></pre> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def __init__(\n    self,\n    protocol: NoiseEnum | list[NoiseEnum],\n    options: dict | list[dict] = dict(),\n) -&gt; None:\n    self.protocol = protocol if isinstance(protocol, list) else [protocol]\n    self.options = options if isinstance(options, list) else [options] * len(self.protocol)\n    self.verify_all_protocols()\n</code></pre>"},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler.append","title":"<code>append(other)</code>","text":"<p>Append noises.</p> PARAMETER DESCRIPTION <code>other</code> <p>The noises to add.</p> <p> TYPE: <code>NoiseHandler | list[NoiseHandler]</code> </p> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def append(self, other: NoiseHandler | list[NoiseHandler]) -&gt; None:\n    \"\"\"Append noises.\n\n    Args:\n        other (NoiseHandler | list[NoiseHandler]): The noises to add.\n    \"\"\"\n    # To avoid overwriting the noise_sources list if an error is raised, make a copy\n    other_list = other if isinstance(other, list) else [other]\n    protocols = self.protocol[:]\n    options = self.options[:]\n\n    for noise in other_list:\n        protocols += noise.protocol\n        options += noise.options\n\n    # init may raise an error\n    temp_handler = NoiseHandler(protocols, options)\n    # if verify passes, replace protocols and options\n    self.protocol = temp_handler.protocol\n    self.options = temp_handler.options\n</code></pre>"},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler.verify_all_protocols","title":"<code>verify_all_protocols()</code>","text":"<p>Make sure all protocols are correct in terms and their combination too.</p> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def verify_all_protocols(self) -&gt; None:\n    \"\"\"Make sure all protocols are correct in terms and their combination too.\"\"\"\n\n    if len(self.protocol) == 0:\n        raise ValueError(\"NoiseHandler should be specified with one valid configuration.\")\n\n    if len(self.protocol) != len(self.options):\n        raise ValueError(\"Specify lists of same length when defining noises.\")\n\n    for protocol, option in zip(self.protocol, self.options):\n        self._verify_single_protocol(protocol, option)\n\n    types = [type(p) for p in self.protocol]\n    unique_types = set(types)\n    if NoiseProtocol.DIGITAL in unique_types and NoiseProtocol.ANALOG in unique_types:\n        raise ValueError(\"Cannot define a config with both Digital and Analog noises.\")\n\n    if NoiseProtocol.ANALOG in unique_types:\n        if NoiseProtocol.READOUT in unique_types:\n            raise ValueError(\"Cannot define a config with both READOUT and Analog noises.\")\n        if types.count(NoiseProtocol.ANALOG) &gt; 1:\n            raise ValueError(\"Multiple Analog Noises are not supported yet.\")\n\n    if NoiseProtocol.READOUT in unique_types:\n        if (\n            not isinstance(self.protocol[-1], NoiseProtocol.READOUT)\n            or types.count(NoiseProtocol.READOUT) &gt; 1\n        ):\n            raise ValueError(\"Only define a NoiseHandler with one READOUT as the last Noise.\")\n</code></pre>"},{"location":"api/operations/","title":"Operations","text":"<p>Operations are common <code>PrimitiveBlocks</code>, these are often called gates elsewhere.</p>"},{"location":"api/operations/#constant-blocks","title":"Constant blocks","text":"<p>CY gate not implemented</p>"},{"location":"api/operations/#qadence.operations.X","title":"<code>X(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The X gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.Y","title":"<code>Y(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Y gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.Z","title":"<code>Z(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Z gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.I","title":"<code>I(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The identity gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.H","title":"<code>H(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hadamard or H gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = (1 / np.sqrt(2)) * (X(target) + Z(target) - np.sqrt(2) * I(target))\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.S","title":"<code>S(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The S / Phase gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.SDagger","title":"<code>SDagger(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hermitian adjoint/conjugate transpose of the S / Phase gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.SWAP","title":"<code>SWAP(control, target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The SWAP gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    a11 = 0.5 * (Z(control) - I(control))\n    a22 = -0.5 * (Z(target) + I(target))\n    a12 = 0.5 * (chain(X(control), Z(control)) + X(control))\n    a21 = 0.5 * (chain(Z(target), X(target)) + X(target))\n    self.generator = (\n        kron(-1.0 * a22, a11) + kron(-1.0 * a11, a22) + kron(a12, a21) + kron(a21, a12)\n    )\n    super().__init__((control, target), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.T","title":"<code>T(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The T gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.TDagger","title":"<code>TDagger(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hermitian adjoint/conjugate transpose of the T gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CNOT","title":"<code>CNOT(control, target, noise=None)</code>","text":"<p>               Bases: <code>ControlBlock</code></p> <p>The CNot, or CX, gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(self, control: int, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    self.generator = kron(N(control), X(target) - I(target))\n    super().__init__((control,), X(target), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CZ","title":"<code>CZ(control, target, noise=None)</code>","text":"<p>               Bases: <code>MCZ</code></p> <p>The CZ gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(self, control: int, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((control,), target, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CPHASE","title":"<code>CPHASE(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCPHASE</code></p> <p>The CPHASE gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#parametrized-blocks","title":"Parametrized blocks","text":""},{"location":"api/operations/#qadence.operations.RX","title":"<code>RX(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Rx gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    # TODO: should we give them more meaningful names? like 'angle'?\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = X(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.RY","title":"<code>RY(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Ry gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = Y(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.RZ","title":"<code>RZ(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Rz gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRX","title":"<code>CRX(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRX</code></p> <p>The CRX gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRY","title":"<code>CRY(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRY</code></p> <p>The CRY gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self, control: int, target: int, parameter: TParameter, noise: NoiseHandler | None = None\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRZ","title":"<code>CRZ(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRZ</code></p> <p>The CRZ gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.PHASE","title":"<code>PHASE(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Parametric Phase / S gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = ParamMap(parameter=parameter)\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#hamiltonian-evolution","title":"Hamiltonian Evolution","text":"<p>AnalogSWAP should be turned into a proper analog block</p>"},{"location":"api/operations/#qadence.operations.HamEvo","title":"<code>HamEvo(generator, parameter, qubit_support=None, duration=None, noise_operators=list())</code>","text":"<p>               Bases: <code>TimeEvolutionBlock</code></p> <p>The Hamiltonian evolution operator U(t).</p> <p>For time-independent Hamiltonians the solution is exact:</p> <pre><code>U(t) = exp(-iGt)\n</code></pre> <p>where G represents an Hermitian generator, or Hamiltonian and t represents the time parameter. For time-dependent Hamiltonians, the solution is obtained by numerical integration of the Schrodinger equation.</p> PARAMETER DESCRIPTION <code>generator</code> <p>Hamiltonian generator, either symbolic as an AbstractBlock, or as a torch.Tensor or numpy.ndarray.</p> <p> TYPE: <code>Union[TGenerator, AbstractBlock]</code> </p> <code>parameter</code> <p>The time parameter for evolution operator. For the time-independent case, it represents the actual value for which the evolution will be evaluated. For the time-dependent case, it should be an instance of TimeParameter to signal the solver the variable that will be integrated over.</p> <p> TYPE: <code>TParameter</code> </p> <code>qubit_support</code> <p>The qubits on which the evolution will be performed on. Only required for generators that are not a composition of blocks.</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>(optional) duration of the evolution in case of time-dependent generator. By default, a FeatureParameter with tag \"duration\" will be initialized, and the value will then be required in the values dict.</p> <p> TYPE: <code>TParameter | None</code> DEFAULT: <code>None</code> </p> <code>noise_operators</code> <p>(optional) the list of jump operators to use when using a shrodinger solver, allowing to perform noisy simulations.</p> <p> TYPE: <code>list[AbstractBlock]</code> DEFAULT: <code>list()</code> </p> <p>Examples:</p> <pre><code>from qadence import X, HamEvo, PI, add, run\nfrom qadence import FeatureParameter, TimeParameter\nimport torch\n\nn_qubits = 3\n\n# Hamiltonian as a block composition\nhamiltonian = add(X(i) for i in range(n_qubits))\nhevo = HamEvo(hamiltonian, parameter=torch.rand(2))\nstate = run(hevo)\n\n# Hamiltonian as a random matrix\nhamiltonian = torch.rand(2, 2, dtype=torch.complex128)\nhevo = HamEvo(hamiltonian, parameter=torch.rand(2), qubit_support=(0,))\nstate = run(hevo)\n\n# Time-dependent Hamiltonian\nt = TimeParameter(\"t\")\nhamiltonian = t * add(X(i) for i in range(n_qubits))\nhevo = HamEvo(hamiltonian, parameter=t)\nstate = run(hevo, values = {\"duration\": torch.tensor(1.0)})\n\n# Adding noise operators\nnoise_ops = [X(0)]\nhevo = HamEvo(hamiltonian, parameter=t, noise_operators=noise_ops)\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/operations/ham_evo.py</code> <pre><code>def __init__(\n    self,\n    generator: Union[TGenerator, AbstractBlock],\n    parameter: TParameter,\n    qubit_support: tuple[int, ...] = None,\n    duration: TParameter | None = None,\n    noise_operators: list[AbstractBlock] = list(),\n):\n    params = {}\n    if qubit_support is None and not isinstance(generator, AbstractBlock):\n        raise ValueError(\"You have to supply a qubit support for non-block generators.\")\n    super().__init__(qubit_support if qubit_support else generator.qubit_support)\n    if isinstance(generator, AbstractBlock):\n        qubit_support = generator.qubit_support\n        if generator.is_parametric:\n            params = {str(e): e for e in expressions(generator)}\n        if generator.is_time_dependent:\n            if isinstance(duration, str):\n                duration = Parameter(duration, trainable=False)\n            elif duration is None:\n                duration = Parameter(\"duration\", trainable=False)\n        if not generator.is_time_dependent and duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n    elif isinstance(generator, torch.Tensor):\n        if duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n        msg = \"Please provide a square generator.\"\n        if len(generator.shape) == 2:\n            assert generator.shape[0] == generator.shape[1], msg\n        elif len(generator.shape) == 3:\n            assert generator.shape[1] == generator.shape[2], msg\n            assert generator.shape[0] == 1, \"Qadence doesnt support batched generators.\"\n        else:\n            raise TypeError(\n                \"Only 2D or 3D generators are supported.\\\n                            In case of a 3D generator, the batch dim\\\n                            is expected to be at dim 0.\"\n            )\n        params = {str(generator.__hash__()): generator}\n    elif isinstance(generator, (sympy.Basic, sympy.Array)):\n        if duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n        params = {str(generator): generator}\n    else:\n        raise TypeError(\n            f\"Generator of type {type(generator)} not supported.\\\n                        If you're using a numpy.ndarray, please cast it to a torch tensor.\"\n        )\n    if duration is not None:\n        params = {\"duration\": Parameter(duration), **params}\n    params = {\"parameter\": Parameter(parameter), **params}\n    self.parameters = ParamMap(**params)\n    self.time_param = parameter\n    self.generator = generator\n    self.duration = duration\n\n    if len(noise_operators) &gt; 0:\n        if not all(\n            [\n                len(set(op.qubit_support + self.qubit_support) - set(self.qubit_support)) == 0\n                for op in noise_operators\n            ]\n        ):\n            raise ValueError(\n                \"Noise operators should be defined\"\n                \" over the same or a subset of the qubit support\"\n            )\n        if True in [op.is_parametric for op in noise_operators]:\n            raise ValueError(\"Parametric operators are not supported\")\n    self.noise_operators = noise_operators\n</code></pre>"},{"location":"api/operations/#qadence.operations.HamEvo.digital_decomposition","title":"<code>digital_decomposition(approximation=LTSOrder.ST4)</code>","text":"<p>Decompose the Hamiltonian evolution into digital gates.</p> PARAMETER DESCRIPTION <code>approximation</code> <p>Choose the type of decomposition. Defaults to \"st4\". Available types are: * 'basic' = apply first-order Trotter formula and decompose each term of     the exponential into digital gates. It is exact only if applied to an     operator whose terms are mutually commuting. * 'st2' = Trotter-Suzuki 2nd order formula for approximating non-commuting     Hamiltonians. * 'st4' = Trotter-Suzuki 4th order formula for approximating non-commuting     Hamiltonians.</p> <p> TYPE: <code>str</code> DEFAULT: <code>ST4</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>a block with the digital decomposition</p> <p> TYPE: <code>AbstractBlock</code> </p> Source code in <code>qadence/operations/ham_evo.py</code> <pre><code>def digital_decomposition(self, approximation: LTSOrder = LTSOrder.ST4) -&gt; AbstractBlock:\n    \"\"\"Decompose the Hamiltonian evolution into digital gates.\n\n    Args:\n        approximation (str, optional): Choose the type of decomposition. Defaults to \"st4\".\n            Available types are:\n            * 'basic' = apply first-order Trotter formula and decompose each term of\n                the exponential into digital gates. It is exact only if applied to an\n                operator whose terms are mutually commuting.\n            * 'st2' = Trotter-Suzuki 2nd order formula for approximating non-commuting\n                Hamiltonians.\n            * 'st4' = Trotter-Suzuki 4th order formula for approximating non-commuting\n                Hamiltonians.\n\n    Returns:\n        AbstractBlock: a block with the digital decomposition\n    \"\"\"\n\n    # psi(t) = exp(-i * H * t * psi0)\n    # psi(t) = exp(-i * lambda * t * psi0)\n    # H = sum(Paulin) + sum(Pauli1*Pauli2)\n    logger.info(\"Quantum simulation of the time-independent Schr\u00f6dinger equation.\")\n\n    blocks = []\n\n    # how to change the type/dict to enum effectively\n\n    # when there is a term including non-commuting matrices use st2 or st4\n\n    # 1) should check that the given generator respects the constraints\n    # single-qubit gates\n\n    assert isinstance(\n        self.generator, AbstractBlock\n    ), \"Only a generator represented as a block can be decomposed\"\n\n    if block_is_qubit_hamiltonian(self.generator):\n        try:\n            block_is_commuting_hamiltonian(self.generator)\n            approximation = LTSOrder.BASIC  # use the simpler approach if the H is commuting\n        except TypeError:\n            logger.warning(\n                \"\"\"Non-commuting terms in the Pauli operator.\n                The Suzuki-Trotter approximation is applied.\"\"\"\n            )\n\n        blocks.extend(\n            lie_trotter_suzuki(\n                block=self.generator,\n                parameter=self.parameters.parameter,\n                order=LTSOrder[approximation],\n            )\n        )\n\n        # 2) return an AbstractBlock instance with the set of gates\n        # resulting from the decomposition\n\n        return chain(*blocks)\n    else:\n        raise NotImplementedError(\n            \"The current digital decomposition can be applied only to Pauli Hamiltonians.\"\n        )\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogSWAP","title":"<code>AnalogSWAP(control, target, parameter=3 * PI / 4)</code>","text":"<p>               Bases: <code>HamEvo</code></p> <p>Single time-independent Hamiltonian evolution over a Rydberg Ising.</p> <p>hamiltonian yielding a SWAP (up to global phase).</p> <p>Derived from Bapat et al. where it is applied to XX-type Hamiltonian</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def __init__(self, control: int, target: int, parameter: TParameter = 3 * PI / 4):\n    rydberg_ising_hamiltonian_generator = (\n        4.0 * kron((I(control) - Z(control)) / 2.0, (I(target) - Z(target)) / 2.0)\n        + (2.0 / 3.0) * np.sqrt(2.0) * X(control)\n        + (2.0 / 3.0) * np.sqrt(2.0) * X(target)\n        + (1.0 + np.sqrt(5.0) / 3) * Z(control)\n        + (1.0 + np.sqrt(5.0) / 3) * Z(target)\n    )\n    super().__init__(rydberg_ising_hamiltonian_generator, parameter, (control, target))\n</code></pre>"},{"location":"api/operations/#analog-blocks","title":"Analog blocks","text":""},{"location":"api/operations/#qadence.operations.AnalogRX","title":"<code>AnalogRX(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog X rotation.</p> <p>Shorthand for <code>AnalogRot</code>:</p> <pre><code>\u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\nAnalogRot(duration=t, omega=\u03a9)\n</code></pre> PARAMETER DESCRIPTION <code>angle</code> <p>Rotation angle [rad]</p> <p> TYPE: <code>float | str | Parameter</code> </p> <code>qubit_support</code> <p>Defines the (local/global) qubit support</p> <p> TYPE: <code>str | QubitSupport | Tuple</code> DEFAULT: <code>'global'</code> </p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRX(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog X rotation.\n\n    Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n\n    ```python\n    \u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\n    AnalogRot(duration=t, omega=\u03a9)\n    ```\n\n    Arguments:\n        angle: Rotation angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n    return _analog_rot(angle, qubit_support, phase=0, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRY","title":"<code>AnalogRY(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog Y rotation.</p> <p>Shorthand for <code>AnalogRot</code>:</p> <p><pre><code>\u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\nAnalogRot(duration=t, omega=\u03a9, phase=-\u03c0/2)\n</code></pre> Arguments:     angle: Rotation angle [rad]     qubit_support: Defines the (local/global) qubit support</p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRY(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog Y rotation.\n\n    Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n\n    ```python\n    \u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\n    AnalogRot(duration=t, omega=\u03a9, phase=-\u03c0/2)\n    ```\n    Arguments:\n        angle: Rotation angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n    return _analog_rot(angle, qubit_support, phase=-PI / 2, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRZ","title":"<code>AnalogRZ(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog Z rotation. Shorthand for <code>AnalogRot</code>: <pre><code>\u03c6=2.4; \u03b4=\u03c0; t = \u03c6/\u03b4 * 100)\nAnalogRot(duration=t, delta=\u03b4, phase=\u03c0/2)\n</code></pre></p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRZ(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog Z rotation. Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n    ```\n    \u03c6=2.4; \u03b4=\u03c0; t = \u03c6/\u03b4 * 100)\n    AnalogRot(duration=t, delta=\u03b4, phase=\u03c0/2)\n    ```\n    \"\"\"\n    q = _cast(QubitSupport, qubit_support)\n    alpha = _cast(Parameter, angle)\n    delta = PI\n    omega = 0\n    duration = alpha / delta * 1000\n    h_norm = sympy.sqrt(omega**2 + delta**2)\n    ps = ParamMap(\n        alpha=alpha, duration=duration, omega=omega, delta=delta, phase=0.0, h_norm=h_norm\n    )\n    return ConstantAnalogRotation(qubit_support=q, parameters=ps, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRot","title":"<code>AnalogRot(duration, omega=0, delta=0, phase=0, qubit_support='global', add_pattern=True)</code>","text":"<p>General analog rotation operation.</p> PARAMETER DESCRIPTION <code>duration</code> <p>Duration of the rotation [ns].</p> <p> TYPE: <code>float | str | Parameter</code> </p> <code>omega</code> <p>Rotation frequency [rad/\u03bcs]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>delta</code> <p>Rotation frequency [rad/\u03bcs]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>phase</code> <p>Phase angle [rad]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>qubit_support</code> <p>Defines the (local/global) qubit support</p> <p> TYPE: <code>str | QubitSupport | Tuple</code> DEFAULT: <code>'global'</code> </p> <code>add_pattern</code> <p>False disables the semi-local addressing pattern for the execution of this specific block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRot(\n    duration: float | str | Parameter,\n    omega: float | str | Parameter = 0,\n    delta: float | str | Parameter = 0,\n    phase: float | str | Parameter = 0,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"General analog rotation operation.\n\n    Arguments:\n        duration: Duration of the rotation [ns].\n        omega: Rotation frequency [rad/\u03bcs]\n        delta: Rotation frequency [rad/\u03bcs]\n        phase: Phase angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n        add_pattern: False disables the semi-local addressing pattern\n            for the execution of this specific block.\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n\n    if omega == 0 and delta == 0:\n        raise ValueError(\"Parameters omega and delta cannot both be 0.\")\n\n    q = _cast(QubitSupport, qubit_support)\n    duration = Parameter(duration)\n    omega = Parameter(omega)\n    delta = Parameter(delta)\n    phase = Parameter(phase)\n    h_norm = sympy.sqrt(omega**2 + delta**2)\n    alpha = duration * h_norm / 1000\n    ps = ParamMap(\n        alpha=alpha, duration=duration, omega=omega, delta=delta, phase=phase, h_norm=h_norm\n    )\n    return ConstantAnalogRotation(parameters=ps, qubit_support=q, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogInteraction","title":"<code>AnalogInteraction(duration, qubit_support='global', add_pattern=True)</code>","text":"<p>Evolution of the interaction term for a register of qubits.</p> <p>Constructs a <code>InteractionBlock</code>.</p> PARAMETER DESCRIPTION <code>duration</code> <p>Time to evolve the interaction for in nanoseconds.</p> <p> TYPE: <code>TNumber | Basic</code> </p> <code>qubit_support</code> <p>Qubits the <code>InteractionBlock</code> is applied to. Can be either <code>\"global\"</code> to evolve the interaction block to all qubits or a tuple of integers.</p> <p> TYPE: <code>str | QubitSupport | tuple</code> DEFAULT: <code>'global'</code> </p> <code>add_pattern</code> <p>False disables the semi-local addressing pattern for the execution of this specific block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>InteractionBlock</code> <p>a <code>InteractionBlock</code></p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogInteraction(\n    duration: TNumber | sympy.Basic,\n    qubit_support: str | QubitSupport | tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; InteractionBlock:\n    \"\"\"Evolution of the interaction term for a register of qubits.\n\n    Constructs a [`InteractionBlock`][qadence.blocks.analog.InteractionBlock].\n\n    Arguments:\n        duration: Time to evolve the interaction for in nanoseconds.\n        qubit_support: Qubits the `InteractionBlock` is applied to. Can be either\n            `\"global\"` to evolve the interaction block to all qubits or a tuple of integers.\n        add_pattern: False disables the semi-local addressing pattern\n            for the execution of this specific block.\n\n    Returns:\n        a `InteractionBlock`\n    \"\"\"\n    q = _cast(QubitSupport, qubit_support)\n    ps = ParamMap(duration=duration)\n    return InteractionBlock(parameters=ps, qubit_support=q, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/parameters/","title":"Parameters","text":""},{"location":"api/parameters/#parameters","title":"Parameters","text":""},{"location":"api/parameters/#qadence.parameters.ParamMap","title":"<code>ParamMap(**kwargs)</code>","text":"<p>Connects UUIDs of parameters to their expressions and names.</p> <p>This class is not user-facing and only needed for more complex block definitions. It provides convenient access to expressions/UUIDs/names needed in different backends.</p> PARAMETER DESCRIPTION <code>kwargs</code> <p>Parameters.</p> <p> TYPE: <code>str | TNumber | Tensor | Basic | Parameter</code> DEFAULT: <code>{}</code> </p> <p>Example: <pre><code>import sympy\nfrom qadence.parameters import ParamMap\n\n(x,y) = sympy.symbols(\"x y\")\nps = ParamMap(omega=2.0, duration=x+y)\n\nprint(f\"{ps.names() = }\")\nprint(f\"{ps.expressions() = }\")\nprint(f\"{ps.uuids() = }\")\n</code></pre> <pre><code>ps.names() = dict_keys(['omega', 'duration'])\nps.expressions() = dict_values([2.00000000000000, x + y])\nps.uuids() = dict_keys(['2a737266-5c73-48f1-91f8-3e4b06303b33', '596ead12-4a7d-40b6-a988-5739e7998ab6'])\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def __init__(self, **kwargs: str | TNumber | Tensor | Basic | Parameter):\n    self._name_dict: dict[str, tuple[str, Basic]] = {}\n    self._uuid_dict: dict[str, str] = {}\n    for name, v in kwargs.items():\n        param = v if isinstance(v, sympy.Basic) else Parameter(v)\n        uuid = str(uuid4())\n        self._name_dict[name] = (uuid, param)\n        self._uuid_dict[uuid] = param\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.Parameter","title":"<code>Parameter</code>","text":"<p>               Bases: <code>Symbol</code></p> <p>A wrapper on top of <code>sympy.Symbol</code>.</p> <p>Includes two additional keywords: <code>trainable</code> and <code>value</code>. This class is to define both feature parameter and variational parameters.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.trainable","title":"<code>trainable</code>  <code>instance-attribute</code>","text":"<p>Trainable parameters are variational parameters.</p> <p>Non-trainable parameters are feature parameters.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.value","title":"<code>value</code>  <code>instance-attribute</code>","text":"<p>(Initial) value of the parameter.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.__new__","title":"<code>__new__(name, **assumptions)</code>","text":"<p>Arguments:</p> <pre><code>name: When given a string only, the class\n    constructs a trainable Parameter with a a randomly initialized value.\n**assumptions: are passed on to the parent class `sympy.Symbol`. Two new assumption\n    kwargs are supported by this constructor: `trainable: bool`, and `value: TNumber`.\n</code></pre> <p>Example: <pre><code>from qadence.parameters import Parameter, VariationalParameter\n\ntheta = Parameter(\"theta\")\nprint(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\nassert not theta.is_number\n\n# you can specify both trainable/value in the constructor\ntheta = Parameter(\"theta\", trainable=True, value=2.0)\nprint(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n\n# VariationalParameter/FeatureParameter are constructing\n# trainable/untrainable Parameters\ntheta = VariationalParameter(\"theta\", value=2.0)\nassert theta == Parameter(\"theta\", trainable=True, value=2.0)\n\n# When provided with a numeric type, Parameter constructs a sympy numeric type\":\nconstant_zero = Parameter(0)\nassert constant_zero.is_number\n\n# When passed a Parameter or a sympy expression, it just returns it.\nexpr = Parameter(\"x\") * Parameter(\"y\")\nprint(f\"{expr=} : {expr.free_symbols}\")\n</code></pre> <pre><code>theta: trainable=True value=0.7871142068782487\ntheta: trainable=True value=2.0\nexpr=x*y : {y, x}\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def __new__(\n    cls, name: str | TNumber | Tensor | Basic | Parameter, **assumptions: Any\n) -&gt; Parameter | Basic | Expr | Array:\n    \"\"\"\n    Arguments:\n\n        name: When given a string only, the class\n            constructs a trainable Parameter with a a randomly initialized value.\n        **assumptions: are passed on to the parent class `sympy.Symbol`. Two new assumption\n            kwargs are supported by this constructor: `trainable: bool`, and `value: TNumber`.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.parameters import Parameter, VariationalParameter\n\n    theta = Parameter(\"theta\")\n    print(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n    assert not theta.is_number\n\n    # you can specify both trainable/value in the constructor\n    theta = Parameter(\"theta\", trainable=True, value=2.0)\n    print(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n\n    # VariationalParameter/FeatureParameter are constructing\n    # trainable/untrainable Parameters\n    theta = VariationalParameter(\"theta\", value=2.0)\n    assert theta == Parameter(\"theta\", trainable=True, value=2.0)\n\n    # When provided with a numeric type, Parameter constructs a sympy numeric type\":\n    constant_zero = Parameter(0)\n    assert constant_zero.is_number\n\n    # When passed a Parameter or a sympy expression, it just returns it.\n    expr = Parameter(\"x\") * Parameter(\"y\")\n    print(f\"{expr=} : {expr.free_symbols}\")\n    ```\n    \"\"\"\n    p: Parameter\n    if isinstance(name, get_args(TNumber)):\n        return sympify(name)\n    elif isinstance(name, Tensor):\n        if name.numel() == 1:\n            return sympify(name)\n        else:\n            return Array(name.detach().numpy())\n    elif isinstance(name, Parameter):\n        p = super().__new__(cls, name.name, **assumptions)\n        p.name = name.name\n        p.trainable = name.trainable\n        p.value = name.value\n        p.is_time = name.is_time\n        return p\n    elif isinstance(name, (Basic, Expr)):\n        if name.is_number:\n            return sympify(evaluate(name))\n        return name\n    elif isinstance(name, str):\n        p = super().__new__(cls, name, **assumptions)\n        p.trainable = assumptions.get(\"trainable\", True)\n        p.value = assumptions.get(\"value\", None)\n        p.is_time = assumptions.get(\"is_time\", False)\n        if p.value is None:\n            p.value = rand(1).item()\n        return p\n    else:\n        raise TypeError(f\"Parameter does not support type {type(name)}\")\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.FeatureParameter","title":"<code>FeatureParameter(name, **kwargs)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=False)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def FeatureParameter(name: str, **kwargs: Any) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=False)`.\"\"\"\n    return Parameter(name, trainable=False, **kwargs)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.TimeParameter","title":"<code>TimeParameter(name)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=False, is_time=True)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def TimeParameter(name: str) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=False, is_time=True)`.\"\"\"\n    return Parameter(name, trainable=False, is_time=True)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.VariationalParameter","title":"<code>VariationalParameter(name, **kwargs)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=True)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def VariationalParameter(name: str, **kwargs: Any) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=True)`.\"\"\"\n    return Parameter(name, trainable=True, **kwargs)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.evaluate","title":"<code>evaluate(expr, values=None, as_torch=False)</code>","text":"<p>Arguments:</p> <pre><code>expr: An expression consisting of Parameters.\nvalues: values dict which contains values for the Parameters,\n    if empty, Parameter.value will be used.\nas_torch: Whether to retrieve a torch-differentiable expression result.\n</code></pre> <p>Example: <pre><code>from qadence.parameters import Parameter, evaluate\n\nexpr = Parameter(\"x\") * Parameter(\"y\")\n\n# Unless specified, Parameter initialized random values\n# Lets evaluate this expression and see what the result is\nres = evaluate(expr)\nprint(res)\n\n# We can also evaluate the expr using a custom dict\nd = {\"x\": 1, \"y\":2}\nres = evaluate(expr, d)\nprint(res)\n\n# Lastly, if we want a differentiable result, lets put the as_torch flag\nres = evaluate(expr, d, as_torch=True)\nprint(res)\n</code></pre> <pre><code>0.19500236218486253\n2.0\ntensor([2])\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def evaluate(expr: Expr, values: dict | None = None, as_torch: bool = False) -&gt; TNumber | Tensor:\n    \"\"\"\n    Arguments:\n\n        expr: An expression consisting of Parameters.\n        values: values dict which contains values for the Parameters,\n            if empty, Parameter.value will be used.\n        as_torch: Whether to retrieve a torch-differentiable expression result.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.parameters import Parameter, evaluate\n\n    expr = Parameter(\"x\") * Parameter(\"y\")\n\n    # Unless specified, Parameter initialized random values\n    # Lets evaluate this expression and see what the result is\n    res = evaluate(expr)\n    print(res)\n\n    # We can also evaluate the expr using a custom dict\n    d = {\"x\": 1, \"y\":2}\n    res = evaluate(expr, d)\n    print(res)\n\n    # Lastly, if we want a differentiable result, lets put the as_torch flag\n    res = evaluate(expr, d, as_torch=True)\n    print(res)\n    ```\n    \"\"\"\n    res: Basic\n    res_value: TNumber | Tensor\n    query: dict[Parameter, TNumber | Tensor] = dict()\n    values = values or dict()\n    if isinstance(expr, Array):\n        return Tensor(expr.tolist())\n    else:\n        if not expr.is_number:\n            for s in expr.free_symbols:\n                if s.name in values.keys():\n                    query[s] = values[s.name]\n                elif hasattr(s, \"value\"):\n                    query[s] = s.value\n                else:\n                    raise ValueError(f\"No value provided for symbol {s.name}\")\n        if as_torch:\n            res_value = make_differentiable(expr)(**{s.name: tensor(v) for s, v in query.items()})\n        else:\n            res = expr.subs(query)\n            res_value = sympy_to_numeric(res)\n        return res_value\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.extract_original_param_entry","title":"<code>extract_original_param_entry(param)</code>","text":"<p>Given an Expression, what was the original \"param\" given by the user? It is either.</p> <p>going to be a numeric value, or a sympy Expression (in case a string was given, it was converted via Parameter(\"string\").</p> Source code in <code>qadence/parameters.py</code> <pre><code>def extract_original_param_entry(\n    param: Expr,\n) -&gt; TNumber | Tensor | Expr:\n    \"\"\"\n    Given an Expression, what was the original \"param\" given by the user? It is either.\n\n    going to be a numeric value, or a sympy Expression (in case a string was given,\n    it was converted via Parameter(\"string\").\n    \"\"\"\n    return param if not param.is_number else evaluate(param)\n</code></pre>"},{"location":"api/parameters/#parameter-embedding","title":"Parameter embedding","text":""},{"location":"api/parameters/#qadence.blocks.embedding.embedding","title":"<code>embedding(block, to_gate_params=False, engine=Engine.TORCH)</code>","text":"<p>Construct embedding function which maps user-facing parameters to either expression-level.</p> <p>parameters or gate-level parameters. The constructed embedding function has the signature:</p> <pre><code> embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n</code></pre> <p>which means that it maps the variational parameter dict <code>params</code> and the feature parameter dict <code>inputs</code> to one new parameter dict <code>embedded_dict</code> which holds all parameters that are needed to execute a circuit on a given backend. There are two different modes for this mapping:</p> <ul> <li>Expression-level parameters: For AD-based optimization. For every unique expression we end   up with one entry in the embedded dict:   <code>len(embedded_dict) == len(unique_parameter_expressions)</code>.</li> <li>Gate-level parameters: For PSR-based optimization or real devices. One parameter for each   gate parameter, regardless if they are based on the same expression. <code>len(embedded_dict) ==   len(parametric_gates)</code>. This is needed because PSR requires to shift the angles of every   gate where the same parameter appears.</li> </ul> PARAMETER DESCRIPTION <code>block</code> <p>parametrized block into which we want to embed parameters.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>to_gate_params</code> <p>A boolean flag whether to generate gate-level parameters or expression-level parameters.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[ParamDictType, Callable[[ParamDictType, ParamDictType], ParamDictType]]</code> <p>A tuple with variational parameter dict and the embedding function.</p> Source code in <code>qadence/blocks/embedding.py</code> <pre><code>def embedding(\n    block: AbstractBlock, to_gate_params: bool = False, engine: Engine = Engine.TORCH\n) -&gt; tuple[\n    ParamDictType,\n    Callable[[ParamDictType, ParamDictType], ParamDictType],\n]:\n    \"\"\"Construct embedding function which maps user-facing parameters to either *expression-level*.\n\n    parameters or *gate-level* parameters. The constructed embedding function has the signature:\n\n         embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n\n    which means that it maps the *variational* parameter dict `params` and the *feature* parameter\n    dict `inputs` to one new parameter dict `embedded_dict` which holds all parameters that are\n    needed to execute a circuit on a given backend. There are two different *modes* for this\n    mapping:\n\n    - *Expression-level* parameters: For AD-based optimization. For every unique expression we end\n      up with one entry in the embedded dict:\n      `len(embedded_dict) == len(unique_parameter_expressions)`.\n    - *Gate-level* parameters: For PSR-based optimization or real devices. One parameter for each\n      gate parameter, regardless if they are based on the same expression. `len(embedded_dict) ==\n      len(parametric_gates)`. This is needed because PSR requires to shift the angles of **every**\n      gate where the same parameter appears.\n\n    Arguments:\n        block: parametrized block into which we want to embed parameters.\n        to_gate_params: A boolean flag whether to generate gate-level parameters or\n            expression-level parameters.\n\n    Returns:\n        A tuple with variational parameter dict and the embedding function.\n    \"\"\"\n    concretize_parameter = _concretize_parameter(engine)\n    if engine == Engine.TORCH:\n        cast_dtype = tensor\n    else:\n        from jax.numpy import array\n\n        cast_dtype = array\n\n    unique_expressions = unique(expressions(block))\n    unique_symbols = [p for p in unique(parameters(block)) if not isinstance(p, sympy.Array)]\n    unique_const_matrices = [e for e in unique_expressions if isinstance(e, sympy.Array)]\n    unique_expressions = [e for e in unique_expressions if not isinstance(e, sympy.Array)]\n\n    # NOTE\n    # there are 3 kinds of parameters in qadence\n    # - non-trainable which are considered as inputs for classical data\n    # - trainable which are the variational parameters to be optimized\n    # - fixed: which are non-trainable parameters with fixed value (e.g. pi/2)\n    #\n    # both non-trainable and trainable parameters can have the same element applied\n    # to different operations in the quantum circuit, e.g. assigning the same parameter\n    # to multiple gates.\n    non_numeric_symbols = [p for p in unique_symbols if not p.is_number]\n    trainable_symbols = [p for p in non_numeric_symbols if p.trainable]\n    constant_expressions = [expr for expr in unique_expressions if expr.is_number]\n    # we dont need to care about constant symbols if they are contained in an symbolic expression\n    # we only care about gate params which are ONLY a constant\n\n    embeddings: dict[sympy.Expr, DifferentiableExpression] = {\n        expr: make_differentiable(expr=expr, engine=engine)\n        for expr in unique_expressions\n        if not expr.is_number\n    }\n\n    uuid_to_expr = uuid_to_expression(block)\n\n    def embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n        embedded_params: dict[sympy.Expr, ArrayLike] = {}\n        for expr, fn in embeddings.items():\n            angle: ArrayLike\n            values = {}\n            for symbol in expr.free_symbols:\n                if symbol.name in inputs:\n                    value = inputs[symbol.name]\n                elif symbol.name in params:\n                    value = params[symbol.name]\n                else:\n                    if symbol.is_time:\n                        value = tensor(1.0)\n                    else:\n                        msg_trainable = \"Trainable\" if symbol.trainable else \"Non-trainable\"\n                        raise KeyError(\n                            f\"{msg_trainable} parameter '{symbol.name}' not found in the \"\n                            f\"inputs list: {list(inputs.keys())} nor the \"\n                            f\"params list: {list(params.keys())}.\"\n                        )\n                values[symbol.name] = value\n            angle = fn(**values)\n            # do not reshape parameters which are multi-dimensional\n            # tensors, such as for example generator matrices\n            if not len(angle.squeeze().shape) &gt; 1:\n                angle = angle.reshape(-1)\n            embedded_params[expr] = angle\n\n        for e in constant_expressions + unique_const_matrices:\n            embedded_params[e] = params[stringify(e)]\n\n        if to_gate_params:\n            gate_lvl_params: ParamDictType = {}\n            for uuid, e in uuid_to_expr.items():\n                gate_lvl_params[uuid] = embedded_params[e]\n            return gate_lvl_params\n        else:\n            embedded_params.update(inputs)\n            for k, v in params.items():\n                if k not in embedded_params:\n                    embedded_params[k] = v\n            out = {\n                stringify(k) if not isinstance(k, str) else k: (\n                    as_tensor(v)[None] if as_tensor(v).ndim == 0 else v\n                )\n                for k, v in embedded_params.items()\n            }\n            return out\n\n    params: ParamDictType\n    params = {\n        p.name: concretize_parameter(value=p.value, trainable=True) for p in trainable_symbols\n    }\n    params.update(\n        {\n            stringify(expr): concretize_parameter(value=evaluate(expr), trainable=False)\n            for expr in constant_expressions\n        }\n    )\n    params.update(\n        {\n            stringify(expr): cast_dtype(nparray(expr.tolist(), dtype=npcdouble))\n            for expr in unique_const_matrices\n        }\n    )\n    return params, embedding_fn\n</code></pre>"},{"location":"api/pasqal_cloud_connection/","title":"Pasqal Cloud Connection","text":""},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.WorkloadNotDoneError","title":"<code>WorkloadNotDoneError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Is raised if a workload is not yet finished running on remote.</p>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.WorkloadSpec","title":"<code>WorkloadSpec(circuit, backend, result_types, parameter_values=None, observable=None)</code>  <code>dataclass</code>","text":"<p>Specification of a workload to be executed on Pasqal Cloud.</p> <p>This data class defines a single workload specification that is to be executed on Pasqal's cloud platform.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The quantum circuit to be executed.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>backend</code> <p>The backend to execute the workload on. Not all backends are available on the cloud platform. Currently the supported backend is <code>BackendName.PYQTORCH</code>.</p> <p> TYPE: <code>BackendName | str</code> </p> <code>result_types</code> <p>The types of result to compute for this workload. The circuit will be run for all result types specified here one by one.</p> <p> TYPE: <code>list[ResultType]</code> </p> <code>parameter_values</code> <p>If the quantum circuit has feature parameters, values for those need to be provided. In the case there are only variational parameters, this field is optional. In the case there are no parameters, this field needs to be <code>None</code>. The parameter values can be either a tensor of dimension 0 or 1, which can differ per parameter. For parameters that are an array, i.e. dimension 1, all array lengths should be equal.</p> <p> TYPE: <code>dict[str, Tensor] | None</code> DEFAULT: <code>None</code> </p> <code>observable</code> <p>Observable that is used when <code>result_types</code> contains <code>ResultType.EXPECTATION</code>. The observable field is mandatory in this case. If not, the value of this field will be ignored. Only a single observable can be passed for cloud submission; providing a list of observables is not supported.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.WorkloadStoppedError","title":"<code>WorkloadStoppedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Is raised when a workload has stopped running on remote for some reason.</p>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection._workload_spec_to_json","title":"<code>_workload_spec_to_json(workload)</code>","text":"<p>Serializes a <code>WorkloadSpec</code> into JSON format.</p> PARAMETER DESCRIPTION <code>workload</code> <p>A <code>WorkloadSpec</code> object, defining the specification of the workload that needs to be uploaded.</p> <p> TYPE: <code>WorkloadSpec</code> </p> RETURNS DESCRIPTION <code>WorkloadSpecJSON</code> <p>Workload specification in JSON format.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def _workload_spec_to_json(workload: WorkloadSpec) -&gt; WorkloadSpecJSON:\n    \"\"\"Serializes a `WorkloadSpec` into JSON format.\n\n    Args:\n        workload: A `WorkloadSpec` object, defining the specification of the workload that needs to\n            be uploaded.\n\n    Returns:\n        Workload specification in JSON format.\n    \"\"\"\n    circuit_json = json.dumps(serialize(workload.circuit))\n    result_types_json = [item.value for item in workload.result_types]\n    config = {\n        \"circuit\": circuit_json,\n        \"result_types\": result_types_json,\n        \"c_values\": _parameter_values_to_json(workload.parameter_values),\n    }\n\n    if workload.observable is not None:\n        config[\"observable\"] = json.dumps(serialize(workload.observable))\n\n    return WorkloadSpecJSON(str(workload.backend), config)\n</code></pre>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.check_status","title":"<code>check_status(connection, workload_id)</code>","text":"<p>Checks if the workload is successfully finished on remote connection.</p> PARAMETER DESCRIPTION <code>connection</code> <p>A <code>pasqal_cloud.SDK</code> instance which is used to connect to the cloud.</p> <p> TYPE: <code>SDK</code> </p> <code>workload_id</code> <p>the id <code>str</code> that is associated with the workload.</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>WorkloadNotDoneError</code> <p>Is raised when the workload status is \"PENDING\", \"RUNNING\" or \"PAUSED\".</p> <code>WorkloadStoppedError</code> <p>Is raise when the workload status is \"CANCELED\", \"TIMED_OUT\" or \"ERROR\".</p> <code>ValueError</code> <p>Is raised when the workload status has an unsupported value.</p> RETURNS DESCRIPTION <code>Workload</code> <p>The workload result if its status is \"DONE\" as a <code>pasqal_cloud.Workload</code> object.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def check_status(connection: SDK, workload_id: str) -&gt; WorkloadResult:\n    \"\"\"Checks if the workload is successfully finished on remote connection.\n\n    Args:\n        connection: A `pasqal_cloud.SDK` instance which is used to connect to the cloud.\n        workload_id: the id `str` that is associated with the workload.\n\n    Raises:\n        WorkloadNotDoneError: Is raised when the workload status is \"PENDING\", \"RUNNING\" or\n            \"PAUSED\".\n        WorkloadStoppedError: Is raise when the workload status is \"CANCELED\", \"TIMED_OUT\" or\n            \"ERROR\".\n        ValueError: Is raised when the workload status has an unsupported value.\n\n    Returns:\n        The workload result if its status is \"DONE\" as a `pasqal_cloud.Workload` object.\n    \"\"\"\n    # TODO Make the function return a \"nice\" result object\n    result = connection.get_workload(workload_id)\n    if result.status == \"DONE\":\n        return result\n    if result.status in (\"PENDING\", \"RUNNING\", \"PAUSED\"):\n        raise WorkloadNotDoneError(\n            f\"Workload with id {workload_id} is not yet finished, the status is {result.status}\"\n        )\n    if result.status in (\"CANCELED\", \"TIMED_OUT\", \"ERROR\"):\n        message = f\"Workload with id {workload_id} couldn't finish, the status is {result.status}.\"\n        if result.status == \"ERROR\":\n            message += f\"The following error(s) occurred {result.errors}\"\n        raise WorkloadStoppedError(message)\n    raise ValueError(\n        f\"Undefined workload status ({result.status}) was returned for workload ({result.id})\"\n    )\n</code></pre>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.get_result","title":"<code>get_result(connection, workload_id, timeout=60.0, refresh_time=1.0)</code>","text":"<p>Repeatedly checks if a workload has finished and returns the result.</p> PARAMETER DESCRIPTION <code>connection</code> <p>A <code>pasqal_cloud.SDK</code> instance which is used to connect to the cloud.</p> <p> TYPE: <code>SDK</code> </p> <code>workload_id</code> <p>the id <code>str</code> that is associated with the workload.</p> <p> TYPE: <code>str</code> </p> <code>timeout</code> <p>Time in seconds after which the function times out. Defaults to 60.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>60.0</code> </p> <code>refresh_time</code> <p>Time in seconds after which the remote is requested to update the status again, when the workload is not finished yet. Defaults to 1.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RAISES DESCRIPTION <code>TimeoutError</code> <p>description</p> RETURNS DESCRIPTION <code>Workload</code> <p>The workload result if its status is \"DONE\" as a <code>pasqal_cloud.Workload</code> object.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def get_result(\n    connection: SDK, workload_id: str, timeout: float = 60.0, refresh_time: float = 1.0\n) -&gt; WorkloadResult:\n    \"\"\"Repeatedly checks if a workload has finished and returns the result.\n\n    Args:\n        connection: A `pasqal_cloud.SDK` instance which is used to connect to the cloud.\n        workload_id: the id `str` that is associated with the workload.\n        timeout: Time in seconds after which the function times out. Defaults to 60.0.\n        refresh_time: Time in seconds after which the remote is requested to update the status\n            again, when the workload is not finished yet. Defaults to 1.0.\n\n    Raises:\n        TimeoutError: _description_\n\n    Returns:\n        The workload result if its status is \"DONE\" as a `pasqal_cloud.Workload` object.\n    \"\"\"\n    max_refresh_count = int(timeout // refresh_time)\n    for _ in range(max_refresh_count):\n        try:\n            result = check_status(connection, workload_id)\n        except WorkloadNotDoneError:\n            time.sleep(refresh_time)\n            continue\n        return result\n    raise TimeoutError(\"Request timed out because it wasn't finished in the specified time. \")\n</code></pre>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.get_workload_spec","title":"<code>get_workload_spec(model, result_types, parameter_values=None, observable=None)</code>","text":"<p>Creates a <code>WorkloadSpec</code> from a quantum model.</p> <p>This function creates a <code>WorkloadSpec</code> from a <code>QuantumModel</code> and the other arguments provided. The circuit, that is extracted from the model, is the original circuit that was used to initialize the model, not the backend converted circuit in <code>model.circuit</code>. The backend set in the model will be used in the workload specification.</p> <p>It is important to note that in case there is an observable defined in the model, it is ignored in the workload specification. To provide an observable to the workload specification, it is only possible to set it in the observable argument of this function.</p> PARAMETER DESCRIPTION <code>model</code> <p>The quantum model that defines the circuit and backend for the workload spec.</p> <p> TYPE: <code>QuantumModel</code> </p> <code>result_types</code> <p>A list of result types that is requested in this workload.</p> <p> TYPE: <code>list[ResultType]</code> </p> <code>parameter_values</code> <p>The parameter values that should be used during execution of the workload.</p> <p> TYPE: <code>dict[str, Tensor] | None</code> DEFAULT: <code>None</code> </p> <code>observable</code> <p>Observable that is used when <code>result_types</code> contains <code>ResultType.EXPECTATION</code>. The observable field is mandatory in this case. If not, the value of this field will be ignored. Only a single observable can be passed for cloud submission; providing a list of observables is not supported.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>WorkloadSpec</code> <p>A <code>WorkloadSpec</code> instance based on the quantum model passed to this function.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def get_workload_spec(\n    model: QuantumModel,\n    result_types: list[ResultType],\n    parameter_values: dict[str, Tensor] | None = None,\n    observable: AbstractBlock | None = None,\n) -&gt; WorkloadSpec:\n    \"\"\"Creates a `WorkloadSpec` from a quantum model.\n\n    This function creates a `WorkloadSpec` from a `QuantumModel` and the other arguments provided.\n    The circuit, that is extracted from the model, is the original circuit that was used to\n    initialize the model, not the backend converted circuit in `model.circuit`. The backend set in\n    the model will be used in the workload specification.\n\n    It is important to note that in case there is an observable defined in the model, it is ignored\n    in the workload specification. To provide an observable to the workload specification, it is\n    only possible to set it in the observable argument of this function.\n\n    Args:\n        model: The quantum model that defines the circuit and backend for the workload spec.\n        result_types: A list of result types that is requested in this workload.\n        parameter_values: The parameter values that should be used during execution of the\n            workload.\n        observable: Observable that is used when `result_types` contains `ResultType.EXPECTATION`.\n            The observable field is mandatory in this case. If not, the value of this field will\n            be ignored. Only a single observable can be passed for cloud submission; providing a\n            list of observables is not supported.\n\n    Returns:\n        A `WorkloadSpec` instance based on the quantum model passed to this function.\n    \"\"\"\n    circuit = model._circuit.original\n    backend = model._backend_name\n    return WorkloadSpec(circuit, backend, result_types, parameter_values, observable)\n</code></pre>"},{"location":"api/pasqal_cloud_connection/#qadence.pasqal_cloud_connection.submit_workload","title":"<code>submit_workload(connection, workload)</code>","text":"<p>Uploads a workload to Pasqal's Cloud and returns the created workload ID.</p> PARAMETER DESCRIPTION <code>connection</code> <p>A <code>pasqal_cloud.SDK</code> instance which is used to connect to the cloud.</p> <p> TYPE: <code>SDK</code> </p> <code>workload</code> <p>A <code>WorkloadSpec</code> object, defining the specification of the workload that needs to be uploaded.</p> <p> TYPE: <code>WorkloadSpec</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A workload id as a <code>str</code>.</p> Source code in <code>qadence/pasqal_cloud_connection.py</code> <pre><code>def submit_workload(connection: SDK, workload: WorkloadSpec) -&gt; str:\n    \"\"\"Uploads a workload to Pasqal's Cloud and returns the created workload ID.\n\n    Args:\n        connection: A `pasqal_cloud.SDK` instance which is used to connect to the cloud.\n        workload: A `WorkloadSpec` object, defining the specification of the workload that needs to\n            be uploaded.\n\n    Returns:\n        A workload id as a `str`.\n    \"\"\"\n    workload_json = _workload_spec_to_json(workload)\n    remote_workload = connection.create_workload(\n        workload_json.workload_type, workload_json.backend_type, workload_json.config\n    )\n    workload_id: str = remote_workload.id\n    return workload_id\n</code></pre>"},{"location":"api/quantumcircuit/","title":"QuantumCircuit","text":""},{"location":"api/quantumcircuit/#quantumcircuit","title":"QuantumCircuit","text":"<p>The abstract <code>QuantumCircuit</code> is the key object in Qadence, as it is what can be executed.</p>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit","title":"<code>QuantumCircuit(support, *blocks)</code>  <code>dataclass</code>","text":"<p>Am abstract QuantumCircuit instance.</p> <p>It needs to be passed to a quantum backend for execution.</p> <p>Arguments:</p> <pre><code>support: `Register` or number of qubits. If an integer is provided, a register is\n    constructed with `Register.all_to_all(x)`\n*blocks: (Possibly multiple) blocks to construct the circuit from.\n</code></pre> Source code in <code>qadence/circuit.py</code> <pre><code>def __init__(self, support: int | Register, *blocks: AbstractBlock):\n    \"\"\"\n    Arguments:\n\n        support: `Register` or number of qubits. If an integer is provided, a register is\n            constructed with `Register.all_to_all(x)`\n        *blocks: (Possibly multiple) blocks to construct the circuit from.\n    \"\"\"\n    self.block = chain(*blocks) if len(blocks) != 1 else blocks[0]\n    self.register = Register(support) if isinstance(support, int) else support\n\n    global_block = isinstance(self.block, AnalogBlock) and self.block.qubit_support.is_global\n    if not global_block and len(self.block) and self.block.n_qubits &gt; self.register.n_qubits:\n        raise ValueError(\n            f\"Register with {self.register.n_qubits} qubits is too small for the \"\n            f\"given block with {self.block.n_qubits} qubits\"\n        )\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.unique_parameters","title":"<code>unique_parameters</code>  <code>property</code>","text":"<p>Return the unique parameters in the circuit.</p> <p>These parameters are the actual user-facing parameters which can be assigned by the user. Multiple gates can contain the same unique parameter</p> RETURNS DESCRIPTION <code>list[Parameter]</code> <p>list[Parameter]: List of unique parameters in the circuit</p>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.dagger","title":"<code>dagger()</code>","text":"<p>Reverse the QuantumCircuit by calling dagger on the block.</p> Source code in <code>qadence/circuit.py</code> <pre><code>def dagger(self) -&gt; QuantumCircuit:\n    \"\"\"Reverse the QuantumCircuit by calling dagger on the block.\"\"\"\n    return QuantumCircuit(self.n_qubits, self.block.dagger())\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.get_blocks_by_tag","title":"<code>get_blocks_by_tag(tag)</code>","text":"<p>Extract one or more blocks using the human-readable tag.</p> <p>This function recursively explores all composite blocks to find all the occurrences of a certain tag in the blocks.</p> PARAMETER DESCRIPTION <code>tag</code> <p>the tag to look for</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>list[AbstractBlock]: The block(s) corresponding to the given tag</p> Source code in <code>qadence/circuit.py</code> <pre><code>def get_blocks_by_tag(self, tag: str) -&gt; list[AbstractBlock]:\n    \"\"\"Extract one or more blocks using the human-readable tag.\n\n    This function recursively explores all composite blocks to find\n    all the occurrences of a certain tag in the blocks.\n\n    Args:\n        tag (str): the tag to look for\n\n    Returns:\n        list[AbstractBlock]: The block(s) corresponding to the given tag\n    \"\"\"\n\n    def _get_block(block: AbstractBlock) -&gt; list[AbstractBlock]:\n        blocks = []\n        if block.tag == tag:\n            blocks += [block]\n        if isinstance(block, CompositeBlock):\n            blocks += flatten(*[_get_block(b) for b in block.blocks])\n        return blocks\n\n    return _get_block(self.block)\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.parameters","title":"<code>parameters()</code>","text":"<p>Extract all parameters for primitive blocks in the circuit.</p> <p>Notice that this function returns all the unique Parameters used in the quantum circuit. These can correspond to constants too.</p> RETURNS DESCRIPTION <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>List[tuple[Parameter]]: A list of tuples containing the Parameter</p> <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>instance of each of the primitive blocks in the circuit or, if the <code>flatten</code></p> <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>flag is set to True, a flattened list of all circuit parameters</p> Source code in <code>qadence/circuit.py</code> <pre><code>def parameters(self) -&gt; list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]:\n    \"\"\"Extract all parameters for primitive blocks in the circuit.\n\n    Notice that this function returns all the unique Parameters used\n    in the quantum circuit. These can correspond to constants too.\n\n    Returns:\n        List[tuple[Parameter]]: A list of tuples containing the Parameter\n        instance of each of the primitive blocks in the circuit or, if the `flatten`\n        flag is set to True, a flattened list of all circuit parameters\n    \"\"\"\n    return parameters(self.block)\n</code></pre>"},{"location":"api/register/","title":"Register","text":""},{"location":"api/register/#quantum-registers","title":"Quantum Registers","text":""},{"location":"api/register/#qadence.register.Register","title":"<code>Register(support, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>","text":"<p>A register of qubits including 2D coordinates.</p> <p>Instantiating the Register class directly is only recommended for building custom registers. For most uses where a predefined lattice is desired it is recommended to use the various class methods available, e.g. <code>Register.triangular_lattice</code>.</p> PARAMETER DESCRIPTION <code>support</code> <p>A NetworkX graph or number of qubits. Nodes can include a <code>\"pos\"</code> attribute such that e.g.: <code>graph.nodes = {0: {\"pos\": (2,3)}, 1: {\"pos\": (0,0)}, ...}</code> which will be used in backends that need qubit coordinates. Passing a number of qubits calls <code>Register.all_to_all(n_qubits)</code>.</p> <p> TYPE: <code>Graph | int</code> </p> <code>spacing</code> <p>Value set as the distance between the two closest qubits. The spacing argument is also available for all the class method constructors.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>1.0</code> </p> <p>Examples: <pre><code>from qadence import Register\n\nreg_all = Register.all_to_all(n_qubits = 4)\nreg_line = Register.line(n_qubits = 4)\nreg_circle = Register.circle(n_qubits = 4)\nreg_squre = Register.square(qubits_side = 2)\nreg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\nreg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\nreg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n</code></pre> </p> Source code in <code>qadence/register.py</code> <pre><code>def __init__(\n    self,\n    support: nx.Graph | int,\n    spacing: float | None = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n):\n    \"\"\"\n    A register of qubits including 2D coordinates.\n\n    Instantiating the Register class directly is only recommended for building custom registers.\n    For most uses where a predefined lattice is desired it is recommended to use the various\n    class methods available, e.g. `Register.triangular_lattice`.\n\n    Arguments:\n        support: A NetworkX graph or number of qubits. Nodes can include a `\"pos\"` attribute\n            such that e.g.: `graph.nodes = {0: {\"pos\": (2,3)}, 1: {\"pos\": (0,0)}, ...}` which\n            will be used in backends that need qubit coordinates. Passing a number of qubits\n            calls `Register.all_to_all(n_qubits)`.\n        spacing: Value set as the distance between the two closest qubits. The spacing\n            argument is also available for all the class method constructors.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\"\n    from qadence import Register\n\n    reg_all = Register.all_to_all(n_qubits = 4)\n    reg_line = Register.line(n_qubits = 4)\n    reg_circle = Register.circle(n_qubits = 4)\n    reg_squre = Register.square(qubits_side = 2)\n    reg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\n    reg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\n    reg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n    ```\n    \"\"\"\n    if device_specs is not None and not isinstance(device_specs, RydbergDevice):\n        raise ValueError(\"Device specs are not valid. Please pass a `RydbergDevice` instance.\")\n\n    self.device_specs = device_specs\n\n    self.graph = support if isinstance(support, nx.Graph) else alltoall_graph(support)\n\n    if spacing is not None and self.min_distance != 0.0:\n        _scale_node_positions(self.graph, self.min_distance, spacing)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.all_node_pairs","title":"<code>all_node_pairs</code>  <code>property</code>","text":"<p>Return a list of all possible qubit pairs in the register.</p>"},{"location":"api/register/#qadence.register.Register.coords","title":"<code>coords</code>  <code>property</code>","text":"<p>Return the dictionary of qubit coordinates.</p>"},{"location":"api/register/#qadence.register.Register.distances","title":"<code>distances</code>  <code>property</code>","text":"<p>Return a dictionary of distances for all qubit pairs in the register.</p>"},{"location":"api/register/#qadence.register.Register.edge_distances","title":"<code>edge_distances</code>  <code>property</code>","text":"<p>Return a dictionary of distances for the qubit pairs that are.</p> <p>connected by an edge in the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.edges","title":"<code>edges</code>  <code>property</code>","text":"<p>Return the EdgeView of the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.min_distance","title":"<code>min_distance</code>  <code>property</code>","text":"<p>Return the minimum distance between two qubts in the register.</p>"},{"location":"api/register/#qadence.register.Register.n_qubits","title":"<code>n_qubits</code>  <code>property</code>","text":"<p>Total number of qubits in the register.</p>"},{"location":"api/register/#qadence.register.Register.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Return the NodeView of the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.support","title":"<code>support</code>  <code>property</code>","text":"<p>Return the set of qubits in the register.</p>"},{"location":"api/register/#qadence.register.Register.all_to_all","title":"<code>all_to_all(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a register with an all-to-all connectivity graph.</p> <p>The graph is projected onto a 2D space and the qubit coordinates are set using a spring layout algorithm.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef all_to_all(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a register with an all-to-all connectivity graph.\n\n    The graph is projected\n    onto a 2D space and the qubit coordinates are set using a spring layout algorithm.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    return cls(alltoall_graph(n_qubits), spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.circle","title":"<code>circle(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a circle register.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef circle(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a circle register.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    graph = nx.grid_2d_graph(n_qubits, 1, periodic=True)\n    graph = nx.relabel_nodes(graph, {(i, 0): i for i in range(n_qubits)})\n    coords = nx.circular_layout(graph)\n    values = {i: {\"pos\": pos} for i, pos in coords.items()}\n    nx.set_node_attributes(graph, values)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.draw","title":"<code>draw(show=True)</code>","text":"<p>Draw the underlying NetworkX graph representing the register.</p> Source code in <code>qadence/register.py</code> <pre><code>def draw(self, show: bool = True) -&gt; None:\n    \"\"\"Draw the underlying NetworkX graph representing the register.\"\"\"\n    coords = {i: n[\"pos\"] for i, n in self.graph.nodes.items()}\n    nx.draw(self.graph, with_labels=True, pos=coords)\n    if show:\n        plt.gcf().show()\n</code></pre>"},{"location":"api/register/#qadence.register.Register.from_coordinates","title":"<code>from_coordinates(coords, lattice=LatticeTopology.ARBITRARY, spacing=None, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a register from a list of qubit coordinates.</p> <p>Each node is added to the underlying graph with the respective coordinates, but the edges are left empty.</p> PARAMETER DESCRIPTION <code>coords</code> <p>List of qubit coordinate tuples.</p> <p> TYPE: <code>list[tuple]</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef from_coordinates(\n    cls,\n    coords: list[tuple],\n    lattice: LatticeTopology | str = LatticeTopology.ARBITRARY,\n    spacing: float | None = None,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a register from a list of qubit coordinates.\n\n    Each node is added to the underlying\n    graph with the respective coordinates, but the edges are left empty.\n\n    Arguments:\n        coords: List of qubit coordinate tuples.\n    \"\"\"\n    graph = nx.Graph()\n    for i, pos in enumerate(coords):\n        graph.add_node(i, pos=pos)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.honeycomb_lattice","title":"<code>honeycomb_lattice(n_cells_row, n_cells_col, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a honeycomb lattice register.</p> <p>Each cell is an hexagon made up of six qubits.</p> PARAMETER DESCRIPTION <code>n_cells_row</code> <p>Number of cells in each row.</p> <p> TYPE: <code>int</code> </p> <code>n_cells_col</code> <p>Number of cells in each column.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef honeycomb_lattice(\n    cls,\n    n_cells_row: int,\n    n_cells_col: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a honeycomb lattice register.\n\n    Each cell is an hexagon made up of six qubits.\n\n    Arguments:\n        n_cells_row: Number of cells in each row.\n        n_cells_col: Number of cells in each column.\n    \"\"\"\n    graph = nx.hexagonal_lattice_graph(n_cells_row, n_cells_col)\n    graph = nx.relabel_nodes(graph, {(i, j): k for k, (i, j) in enumerate(graph.nodes)})\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.line","title":"<code>line(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a line register.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef line(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a line register.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    return cls(line_graph(n_qubits), spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.rescale_coords","title":"<code>rescale_coords(scaling)</code>","text":"<p>Rescale the coordinates of all qubits in the register.</p> PARAMETER DESCRIPTION <code>scaling</code> <p>Scaling value.</p> <p> TYPE: <code>float</code> </p> Source code in <code>qadence/register.py</code> <pre><code>def rescale_coords(self, scaling: float) -&gt; Register:\n    \"\"\"\n    Rescale the coordinates of all qubits in the register.\n\n    Arguments:\n        scaling: Scaling value.\n    \"\"\"\n    g = deepcopy(self.graph)\n    _scale_node_positions(g, min_distance=1.0, spacing=scaling)\n    return Register(g, spacing=None, device_specs=self.device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.square","title":"<code>square(qubits_side, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a square register.</p> PARAMETER DESCRIPTION <code>qubits_side</code> <p>Number of qubits on one side of the square.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef square(\n    cls,\n    qubits_side: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a square register.\n\n    Arguments:\n        qubits_side: Number of qubits on one side of the square.\n    \"\"\"\n    n_points = 4 * (qubits_side - 1)\n\n    def gen_points() -&gt; np.ndarray:\n        rotate_left = np.array([[0.0, -1.0], [1.0, 0.0]])\n        increment = np.array([0.0, 1.0])\n\n        points = [np.array([0.0, 0.0])]\n        counter = 1\n        while len(points) &lt; n_points:\n            points.append(points[-1] + increment)\n\n            counter = (counter + 1) % qubits_side\n            if counter == 0:\n                increment = rotate_left.dot(increment)\n                counter = 1\n        points = np.array(points)  # type: ignore[assignment]\n        points -= np.mean(points, axis=0)\n\n        return points  # type: ignore[return-value]\n\n    graph = nx.grid_2d_graph(n_points, 1, periodic=True)\n    graph = nx.relabel_nodes(graph, {(i, 0): i for i in range(n_points)})\n    values = {i: {\"pos\": point} for i, point in zip(graph.nodes, gen_points())}\n    nx.set_node_attributes(graph, values)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.triangular_lattice","title":"<code>triangular_lattice(n_cells_row, n_cells_col, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a triangular lattice register.</p> <p>Each cell is a triangle made up of three qubits.</p> PARAMETER DESCRIPTION <code>n_cells_row</code> <p>Number of cells in each row.</p> <p> TYPE: <code>int</code> </p> <code>n_cells_col</code> <p>Number of cells in each column.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef triangular_lattice(\n    cls,\n    n_cells_row: int,\n    n_cells_col: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a triangular lattice register.\n\n    Each cell is a triangle made up of three qubits.\n\n    Arguments:\n        n_cells_row: Number of cells in each row.\n        n_cells_col: Number of cells in each column.\n    \"\"\"\n    return cls(triangular_lattice_graph(n_cells_row, n_cells_col), spacing, device_specs)\n</code></pre>"},{"location":"api/serialization/","title":"Serialization","text":""},{"location":"api/serialization/#serialization","title":"Serialization","text":""},{"location":"api/serialization/#qadence.serialization.SerializationModel","title":"<code>SerializationModel(d=dict())</code>  <code>dataclass</code>","text":"<p>A serialization model class to serialize data from <code>QuantumModel</code>s,.</p> <p><code>torch.nn.Module</code> and similar structures. The data included in the serialization logic includes: the <code>AbstractBlock</code> and its children classes, <code>QuantumCircuit</code>, <code>Register</code>, and <code>sympy</code> expressions (including <code>Parameter</code> class from <code>qadence.parameters</code>).</p> <p>A children class must define the <code>value</code> attribute type and how to handle it, since it is the main property for the class to be used by the serialization process. For instance:</p> <pre><code>@dataclass\nclass QuantumCircuitSerialization(SerializationModel):\n    value: QuantumCircuit = dataclass_field(init=False)\n\n    def __post_init__(self) -&gt; None:\n        self.value = (\n            QuantumCircuit._from_dict(self.d)\n            if isinstance(self.d, dict)\n            else self.d\n        )\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.deserialize","title":"<code>deserialize(d, as_torch=False)</code>","text":"<p>Supported Types:</p> <p>AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module Deserializes a dict to one of the supported types.</p> PARAMETER DESCRIPTION <code>d</code> <p>A dict containing a serialized object.</p> <p> TYPE: <code>dict</code> </p> <code>as_torch</code> <p>Whether to transform to torch for the deserialized object.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     AbstractBlock, QuantumCircuit, QuantumModel, Register, torch.nn.Module.</p> <p>Examples: <pre><code>import torch\nfrom qadence import serialize, deserialize, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nblock_dict = serialize(myblock)\nprint(block_dict)\n\n## Lets use myblock in a QuantumCircuit and serialize it.\n\nqc = QuantumCircuit(n_qubits, myblock)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n## Finally, let's wrap it in a QuantumModel\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n# Lets check if the loaded QuantumModel returns the same expectation\nassert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n</code></pre> <pre><code>{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': 'HEA', 'blocks': [{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('cb56c0b3-e031-48f8-8eea-fa5520991f06', {'name': 'theta_0', 'expression': \"Parameter('theta_0')\", 'symbols': {'theta_0': {'name': 'theta_0', 'trainable': 'True', 'value': '0.7678106685034964'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('35c711c9-07e6-40c4-95ed-cba3a9171acf', {'name': 'theta_1', 'expression': \"Parameter('theta_1')\", 'symbols': {'theta_1': {'name': 'theta_1', 'trainable': 'True', 'value': '0.3566015853800468'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RY', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('164bc8d2-8fd9-4f63-952c-51ceac48f915', {'name': 'theta_2', 'expression': \"Parameter('theta_2')\", 'symbols': {'theta_2': {'name': 'theta_2', 'trainable': 'True', 'value': '0.8984062223169174'}}})}}, 'noise': None}, {'type': 'RY', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('e991d67f-1a1a-4a70-8b49-63ed5e971f5a', {'name': 'theta_3', 'expression': \"Parameter('theta_3')\", 'symbols': {'theta_3': {'name': 'theta_3', 'trainable': 'True', 'value': '0.9685041769662228'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('9039d99e-e15c-4108-aed4-ecc0c9dceb5a', {'name': 'theta_4', 'expression': \"Parameter('theta_4')\", 'symbols': {'theta_4': {'name': 'theta_4', 'trainable': 'True', 'value': '0.21431111346590004'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('5791a89f-00ef-4f6a-a0ab-6b1ff045e414', {'name': 'theta_5', 'expression': \"Parameter('theta_5')\", 'symbols': {'theta_5': {'name': 'theta_5', 'trainable': 'True', 'value': '0.0023027718890514803'}}})}}, 'noise': None}]}]}, {'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'CNOT', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'X', 'qubit_support': (1,), 'tag': None, 'noise': None}], 'noise': None}]}]}]}\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def deserialize(d: dict, as_torch: bool = False) -&gt; SUPPORTED_TYPES:\n    \"\"\"\n    Supported Types:\n\n    AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module\n    Deserializes a dict to one of the supported types.\n\n    Arguments:\n        d (dict): A dict containing a serialized object.\n        as_torch (bool): Whether to transform to torch for the deserialized object.\n    Returns:\n        AbstractBlock, QuantumCircuit, QuantumModel, Register, torch.nn.Module.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence import serialize, deserialize, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    block_dict = serialize(myblock)\n    print(block_dict)\n\n    ## Lets use myblock in a QuantumCircuit and serialize it.\n\n    qc = QuantumCircuit(n_qubits, myblock)\n    qc_dict = serialize(qc)\n    qc_deserialized = deserialize(qc_dict)\n    assert qc == qc_deserialized\n\n    ## Finally, let's wrap it in a QuantumModel\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\n    qm_dict = serialize(qm)\n    qm_deserialized = deserialize(qm_dict)\n    # Lets check if the loaded QuantumModel returns the same expectation\n    assert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n    ```\n    \"\"\"\n    obj: SerializationModel\n    if d.get(\"expression\"):\n        obj = ExpressionSerialization(d)\n    elif d.get(\"block\") and d.get(\"register\"):\n        obj = QuantumCircuitSerialization(d)\n    elif d.get(\"graph\"):\n        obj = RegisterSerialization(d)\n    elif d.get(\"type\"):\n        obj = BlockTypeSerialization(d)\n    else:\n        obj = ModelSerialization(d, as_torch=as_torch)\n    return obj.value\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.load","title":"<code>load(file_path, map_location='cpu')</code>","text":"<p>Same as serialize/deserialize but for storing/loading files.</p> <p>Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register Loads a .json or .pt file to one of the supported types.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>The name of the file.</p> <p> TYPE: <code>str</code> </p> <code>map_location</code> <p>In case of a .pt file, on which device to load the object (cpu,cuda).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cpu'</code> </p> <p>Returns:     A object of type AbstractBlock, QuantumCircuit, QuantumModel, Register.</p> <p>Examples: <pre><code>import torch\nfrom pathlib import Path\nimport os\n\nfrom qadence import save, load, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nqc = QuantumCircuit(n_qubits, myblock)\n# Lets store the circuit in a json file\nsave(qc, '.', 'circ')\nloaded_qc = load(Path('circ.json'))\nqc == loaded_qc\nos.remove('circ.json')\n## Let's wrap it in a QuantumModel and store that\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\nsave(qm, folder= '.',file_name= 'quantum_model')\nqm_loaded = load('quantum_model.json')\nos.remove('quantum_model.json')\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def load(file_path: str | Path, map_location: str = \"cpu\") -&gt; SUPPORTED_TYPES:\n    \"\"\"\n    Same as serialize/deserialize but for storing/loading files.\n\n    Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register\n    Loads a .json or .pt file to one of the supported types.\n\n    Arguments:\n        file_path (str): The name of the file.\n        map_location (str): In case of a .pt file, on which device to load the object (cpu,cuda).\n    Returns:\n        A object of type AbstractBlock, QuantumCircuit, QuantumModel, Register.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from pathlib import Path\n    import os\n\n    from qadence import save, load, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    qc = QuantumCircuit(n_qubits, myblock)\n    # Lets store the circuit in a json file\n    save(qc, '.', 'circ')\n    loaded_qc = load(Path('circ.json'))\n    qc == loaded_qc\n    os.remove('circ.json')\n    ## Let's wrap it in a QuantumModel and store that\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n    save(qm, folder= '.',file_name= 'quantum_model')\n    qm_loaded = load('quantum_model.json')\n    os.remove('quantum_model.json')\n    ```\n    \"\"\"\n    d = {}\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if not os.path.exists(file_path):\n        logger.error(f\"File {file_path} not found.\")\n        raise FileNotFoundError\n    FORMAT = file_extension(file_path)\n    _, _, load_fn, _ = FORMAT_DICT[FORMAT]  # type: ignore[index]\n    try:\n        d = load_fn(file_path, map_location)\n        logger.debug(f\"Successfully loaded {d} from {file_path}.\")\n    except Exception as e:\n        logger.error(f\"Unable to load Object from {file_path} due to {e}\")\n    return deserialize(d)\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.parse_expr_fn","title":"<code>parse_expr_fn(code)</code>","text":"<p>A parsing expressions function that checks whether a given code is valid on.</p> <p>the parsing grammar. The grammar is defined to be compatible with <code>sympy</code> expressions, such as <code>Float('-0.33261030434342942', precision=53)</code>, while avoiding code injection such as <code>2*3</code> or <code>__import__('os').system('ls -la')</code>.</p> PARAMETER DESCRIPTION <code>code</code> <p>code to be parsed and checked.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Boolean indicating whether the code matches the defined grammar or not.</p> Source code in <code>qadence/serialization.py</code> <pre><code>def parse_expr_fn(code: str) -&gt; bool:\n    \"\"\"\n    A parsing expressions function that checks whether a given code is valid on.\n\n    the parsing grammar. The grammar is defined to be compatible with `sympy`\n    expressions, such as `Float('-0.33261030434342942', precision=53)`, while\n    avoiding code injection such as `2*3` or `__import__('os').system('ls -la')`.\n\n    Args:\n        code (str): code to be parsed and checked.\n\n    Returns:\n        Boolean indicating whether the code matches the defined grammar or not.\n    \"\"\"\n\n    parser = _parsing_serialize_expr\n    try:\n        parser.parse(code)\n    except NoMatch:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.save","title":"<code>save(obj, folder, file_name='', format=SerializationFormat.JSON)</code>","text":"<p>Same as serialize/deserialize but for storing/loading files.</p> <p>Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module Saves a qadence object to a json/.pt.</p> PARAMETER DESCRIPTION <code>obj</code> <pre><code>Either AbstractBlock, QuantumCircuit, QuantumModel, Register.\n</code></pre> <p> TYPE: <code>AbstractBlock | QuantumCircuit | QuantumModel | Register</code> </p> <code>file_name</code> <p>The name of the file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>format</code> <p>The type of file to save.</p> <p> TYPE: <code>str</code> DEFAULT: <code>JSON</code> </p> <p>Returns:     None.</p> <p>Examples: <pre><code>import torch\nfrom pathlib import Path\nimport os\n\nfrom qadence import save, load, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nqc = QuantumCircuit(n_qubits, myblock)\n# Lets store the circuit in a json file\nsave(qc, '.', 'circ')\nloaded_qc = load(Path('circ.json'))\nqc == loaded_qc\nos.remove('circ.json')\n## Let's wrap it in a QuantumModel and store that\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\nsave(qm, folder= '.',file_name= 'quantum_model')\nqm_loaded = load('quantum_model.json')\nos.remove('quantum_model.json')\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def save(\n    obj: SUPPORTED_TYPES,\n    folder: str | Path,\n    file_name: str = \"\",\n    format: SerializationFormat = SerializationFormat.JSON,\n) -&gt; None:\n    \"\"\"\n    Same as serialize/deserialize but for storing/loading files.\n\n    Supported types:\n    AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module\n    Saves a qadence object to a json/.pt.\n\n    Arguments:\n        obj (AbstractBlock | QuantumCircuit | QuantumModel | Register):\n                Either AbstractBlock, QuantumCircuit, QuantumModel, Register.\n        file_name (str): The name of the file.\n        format (str): The type of file to save.\n    Returns:\n        None.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from pathlib import Path\n    import os\n\n    from qadence import save, load, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    qc = QuantumCircuit(n_qubits, myblock)\n    # Lets store the circuit in a json file\n    save(qc, '.', 'circ')\n    loaded_qc = load(Path('circ.json'))\n    qc == loaded_qc\n    os.remove('circ.json')\n    ## Let's wrap it in a QuantumModel and store that\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n    save(qm, folder= '.',file_name= 'quantum_model')\n    qm_loaded = load('quantum_model.json')\n    os.remove('quantum_model.json')\n    ```\n    \"\"\"\n    if not isinstance(obj, get_args(SUPPORTED_TYPES)):\n        logger.error(f\"Serialization of object type {type(obj)} not supported.\")\n    folder = Path(folder)\n    if not folder.is_dir():\n        logger.error(NotADirectoryError)\n    if file_name == \"\":\n        file_name = type(obj).__name__\n    try:\n        suffix, save_fn, _, save_params = FORMAT_DICT[format]\n        d = serialize(obj, save_params)\n        file_path = folder / Path(file_name + suffix)\n        save_fn(d, file_path)\n        logger.debug(f\"Successfully saved {obj} from to {folder}.\")\n    except Exception as e:\n        logger.error(f\"Unable to write {type(obj)} to disk due to {e}\")\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.serialize","title":"<code>serialize(obj, save_params=False)</code>","text":"<p>Supported Types:</p> <p>AbstractBlock | QuantumCircuit | QuantumModel | torch.nn.Module | Register | Module Serializes a qadence object to a dictionary.</p> PARAMETER DESCRIPTION <code>obj</code> <p> TYPE: <code>AbstractBlock | QuantumCircuit | QuantumModel | Register | Module</code> </p> <p>Returns:     A dict.</p> <p>Examples: <pre><code>import torch\nfrom qadence import serialize, deserialize, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nblock_dict = serialize(myblock)\nprint(block_dict)\n\n## Lets use myblock in a QuantumCircuit and serialize it.\n\nqc = QuantumCircuit(n_qubits, myblock)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n## Finally, let's wrap it in a QuantumModel\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n# Lets check if the loaded QuantumModel returns the same expectation\nassert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n</code></pre> <pre><code>{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': 'HEA', 'blocks': [{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('84d84f15-fd91-40a5-a5d7-9a7444f21793', {'name': 'theta_0', 'expression': \"Parameter('theta_0')\", 'symbols': {'theta_0': {'name': 'theta_0', 'trainable': 'True', 'value': '0.730783666620393'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('9da153cd-c8a0-4ff0-997b-9a03f4a171a9', {'name': 'theta_1', 'expression': \"Parameter('theta_1')\", 'symbols': {'theta_1': {'name': 'theta_1', 'trainable': 'True', 'value': '0.9688001905158695'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RY', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('dc507ae9-18e2-48fb-b3b2-0dfa55a4e23c', {'name': 'theta_2', 'expression': \"Parameter('theta_2')\", 'symbols': {'theta_2': {'name': 'theta_2', 'trainable': 'True', 'value': '0.41792876795194445'}}})}}, 'noise': None}, {'type': 'RY', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('3265cdf8-f942-4053-b68b-423f294ad351', {'name': 'theta_3', 'expression': \"Parameter('theta_3')\", 'symbols': {'theta_3': {'name': 'theta_3', 'trainable': 'True', 'value': '0.8024262327769821'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('47dfd94e-3919-4886-95a2-fe6cce6b249f', {'name': 'theta_4', 'expression': \"Parameter('theta_4')\", 'symbols': {'theta_4': {'name': 'theta_4', 'trainable': 'True', 'value': '0.07259653069709826'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('6257a5a6-068c-4b86-8a38-e28b9fa1a1a6', {'name': 'theta_5', 'expression': \"Parameter('theta_5')\", 'symbols': {'theta_5': {'name': 'theta_5', 'trainable': 'True', 'value': '0.24376541039758137'}}})}}, 'noise': None}]}]}, {'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'CNOT', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'X', 'qubit_support': (1,), 'tag': None, 'noise': None}], 'noise': None}]}]}]}\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def serialize(obj: SUPPORTED_TYPES, save_params: bool = False) -&gt; dict:\n    \"\"\"\n    Supported Types:\n\n    AbstractBlock | QuantumCircuit | QuantumModel | torch.nn.Module | Register | Module\n    Serializes a qadence object to a dictionary.\n\n    Arguments:\n        obj (AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module):\n    Returns:\n        A dict.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence import serialize, deserialize, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    block_dict = serialize(myblock)\n    print(block_dict)\n\n    ## Lets use myblock in a QuantumCircuit and serialize it.\n\n    qc = QuantumCircuit(n_qubits, myblock)\n    qc_dict = serialize(qc)\n    qc_deserialized = deserialize(qc_dict)\n    assert qc == qc_deserialized\n\n    ## Finally, let's wrap it in a QuantumModel\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\n    qm_dict = serialize(qm)\n    qm_deserialized = deserialize(qm_dict)\n    # Lets check if the loaded QuantumModel returns the same expectation\n    assert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n    ```\n    \"\"\"\n    if not isinstance(obj, get_args(SUPPORTED_TYPES)):\n        logger.error(TypeError(f\"Serialization of object type {type(obj)} not supported.\"))\n\n    d: dict = dict()\n    try:\n        if isinstance(obj, core.Expr):\n            symb_dict = dict()\n            expr_dict = {\"name\": str(obj), \"expression\": srepr(obj)}\n            symbs: set[Parameter | core.Basic] = obj.free_symbols\n            if symbs:\n                symb_dict = {\"symbols\": {str(s): s._to_dict() for s in symbs}}\n            d = {**expr_dict, **symb_dict}\n        else:\n            if hasattr(obj, \"_to_dict\"):\n                model_to_dict: Callable = obj._to_dict\n                d = (\n                    model_to_dict(save_params)\n                    if isinstance(obj, torch.nn.Module)\n                    else model_to_dict()\n                )\n            elif hasattr(obj, \"state_dict\"):\n                d = {type(obj).__name__: obj.state_dict()}\n            else:\n                raise ValueError(f\"Cannot serialize object {obj}.\")\n    except Exception as e:\n        logger.error(f\"Serialization of object {obj} failed due to {e}\")\n    return d\n</code></pre>"},{"location":"api/states/","title":"State preparation","text":""},{"location":"api/states/#state-preparation-routines","title":"State Preparation Routines","text":""},{"location":"api/states/#qadence.states._rand_haar_slow","title":"<code>_rand_haar_slow(n_qubits)</code>","text":"<p>Detailed in https://arxiv.org/pdf/math-ph/0609050.pdf.</p> <p>Textbook implementation, but very expensive. For 12 qubits it takes several seconds. For 1 qubit it seems to produce the same distribution as the measure above.</p> Source code in <code>qadence/states.py</code> <pre><code>def _rand_haar_slow(n_qubits: int) -&gt; Tensor:\n    \"\"\"\n    Detailed in https://arxiv.org/pdf/math-ph/0609050.pdf.\n\n    Textbook implementation, but very expensive. For 12 qubits it takes several seconds.\n    For 1 qubit it seems to produce the same distribution as the measure above.\n    \"\"\"\n    N = 2**n_qubits\n    A = torch.zeros(N, N, dtype=DTYPE).normal_(0, 1)\n    B = torch.zeros(N, N, dtype=DTYPE).normal_(0, 1)\n    Z = A + 1.0j * B\n    Q, R = torch.linalg.qr(Z)\n    Lambda = torch.diag(torch.diag(R) / torch.diag(R).abs())\n    haar_unitary = torch.matmul(Q, Lambda)\n    return torch.matmul(haar_unitary, zero_state(n_qubits).squeeze(0)).unsqueeze(0)\n</code></pre>"},{"location":"api/states/#qadence.states.density_mat","title":"<code>density_mat(state)</code>","text":"<p>Computes the density matrix from a pure state vector.</p> PARAMETER DESCRIPTION <code>state</code> <p>The pure state vector :math:<code>|\\psi\\rangle</code>.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The density matrix :math:<code>\\rho = |\\psi \\rangle \\langle\\psi|</code>.</p> <p> TYPE: <code>DensityMatrix</code> </p> Source code in <code>qadence/states.py</code> <pre><code>def density_mat(state: Tensor) -&gt; DensityMatrix:\n    \"\"\"\n    Computes the density matrix from a pure state vector.\n\n    Arguments:\n        state: The pure state vector :math:`|\\\\psi\\\\rangle`.\n\n    Returns:\n        Tensor: The density matrix :math:`\\\\rho = |\\psi \\\\rangle \\\\langle\\\\psi|`.\n    \"\"\"\n    if isinstance(state, DensityMatrix):\n        return state\n    return DensityMatrix(torch.einsum(\"bi,bj-&gt;bij\", (state, state.conj())))\n</code></pre>"},{"location":"api/states/#qadence.states.ghz_block","title":"<code>ghz_block(n_qubits)</code>","text":"<p>Generates the abstract ghz state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>A ChainBlock representing the GHZ state.</p> <p>Examples: <pre><code>from qadence.states import ghz_block\n\nblock = ghz_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 CNOT(0, 1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def ghz_block(n_qubits: int) -&gt; ChainBlock:\n    \"\"\"\n    Generates the abstract ghz state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A ChainBlock representing the GHZ state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import ghz_block\n\n    block = ghz_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    cnots = chain(CNOT(i - 1, i) for i in range(1, n_qubits))\n    return chain(H(0), cnots)\n</code></pre>"},{"location":"api/states/#qadence.states.ghz_state","title":"<code>ghz_state(n_qubits, batch_size=1)</code>","text":"<p>Creates a GHZ state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>How many bitstrings to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import ghz_state\n\nprint(ghz_state(n_qubits=2, batch_size=2))\n</code></pre> <pre><code>tensor([[0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j],\n        [0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def ghz_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Creates a GHZ state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): How many bitstrings to use.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import ghz_state\n\n    print(ghz_state(n_qubits=2, batch_size=2))\n    ```\n    \"\"\"\n    norm = 1 / torch.sqrt(torch.tensor(2))\n    return norm * (zero_state(n_qubits, batch_size) + one_state(n_qubits, batch_size))\n</code></pre>"},{"location":"api/states/#qadence.states.is_normalized","title":"<code>is_normalized(wf, atol=NORMALIZATION_ATOL)</code>","text":"<p>Checks if a wave function is normalized.</p> PARAMETER DESCRIPTION <code>wf</code> <p>The wave function as a torch tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>atol</code> <p>The tolerance.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>NORMALIZATION_ATOL</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, is_normalized\n\nprint(is_normalized(uniform_state(2)))\n</code></pre> <pre><code>True\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def is_normalized(wf: Tensor, atol: float = NORMALIZATION_ATOL) -&gt; bool:\n    \"\"\"\n    Checks if a wave function is normalized.\n\n    Arguments:\n        wf (torch.Tensor): The wave function as a torch tensor.\n        atol (float) : The tolerance.\n\n    Returns:\n        A bool.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, is_normalized\n\n    print(is_normalized(uniform_state(2)))\n    ```\n    \"\"\"\n    if wf.dim() == 1:\n        wf = wf.unsqueeze(0)\n    sum_probs: Tensor = (wf.abs() ** 2).sum(dim=1)\n    ones = torch.ones_like(sum_probs)\n    return torch.allclose(sum_probs, ones, rtol=0.0, atol=atol)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/states/#qadence.states.normalize","title":"<code>normalize(wf)</code>","text":"<p>Normalizes a wavefunction or batch of wave functions.</p> PARAMETER DESCRIPTION <code>wf</code> <p>Normalized wavefunctions.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, normalize\n\nprint(normalize(uniform_state(2, 2)))\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j],\n        [0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def normalize(wf: Tensor) -&gt; Tensor:\n    \"\"\"\n    Normalizes a wavefunction or batch of wave functions.\n\n    Arguments:\n        wf (torch.Tensor): Normalized wavefunctions.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, normalize\n\n    print(normalize(uniform_state(2, 2)))\n    ```\n    \"\"\"\n    if wf.dim() == 1:\n        return wf / torch.sqrt((wf.abs() ** 2).sum())\n    else:\n        return wf / torch.sqrt((wf.abs() ** 2).sum(1)).unsqueeze(1)\n</code></pre>"},{"location":"api/states/#qadence.states.one_block","title":"<code>one_block(n_qubits)</code>","text":"<p>Generates the abstract one state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the one state.</p> <p>Examples: <pre><code>from qadence.states import one_block\n\nblock = one_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def one_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract one state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the one state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import one_block\n\n    block = one_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(X, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.one_state","title":"<code>one_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the one state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import one_state\n\nstate = one_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def one_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the one state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): The batch size.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import one_state\n\n    state = one_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    bitstring = \"1\" * n_qubits\n    return _state_from_bitstring(bitstring, batch_size)\n</code></pre>"},{"location":"api/states/#qadence.states.overlap","title":"<code>overlap(s0, s1)</code>","text":"<p>Computes the exact overlap between two statevectors.</p> PARAMETER DESCRIPTION <code>s0</code> <p>A statevector or batch of statevectors.</p> <p> TYPE: <code>Tensor</code> </p> <code>s1</code> <p>A statevector or batch of statevectors.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor with the result.</p> <p>Examples: <pre><code>from qadence.states import rand_bitstring\n\nprint(rand_bitstring(N=8))\n</code></pre> <pre><code>10110001\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def overlap(s0: torch.Tensor, s1: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the exact overlap between two statevectors.\n\n    Arguments:\n        s0 (torch.Tensor): A statevector or batch of statevectors.\n        s1 (torch.Tensor): A statevector or batch of statevectors.\n\n    Returns:\n        A torch.Tensor with the result.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_bitstring\n\n    print(rand_bitstring(N=8))\n    ```\n    \"\"\"\n    from qadence.overlap import overlap_exact\n\n    return overlap_exact(s0, s1)\n</code></pre>"},{"location":"api/states/#qadence.states.pmf","title":"<code>pmf(wf)</code>","text":"<p>Converts a wave function into a torch Distribution.</p> PARAMETER DESCRIPTION <code>wf</code> <p>The wave function as a torch tensor.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Distribution</code> <p>A torch.distributions.Distribution.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, pmf\n\nprint(pmf(uniform_state(2)).probs)\n</code></pre> <pre><code>tensor([[0.2500, 0.2500, 0.2500, 0.2500]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def pmf(wf: Tensor) -&gt; Distribution:\n    \"\"\"\n    Converts a wave function into a torch Distribution.\n\n    Arguments:\n        wf (torch.Tensor): The wave function as a torch tensor.\n\n    Returns:\n        A torch.distributions.Distribution.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, pmf\n\n    print(pmf(uniform_state(2)).probs)\n    ```\n    \"\"\"\n    return Categorical(torch.abs(torch.pow(wf, 2)))\n</code></pre>"},{"location":"api/states/#qadence.states.product_block","title":"<code>product_block(bitstring)</code>","text":"<p>Creates an abstract product state from a bitstring.</p> PARAMETER DESCRIPTION <code>bitstring</code> <p>A bitstring.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the product state.</p> <p>Examples: <pre><code>from qadence.states import product_block\n\nprint(product_block(\"1100\"))\n</code></pre> <pre><code>KronBlock(0,1,2,3)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u251c\u2500\u2500 I(2)\n\u2514\u2500\u2500 I(3)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def product_block(bitstring: str) -&gt; KronBlock:\n    \"\"\"\n    Creates an abstract product state from a bitstring.\n\n    Arguments:\n        bitstring (str): A bitstring.\n\n    Returns:\n        A KronBlock representing the product state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import product_block\n\n    print(product_block(\"1100\"))\n    ```\n    \"\"\"\n    return _block_from_bitstring(bitstring)\n</code></pre>"},{"location":"api/states/#qadence.states.product_state","title":"<code>product_state(bitstring, batch_size=1, endianness=Endianness.BIG, backend=BackendName.PYQTORCH)</code>","text":"<p>Creates a product state from a bitstring.</p> PARAMETER DESCRIPTION <code>bitstring</code> <p>A bitstring.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1</code> </p> <code>backend</code> <p>The backend to use. Default is \"pyqtorch\".</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import product_state\n\nprint(product_state(\"1100\", backend=\"pyqtorch\"))\nprint(product_state(\"1100\", backend=\"horqrux\"))\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n         1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j\n 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>@singledispatch\ndef product_state(\n    bitstring: str,\n    batch_size: int = 1,\n    endianness: Endianness = Endianness.BIG,\n    backend: BackendName = BackendName.PYQTORCH,\n) -&gt; ArrayLike:\n    \"\"\"\n    Creates a product state from a bitstring.\n\n    Arguments:\n        bitstring (str): A bitstring.\n        batch_size (int) : Batch size.\n        backend (BackendName): The backend to use. Default is \"pyqtorch\".\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import product_state\n\n    print(product_state(\"1100\", backend=\"pyqtorch\"))\n    print(product_state(\"1100\", backend=\"horqrux\"))\n    ```\n    \"\"\"\n    if batch_size:\n        logger.debug(\n            \"The input `batch_size` is going to be deprecated. \"\n            \"For now, default batch_size is set to 1.\"\n        )\n    return run(product_block(bitstring), backend=backend, endianness=endianness)\n</code></pre>"},{"location":"api/states/#qadence.states.rand_bitstring","title":"<code>rand_bitstring(N)</code>","text":"<p>Creates a random bistring.</p> PARAMETER DESCRIPTION <code>N</code> <p>The length of the bitstring.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A string.</p> <p>Examples: <pre><code>from qadence.states import rand_bitstring\n\nprint(rand_bitstring(N=8))\n</code></pre> <pre><code>11111110\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_bitstring(N: int) -&gt; str:\n    \"\"\"\n    Creates a random bistring.\n\n    Arguments:\n        N (int): The length of the bitstring.\n\n    Returns:\n        A string.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_bitstring\n\n    print(rand_bitstring(N=8))\n    ```\n    \"\"\"\n    return \"\".join(str(random.randint(0, 1)) for _ in range(N))\n</code></pre>"},{"location":"api/states/#qadence.states.rand_product_block","title":"<code>rand_product_block(n_qubits)</code>","text":"<p>Creates a block representing a random abstract product state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the product state.</p> <p>Examples: <pre><code>from qadence.states import rand_product_block\n\nprint(rand_product_block(n_qubits=2))\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 I(0)\n\u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_product_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Creates a block representing a random abstract product state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the product state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_product_block\n\n    print(rand_product_block(n_qubits=2))\n    ```\n    \"\"\"\n    return product_block(rand_bitstring(n_qubits))\n</code></pre>"},{"location":"api/states/#qadence.states.rand_product_state","title":"<code>rand_product_state(n_qubits, batch_size=1)</code>","text":"<p>Creates a random product state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>How many bitstrings to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import rand_product_state\n\nprint(rand_product_state(n_qubits=2, batch_size=2))\n</code></pre> <pre><code>tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_product_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Creates a random product state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): How many bitstrings to use.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_product_state\n\n    print(rand_product_state(n_qubits=2, batch_size=2))\n    ```\n    \"\"\"\n    wf_batch = torch.zeros(batch_size, 2**n_qubits, dtype=DTYPE)\n    rand_pos = torch.randint(0, 2**n_qubits, (batch_size,))\n    wf_batch[torch.arange(batch_size), rand_pos] = torch.tensor(1.0 + 0j, dtype=DTYPE)\n    return wf_batch\n</code></pre>"},{"location":"api/states/#qadence.states.random_state","title":"<code>random_state(n_qubits, batch_size=1, backend=BackendName.PYQTORCH, type=StateGeneratorType.HAAR_MEASURE_FAST)</code>","text":"<p>Generates a random state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>backend</code> <p>The backend to use.</p> <p> TYPE: <code>str</code> DEFAULT: <code>PYQTORCH</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>type</code> <p>StateGeneratorType.</p> <p> DEFAULT: <code>HAAR_MEASURE_FAST</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import random_state, StateGeneratorType\nfrom qadence.states import random_state, is_normalized, pmf\nfrom qadence.types import BackendName\nfrom torch.distributions import Distribution\n\n### We have the following options:\nprint([g.value for g in StateGeneratorType])\n\nn_qubits = 2\n# The default is StateGeneratorType.HAAR_MEASURE_FAST\nstate = random_state(n_qubits=n_qubits)\nprint(state)\n\n### Lets initialize a state using random rotations, i.e., StateGeneratorType.RANDOM_ROTATIONS.\nrandom = random_state(n_qubits=n_qubits, type=StateGeneratorType.RANDOM_ROTATIONS)\nprint(random)\n</code></pre> <pre><code>['RandomRotations', 'HaarMeasureFast', 'HaarMeasureSlow']\ntensor([[ 0.0197-0.6124j,  0.3357+0.2945j, -0.0681+0.5040j, -0.2633-0.3117j]])\ntensor([[0.9720+0.0000j, 0.0608+0.0000j, 0.0000+0.2265j, 0.0000+0.0142j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def random_state(\n    n_qubits: int,\n    batch_size: int = 1,\n    backend: str = BackendName.PYQTORCH,\n    type: StateGeneratorType = StateGeneratorType.HAAR_MEASURE_FAST,\n) -&gt; Tensor:\n    \"\"\"\n    Generates a random state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        backend (str): The backend to use.\n        batch_size (int): The batch size.\n        type : StateGeneratorType.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import random_state, StateGeneratorType\n    from qadence.states import random_state, is_normalized, pmf\n    from qadence.types import BackendName\n    from torch.distributions import Distribution\n\n    ### We have the following options:\n    print([g.value for g in StateGeneratorType])\n\n    n_qubits = 2\n    # The default is StateGeneratorType.HAAR_MEASURE_FAST\n    state = random_state(n_qubits=n_qubits)\n    print(state)\n\n    ### Lets initialize a state using random rotations, i.e., StateGeneratorType.RANDOM_ROTATIONS.\n    random = random_state(n_qubits=n_qubits, type=StateGeneratorType.RANDOM_ROTATIONS)\n    print(random)\n    ```\n    \"\"\"\n\n    if type == StateGeneratorType.HAAR_MEASURE_FAST:\n        state = concat(tuple(_rand_haar_fast(n_qubits) for _ in range(batch_size)), dim=0)\n    elif type == StateGeneratorType.HAAR_MEASURE_SLOW:\n        state = concat(tuple(_rand_haar_slow(n_qubits) for _ in range(batch_size)), dim=0)\n    elif type == StateGeneratorType.RANDOM_ROTATIONS:\n        state = run(_abstract_random_state(n_qubits, batch_size))  # type: ignore\n    assert all(list(map(is_normalized, state)))\n    return state\n</code></pre>"},{"location":"api/states/#qadence.states.uniform_block","title":"<code>uniform_block(n_qubits)</code>","text":"<p>Generates the abstract uniform state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the uniform state.</p> <p>Examples: <pre><code>from qadence.states import uniform_block\n\nblock = uniform_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 H(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def uniform_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract uniform state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the uniform state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_block\n\n    block = uniform_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(H, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.uniform_state","title":"<code>uniform_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the uniform state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import uniform_state\n\nstate = uniform_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def uniform_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the uniform state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): The batch size.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state\n\n    state = uniform_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    norm = 1 / torch.sqrt(torch.tensor(2**n_qubits))\n    return norm * torch.ones(batch_size, 2**n_qubits, dtype=DTYPE)\n</code></pre>"},{"location":"api/states/#qadence.states.zero_block","title":"<code>zero_block(n_qubits)</code>","text":"<p>Generates the abstract zero state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the zero state.</p> <p>Examples: <pre><code>from qadence.states import zero_block\n\nblock = zero_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 I(0)\n\u2514\u2500\u2500 I(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def zero_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract zero state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the zero state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import zero_block\n\n    block = zero_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(I, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.zero_state","title":"<code>zero_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the zero state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits for which the zero state is to be generated.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size for the zero state.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import zero_state\n\nstate = zero_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def zero_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the zero state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits for which the zero state is to be generated.\n        batch_size (int): The batch size for the zero state.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import zero_state\n\n    state = zero_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    bitstring = \"0\" * n_qubits\n    return _state_from_bitstring(bitstring, batch_size)\n</code></pre>"},{"location":"api/transpile/","title":"Transpilation","text":"<p>Contains functions that operate on blocks and circuits to <code>transpile</code> them to new blocks/circuits.</p>"},{"location":"api/transpile/#qadence.transpile.transpile.transpile","title":"<code>transpile(*fs)</code>","text":"<pre><code>transpile(*fs: Callable[[AbstractBlock], AbstractBlock]) -&gt; Callable[[AbstractBlock], AbstractBlock]\n</code></pre><pre><code>transpile(*fs: Callable[[QuantumCircuit], QuantumCircuit]) -&gt; Callable[[QuantumCircuit], QuantumCircuit]\n</code></pre> <p><code>AbstractBlock</code> or <code>QuantumCircuit</code> transpilation.</p> <p>Compose functions that accept a circuit/block and returns a circuit/block.</p> PARAMETER DESCRIPTION <code>*fs</code> <p>composable functions that either map blocks to blocks (<code>Callable[[AbstractBlock], AbstractBlock]</code>) or circuits to circuits (<code>Callable[[QuantumCircuit], QuantumCircuit]</code>).</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>Composed function.</p> <p>Examples:</p> <p>Flatten a block of nested chains and krons: <pre><code>from qadence import *\nfrom qadence.transpile import transpile, flatten, scale_primitive_blocks_only\n\nb = chain(2 * chain(chain(X(0), Y(0))), kron(kron(X(0), X(1))))\nprint(b)\n\n# both flatten and scale_primitive_blocks_only are functions that accept and\n# return a block\nt = transpile(flatten, scale_primitive_blocks_only)(b)\nprint(t)\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 [mul: 2] \n\u2502   \u2514\u2500\u2500 ChainBlock(0)\n\u2502       \u2514\u2500\u2500 ChainBlock(0)\n\u2502           \u251c\u2500\u2500 X(0)\n\u2502           \u2514\u2500\u2500 Y(0)\n\u2514\u2500\u2500 KronBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u251c\u2500\u2500 X(0)\n        \u2514\u2500\u2500 X(1)\n\nChainBlock(0,1)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u251c\u2500\u2500 Y(0)\n\u2514\u2500\u2500 KronBlock(0,1)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 X(1)\n</code></pre> </p> <p>We also proved a decorator to easily turn a function <code>Callable[[AbstractBlock], AbstractBlock]</code> into a <code>Callable[[QuantumCircuit], QuantumCircuit]</code> to be used in circuit transpilation. <pre><code>from qadence import *\nfrom qadence.transpile import transpile, blockfn_to_circfn, flatten\n\n# We want to pass this circuit to `transpile` instead of a block,\n# so we need functions that map from a circuit to a circuit.\ncirc = QuantumCircuit(2, chain(chain(X(0), chain(X(1)))))\n\n@blockfn_to_circfn\ndef fn(block):\n    # un-decorated function accepts a block and returns a block\n    return block * block\n\ntransp = transpile(\n    # the decorated function accepts a circuit and returns a circuit\n    fn,\n    # already existing functions can also be decorated\n    blockfn_to_circfn(flatten)\n)\nprint(transp(circ))\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 X(0)\n\u2502   \u2514\u2500\u2500 X(1)\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/transpile/transpile.py</code> <pre><code>def transpile(*fs: Callable) -&gt; Callable:\n    \"\"\"`AbstractBlock` or `QuantumCircuit` transpilation.\n\n    Compose functions that\n    accept a circuit/block and returns a circuit/block.\n\n    Arguments:\n        *fs: composable functions that either map blocks to blocks\n            (`Callable[[AbstractBlock], AbstractBlock]`)\n            or circuits to circuits (`Callable[[QuantumCircuit], QuantumCircuit]`).\n\n    Returns:\n        Composed function.\n\n    Examples:\n\n    Flatten a block of nested chains and krons:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import *\n    from qadence.transpile import transpile, flatten, scale_primitive_blocks_only\n\n    b = chain(2 * chain(chain(X(0), Y(0))), kron(kron(X(0), X(1))))\n    print(b)\n    print() # markdown-exec: hide\n\n    # both flatten and scale_primitive_blocks_only are functions that accept and\n    # return a block\n    t = transpile(flatten, scale_primitive_blocks_only)(b)\n    print(t)\n    ```\n\n    We also proved a decorator to easily turn a function `Callable[[AbstractBlock], AbstractBlock]`\n    into a `Callable[[QuantumCircuit], QuantumCircuit]` to be used in circuit transpilation.\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import *\n    from qadence.transpile import transpile, blockfn_to_circfn, flatten\n\n    # We want to pass this circuit to `transpile` instead of a block,\n    # so we need functions that map from a circuit to a circuit.\n    circ = QuantumCircuit(2, chain(chain(X(0), chain(X(1)))))\n\n    @blockfn_to_circfn\n    def fn(block):\n        # un-decorated function accepts a block and returns a block\n        return block * block\n\n    transp = transpile(\n        # the decorated function accepts a circuit and returns a circuit\n        fn,\n        # already existing functions can also be decorated\n        blockfn_to_circfn(flatten)\n    )\n    print(transp(circ))\n    ```\n    \"\"\"\n    return lambda x: reduce(lambda acc, f: f(acc), reversed(fs), x)\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.chain_single_qubit_ops","title":"<code>chain_single_qubit_ops(block)</code>","text":"<p>Transpile a chain of krons into a kron of chains of single qubit operations.</p> <p>Examples: <pre><code>from qadence import hea\nfrom qadence.transpile.block import chain_single_qubit_ops\n\n# Consider a single HEA layer\nblock = hea(2,1)\nprint(block)\n\n# After applying chain_single_qubit_ops, we get:\nprint(chain_single_qubit_ops(block))\n</code></pre> <pre><code>ChainBlock(0,1) [tag: HEA]\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n\u2502   \u2502   \u2514\u2500\u2500 RX(1) [params: ['theta_1']]\n\u2502   \u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_2']]\n\u2502   \u2502   \u2514\u2500\u2500 RY(1) [params: ['theta_3']]\n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 RX(0) [params: ['theta_4']]\n\u2502       \u2514\u2500\u2500 RX(1) [params: ['theta_5']]\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u2514\u2500\u2500 CNOT(0, 1)\nChainBlock(0,1)\n\u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u251c\u2500\u2500 ChainBlock(0)\n\u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n\u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_2']]\n\u2502   \u2502   \u2514\u2500\u2500 RX(0) [params: ['theta_4']]\n\u2502   \u2514\u2500\u2500 ChainBlock(1)\n\u2502       \u251c\u2500\u2500 RX(1) [params: ['theta_1']]\n\u2502       \u251c\u2500\u2500 RY(1) [params: ['theta_3']]\n\u2502       \u2514\u2500\u2500 RX(1) [params: ['theta_5']]\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u2514\u2500\u2500 CNOT(0, 1)\n</code></pre></p> Source code in <code>qadence/transpile/block.py</code> <pre><code>def chain_single_qubit_ops(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Transpile a chain of krons into a kron of chains of single qubit operations.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import hea\n    from qadence.transpile.block import chain_single_qubit_ops\n\n    # Consider a single HEA layer\n    block = hea(2,1)\n    print(block)\n\n    # After applying chain_single_qubit_ops, we get:\n    print(chain_single_qubit_ops(block))\n    ```\n    \"\"\"\n    if is_chain_of_primitivekrons(block):\n        try:\n            return kron(*map(lambda bs: chain(*bs), zip(*block)))  # type: ignore[misc]\n        except Exception as e:\n            logger.debug(\n                f\"Unable to transpile {block} using chain_single_qubit_ops\\\n                         due to {e}. Returning original circuit.\"\n            )\n            return block\n\n    elif isinstance(block, CompositeBlock):\n        return _construct(type(block), tuple(chain_single_qubit_ops(b) for b in block.blocks))\n    else:\n        return block\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.scale_primitive_blocks_only","title":"<code>scale_primitive_blocks_only(block, scale=None)</code>","text":"<p>Push the scale all the way down into the leaves of the block tree.</p> <p>When given a scaled CompositeBlock consisting of several PrimitiveBlocks.</p> PARAMETER DESCRIPTION <code>block</code> <p>The block to be transpiled.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>scale</code> <p>An optional scale parameter. Only to be used for recursive calls internally.</p> <p> TYPE: <code>Basic</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>A block of the same type where the scales have been moved into the subblocks.</p> <p> TYPE: <code>AbstractBlock</code> </p> <p>Examples:</p> <p>There are two different cases: <code>ChainBlock</code>s/<code>KronBlock</code>s: Only the first subblock needs to be scaled because chains/krons represent multiplications. <pre><code>from qadence import chain, X, RX\nfrom qadence.transpile import scale_primitive_blocks_only\nb = 2 * chain(X(0), RX(0, \"theta\"))\nprint(b)\n# After applying scale_primitive_blocks_only\nprint(scale_primitive_blocks_only(b))\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 ChainBlock(0)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\nChainBlock(0)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 RX(0) [params: ['theta']]\n</code></pre></p> <p><code>AddBlock</code>s: Consider 2 * add(X(0), RX(0, \"theta\")).  The scale needs to be added to all subblocks.  We get add(2 * X(0), 2 * RX(0, \"theta\")). <pre><code>from qadence import add, X, RX\nfrom qadence.transpile import scale_primitive_blocks_only\nb = 2 * add(X(0), RX(0, \"theta\"))\nprint(b)\n# After applying scale_primitive_blocks_only\nprint(scale_primitive_blocks_only(b))\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 AddBlock(0)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\nAddBlock(0)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 [mul: 2.000] \n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\n</code></pre></p> Source code in <code>qadence/transpile/block.py</code> <pre><code>@singledispatch\ndef scale_primitive_blocks_only(block: AbstractBlock, scale: sympy.Basic = None) -&gt; AbstractBlock:\n    \"\"\"Push the scale all the way down into the leaves of the block tree.\n\n    When given a scaled CompositeBlock consisting of several PrimitiveBlocks.\n\n    Arguments:\n        block: The block to be transpiled.\n        scale: An optional scale parameter. Only to be used for recursive calls internally.\n\n    Returns:\n        AbstractBlock: A block of the same type where the scales have been moved into the subblocks.\n\n    Examples:\n\n    There are two different cases:\n    `ChainBlock`s/`KronBlock`s: Only the first subblock needs to be scaled because chains/krons\n    represent multiplications.\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import chain, X, RX\n    from qadence.transpile import scale_primitive_blocks_only\n    b = 2 * chain(X(0), RX(0, \"theta\"))\n    print(b)\n    # After applying scale_primitive_blocks_only\n    print(scale_primitive_blocks_only(b))\n    ```\n\n    `AddBlock`s: Consider 2 * add(X(0), RX(0, \"theta\")).  The scale needs to be added to all\n    subblocks.  We get add(2 * X(0), 2 * RX(0, \"theta\")).\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import add, X, RX\n    from qadence.transpile import scale_primitive_blocks_only\n    b = 2 * add(X(0), RX(0, \"theta\"))\n    print(b)\n    # After applying scale_primitive_blocks_only\n    print(scale_primitive_blocks_only(b))\n    ```\n    \"\"\"\n    raise NotImplementedError(f\"scale_primitive_blocks_only is not implemented for {type(block)}\")\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.set_trainable","title":"<code>set_trainable(blocks, value=True, inplace=True)</code>","text":"<p>Set the trainability of all parameters in a block to a given value.</p> PARAMETER DESCRIPTION <code>blocks</code> <p>Block or list of blocks for which to set the trainable attribute</p> <p> TYPE: <code>AbstractBlock | list[AbstractBlock]</code> </p> <code>value</code> <p>The value of the trainable attribute to assign to the input blocks</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <p>Whether to modify the block(s) in place or not. Currently, only</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>if the <code>inplace</code> argument is set to False, the function will raise  this exception</p> RETURNS DESCRIPTION <code>AbstractBlock | list[AbstractBlock]</code> <p>AbstractBlock | list[AbstractBlock]: the input block or list of blocks with the trainable attribute set to the given value</p> Source code in <code>qadence/transpile/block.py</code> <pre><code>def set_trainable(\n    blocks: AbstractBlock | list[AbstractBlock], value: bool = True, inplace: bool = True\n) -&gt; AbstractBlock | list[AbstractBlock]:\n    \"\"\"Set the trainability of all parameters in a block to a given value.\n\n    Args:\n        blocks (AbstractBlock | list[AbstractBlock]): Block or list of blocks for which\n            to set the trainable attribute\n        value (bool, optional): The value of the trainable attribute to assign to the input blocks\n        inplace (bool, optional): Whether to modify the block(s) in place or not. Currently, only\n\n    Raises:\n        NotImplementedError: if the `inplace` argument is set to False, the function will\n            raise  this exception\n\n    Returns:\n        AbstractBlock | list[AbstractBlock]: the input block or list of blocks with the trainable\n            attribute set to the given value\n    \"\"\"\n\n    if isinstance(blocks, AbstractBlock):\n        blocks = [blocks]\n\n    if inplace:\n        for block in blocks:\n            params: list[sympy.Basic] = parameters(block)\n            for p in params:\n                if not p.is_number:\n                    p.trainable = value\n    else:\n        raise NotImplementedError(\"Not inplace set_trainable is not yet available\")\n\n    return blocks if len(blocks) &gt; 1 else blocks[0]\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.validate","title":"<code>validate(block)</code>","text":"<p>Moves a block from global to local qubit numbers by adding PutBlocks.</p> <p>Reassigns qubit locations appropriately.</p>"},{"location":"api/transpile/#qadence.transpile.block.validate--example","title":"Example","text":"<pre><code>from qadence.blocks import chain\nfrom qadence.operations import X\nfrom qadence.transpile import validate\n\nx = chain(chain(X(0)), chain(X(1)))\nprint(x)\nprint(validate(x))\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 ChainBlock(0)\n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 ChainBlock(1)\n    \u2514\u2500\u2500 X(1)\nChainBlock(0,1)\n\u251c\u2500\u2500 put on (0)\n\u2502   \u2514\u2500\u2500 ChainBlock(0)\n\u2502       \u2514\u2500\u2500 put on (0)\n\u2502           \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 put on (1)\n    \u2514\u2500\u2500 ChainBlock(0)\n        \u2514\u2500\u2500 put on (0)\n            \u2514\u2500\u2500 X(0)\n</code></pre> Source code in <code>qadence/transpile/block.py</code> <pre><code>def validate(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Moves a block from global to local qubit numbers by adding PutBlocks.\n\n    Reassigns qubit locations appropriately.\n\n    # Example\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence.blocks import chain\n    from qadence.operations import X\n    from qadence.transpile import validate\n\n    x = chain(chain(X(0)), chain(X(1)))\n    print(x)\n    print(validate(x))\n    ```\n    \"\"\"\n    vblock: AbstractBlock\n    from qadence.transpile import reassign\n\n    if isinstance(block, ControlBlock):\n        vblock = deepcopy(block)\n        b: AbstractBlock\n        (b,) = block.blocks\n        b = reassign(b, {i: i - min(b.qubit_support) for i in b.qubit_support})\n        b = validate(b)\n        vblock.blocks = (b,)  # type: ignore[assignment]\n\n    elif isinstance(block, CompositeBlock):\n        blocks = []\n        for b in block.blocks:\n            mi, ma = min(b.qubit_support), max(b.qubit_support)\n            nb = reassign(b, {i: i - min(b.qubit_support) for i in b.qubit_support})\n            nb = validate(nb)\n            nb = PutBlock(nb, tuple(range(mi, ma + 1)))\n            blocks.append(nb)\n        try:\n            vblock = _construct(type(block), tuple(blocks))\n        except AssertionError as e:\n            if str(e) == \"Make sure blocks act on distinct qubits!\":\n                vblock = chain(*blocks)\n            else:\n                raise e\n\n    elif isinstance(block, PrimitiveBlock):\n        vblock = deepcopy(block)\n\n    else:\n        raise NotImplementedError\n\n    vblock.tag = block.tag\n    return vblock\n</code></pre>"},{"location":"api/types/","title":"Types","text":""},{"location":"api/types/#qadence-types","title":"Qadence Types","text":""},{"location":"api/types/#qadence.types.TArray","title":"<code>TArray = Union[Iterable, Tensor, np.ndarray]</code>  <code>module-attribute</code>","text":"<p>Union of common array types.</p>"},{"location":"api/types/#qadence.types.TGenerator","title":"<code>TGenerator = Union[Tensor, sympy.Array, sympy.Basic]</code>  <code>module-attribute</code>","text":"<p>Union of torch tensors and numpy arrays.</p>"},{"location":"api/types/#qadence.types.TNumber","title":"<code>TNumber = Union[int, float, complex, np.int64, np.float64]</code>  <code>module-attribute</code>","text":"<p>Union of python and numpy numeric types.</p>"},{"location":"api/types/#qadence.types.TParameter","title":"<code>TParameter = Union[TNumber, Tensor, sympy.Basic, str]</code>  <code>module-attribute</code>","text":"<p>Union of numbers, tensors, and parameter types.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo","title":"<code>AlgoHEvo</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Hamiltonian Evolution algorithms that can be used by the backend.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.EIG","title":"<code>EIG = 'EIG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using Hamiltonian diagonalization.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.EXP","title":"<code>EXP = 'EXP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using torch.matrix_exp on the generator matrix.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.RK4","title":"<code>RK4 = 'RK4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>4th order Runge-Kutta approximation.</p>"},{"location":"api/types/#qadence.types.AnalogNoise","title":"<code>AnalogNoise</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of noise protocol.</p>"},{"location":"api/types/#qadence.types.AnsatzType","title":"<code>AnsatzType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Ansatz types for variational circuits.</p>"},{"location":"api/types/#qadence.types.AnsatzType.ALA","title":"<code>ALA = 'ala'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Alternating Layer Ansatz.</p>"},{"location":"api/types/#qadence.types.AnsatzType.HEA","title":"<code>HEA = 'hea'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hardware-efficient ansatz.</p>"},{"location":"api/types/#qadence.types.AnsatzType.IIA","title":"<code>IIA = 'iia'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Identity-Initialised Ansatz.</p>"},{"location":"api/types/#qadence.types.BasisSet","title":"<code>BasisSet</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Basis set for feature maps.</p>"},{"location":"api/types/#qadence.types.BasisSet.CHEBYSHEV","title":"<code>CHEBYSHEV = 'Chebyshev'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Chebyshev polynomials of the first kind.</p>"},{"location":"api/types/#qadence.types.BasisSet.FOURIER","title":"<code>FOURIER = 'Fourier'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fourier basis set.</p>"},{"location":"api/types/#qadence.types.DeviceType","title":"<code>DeviceType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported types of devices for Pulser backend.</p>"},{"location":"api/types/#qadence.types.DeviceType.IDEALIZED","title":"<code>IDEALIZED = 'IdealDevice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Idealized device, least realistic.</p>"},{"location":"api/types/#qadence.types.DeviceType.REALISTIC","title":"<code>REALISTIC = 'RealisticDevice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Device with realistic specs.</p>"},{"location":"api/types/#qadence.types.Endianness","title":"<code>Endianness</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The endianness convention to use.</p>"},{"location":"api/types/#qadence.types.Endianness.BIG","title":"<code>BIG = 'Big'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use Big endianness.</p>"},{"location":"api/types/#qadence.types.Endianness.LITTLE","title":"<code>LITTLE = 'Little'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use little endianness.</p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool","title":"<code>ExperimentTrackingTool</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool.MLFLOW","title":"<code>MLFLOW = 'mlflow'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the ml-flow experiment tracker.</p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool.TENSORBOARD","title":"<code>TENSORBOARD = 'tensorboard'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the tensorboard experiment tracker.</p>"},{"location":"api/types/#qadence.types.FigFormat","title":"<code>FigFormat</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available output formats for exporting visualized circuits to a file.</p>"},{"location":"api/types/#qadence.types.FigFormat.PDF","title":"<code>PDF = 'PDF'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PDF format.</p>"},{"location":"api/types/#qadence.types.FigFormat.PNG","title":"<code>PNG = 'PNG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PNG format.</p>"},{"location":"api/types/#qadence.types.FigFormat.SVG","title":"<code>SVG = 'SVG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>SVG format.</p>"},{"location":"api/types/#qadence.types.GenDAQC","title":"<code>GenDAQC</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The type of interaction for the DAQC transform.</p>"},{"location":"api/types/#qadence.types.GenDAQC.NN","title":"<code>NN = 'NN'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NN</p>"},{"location":"api/types/#qadence.types.GenDAQC.ZZ","title":"<code>ZZ = 'ZZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ZZ</p>"},{"location":"api/types/#qadence.types.InputDiffMode","title":"<code>InputDiffMode</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Derivative modes w.r.t inputs of UFAs.</p>"},{"location":"api/types/#qadence.types.InputDiffMode.AD","title":"<code>AD = 'ad'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Reverse automatic differentiation.</p>"},{"location":"api/types/#qadence.types.InputDiffMode.FD","title":"<code>FD = 'fd'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Central finite differencing.</p>"},{"location":"api/types/#qadence.types.Interaction","title":"<code>Interaction</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Interaction types used in.</p> <ul> <li><code>RydbergDevice</code>.</li> <li><code>hamiltonian_factory</code>.</li> </ul>"},{"location":"api/types/#qadence.types.Interaction.NN","title":"<code>NN = 'NN'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NN-Ising Interaction, N=(I-Z)/2.</p>"},{"location":"api/types/#qadence.types.Interaction.XY","title":"<code>XY = 'XY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>XY Interaction.</p>"},{"location":"api/types/#qadence.types.Interaction.XYZ","title":"<code>XYZ = 'XYZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>XYZ Interaction.</p>"},{"location":"api/types/#qadence.types.Interaction.ZZ","title":"<code>ZZ = 'ZZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ZZ-Ising Interaction.</p>"},{"location":"api/types/#qadence.types.LTSOrder","title":"<code>LTSOrder</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Lie-Trotter-Suzuki approximation order.</p>"},{"location":"api/types/#qadence.types.LTSOrder.BASIC","title":"<code>BASIC = 'BASIC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basic.</p>"},{"location":"api/types/#qadence.types.LTSOrder.ST2","title":"<code>ST2 = 'ST2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ST2.</p>"},{"location":"api/types/#qadence.types.LTSOrder.ST4","title":"<code>ST4 = 'ST4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ST4.</p>"},{"location":"api/types/#qadence.types.LatticeTopology","title":"<code>LatticeTopology</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Lattice topologies to choose from for the register.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.ALL_TO_ALL","title":"<code>ALL_TO_ALL = 'all_to_all'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>All to all- connected lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.ARBITRARY","title":"<code>ARBITRARY = 'arbitrary'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Arbitrarily-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.CIRCLE","title":"<code>CIRCLE = 'circle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Circular lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.HONEYCOMB_LATTICE","title":"<code>HONEYCOMB_LATTICE = 'honeycomb_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Honeycomb-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.LINE","title":"<code>LINE = 'line'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Line-format lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.RECTANGULAR_LATTICE","title":"<code>RECTANGULAR_LATTICE = 'rectangular_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Rectangular-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.SQUARE","title":"<code>SQUARE = 'square'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Square lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.TRIANGULAR_LATTICE","title":"<code>TRIANGULAR_LATTICE = 'triangular_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Triangular-shaped shape.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy","title":"<code>MultivariateStrategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Multivariate strategy for feature maps.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy.PARALLEL","title":"<code>PARALLEL = 'Parallel'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parallel strategy.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy.SERIES","title":"<code>SERIES = 'Series'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Serial strategy.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol","title":"<code>NoiseProtocol()</code>  <code>dataclass</code>","text":"<p>Type of noise protocol.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.ANALOG","title":"<code>ANALOG = AnalogNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied in analog blocks.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.DIGITAL","title":"<code>DIGITAL = DigitalNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied to digital blocks.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.READOUT","title":"<code>READOUT = ReadoutNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied on outputs of quantum programs.</p>"},{"location":"api/types/#qadence.types.ObservableTransform","title":"<code>ObservableTransform</code>","text":"<p>Observable transformation type.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.NONE","title":"<code>NONE = 'none'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>No transformation.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.RANGE","title":"<code>RANGE = 'range'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the given values as min and max.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.SCALE","title":"<code>SCALE = 'scale'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the given values as scale and shift.</p>"},{"location":"api/types/#qadence.types.OpName","title":"<code>OpName</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>A list of all available of digital-analog operations.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGENTANG","title":"<code>ANALOGENTANG = 'AnalogEntanglement'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog entanglement operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGINTERACTION","title":"<code>ANALOGINTERACTION = 'AnalogInteraction'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog interaction operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRX","title":"<code>ANALOGRX = 'AnalogRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RX operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRY","title":"<code>ANALOGRY = 'AnalogRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RY operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRZ","title":"<code>ANALOGRZ = 'AnalogRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RZ operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGSWAP","title":"<code>ANALOGSWAP = 'AnalogSWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog SWAP operation.</p>"},{"location":"api/types/#qadence.types.OpName.CNOT","title":"<code>CNOT = 'CNOT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The CNOT gate.</p>"},{"location":"api/types/#qadence.types.OpName.CPHASE","title":"<code>CPHASE = 'CPHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The controlled PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRX","title":"<code>CRX = 'CRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRY","title":"<code>CRY = 'CRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Controlled RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRZ","title":"<code>CRZ = 'CRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.CSWAP","title":"<code>CSWAP = 'CSWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control SWAP gate.</p>"},{"location":"api/types/#qadence.types.OpName.CZ","title":"<code>CZ = 'CZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The CZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.ENTANGLE","title":"<code>ENTANGLE = 'entangle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The entanglement operation.</p>"},{"location":"api/types/#qadence.types.OpName.H","title":"<code>H = 'H'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Hadamard gate.</p>"},{"location":"api/types/#qadence.types.OpName.HAMEVO","title":"<code>HAMEVO = 'HamEvo'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Hamiltonian Evolution operation.</p>"},{"location":"api/types/#qadence.types.OpName.I","title":"<code>I = 'I'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Identity gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCPHASE","title":"<code>MCPHASE = 'MCPHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRX","title":"<code>MCRX = 'MCRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRY","title":"<code>MCRY = 'MCRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRZ","title":"<code>MCRZ = 'MCRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCZ","title":"<code>MCZ = 'MCZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol CZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.N","title":"<code>N = 'N'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The N = (1/2)(I-Z) operator.</p>"},{"location":"api/types/#qadence.types.OpName.PHASE","title":"<code>PHASE = 'PHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.PROJ","title":"<code>PROJ = 'Projector'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The projector operation.</p>"},{"location":"api/types/#qadence.types.OpName.RX","title":"<code>RX = 'RX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.RY","title":"<code>RY = 'RY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.RZ","title":"<code>RZ = 'RZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.S","title":"<code>S = 'S'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The S gate.</p>"},{"location":"api/types/#qadence.types.OpName.SDAGGER","title":"<code>SDAGGER = 'SDagger'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The S dagger gate.</p>"},{"location":"api/types/#qadence.types.OpName.SWAP","title":"<code>SWAP = 'SWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The SWAP gate.</p>"},{"location":"api/types/#qadence.types.OpName.T","title":"<code>T = 'T'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The T gate.</p>"},{"location":"api/types/#qadence.types.OpName.TDAGGER","title":"<code>TDAGGER = 'TDagger'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The T dagger gate.</p>"},{"location":"api/types/#qadence.types.OpName.TOFFOLI","title":"<code>TOFFOLI = 'Toffoli'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Toffoli gate.</p>"},{"location":"api/types/#qadence.types.OpName.U","title":"<code>U = 'U'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The U gate.</p>"},{"location":"api/types/#qadence.types.OpName.X","title":"<code>X = 'X'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The X gate.</p>"},{"location":"api/types/#qadence.types.OpName.Y","title":"<code>Y = 'Y'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Y gate.</p>"},{"location":"api/types/#qadence.types.OpName.Z","title":"<code>Z = 'Z'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Z gate.</p>"},{"location":"api/types/#qadence.types.OpName.ZERO","title":"<code>ZERO = 'Zero'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The zero gate.</p>"},{"location":"api/types/#qadence.types.OverlapMethod","title":"<code>OverlapMethod</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Overlap Methods to choose from.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.COMPUTE_UNCOMPUTE","title":"<code>COMPUTE_UNCOMPUTE = 'compute_uncompute'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Compute-uncompute.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.EXACT","title":"<code>EXACT = 'exact'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exact.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.HADAMARD_TEST","title":"<code>HADAMARD_TEST = 'hadamard_test'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hadamard-test.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.JENSEN_SHANNON","title":"<code>JENSEN_SHANNON = 'jensen_shannon'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Jensen-shannon.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.SWAP_TEST","title":"<code>SWAP_TEST = 'swap_test'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Swap-test.</p>"},{"location":"api/types/#qadence.types.ParameterType","title":"<code>ParameterType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Parameter types available in qadence.</p>"},{"location":"api/types/#qadence.types.ParameterType.FEATURE","title":"<code>FEATURE = 'Feature'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>FeatureParameters act as input and are not trainable.</p>"},{"location":"api/types/#qadence.types.ParameterType.FIXED","title":"<code>FIXED = 'Fixed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fixed/ constant parameters are neither trainable nor act as input.</p>"},{"location":"api/types/#qadence.types.ParameterType.VARIATIONAL","title":"<code>VARIATIONAL = 'Variational'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>VariationalParameters are trainable.</p>"},{"location":"api/types/#qadence.types.QubitSupportType","title":"<code>QubitSupportType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Qubit support types.</p>"},{"location":"api/types/#qadence.types.QubitSupportType.GLOBAL","title":"<code>GLOBAL = 'global'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use global qubit support.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise","title":"<code>ReadoutNoise</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of readout protocol.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise.CORRELATED","title":"<code>CORRELATED = 'Correlated Readout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using a confusion matrix (2n, 2n) for corrupting bitstrings values.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise.INDEPENDENT","title":"<code>INDEPENDENT = 'Independent Readout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Simple readout protocols where each qubit is corrupted independently.</p>"},{"location":"api/types/#qadence.types.ResultType","title":"<code>ResultType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available data types for generating certain results.</p>"},{"location":"api/types/#qadence.types.ResultType.NUMPY","title":"<code>NUMPY = 'Numpy'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Numpy Array Type.</p>"},{"location":"api/types/#qadence.types.ResultType.STRING","title":"<code>STRING = 'String'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String Type.</p>"},{"location":"api/types/#qadence.types.ResultType.TORCH","title":"<code>TORCH = 'Torch'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Torch Tensor Type.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling","title":"<code>ReuploadScaling</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Scaling for data reuploads in feature maps.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.CONSTANT","title":"<code>CONSTANT = 'Constant'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Constant scaling.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.EXP","title":"<code>EXP = 'Exponential'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exponentially increasing scaling.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.TOWER","title":"<code>TOWER = 'Tower'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Linearly increasing scaling.</p>"},{"location":"api/types/#qadence.types.SerializationFormat","title":"<code>SerializationFormat</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available serialization formats for circuits.</p>"},{"location":"api/types/#qadence.types.SerializationFormat.JSON","title":"<code>JSON = 'JSON'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Json format.</p>"},{"location":"api/types/#qadence.types.SerializationFormat.PT","title":"<code>PT = 'PT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PT format used by Torch.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType","title":"<code>StateGeneratorType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Methods to generate random states.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.HAAR_MEASURE_FAST","title":"<code>HAAR_MEASURE_FAST = 'HaarMeasureFast'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>HaarMeasure.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.HAAR_MEASURE_SLOW","title":"<code>HAAR_MEASURE_SLOW = 'HaarMeasureSlow'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>HaarMeasure non-optimized version.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.RANDOM_ROTATIONS","title":"<code>RANDOM_ROTATIONS = 'RandomRotations'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random Rotations.</p>"},{"location":"api/types/#qadence.types.StrEnum","title":"<code>StrEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"api/types/#qadence.types.StrEnum.__str__","title":"<code>__str__()</code>","text":"<p>Used when dumping enum fields in a schema.</p> Source code in <code>qadence/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Used when dumping enum fields in a schema.\"\"\"\n    ret: str = self.value\n    return ret\n</code></pre>"},{"location":"api/types/#qadence.types.Strategy","title":"<code>Strategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Computing paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.ANALOG","title":"<code>ANALOG = 'Analog'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the analog paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.BDAQC","title":"<code>BDAQC = 'bDAQC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the banged digital-analog QC paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.DIGITAL","title":"<code>DIGITAL = 'Digital'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the digital paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.RYDBERG","title":"<code>RYDBERG = 'Rydberg'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the Rydberg QC paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.SDAQC","title":"<code>SDAQC = 'sDAQC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the step-wise digital-analog QC paradigm.</p>"},{"location":"api/types/#qadence.types.TensorType","title":"<code>TensorType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Tensor Types for converting blocks to tensors.</p>"},{"location":"api/types/#qadence.types.TensorType.DENSE","title":"<code>DENSE = 'Dense'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a block to a dense tensor.</p>"},{"location":"api/types/#qadence.types.TensorType.SPARSE","title":"<code>SPARSE = 'Sparse'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a observable block to a sparse tensor.</p>"},{"location":"api/types/#qadence.types.TensorType.SPARSEDIAGONAL","title":"<code>SPARSEDIAGONAL = 'SparseDiagonal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a diagonal observable block to a sparse diagonal if possible.</p>"},{"location":"api/types/#qadence.types._BackendName","title":"<code>_BackendName</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The available backends for running circuits.</p>"},{"location":"api/types/#qadence.types._BackendName.HORQRUX","title":"<code>HORQRUX = 'horqrux'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The horqrux backend.</p>"},{"location":"api/types/#qadence.types._BackendName.PULSER","title":"<code>PULSER = 'pulser'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Pulser backend.</p>"},{"location":"api/types/#qadence.types._BackendName.PYQTORCH","title":"<code>PYQTORCH = 'pyqtorch'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Pyqtorch backend.</p>"},{"location":"api/types/#qadence.types._DiffMode","title":"<code>_DiffMode</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Differentiation modes to choose from.</p>"},{"location":"api/types/#qadence.types._DiffMode.AD","title":"<code>AD = 'ad'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Automatic Differentiation.</p>"},{"location":"api/types/#qadence.types._DiffMode.ADJOINT","title":"<code>ADJOINT = 'adjoint'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Adjoint Differentiation.</p>"},{"location":"api/types/#qadence.types._DiffMode.GPSR","title":"<code>GPSR = 'gpsr'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basic generalized parameter shift rule.</p>"},{"location":"api/backends/backend/","title":"Abstract backend","text":""},{"location":"api/backends/backend/#qadence.backend.Backend","title":"<code>Backend(name, supports_ad, support_bp, supports_adjoint, is_remote, with_measurements, native_endianness, engine, with_noise, config)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract class that defines the interface for the backends.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>backend unique string identifier</p> <p> TYPE: <code>BackendName</code> </p> <code>supports_ad</code> <p>whether or not the backend has a native autograd</p> <p> TYPE: <code>bool</code> </p> <code>supports_bp</code> <p>whether or not the backend has a native backprop</p> <p> TYPE: <code>bool</code> </p> <code>supports_adjoint</code> <p>Does the backend support native adjoint differentation.</p> <p> TYPE: <code>bool</code> </p> <code>is_remote</code> <p>whether computations are executed locally or remotely on this backend, useful when using cloud platforms where credentials are needed for example.</p> <p> TYPE: <code>bool</code> </p> <code>with_measurements</code> <p>whether it supports counts or not</p> <p> TYPE: <code>bool</code> </p> <code>with_noise</code> <p>whether to add realistic noise or not</p> <p> TYPE: <code>bool</code> </p> <code>native_endianness</code> <p>The native endianness of the backend</p> <p> TYPE: <code>Endianness</code> </p> <code>engine</code> <p>The underlying (native) automatic differentiation engine of the backend.</p> <p> TYPE: <code>Engine</code> </p>"},{"location":"api/backends/backend/#qadence.backend.Backend.circuit","title":"<code>circuit(circuit)</code>  <code>abstractmethod</code>","text":"<p>Converts an abstract <code>QuantumCircuit</code> to the native backend representation.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A circuit, for example: <code>QuantumCircuit(2, X(0))</code></p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>A converted circuit <code>c</code>. You can access the original, arbstract circuit via <code>c.abstract</code></p> <code>ConvertedCircuit</code> <p>and the converted (or backend native) circuit via <code>c.native</code>.</p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Converts an abstract `QuantumCircuit` to the native backend representation.\n\n    Arguments:\n        circuit: A circuit, for example: `QuantumCircuit(2, X(0))`\n\n    Returns:\n        A converted circuit `c`. You can access the original, arbstract circuit via `c.abstract`\n        and the converted (or backend *native*) circuit via `c.native`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.convert","title":"<code>convert(circuit, observable=None)</code>","text":"<p>Convert an abstract circuit and an optional observable to their native representation.</p> <p>Additionally, this function constructs an embedding function which maps from user-facing parameters to device parameters (read more on parameter embedding here).</p> Source code in <code>qadence/backend.py</code> <pre><code>def convert(\n    self, circuit: QuantumCircuit, observable: list[AbstractBlock] | AbstractBlock | None = None\n) -&gt; Converted:\n    \"\"\"Convert an abstract circuit and an optional observable to their native representation.\n\n    Additionally, this function constructs an embedding function which maps from\n    user-facing parameters to device parameters (read more on parameter embedding\n    [here][qadence.blocks.embedding.embedding]).\n    \"\"\"\n\n    def check_observable(obs_obj: Any) -&gt; AbstractBlock:\n        if isinstance(obs_obj, QubitOperator):\n            from qadence.blocks.manipulate import from_openfermion\n\n            assert len(obs_obj.terms) &gt; 0, \"Make sure to give a non-empty qubit hamiltonian\"\n\n            return from_openfermion(obs_obj)\n\n        elif isinstance(obs_obj, (CompositeBlock, PrimitiveBlock, ScaleBlock)):\n            from qadence.blocks.utils import block_is_qubit_hamiltonian\n\n            assert block_is_qubit_hamiltonian(\n                obs_obj\n            ), \"Make sure the QubitHamiltonian consists only of Pauli operators X, Y, Z, I\"\n            return obs_obj\n        raise TypeError(\n            \"qubit_hamiltonian should be a Pauli-like AbstractBlock or a QubitOperator\"\n        )\n\n    conv_circ = self.circuit(circuit)\n    circ_params, circ_embedding_fn = embedding(\n        conv_circ.abstract.block, self.config._use_gate_params, self.engine\n    )\n    params = circ_params\n    if observable is not None:\n        observable = observable if isinstance(observable, list) else [observable]\n        conv_obs = []\n        obs_embedding_fn_list = []\n\n        for obs in observable:\n            obs = check_observable(obs)\n            c_obs = self.observable(obs, max(circuit.n_qubits, obs.n_qubits))\n            obs_params, obs_embedding_fn = embedding(\n                c_obs.abstract, self.config._use_gate_params, self.engine\n            )\n            params.update(obs_params)\n            obs_embedding_fn_list.append(obs_embedding_fn)\n            conv_obs.append(c_obs)\n\n        def embedding_fn_dict(a: dict, b: dict) -&gt; dict:\n            embedding_dict = circ_embedding_fn(a, b)\n            for o in obs_embedding_fn_list:\n                embedding_dict.update(o(a, b))\n            return embedding_dict\n\n        return Converted(conv_circ, conv_obs, embedding_fn_dict, params)\n\n    def embedding_fn(a: dict, b: dict) -&gt; dict:\n        return circ_embedding_fn(a, b)\n\n    return Converted(conv_circ, None, embedding_fn, params)\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>  <code>abstractmethod</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.observable","title":"<code>observable(observable, n_qubits)</code>  <code>abstractmethod</code>","text":"<p>Converts an abstract observable (which is just an <code>AbstractBlock</code>) to the native backend.</p> <p>representation.</p> PARAMETER DESCRIPTION <code>observable</code> <p>An observable.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits the observable covers. This is typically <code>circuit.n_qubits</code>.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ConvertedObservable</code> <p>A converted observable <code>o</code>. You can access the original, arbstract observable via</p> <code>ConvertedObservable</code> <p><code>o.abstract</code> and the converted (or backend native) observable via <code>o.native</code>.</p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef observable(self, observable: AbstractBlock, n_qubits: int) -&gt; ConvertedObservable:\n    \"\"\"Converts an abstract observable (which is just an `AbstractBlock`) to the native backend.\n\n    representation.\n\n    Arguments:\n        observable: An observable.\n        n_qubits: Number of qubits the observable covers. This is typically `circuit.n_qubits`.\n\n    Returns:\n        A converted observable `o`. You can access the original, arbstract observable via\n        `o.abstract` and the converted (or backend *native*) observable via `o.native`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.run","title":"<code>run(circuit, param_values={}, state=None, endianness=Endianness.BIG, *args, **kwargs)</code>","text":"<p>Run a circuit and return the resulting wave function.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>dict[str, ArrayLike]</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting wavefunction.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>A list of Counter objects where each key represents a bitstring</p> <code>ArrayLike</code> <p>and its value the number of times it has been sampled from the given wave function.</p> Source code in <code>qadence/backend.py</code> <pre><code>def run(\n    self,\n    circuit: ConvertedCircuit,\n    param_values: dict[str, ArrayLike] = {},\n    state: Tensor | None = None,\n    endianness: Endianness = Endianness.BIG,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ArrayLike:\n    \"\"\"Run a circuit and return the resulting wave function.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        endianness: Endianness of the resulting wavefunction.\n\n    Returns:\n        A list of Counter objects where each key represents a bitstring\n        and its value the number of times it has been sampled from the given wave function.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.sample","title":"<code>sample(circuit, param_values={}, n_shots=1000, state=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>  <code>abstractmethod</code>","text":"<p>Sample bit strings.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>n_shots</code> <p>Number of shots to sample.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>An error mitigation protocol to apply.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    circuit: ConvertedCircuit,\n    param_values: dict[str, Tensor] = {},\n    n_shots: int = 1000,\n    state: ArrayLike | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; list[Counter]:\n    \"\"\"Sample bit strings.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        n_shots: Number of shots to sample.\n        state: Initial state.\n        noise: A noise model to use.\n        mitigation: An error mitigation protocol to apply.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration","title":"<code>BackendConfiguration(_use_gate_params=True, use_sparse_observable=False, use_gradient_checkpointing=False, use_single_qubit_composition=False, transpilation_passes=None)</code>  <code>dataclass</code>","text":""},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration.available_options","title":"<code>available_options()</code>","text":"<p>Return as a string the available fields with types of the configuration.</p> RETURNS DESCRIPTION <code>str</code> <p>a string with all the available fields, one per line</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>def available_options(self) -&gt; str:\n    \"\"\"Return as a string the available fields with types of the configuration.\n\n    Returns:\n        str: a string with all the available fields, one per line\n    \"\"\"\n    conf_msg = \"\"\n    for _field in fields(self):\n        if not _field.name.startswith(\"_\"):\n            conf_msg += (\n                f\"Name: {_field.name} - Type: {_field.type} - Default value: {_field.default}\\n\"\n            )\n    return conf_msg\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration.get_param_name","title":"<code>get_param_name(blk)</code>","text":"<p>Return parameter names for the current backend.</p> <p>Depending on which backend is in use this function returns either UUIDs or expressions of parameters.</p> Source code in <code>qadence/backend.py</code> <pre><code>def get_param_name(self, blk: AbstractBlock) -&gt; Tuple[str, ...]:\n    \"\"\"Return parameter names for the current backend.\n\n    Depending on which backend is in use this\n    function returns either UUIDs or expressions of parameters.\n    \"\"\"\n    param_ids: Tuple\n    # FIXME: better type hiearchy?\n    types = (TimeEvolutionBlock, ParametricBlock, ConstantAnalogRotation, InteractionBlock)\n    if not isinstance(blk, types):\n        raise TypeError(f\"Can not infer param name from {type(blk)}\")\n    else:\n        if self._use_gate_params:\n            param_ids = tuple(blk.parameters.uuids())\n        else:\n            param_ids = tuple(map(stringify, blk.parameters.expressions()))\n    return param_ids\n</code></pre>"},{"location":"api/backends/differentiable/","title":"DifferentiableBackend","text":""},{"location":"api/backends/differentiable/#qadence.engines.torch.differentiable_backend.DifferentiableBackend","title":"<code>DifferentiableBackend(backend, diff_mode=DiffMode.AD, **psr_args)</code>","text":"<p>               Bases: <code>DifferentiableBackend</code></p> <p>A class which wraps a QuantumBackend with the automatic differentation engine TORCH.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of the QuantumBackend type perform execution.</p> <p> TYPE: <code>Backend</code> </p> <code>diff_mode</code> <p>A differentiable mode supported by the differentiation engine.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>**psr_args</code> <p>Arguments that will be passed on to <code>DifferentiableExpectation</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>{}</code> </p> Source code in <code>qadence/engines/torch/differentiable_backend.py</code> <pre><code>def __init__(\n    self,\n    backend: QuantumBackend,\n    diff_mode: DiffMode = DiffMode.AD,\n    **psr_args: int | float | None,\n) -&gt; None:\n    super().__init__(backend=backend, engine=Engine.TORCH, diff_mode=diff_mode)\n    self.psr_args = psr_args\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.torch.differentiable_backend.DifferentiableBackend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>observable</code> <p>A converted observable as returned by <code>backend.observable</code>.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>The error mitigation to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/engines/torch/differentiable_backend.py</code> <pre><code>def expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        observable: A converted observable as returned by `backend.observable`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: The error mitigation to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    observable = observable if isinstance(observable, list) else [observable]\n    differentiable_expectation = DifferentiableExpectation(\n        backend=self.backend,\n        circuit=circuit,\n        observable=observable,\n        param_values=param_values,\n        state=state,\n        measurement=measurement,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n\n    if self.diff_mode == DiffMode.AD:\n        expectation = differentiable_expectation.ad\n    elif self.diff_mode == DiffMode.ADJOINT:\n        expectation = differentiable_expectation.adjoint\n    else:\n        try:\n            fns = get_gpsr_fns()\n            psr_fn = fns[self.diff_mode]\n        except KeyError:\n            raise ValueError(f\"{self.diff_mode} differentiation mode is not supported\")\n        expectation = partial(differentiable_expectation.psr, psr_fn=psr_fn, **self.psr_args)\n    return expectation()\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.jax.differentiable_backend.DifferentiableBackend","title":"<code>DifferentiableBackend(backend, diff_mode=DiffMode.AD, **psr_args)</code>","text":"<p>               Bases: <code>DifferentiableBackend</code></p> <p>A class which wraps a QuantumBackend with the automatic differentation engine JAX.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of the QuantumBackend type perform execution.</p> <p> TYPE: <code>Backend</code> </p> <code>diff_mode</code> <p>A differentiable mode supported by the differentiation engine.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>**psr_args</code> <p>Arguments that will be passed on to <code>DifferentiableExpectation</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>{}</code> </p> Source code in <code>qadence/engines/jax/differentiable_backend.py</code> <pre><code>def __init__(\n    self,\n    backend: Backend,\n    diff_mode: DiffMode = DiffMode.AD,\n    **psr_args: int | float | None,\n) -&gt; None:\n    super().__init__(backend=backend, engine=Engine.JAX, diff_mode=diff_mode)\n    self.psr_args = psr_args\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.jax.differentiable_backend.DifferentiableBackend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>observable</code> <p>A converted observable as returned by <code>backend.observable</code>.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>The error mitigation to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/engines/jax/differentiable_backend.py</code> <pre><code>def expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        observable: A converted observable as returned by `backend.observable`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: The error mitigation to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    observable = observable if isinstance(observable, list) else [observable]\n\n    if self.diff_mode == DiffMode.AD:\n        expectation = self.backend.expectation(circuit, observable, param_values, state)\n    else:\n        expectation = DifferentiableExpectation(\n            backend=self.backend,\n            circuit=circuit,\n            observable=observable,\n            param_values=param_values,\n            state=state,\n            measurement=measurement,\n            noise=noise,\n            mitigation=mitigation,\n            endianness=endianness,\n        ).psr()\n    return expectation\n</code></pre>"},{"location":"api/backends/pulser/","title":"Pulser","text":"<p>The Pulser backend features a basic integration with the pulse-level programming interface Pulser. This backend offers for now few simple operations which are translated into a valid, non time-dependent pulse sequence. In particular, one has access to:</p> <ul> <li>analog rotations: <code>AnalogRx</code> and <code>AnalogRy</code> blocks</li> <li>free evolution blocks (basically no pulse, just interaction): <code>AnalogWait</code> block</li> <li>a block for creating entangled states: <code>AnalogEntanglement</code></li> <li>digital rotation <code>Rx</code> and <code>Ry</code></li> </ul>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend.Backend","title":"<code>Backend(name=BackendName.PULSER, supports_ad=False, support_bp=False, supports_adjoint=False, is_remote=False, with_measurements=True, native_endianness=Endianness.BIG, engine=Engine.TORCH, with_noise=False, config=Configuration())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Backend</code></p> <p>The Pulser backend.</p>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend._convert_init_state","title":"<code>_convert_init_state(state)</code>","text":"<p>Flip and squeeze initial state consistent with Pulser convention.</p> Source code in <code>qadence/backends/pulser/backend.py</code> <pre><code>def _convert_init_state(state: Tensor) -&gt; np.ndarray:\n    \"\"\"Flip and squeeze initial state consistent with Pulser convention.\"\"\"\n    if state.shape[0] &gt; 1:\n        raise ValueError(\"Pulser backend only supports initial states with batch size 1.\")\n    return np.flip(state.cpu().squeeze().numpy())\n</code></pre>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend.create_register","title":"<code>create_register(register)</code>","text":"<p>Convert Qadence Register to Pulser Register.</p> Source code in <code>qadence/backends/pulser/backend.py</code> <pre><code>def create_register(register: Register) -&gt; PulserRegister:\n    \"\"\"Convert Qadence Register to Pulser Register.\"\"\"\n    coords = np.array(list(register.coords.values()))\n    return PulserRegister.from_coordinates(coords)\n</code></pre>"},{"location":"api/backends/pyqtorch/","title":"PyQTorch","text":"<p>Fast differentiable statevector emulator based on PyTorch. The code is open source, hosted on Github and maintained by Pasqal.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend","title":"<code>Backend(name=BackendName.PYQTORCH, supports_ad=True, support_bp=True, supports_adjoint=True, is_remote=False, with_measurements=True, native_endianness=Endianness.BIG, engine=Engine.TORCH, with_noise=False, config=Configuration())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Backend</code></p> <p>PyQTorch backend.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend.circuit","title":"<code>circuit(circuit)</code>","text":"<p>Return the converted circuit.</p> <p>Note that to get a representation with noise, noise should be passed within the config.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Original circuit</p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>ConvertedCircuit instance for backend.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Return the converted circuit.\n\n    Note that to get a representation with noise, noise\n    should be passed within the config.\n\n    Args:\n        circuit (QuantumCircuit): Original circuit\n\n    Returns:\n        ConvertedCircuit: ConvertedCircuit instance for backend.\n    \"\"\"\n    passes = self.config.transpilation_passes\n    if passes is None:\n        passes = default_passes(self.config)\n\n    original_circ = circuit\n    if len(passes) &gt; 0:\n        circuit = transpile(*passes)(circuit)\n    # Setting noise in the circuit.\n    if self.config.noise:\n        set_noise(circuit, self.config.noise)\n\n    ops = convert_block(circuit.block, n_qubits=circuit.n_qubits, config=self.config)\n    readout_noise = (\n        convert_readout_noise(circuit.n_qubits, self.config.noise)\n        if self.config.noise\n        else None\n    )\n    if self.config.dropout_probability == 0:\n        native = pyq.QuantumCircuit(\n            circuit.n_qubits,\n            ops,\n            readout_noise,\n        )\n    else:\n        native = pyq.DropoutQuantumCircuit(\n            circuit.n_qubits,\n            ops,\n            readout_noise,\n            dropout_prob=self.config.dropout_probability,\n            dropout_mode=self.config.dropout_mode,\n        )\n    return ConvertedCircuit(native=native, abstract=circuit, original=original_circ)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend.convert","title":"<code>convert(circuit, observable=None)</code>","text":"<p>Convert an abstract circuit and an optional observable to their native representation.</p> <p>Additionally, this function constructs an embedding function which maps from user-facing parameters to device parameters (read more on parameter embedding here).</p> Source code in <code>qadence/backend.py</code> <pre><code>def convert(\n    self, circuit: QuantumCircuit, observable: list[AbstractBlock] | AbstractBlock | None = None\n) -&gt; Converted:\n    \"\"\"Convert an abstract circuit and an optional observable to their native representation.\n\n    Additionally, this function constructs an embedding function which maps from\n    user-facing parameters to device parameters (read more on parameter embedding\n    [here][qadence.blocks.embedding.embedding]).\n    \"\"\"\n\n    def check_observable(obs_obj: Any) -&gt; AbstractBlock:\n        if isinstance(obs_obj, QubitOperator):\n            from qadence.blocks.manipulate import from_openfermion\n\n            assert len(obs_obj.terms) &gt; 0, \"Make sure to give a non-empty qubit hamiltonian\"\n\n            return from_openfermion(obs_obj)\n\n        elif isinstance(obs_obj, (CompositeBlock, PrimitiveBlock, ScaleBlock)):\n            from qadence.blocks.utils import block_is_qubit_hamiltonian\n\n            assert block_is_qubit_hamiltonian(\n                obs_obj\n            ), \"Make sure the QubitHamiltonian consists only of Pauli operators X, Y, Z, I\"\n            return obs_obj\n        raise TypeError(\n            \"qubit_hamiltonian should be a Pauli-like AbstractBlock or a QubitOperator\"\n        )\n\n    conv_circ = self.circuit(circuit)\n    circ_params, circ_embedding_fn = embedding(\n        conv_circ.abstract.block, self.config._use_gate_params, self.engine\n    )\n    params = circ_params\n    if observable is not None:\n        observable = observable if isinstance(observable, list) else [observable]\n        conv_obs = []\n        obs_embedding_fn_list = []\n\n        for obs in observable:\n            obs = check_observable(obs)\n            c_obs = self.observable(obs, max(circuit.n_qubits, obs.n_qubits))\n            obs_params, obs_embedding_fn = embedding(\n                c_obs.abstract, self.config._use_gate_params, self.engine\n            )\n            params.update(obs_params)\n            obs_embedding_fn_list.append(obs_embedding_fn)\n            conv_obs.append(c_obs)\n\n        def embedding_fn_dict(a: dict, b: dict) -&gt; dict:\n            embedding_dict = circ_embedding_fn(a, b)\n            for o in obs_embedding_fn_list:\n                embedding_dict.update(o(a, b))\n            return embedding_dict\n\n        return Converted(conv_circ, conv_obs, embedding_fn_dict, params)\n\n    def embedding_fn(a: dict, b: dict) -&gt; dict:\n        return circ_embedding_fn(a, b)\n\n    return Converted(conv_circ, None, embedding_fn, params)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_block_and_readout_noises","title":"<code>set_block_and_readout_noises(circuit, noise, config)</code>","text":"<p>Add noise on blocks and readout on circuit.</p> <p>We first start by adding noise to the abstract blocks. Then we do a conversion to their native representation. Finally, we add readout.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>noise</code> <p>Noise to add.</p> <p> TYPE: <code>NoiseHandler | None</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_block_and_readout_noises(\n    circuit: ConvertedCircuit, noise: NoiseHandler | None, config: Configuration\n) -&gt; None:\n    \"\"\"Add noise on blocks and readout on circuit.\n\n    We first start by adding noise to the abstract blocks. Then we do a conversion to their\n    native representation. Finally, we add readout.\n\n    Args:\n        circuit (ConvertedCircuit): Input circuit.\n        noise (NoiseHandler | None): Noise to add.\n    \"\"\"\n    if noise:\n        set_noise(circuit, noise)\n        set_noise_abstract_to_native(circuit, config)\n        set_readout_noise(circuit, noise)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_noise_abstract_to_native","title":"<code>set_noise_abstract_to_native(circuit, config)</code>","text":"<p>Set noise in native blocks from the abstract ones with noise.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input converted circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_noise_abstract_to_native(circuit: ConvertedCircuit, config: Configuration) -&gt; None:\n    \"\"\"Set noise in native blocks from the abstract ones with noise.\n\n    Args:\n        circuit (ConvertedCircuit): Input converted circuit.\n    \"\"\"\n    ops = convert_block(circuit.abstract.block, n_qubits=circuit.native.n_qubits, config=config)\n    circuit.native = pyq.QuantumCircuit(circuit.native.n_qubits, ops, circuit.native.readout_noise)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_readout_noise","title":"<code>set_readout_noise(circuit, noise)</code>","text":"<p>Set readout noise in place in native.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input converted circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>noise</code> <p>Noise.</p> <p> TYPE: <code>NoiseHandler | None</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_readout_noise(circuit: ConvertedCircuit, noise: NoiseHandler) -&gt; None:\n    \"\"\"Set readout noise in place in native.\n\n    Args:\n        circuit (ConvertedCircuit):  Input converted circuit.\n        noise (NoiseHandler | None): Noise.\n    \"\"\"\n    readout = convert_readout_noise(circuit.abstract.n_qubits, noise)\n    if readout:\n        circuit.native.readout_noise = readout\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration","title":"<code>Configuration(_use_gate_params=True, use_sparse_observable=False, use_gradient_checkpointing=False, use_single_qubit_composition=False, transpilation_passes=None, algo_hevo=AlgoHEvo.EXP, ode_solver=SolverType.DP5_SE, n_steps_hevo=100, loop_expectation=False, noise=None, dropout_probability=0.0, dropout_mode=DropoutMode.ROTATIONAL)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BackendConfiguration</code></p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.algo_hevo","title":"<code>algo_hevo = AlgoHEvo.EXP</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine which kind of Hamiltonian evolution algorithm to use.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.dropout_mode","title":"<code>dropout_mode = DropoutMode.ROTATIONAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of quantum dropout to perform.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.dropout_probability","title":"<code>dropout_probability = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantum dropout probability (0 means no dropout).</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.loop_expectation","title":"<code>loop_expectation = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When computing batches of expectation values, only allocate one wavefunction.</p> <p>Loop over the batch of parameters to only allocate a single wavefunction at any given time.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.n_steps_hevo","title":"<code>n_steps_hevo = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default number of steps for the Hamiltonian evolution.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.noise","title":"<code>noise = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NoiseHandler containing readout noise applied in backend.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.ode_solver","title":"<code>ode_solver = SolverType.DP5_SE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine which ODE solver to use for time-dependent blocks.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.use_gradient_checkpointing","title":"<code>use_gradient_checkpointing = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use gradient checkpointing.</p> <p>Recommended for higher-order optimization tasks.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.use_single_qubit_composition","title":"<code>use_single_qubit_composition = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Composes chains of single qubit gates into a single matmul if possible.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.supported_gates","title":"<code>supported_gates = list(set(OpName.list()) - set([OpName.TDAGGER]))</code>  <code>module-attribute</code>","text":"<p>The set of supported gates.</p> <p>Tdagger is currently not supported.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_block","title":"<code>convert_block(block, n_qubits=None, config=None)</code>","text":"<p>Convert block to native Pyqtorch representation.</p> PARAMETER DESCRIPTION <code>block</code> <p>Block to convert.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Backend configuration instance. Defaults to None.</p> <p> TYPE: <code>Configuration</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>For non supported blocks.</p> RETURNS DESCRIPTION <code>Sequence[Module | Tensor | str | Expr]</code> <p>Sequence[Module | Tensor | str | sympy.Expr]: List of native operations.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_block(\n    block: AbstractBlock,\n    n_qubits: int = None,\n    config: Configuration = None,\n) -&gt; Sequence[Module | Tensor | str | sympy.Expr]:\n    \"\"\"Convert block to native Pyqtorch representation.\n\n    Args:\n        block (AbstractBlock): Block to convert.\n        n_qubits (int, optional): Number of qubits. Defaults to None.\n        config (Configuration, optional): Backend configuration instance. Defaults to None.\n\n    Raises:\n        NotImplementedError: For non supported blocks.\n\n    Returns:\n        Sequence[Module | Tensor | str | sympy.Expr]: List of native operations.\n    \"\"\"\n    if isinstance(block, (Tensor, str, sympy.Expr)):  # case for hamevo generators\n        if isinstance(block, Tensor):\n            block = block.permute(1, 2, 0)  # put batch size in the back\n        return [block]\n    qubit_support = block.qubit_support\n    if n_qubits is None:\n        n_qubits = max(qubit_support) + 1\n\n    if config is None:\n        config = Configuration()\n\n    noise: NoiseHandler | None = None\n    if hasattr(block, \"noise\") and block.noise:\n        noise = convert_digital_noise(block.noise)\n\n    if isinstance(block, ScaleBlock):\n        scaled_ops = convert_block(block.block, n_qubits, config)\n        scale = extract_parameter(block, config=config)\n\n        # replace underscore by dot when underscore is between two numbers in string\n        if isinstance(scale, str):\n            scale = replace_underscore_floats(scale)\n\n        if isinstance(scale, str) and not config._use_gate_params:\n            param = sympy_to_pyq(sympy.parse_expr(scale))\n        else:\n            param = scale\n\n        return [pyq.Scale(pyq.Sequence(scaled_ops), param)]\n\n    elif isinstance(block, TimeEvolutionBlock):\n        duration = block.duration  # type: ignore [attr-defined]\n        if getattr(block.generator, \"is_time_dependent\", False):\n            config._use_gate_params = False\n            duration = config.get_param_name(block)[1]\n            generator = convert_block(block.generator, config=config)[0]  # type: ignore [arg-type]\n        elif isinstance(block.generator, sympy.Basic):\n            generator = config.get_param_name(block)[1]\n\n        elif isinstance(block.generator, Tensor):\n            m = block.generator.to(dtype=cdouble)\n            generator = convert_block(\n                MatrixBlock(\n                    m,\n                    qubit_support=qubit_support,\n                    check_unitary=False,\n                    check_hermitian=True,\n                )\n            )[0]\n        else:\n            generator = convert_block(block.generator, n_qubits, config)[0]  # type: ignore[arg-type]\n        time_param = config.get_param_name(block)[0]\n\n        # convert noise operators here\n        noise_operators: list = [\n            convert_block(noise_block, config=config)[0] for noise_block in block.noise_operators\n        ]\n        if len(noise_operators) &gt; 0:\n            # squeeze batch size for noise operators\n            noise_operators = [\n                pyq_op.tensor(full_support=qubit_support).squeeze(-1) for pyq_op in noise_operators\n            ]\n\n        return [\n            pyq.HamiltonianEvolution(\n                qubit_support=qubit_support,\n                generator=generator,\n                time=time_param,\n                cache_length=0,\n                duration=duration,\n                solver=config.ode_solver,\n                steps=config.n_steps_hevo,\n                noise=noise_operators if len(noise_operators) &gt; 0 else None,\n            )\n        ]\n\n    elif isinstance(block, MatrixBlock):\n        return [pyq.primitives.Primitive(block.matrix, block.qubit_support, noise=noise)]\n    elif isinstance(block, CompositeBlock):\n        ops = list(flatten(*(convert_block(b, n_qubits, config) for b in block.blocks)))\n        if isinstance(block, AddBlock):\n            return [pyq.Add(ops)]  # add\n        elif is_single_qubit_chain(block) and config.use_single_qubit_composition:\n            return [pyq.Merge(ops)]  # for chains of single qubit ops on the same qubit\n        else:\n            return [pyq.Sequence(ops)]  # for kron and chain\n    elif isinstance(block, tuple(non_unitary_gateset)):\n        if isinstance(block, ProjectorBlock):\n            projector = getattr(pyq, block.name)\n            if block.name == OpName.N:\n                return [projector(target=qubit_support, noise=noise)]\n            else:\n                return [\n                    projector(\n                        qubit_support=qubit_support,\n                        ket=block.ket,\n                        bra=block.bra,\n                        noise=noise,\n                    )\n                ]\n        else:\n            return [getattr(pyq, block.name)(qubit_support[0])]\n    elif isinstance(block, tuple(single_qubit_gateset)):\n        pyq_cls = getattr(pyq, block.name)\n        if isinstance(block, ParametricBlock):\n            if isinstance(block, U):\n                op = pyq_cls(\n                    qubit_support[0],\n                    *config.get_param_name(block),\n                    noise=noise,\n                )\n            else:\n                param = extract_parameter(block, config)\n                op = pyq_cls(qubit_support[0], param, noise=noise)\n        else:\n            op = pyq_cls(qubit_support[0], noise=noise)  # type: ignore [attr-defined]\n        return [op]\n    elif isinstance(block, tuple(two_qubit_gateset)):\n        pyq_cls = getattr(pyq, block.name)\n        if isinstance(block, ParametricBlock):\n            op = pyq_cls(\n                qubit_support[0],\n                qubit_support[1],\n                extract_parameter(block, config),\n                noise=noise,\n            )\n        else:\n            op = pyq_cls(\n                qubit_support[0], qubit_support[1], noise=noise  # type: ignore [attr-defined]\n            )\n        return [op]\n    elif isinstance(block, tuple(three_qubit_gateset) + tuple(multi_qubit_gateset)):\n        block_name = block.name[1:] if block.name.startswith(\"M\") else block.name\n        pyq_cls = getattr(pyq, block_name)\n        if isinstance(block, ParametricBlock):\n            op = pyq_cls(\n                qubit_support[:-1],\n                qubit_support[-1],\n                extract_parameter(block, config),\n                noise=noise,\n            )\n        else:\n            if \"CSWAP\" in block_name:\n                op = pyq_cls(\n                    qubit_support[:-2], qubit_support[-2:], noise=noise  # type: ignore [attr-defined]\n                )\n            else:\n                op = pyq_cls(\n                    qubit_support[:-1], qubit_support[-1], noise=noise  # type: ignore [attr-defined]\n                )\n        return [op]\n    else:\n        raise NotImplementedError(\n            f\"Non supported operation of type {type(block)}. \"\n            \"In case you are trying to run an `AnalogBlock`, make sure you \"\n            \"specify the `device_specs` in your `Register` first.\"\n        )\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_digital_noise","title":"<code>convert_digital_noise(noise)</code>","text":"<p>Convert the digital noise into pyqtorch NoiseProtocol.</p> PARAMETER DESCRIPTION <code>noise</code> <p>Noise to convert.</p> <p> TYPE: <code>NoiseHandler</code> </p> RETURNS DESCRIPTION <code>DigitalNoiseProtocol | None</code> <p>pyq.noise.DigitalNoiseProtocol | None: Pyqtorch native noise protocol if there are any digital noise protocols.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_digital_noise(noise: NoiseHandler) -&gt; pyq.noise.DigitalNoiseProtocol | None:\n    \"\"\"Convert the digital noise into pyqtorch NoiseProtocol.\n\n    Args:\n        noise (NoiseHandler): Noise to convert.\n\n    Returns:\n        pyq.noise.DigitalNoiseProtocol | None: Pyqtorch native noise protocol\n            if there are any digital noise protocols.\n    \"\"\"\n    digital_part = noise.filter(NoiseProtocol.DIGITAL)\n    if digital_part is None:\n        return None\n    return pyq.noise.DigitalNoiseProtocol(\n        [\n            pyq.noise.DigitalNoiseProtocol(proto, option.get(\"error_probability\"))\n            for proto, option in zip(digital_part.protocol, digital_part.options)\n        ]\n    )\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_readout_noise","title":"<code>convert_readout_noise(n_qubits, noise)</code>","text":"<p>Convert the readout noise into pyqtorch ReadoutNoise.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Number of qubits</p> <p> TYPE: <code>int</code> </p> <code>noise</code> <p>Noise to convert.</p> <p> TYPE: <code>NoiseHandler</code> </p> RETURNS DESCRIPTION <code>ReadoutNoise | None</code> <p>pyq.noise.ReadoutNoise | None: Pyqtorch native ReadoutNoise instance if readout is is noise.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_readout_noise(n_qubits: int, noise: NoiseHandler) -&gt; pyq.noise.ReadoutNoise | None:\n    \"\"\"Convert the readout noise into pyqtorch ReadoutNoise.\n\n    Args:\n        n_qubits (int): Number of qubits\n        noise (NoiseHandler):  Noise to convert.\n\n    Returns:\n        pyq.noise.ReadoutNoise | None: Pyqtorch native ReadoutNoise instance\n            if readout is is noise.\n    \"\"\"\n    readout_part = noise.filter(NoiseProtocol.READOUT)\n    if readout_part is None:\n        return None\n\n    if readout_part.protocol[0] == NoiseProtocol.READOUT.INDEPENDENT:\n        return pyq.noise.ReadoutNoise(n_qubits, **readout_part.options[0])\n    else:\n        return pyq.noise.CorrelatedReadoutNoise(**readout_part.options[0])\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.extract_parameter","title":"<code>extract_parameter(block, config)</code>","text":"<p>Extract the parameter as string or its tensor value.</p> PARAMETER DESCRIPTION <code>block</code> <p>Block to extract parameter from.</p> <p> TYPE: <code>ScaleBlock | ParametricBlock</code> </p> <code>config</code> <p>Configuration instance.</p> <p> TYPE: <code>Configuration</code> </p> RETURNS DESCRIPTION <code>str | Tensor</code> <p>str | Tensor: Parameter value or symbol.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def extract_parameter(block: ScaleBlock | ParametricBlock, config: Configuration) -&gt; str | Tensor:\n    \"\"\"Extract the parameter as string or its tensor value.\n\n    Args:\n        block (ScaleBlock | ParametricBlock): Block to extract parameter from.\n        config (Configuration): Configuration instance.\n\n    Returns:\n        str | Tensor: Parameter value or symbol.\n    \"\"\"\n    if not block.is_parametric:\n        tensor_val = tensor([block.parameters.parameter], dtype=complex64)\n        return (\n            tensor([block.parameters.parameter], dtype=float64)\n            if torch.all(tensor_val.imag == 0)\n            else tensor_val\n        )\n\n    return config.get_param_name(block)[0]\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.replace_underscore_floats","title":"<code>replace_underscore_floats(s)</code>","text":"<p>Replace underscores with periods for all floats in given string.</p> <p>Needed for correct parsing of string by sympy parser.</p> PARAMETER DESCRIPTION <code>s</code> <p>string expression</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>transformed string expression</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def replace_underscore_floats(s: str) -&gt; str:\n    \"\"\"Replace underscores with periods for all floats in given string.\n\n    Needed for correct parsing of string by sympy parser.\n\n    Args:\n        s (str): string expression\n\n    Returns:\n        str: transformed string expression\n    \"\"\"\n\n    # Regular expression to match floats written with underscores instead of dots\n    float_with_underscore_pattern = r\"\"\"\n        (?&lt;!\\w)            # Negative lookbehind to ensure not part of a word\n        -?                 # Optional negative sign\n        \\d+                # One or more digits (before underscore)\n        _                  # The underscore acting as decimal separator\n        \\d+                # One or more digits (after underscore)\n        ([eE][-+]?\\d+)?    # Optional exponent part for scientific notation\n        (?!\\w)             # Negative lookahead to ensure not part of a word\n    \"\"\"\n\n    # Function to replace the underscore with a dot\n    def underscore_to_dot(match: re.Match) -&gt; Any:\n        return match.group(0).replace(\"_\", \".\")\n\n    # Compile the regular expression\n    pattern = re.compile(float_with_underscore_pattern, re.VERBOSE)\n\n    return pattern.sub(underscore_to_dot, s)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.sympy_to_pyq","title":"<code>sympy_to_pyq(expr)</code>","text":"<p>Convert sympy expression to pyqtorch ConcretizedCallable object.</p> PARAMETER DESCRIPTION <code>expr</code> <p>sympy expression</p> <p> TYPE: <code>Expr</code> </p> RETURNS DESCRIPTION <code>ConcretizedCallable</code> <p>expression encoded as ConcretizedCallable</p> <p> TYPE: <code>ConcretizedCallable | Tensor</code> </p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def sympy_to_pyq(expr: sympy.Expr) -&gt; ConcretizedCallable | Tensor:\n    \"\"\"Convert sympy expression to pyqtorch ConcretizedCallable object.\n\n    Args:\n        expr (sympy.Expr): sympy expression\n\n    Returns:\n        ConcretizedCallable: expression encoded as ConcretizedCallable\n    \"\"\"\n\n    # base case - independent argument\n    if len(expr.args) == 0:\n        try:\n            res = torch.as_tensor(float(expr))\n        except Exception as e:\n            res = str(expr)\n\n            if \"/\" in res:  # Found a rational\n                res = torch.as_tensor(float(sympy.Rational(res).evalf()))\n        return res\n\n    # Recursively iterate through current function arguments\n    all_results = []\n    for arg in expr.args:\n        res = sympy_to_pyq(arg)\n        all_results.append(res)\n\n    # deal with multi-argument (&gt;2) sympy functions: converting to nested\n    # ConcretizedCallable objects\n    if len(all_results) &gt; 2:\n\n        def fn(x: str | ConcretizedCallable, y: str | ConcretizedCallable) -&gt; Callable:\n            return partial(ConcretizedCallable, call_name=SYMPY_TO_PYQ_MAPPING[expr.func])(  # type: ignore [no-any-return]\n                abstract_args=[x, y]\n            )\n\n        concretized_callable = reduce(fn, all_results)\n    else:\n        concretized_callable = ConcretizedCallable(SYMPY_TO_PYQ_MAPPING[expr.func], all_results)\n    return concretized_callable\n</code></pre>"},{"location":"content/backends/","title":"Backends","text":"<p>Backends allow execution of Qadence abstract quantum circuits. They could be chosen from a variety of simulators, emulators and hardware and can enable circuit differentiability. The primary way to interact and configure a backend is via the high-level API <code>QuantumModel</code>.</p> <p>Not all backends are equivalent</p> <p>Not all backends support the same set of operations, especially while executing analog blocks. Qadence will throw descriptive errors in such cases.</p>"},{"location":"content/backends/#execution-backends","title":"Execution backends","text":"<p>PyQTorch: An efficient, large-scale simulator designed for quantum machine learning, seamlessly integrated with the popular PyTorch deep learning framework for automatic differentiability. It also offers analog computing for time-(in)dependent pulses. See <code>PyQTorchBackend</code>.</p> <p>Pulser: A Python library for pulse-level/analog control of neutral atom devices. Execution via QuTiP. See <code>PulserBackend</code>.</p> <p>More: Proprietary Qadence extensions provide more high-performance backends based on tensor networks or differentiation engines. For more enquiries, please contact: <code>info@pasqal.com</code>.</p>"},{"location":"content/backends/#differentiation-backend","title":"Differentiation backend","text":"<p>The <code>DifferentiableBackend</code> class enables different differentiation modes for the given backend. This can be chosen from two types:</p> <ul> <li>Automatic differentiation (AD): available for PyTorch based backends (PyQTorch).</li> <li>Parameter Shift Rules (PSR): available for all backends. See this section for more information on differentiability and PSR.</li> </ul> <p>In practice, only a <code>diff_mode</code> should be provided in the <code>QuantumModel</code>. Please note that <code>diff_mode</code> defaults to <code>None</code>:</p> <pre><code>import sympy\nimport torch\nfrom qadence import Parameter, RX, RZ, Z, CNOT, QuantumCircuit, QuantumModel, chain, BackendName, DiffMode\n\nx = Parameter(\"x\", trainable=False)\ny = Parameter(\"y\", trainable=False)\nfm = chain(\n    RX(0, 3 * x),\n    RX(0, x),\n    RZ(1, sympy.exp(y)),\n    RX(0, 3.14),\n    RZ(1, \"theta\")\n)\n\nansatz = CNOT(0, 1)\nblock = chain(fm, ansatz)\n\ncircuit = QuantumCircuit(2, block)\n\nobservable = Z(0)\n\n# DiffMode.GPSR is available for any backend.\n# DiffMode.AD is only available for natively differentiable backends.\nmodel = QuantumModel(circuit, observable, backend=BackendName.PYQTORCH, diff_mode=DiffMode.GPSR)\n\n# Get some values for the feature parameters.\nvalues = {\"x\": (x := torch.tensor([0.5], requires_grad=True)), \"y\": torch.tensor([0.1])}\n\n# Compute expectation.\nexp = model.expectation(values)\n\n# Differentiate the expectation wrt x.\ndexp_dx = torch.autograd.grad(exp, x, torch.ones_like(exp))\n</code></pre> <pre><code>dexp_dx = (tensor([3.6398]),)\n</code></pre>"},{"location":"content/backends/#low-level-backend_factory-interface","title":"Low-level <code>backend_factory</code> interface","text":"<p>Every backend in Qadence inherits from the abstract <code>Backend</code> class: <code>Backend</code> and implement the following methods:</p> <ul> <li><code>run</code>: propagate the initial state according to the quantum circuit and return the final wavefunction object.</li> <li><code>sample</code>: sample from a circuit.</li> <li><code>expectation</code>: computes the expectation of a circuit given an observable.</li> <li><code>convert</code>: convert the abstract <code>QuantumCircuit</code> object to its backend-native representation including a backend specific parameter embedding function.</li> </ul> <p>Backends are purely functional objects which take as input the values for the circuit parameters and return the desired output from a call to a method. In order to use a backend directly, embedded parameters must be supplied as they are returned by the backend specific embedding function.</p> <p>Here is a simple demonstration of the use of the PyQTorch backend to execute a circuit in non-differentiable mode:</p> <pre><code>from qadence import QuantumCircuit, FeatureParameter, RX, RZ, CNOT, hea, chain\n\n# Construct a feature map.\nx = FeatureParameter(\"x\")\nz = FeatureParameter(\"y\")\nfm = chain(RX(0, 3 * x), RZ(1, z), CNOT(0, 1))\n\n# Construct a circuit with an hardware-efficient ansatz.\ncircuit = QuantumCircuit(3, fm, hea(3,1))\n</code></pre> <p>The abstract <code>QuantumCircuit</code> can now be converted to its native representation via the PyQTorch backend.</p> <pre><code>from qadence import backend_factory\n\n# Use only PyQtorch in non-differentiable mode:\nbackend = backend_factory(\"pyqtorch\")\n\n# The `Converted` object\n# (contains a `ConvertedCircuit` with the original and native representation)\nconv = backend.convert(circuit)\n</code></pre> <pre><code>conv.circuit.original = ChainBlock(0,1,2)\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 RX(0) [params: ['3*x']]\n\u2502   \u251c\u2500\u2500 RZ(1) [params: ['y']]\n\u2502   \u2514\u2500\u2500 CNOT(0, 1)\n\u2514\u2500\u2500 ChainBlock(0,1,2) [tag: HEA]\n    \u251c\u2500\u2500 ChainBlock(0,1,2)\n    \u2502   \u251c\u2500\u2500 KronBlock(0,1,2)\n    \u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n    \u2502   \u2502   \u251c\u2500\u2500 RX(1) [params: ['theta_1']]\n    \u2502   \u2502   \u2514\u2500\u2500 RX(2) [params: ['theta_2']]\n    \u2502   \u251c\u2500\u2500 KronBlock(0,1,2)\n    \u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_3']]\n    \u2502   \u2502   \u251c\u2500\u2500 RY(1) [params: ['theta_4']]\n    \u2502   \u2502   \u2514\u2500\u2500 RY(2) [params: ['theta_5']]\n    \u2502   \u2514\u2500\u2500 KronBlock(0,1,2)\n    \u2502       \u251c\u2500\u2500 RX(0) [params: ['theta_6']]\n    \u2502       \u251c\u2500\u2500 RX(1) [params: ['theta_7']]\n    \u2502       \u2514\u2500\u2500 RX(2) [params: ['theta_8']]\n    \u2514\u2500\u2500 ChainBlock(0,1,2)\n        \u251c\u2500\u2500 KronBlock(0,1)\n        \u2502   \u2514\u2500\u2500 CNOT(0, 1)\n        \u2514\u2500\u2500 KronBlock(1,2)\n            \u2514\u2500\u2500 CNOT(1, 2)\nconv.circuit.native = QuantumCircuit(\n  (operations): ModuleList(\n    (0): Sequence(\n      (operations): ModuleList(\n        (0): Sequence(\n          (operations): ModuleList(\n            (0): RX(target: (0,), param: b82859bb-b1a0-43c6-a17c-0397185e140b)\n            (1): RZ(target: (1,), param: 3060ec1b-1cfe-48a7-be3f-06c74dddd609)\n            (2): CNOT(control: (0,), target: (1,))\n          )\n        )\n        (1): Sequence(\n          (operations): ModuleList(\n            (0): Sequence(\n              (operations): ModuleList(\n                (0): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (0,), param: 7febe7d3-48a8-46e7-a913-5dae2de11b45)\n                    (1): RY(target: (0,), param: aafe5b83-2fe3-46b8-ba14-3626a5d7c6d0)\n                    (2): RX(target: (0,), param: 349d8210-8615-47c1-8fa2-3507d38ba2bd)\n                  )\n                )\n                (1): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (1,), param: 92a67e4c-edd0-4a65-ae9b-bab5d2316ddd)\n                    (1): RY(target: (1,), param: 150b57e0-a1f8-46dd-86cf-b6db18d9b1e4)\n                    (2): RX(target: (1,), param: 406a66fa-b6f1-422b-bfea-f0dc28f0cb22)\n                  )\n                )\n                (2): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (2,), param: 0100d2f4-e186-4147-a99d-b3d653e0675e)\n                    (1): RY(target: (2,), param: 1da07f4f-7df4-450a-a07c-962bd0d9f172)\n                    (2): RX(target: (2,), param: 100840da-37f3-444e-9ba3-fd89562a3133)\n                  )\n                )\n              )\n            )\n            (1): Sequence(\n              (operations): ModuleList(\n                (0): Sequence(\n                  (operations): ModuleList(\n                    (0): CNOT(control: (0,), target: (1,))\n                  )\n                )\n                (1): Sequence(\n                  (operations): ModuleList(\n                    (0): CNOT(control: (1,), target: (2,))\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n)\n</code></pre> <p>Additionally, <code>Converted</code> contains all fixed and variational parameters, as well as an embedding function which accepts feature parameters to construct a dictionary of circuit native parameters. These are needed as each backend uses a different representation of the circuit parameters:</p> <pre><code>import torch\n\n# Contains fixed parameters and variational (from the HEA)\nconv.params\n\ninputs = {\"x\": torch.tensor([1., 1.]), \"y\":torch.tensor([2., 2.])}\n\n# get all circuit parameters (including feature params)\nembedded = conv.embedding_fn(conv.params, inputs)\n</code></pre> <pre><code>conv.params = {\n  theta_7: tensor([0.1238], requires_grad=True)\n  theta_1: tensor([0.4164], requires_grad=True)\n  theta_6: tensor([0.2781], requires_grad=True)\n  theta_3: tensor([0.5660], requires_grad=True)\n  theta_5: tensor([0.3560], requires_grad=True)\n  theta_8: tensor([0.7668], requires_grad=True)\n  theta_0: tensor([0.4556], requires_grad=True)\n  theta_4: tensor([0.3843], requires_grad=True)\n  theta_2: tensor([0.3559], requires_grad=True)\n}\nembedded = {\n  b82859bb-b1a0-43c6-a17c-0397185e140b: tensor([3., 3.], grad_fn=&lt;ViewBackward0&gt;)\n  3060ec1b-1cfe-48a7-be3f-06c74dddd609: tensor([2., 2.])\n  7febe7d3-48a8-46e7-a913-5dae2de11b45: tensor([0.4556], grad_fn=&lt;ViewBackward0&gt;)\n  aafe5b83-2fe3-46b8-ba14-3626a5d7c6d0: tensor([0.5660], grad_fn=&lt;ViewBackward0&gt;)\n  349d8210-8615-47c1-8fa2-3507d38ba2bd: tensor([0.2781], grad_fn=&lt;ViewBackward0&gt;)\n  92a67e4c-edd0-4a65-ae9b-bab5d2316ddd: tensor([0.4164], grad_fn=&lt;ViewBackward0&gt;)\n  150b57e0-a1f8-46dd-86cf-b6db18d9b1e4: tensor([0.3843], grad_fn=&lt;ViewBackward0&gt;)\n  406a66fa-b6f1-422b-bfea-f0dc28f0cb22: tensor([0.1238], grad_fn=&lt;ViewBackward0&gt;)\n  0100d2f4-e186-4147-a99d-b3d653e0675e: tensor([0.3559], grad_fn=&lt;ViewBackward0&gt;)\n  1da07f4f-7df4-450a-a07c-962bd0d9f172: tensor([0.3560], grad_fn=&lt;ViewBackward0&gt;)\n  100840da-37f3-444e-9ba3-fd89562a3133: tensor([0.7668], grad_fn=&lt;ViewBackward0&gt;)\n}\n</code></pre> <p>With the embedded parameters, <code>QuantumModel</code> methods are accessible:</p> <pre><code>output = backend.run(conv.circuit, embedded)\nprint(f\"{output = }\")\n</code></pre> <pre><code>output = tensor([[ 0.1124-0.1251j, -0.0510-0.0990j,  0.2156+0.0420j,  0.0563+0.3270j,\n         -0.6374-0.3222j, -0.3490+0.3186j,  0.1386+0.0371j,  0.0219+0.2157j],\n        [ 0.1124-0.1251j, -0.0510-0.0990j,  0.2156+0.0420j,  0.0563+0.3270j,\n         -0.6374-0.3222j, -0.3490+0.3186j,  0.1386+0.0371j,  0.0219+0.2157j]],\n       grad_fn=&lt;TBackward0&gt;)\n</code></pre>"},{"location":"content/backends/#lower-level-the-backend-representation","title":"Lower-level: the <code>Backend</code> representation","text":"<p>If there is a requirement to work with a specific backend, it is possible to access directly the native circuit. For example, should one wish to use PyQtorch noise features directly instead of using the <code>NoiseHandler</code> interface from Qadence:</p> <pre><code>from pyqtorch.noise import Depolarizing\n\ninputs = {\"x\": torch.rand(1), \"y\":torch.rand(1)}\nembedded = conv.embedding_fn(conv.params, inputs)\n\n# Define a noise channel on qubit 0\nnoise = Depolarizing(0, error_probability=0.1)\n\n# Add noise to circuit\nconv.circuit.native.operations.append(noise)\n</code></pre> <p>When running With noise, one can see that the output is a density matrix:</p> <pre><code>density_result = backend.run(conv.circuit, embedded)\nprint(density_result.shape)\n</code></pre> <pre><code>torch.Size([1, 8, 8])\n</code></pre>"},{"location":"content/block_system/","title":"Block system","text":"<p>Quantum programs in Qadence are constructed using a block-system, with an emphasis on composability of primitive blocks to obtain larger, composite blocks. This functional approach is different from other frameworks which follow a more object-oriented way to construct circuits and express programs.</p>"},{"location":"content/block_system/#primitive-blocks","title":"Primitive blocks","text":"<p>A <code>PrimitiveBlock</code> represents a digital or an analog time-evolution quantum operation applied to a qubit support. Programs can always be decomposed down into a sequence of <code>PrimitiveBlock</code> elements.</p> <p>Two canonical examples of digital primitive blocks are the parametrized <code>RX</code> and the <code>CNOT</code> gates:</p> <pre><code>from qadence import chain, RX, CNOT\n\nrx = RX(0, 0.5)\ncnot = CNOT(0, 1)\n\nblock = chain(rx, cnot)\n</code></pre> %3 ca56dd3314c843da8af1d4637d2175f1 0 3b47266dde234b9e81ff58ab35f35cd9 RX(0.5) ca56dd3314c843da8af1d4637d2175f1--3b47266dde234b9e81ff58ab35f35cd9 1cfc385ef9264b3daf0f0d5b5b3d11e0 1 b07cad85e70a49f3baa08ce1d86ebef0 3b47266dde234b9e81ff58ab35f35cd9--b07cad85e70a49f3baa08ce1d86ebef0 b47f780100fa400ca457fce5f43c82b6 b07cad85e70a49f3baa08ce1d86ebef0--b47f780100fa400ca457fce5f43c82b6 d3e642651dca410fa16fbc5373af98fd 593a30055e454d7dbf94539c8d50042d 1cfc385ef9264b3daf0f0d5b5b3d11e0--593a30055e454d7dbf94539c8d50042d d9e6d9a7b7eb49efae62e9c565087404 X 593a30055e454d7dbf94539c8d50042d--d9e6d9a7b7eb49efae62e9c565087404 d9e6d9a7b7eb49efae62e9c565087404--b07cad85e70a49f3baa08ce1d86ebef0 d9e6d9a7b7eb49efae62e9c565087404--d3e642651dca410fa16fbc5373af98fd <p>A list of all available primitive operations can be found here.</p> How to visualize blocks <p>There are two ways to display blocks in a Python interpreter: either as a tree in ASCII format using <code>print</code>:</p> <pre><code>from qadence import X, Y, kron\n\nkron_block = kron(X(0), Y(1))\nprint(kron_block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 Y(1)\n</code></pre> <p>Or using the visualization package:</p> <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nkron_block = kron(X(0), Y(1))\n# display(kron_block)\n</code></pre> %3 0476071c86cd45ed9734dc3cc99267a2 0 ffce24a97fdd4358ae705bab03e11235 X 0476071c86cd45ed9734dc3cc99267a2--ffce24a97fdd4358ae705bab03e11235 9d7800af0584490ab355558b8dd021d9 1 caa313053c784d12a1f2bf3a87a7a1b4 ffce24a97fdd4358ae705bab03e11235--caa313053c784d12a1f2bf3a87a7a1b4 af3feafaa41e42c68db1d2ff963f38d6 b39cd3a9980e493fb4377911a7639813 Y 9d7800af0584490ab355558b8dd021d9--b39cd3a9980e493fb4377911a7639813 b39cd3a9980e493fb4377911a7639813--af3feafaa41e42c68db1d2ff963f38d6"},{"location":"content/block_system/#composite-blocks","title":"Composite Blocks","text":"<p>Programs can be expressed by composing blocks to result in a larger <code>CompositeBlock</code> using three fundamental operations: chain, kron, and add.</p> <ul> <li>chain applies a set of blocks in sequence, which can have overlapping qubit supports, and results in a <code>ChainBlock</code> type. It is akin to applying a matrix product of the sub-blocks, and can also be used with the <code>*</code> operator.</li> <li>kron applies a set of blocks in parallel, requiring disjoint qubit support, and results in a <code>KronBlock</code> type. This is akin to applying a tensor product of the sub-blocks, and can also be used with the <code>@</code> operator.</li> <li>add performs a direct sum of the operators, and results in an <code>AddBlock</code> type. Blocks constructed this way are typically non-unitary, as is the case for Hamiltonians which can be constructed through sums of Pauli strings. Addition can also be performed directly with the <code>+</code> operator.</li> </ul> <pre><code>from qadence import X, Y, chain, kron\n\nchain_0 = chain(X(0), Y(0))\nchain_1 = chain(X(1), Y(1))\n\nkron_block = kron(chain_0, chain_1)\n</code></pre> %3 03002cc0f5c84167b85fcc7d60e2eb85 0 547d6a119c274b4799d72b7bb253fc0e X 03002cc0f5c84167b85fcc7d60e2eb85--547d6a119c274b4799d72b7bb253fc0e 87c6e5311a6241ef8ee87466e150a725 1 48437c4e2ae6434f9988f8e2597896ee Y 547d6a119c274b4799d72b7bb253fc0e--48437c4e2ae6434f9988f8e2597896ee a130db4779aa4b3f818b032a6176a385 48437c4e2ae6434f9988f8e2597896ee--a130db4779aa4b3f818b032a6176a385 ae702f0233354b109bcd47b679f62679 72249066a31447c2b506b9e4ed5d01e8 X 87c6e5311a6241ef8ee87466e150a725--72249066a31447c2b506b9e4ed5d01e8 7683bf42cbc545a1909bc0baf6521b86 Y 72249066a31447c2b506b9e4ed5d01e8--7683bf42cbc545a1909bc0baf6521b86 7683bf42cbc545a1909bc0baf6521b86--ae702f0233354b109bcd47b679f62679 <p>All composition functions support list comprehension syntax. Below we exemplify the creation of an XY Hamiltonian for qubits laid out on a line.</p> <pre><code>from qadence import X, Y, add\n\ndef xy_int(i: int, j: int):\n    return (1/2) * (X(i)@X(j) + Y(i)@Y(j))\n\nn_qubits = 3\n\nxy_ham = add(xy_int(i, i+1) for i in range(n_qubits-1))\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 AddBlock(0,1)\n\u2502       \u251c\u2500\u2500 KronBlock(0,1)\n\u2502       \u2502   \u251c\u2500\u2500 X(0)\n\u2502       \u2502   \u2514\u2500\u2500 X(1)\n\u2502       \u2514\u2500\u2500 KronBlock(0,1)\n\u2502           \u251c\u2500\u2500 Y(0)\n\u2502           \u2514\u2500\u2500 Y(1)\n\u2514\u2500\u2500 [mul: 0.500] \n    \u2514\u2500\u2500 AddBlock(1,2)\n        \u251c\u2500\u2500 KronBlock(1,2)\n        \u2502   \u251c\u2500\u2500 X(1)\n        \u2502   \u2514\u2500\u2500 X(2)\n        \u2514\u2500\u2500 KronBlock(1,2)\n            \u251c\u2500\u2500 Y(1)\n            \u2514\u2500\u2500 Y(2)\n</code></pre> <p>Qadence blocks can be directly translated to matrix form by calling <code>block.tensor()</code>. Note that first dimension is the batch dimension, following PyTorch conventions. This becomes relevant if the block are parameterized and batched input values are passed, as we will see later.</p> <pre><code>from qadence import X, Y\n\nxy = (1/2) * (X(0)@X(1) + Y(0)@Y(1))\n\nprint(xy.tensor().real)\n</code></pre> <pre><code>tensor([[[0., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [0., 1., 0., 0.],\n         [0., 0., 0., 0.]]])\n</code></pre> <p>For a final example of the flexibility of functional block composition, below is an implementation of the Quantum Fourier Transform on an arbitrary qubit support.</p> <pre><code>from qadence import H, CPHASE, PI, chain, kron\n\ndef qft_layer(qs: tuple, l: int):\n    cphases = chain(CPHASE(qs[j], qs[l], PI/2**(j-l)) for j in range(l+1, len(qs)))\n    return H(qs[l]) * cphases\n\ndef qft(qs: tuple):\n    return chain(qft_layer(qs, l) for l in range(len(qs)))\n</code></pre> %3 7661718bf83941ccba3692887d954d17 0 73c5bea47f1843529cb8b17b6930b90f H 7661718bf83941ccba3692887d954d17--73c5bea47f1843529cb8b17b6930b90f 9c40932d5ad5417386590e019a8bb17e 1 4185b157608f46919c9dda1922360ee3 PHASE(1.571) 73c5bea47f1843529cb8b17b6930b90f--4185b157608f46919c9dda1922360ee3 6e43d4bb871148a09b0ac1d050232727 PHASE(0.785) 4185b157608f46919c9dda1922360ee3--6e43d4bb871148a09b0ac1d050232727 ae82abc5f02c426ead3141f42b468e04 4185b157608f46919c9dda1922360ee3--ae82abc5f02c426ead3141f42b468e04 8fadd9d62bac4a858efc482e2b85e9e3 6e43d4bb871148a09b0ac1d050232727--8fadd9d62bac4a858efc482e2b85e9e3 24ec0c712c874c9cadd8ebec5f745545 6e43d4bb871148a09b0ac1d050232727--24ec0c712c874c9cadd8ebec5f745545 2955cbfe64454e629acea5bef4b588e8 8fadd9d62bac4a858efc482e2b85e9e3--2955cbfe64454e629acea5bef4b588e8 ddbaee31c2904fb9894661ff419bbeb5 2955cbfe64454e629acea5bef4b588e8--ddbaee31c2904fb9894661ff419bbeb5 20c45e959faf4f84b3aadb2888e6ff1d ddbaee31c2904fb9894661ff419bbeb5--20c45e959faf4f84b3aadb2888e6ff1d d4b4f419b20449bea98fb92c21470be4 3e809f24b1624c648367db295562b07e 9c40932d5ad5417386590e019a8bb17e--3e809f24b1624c648367db295562b07e 687d9877d9c24de89017bc6d4129e931 2 3e809f24b1624c648367db295562b07e--ae82abc5f02c426ead3141f42b468e04 3aa79b4c73284798819e9dd7a7630520 ae82abc5f02c426ead3141f42b468e04--3aa79b4c73284798819e9dd7a7630520 c9e9a666e9604c98898d2432d9732850 H 3aa79b4c73284798819e9dd7a7630520--c9e9a666e9604c98898d2432d9732850 f5338c7bce0444fa99caf02d0b7ac472 PHASE(1.571) c9e9a666e9604c98898d2432d9732850--f5338c7bce0444fa99caf02d0b7ac472 30035d553f7c4d43931ee2f3b32ff227 f5338c7bce0444fa99caf02d0b7ac472--30035d553f7c4d43931ee2f3b32ff227 b374f8a36bec4372a6ef7ed95c64d469 f5338c7bce0444fa99caf02d0b7ac472--b374f8a36bec4372a6ef7ed95c64d469 30035d553f7c4d43931ee2f3b32ff227--d4b4f419b20449bea98fb92c21470be4 8587acf6e87949a49b7b9ace2cc42ce5 38e07fec0ba6479ca996ff84abf25b3b 687d9877d9c24de89017bc6d4129e931--38e07fec0ba6479ca996ff84abf25b3b e1b059799a3f44dd99e5253be964e2fc 38e07fec0ba6479ca996ff84abf25b3b--e1b059799a3f44dd99e5253be964e2fc e1b059799a3f44dd99e5253be964e2fc--24ec0c712c874c9cadd8ebec5f745545 862703f4bede46ebbe298193c094d0f6 24ec0c712c874c9cadd8ebec5f745545--862703f4bede46ebbe298193c094d0f6 862703f4bede46ebbe298193c094d0f6--b374f8a36bec4372a6ef7ed95c64d469 df1cfe32388f46e1844271ffbc133fe0 H b374f8a36bec4372a6ef7ed95c64d469--df1cfe32388f46e1844271ffbc133fe0 df1cfe32388f46e1844271ffbc133fe0--8587acf6e87949a49b7b9ace2cc42ce5 <p>Other functionalities are directly built in the block system. For example, the inverse operation can be created with the <code>dagger()</code> method.</p> <pre><code>qft_inv = qft((0, 1, 2)).dagger()\n</code></pre> %3 4d22c1d642144fc48fcf392e41ae3e6a 0 483079fe3bab4b0894090e6b0b5afe46 4d22c1d642144fc48fcf392e41ae3e6a--483079fe3bab4b0894090e6b0b5afe46 362c4d4e0add4114b6b3bdf6eda03bcf 1 80959f566cfd4b86985c4641018e5a41 483079fe3bab4b0894090e6b0b5afe46--80959f566cfd4b86985c4641018e5a41 f1d84dddb11c4bd7972b62bb182bc36d 80959f566cfd4b86985c4641018e5a41--f1d84dddb11c4bd7972b62bb182bc36d ae9ceff8077e4ccbb2b9b4d7aadf820b PHASE(-0.785) f1d84dddb11c4bd7972b62bb182bc36d--ae9ceff8077e4ccbb2b9b4d7aadf820b 8c3e8b9c6054446d91c5070f7f2e974f PHASE(-1.571) ae9ceff8077e4ccbb2b9b4d7aadf820b--8c3e8b9c6054446d91c5070f7f2e974f f176474e4e154135bb4acf0ed5a79198 ae9ceff8077e4ccbb2b9b4d7aadf820b--f176474e4e154135bb4acf0ed5a79198 a2c3142338d34b379d5f2829ee55b4b1 H 8c3e8b9c6054446d91c5070f7f2e974f--a2c3142338d34b379d5f2829ee55b4b1 0d1b6a22a49a4dc0b0209ee6e7cab8b6 8c3e8b9c6054446d91c5070f7f2e974f--0d1b6a22a49a4dc0b0209ee6e7cab8b6 d6e11bd6c2134c0091f9a92905065932 a2c3142338d34b379d5f2829ee55b4b1--d6e11bd6c2134c0091f9a92905065932 d2e31a688f3945f99cf4262a43a1ad88 cf3859e662694f5bb9c4baf32ddbb85b 362c4d4e0add4114b6b3bdf6eda03bcf--cf3859e662694f5bb9c4baf32ddbb85b eee17b3c404a456f98445191899b107d 2 429e7dedc4754caabfc4d5516598117a PHASE(-1.571) cf3859e662694f5bb9c4baf32ddbb85b--429e7dedc4754caabfc4d5516598117a 684552d4798b48ab8b5c8bcee261bfd8 H 429e7dedc4754caabfc4d5516598117a--684552d4798b48ab8b5c8bcee261bfd8 8c99a4cbfc67491fa911e6ef6e44defe 429e7dedc4754caabfc4d5516598117a--8c99a4cbfc67491fa911e6ef6e44defe 3b557e27b61447148f9d70c594f2b451 684552d4798b48ab8b5c8bcee261bfd8--3b557e27b61447148f9d70c594f2b451 3b557e27b61447148f9d70c594f2b451--0d1b6a22a49a4dc0b0209ee6e7cab8b6 a6c608d9cbf5457490b399ae9a1c593c 0d1b6a22a49a4dc0b0209ee6e7cab8b6--a6c608d9cbf5457490b399ae9a1c593c a6c608d9cbf5457490b399ae9a1c593c--d2e31a688f3945f99cf4262a43a1ad88 4e8f90e993e142128505e37747685c47 bf0ea116db224608a0fb0e93ebe14561 H eee17b3c404a456f98445191899b107d--bf0ea116db224608a0fb0e93ebe14561 bf0ea116db224608a0fb0e93ebe14561--8c99a4cbfc67491fa911e6ef6e44defe 655b675c11a44852957aff9470bc13f5 8c99a4cbfc67491fa911e6ef6e44defe--655b675c11a44852957aff9470bc13f5 655b675c11a44852957aff9470bc13f5--f176474e4e154135bb4acf0ed5a79198 8ef8d0619df64c7bb05f0c5c8217b07f f176474e4e154135bb4acf0ed5a79198--8ef8d0619df64c7bb05f0c5c8217b07f bdb6fe1139104eb99b81b10e41ca964e 8ef8d0619df64c7bb05f0c5c8217b07f--bdb6fe1139104eb99b81b10e41ca964e bdb6fe1139104eb99b81b10e41ca964e--4e8f90e993e142128505e37747685c47"},{"location":"content/block_system/#digital-analog-composition","title":"Digital-analog composition","text":"<p>In Qadence, analog operations are first-class citizens. An analog operation is one whose unitary is best described by the evolution of some hermitian generator, or Hamiltonian, acting on an arbitrary number of qubits. Qadence provides the <code>HamEvo</code> class to initialize analog operations. For a time-independent generator \\(\\mathcal{H}\\) and some time variable \\(t\\), <code>HamEvo(H, t)</code> represents the evolution operator \\(\\exp(-i\\mathcal{H}t)\\).</p> <p>Analog operations constitute a generalization of digital operations, and all digital operations can also be represented as the evolution of some hermitian generator. For example, the <code>RX</code> gate is the evolution of <code>X</code>.</p> <pre><code>from qadence import X, RX, HamEvo, PI\nfrom torch import allclose\n\nangle = PI/2\n\nblock_digital = RX(0, angle)\n\nblock_analog = HamEvo(0.5*X(0), angle)\n\nprint(allclose(block_digital.tensor(), block_analog.tensor()))\n</code></pre> <pre><code>True\n</code></pre> <p>As seen in the previous section, arbitrary Hamiltonians can be constructed using Pauli operators. Their evolution can be combined with other arbitrary digital operations and incorporated into any quantum program.</p> <pre><code>from qadence import X, Y, RX, HamEvo\nfrom qadence import add, kron, PI\n\ndef xy_int(i: int, j: int):\n    return (1/2) * (X(i)@X(j) + Y(i)@Y(j))\n\nn_qubits = 3\n\nxy_ham = add(xy_int(i, i+1) for i in range(n_qubits-1))\n\nanalog_evo = HamEvo(xy_ham, 1.0)\n\ndigital_block = kron(RX(i, i*PI/2) for i in range(n_qubits))\n\nprogram = digital_block * analog_evo * digital_block\n</code></pre> %3 cluster_f0ed182c7a7e4919aa021356c06b6efd 049b4cf448ea4f96804def5b80340976 0 d63d9729d8064214a7f36c8411556de9 RX(0.0) 049b4cf448ea4f96804def5b80340976--d63d9729d8064214a7f36c8411556de9 31fed87df7ae4a3ba04bc71568712820 1 377581544bac4da7b368b5df91ad4dff HamEvo d63d9729d8064214a7f36c8411556de9--377581544bac4da7b368b5df91ad4dff cbdfb2dcc1cc4e7db82079a7aa320729 RX(0.0) 377581544bac4da7b368b5df91ad4dff--cbdfb2dcc1cc4e7db82079a7aa320729 78fdc914bc704610a2f46a1edb1be083 cbdfb2dcc1cc4e7db82079a7aa320729--78fdc914bc704610a2f46a1edb1be083 67fa55876b1b49b08811dc60072b2ce7 8c83b13f36564cb59433ad1cd5c262e4 RX(1.571) 31fed87df7ae4a3ba04bc71568712820--8c83b13f36564cb59433ad1cd5c262e4 2b05e7929cfe47e7ba80e51ee9ac8ae1 2 9a27b00da4de4cfca913f1920f5158ed t = 1.000 8c83b13f36564cb59433ad1cd5c262e4--9a27b00da4de4cfca913f1920f5158ed a15032aa443d486688825c919626c72f RX(1.571) 9a27b00da4de4cfca913f1920f5158ed--a15032aa443d486688825c919626c72f a15032aa443d486688825c919626c72f--67fa55876b1b49b08811dc60072b2ce7 345b90f49348483ca4490459d41bfd73 bc709a4647bf4d0ab05a62c39d9750e8 RX(3.142) 2b05e7929cfe47e7ba80e51ee9ac8ae1--bc709a4647bf4d0ab05a62c39d9750e8 0097b5cd0eaf4cad88f393ec29792b44 bc709a4647bf4d0ab05a62c39d9750e8--0097b5cd0eaf4cad88f393ec29792b44 942d438a91374314bbd267e96b76d4d1 RX(3.142) 0097b5cd0eaf4cad88f393ec29792b44--942d438a91374314bbd267e96b76d4d1 942d438a91374314bbd267e96b76d4d1--345b90f49348483ca4490459d41bfd73"},{"location":"content/block_system/#block-execution","title":"Block execution","text":"<p>To quickly run block operations and access wavefunctions, samples or expectation values of observables, one can use the convenience functions <code>run</code>, <code>sample</code> and <code>expectation</code>.</p> <pre><code>from qadence import kron, add, H, Z, run, sample, expectation\n\nn_qubits = 2\n\n# Prepares a uniform state\nh_block = kron(H(i) for i in range(n_qubits))\n\nwf = run(h_block)\n\nxs = sample(h_block, n_shots=1000)\n\nobs = add(Z(i) for i in range(n_qubits))\nex = expectation(h_block, obs)\n</code></pre> <pre><code>wf = tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\nxs = [OrderedCounter({'10': 273, '01': 258, '11': 237, '00': 232})]\nex = tensor([[0.]])\n</code></pre>"},{"location":"content/block_system/#execution-via-quantumcircuit-and-quantummodel","title":"Execution via <code>QuantumCircuit</code> and <code>QuantumModel</code>","text":"<p>More fine-grained control and better performance is provided via the high-level <code>QuantumModel</code> abstraction. Quantum programs in Qadence are constructed in two steps:</p> <ol> <li>Build a <code>QuantumCircuit</code> which ties together a composite block and a register.</li> <li>Define a <code>QuantumModel</code> which differentiates, compiles and executes the circuit.</li> </ol> <p>Execution of more complex Qadence programs will be explored in the next tutorials.</p>"},{"location":"content/block_system/#adding-noise-to-gates","title":"Adding noise to gates","text":"<p>It is possible to add noise to gates. Please refer to the noise tutorial here.</p>"},{"location":"content/hamiltonians/","title":"Constructing arbitrary Hamiltonians","text":"<p>At the heart of digital-analog quantum computing is the description and execution of analog blocks, which represent a set of interacting qubits under some interaction Hamiltonian. For this purpose, Qadence relies on the <code>hamiltonian_factory</code> function to create arbitrary Hamiltonian blocks to be used as generators of <code>HamEvo</code> or as observables to be measured.</p>"},{"location":"content/hamiltonians/#arbitrary-all-to-all-hamiltonians","title":"Arbitrary all-to-all Hamiltonians","text":"<p>Arbitrary all-to-all interaction Hamiltonians can be easily created by passing the number of qubits in the first argument. The type of <code>interaction</code> can be chosen from the available ones in the <code>Interaction</code> enum type.</p> <pre><code>from qadence import hamiltonian_factory\nfrom qadence import N, X, Y, Z\nfrom qadence import Interaction\n\nn_qubits = 3\n\nhamilt = hamiltonian_factory(n_qubits, interaction=Interaction.ZZ)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(2)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 Z(1)\n        \u2514\u2500\u2500 Z(2)\n</code></pre> <p>Alternatively, a custom interaction function can also be defined. The input should be two integer indices \\(i\\) and \\(j\\) and it should return a composition of pauli terms representing the interaction between qubits \\(i\\) and \\(j\\):</p> <pre><code>def custom_int(i: int, j: int):\n    return X(i) @ X(j) + Y(i) @ Y(j)\n\nn_qubits = 2\n\nhamilt = hamiltonian_factory(n_qubits, interaction=custom_int)\n</code></pre> <pre><code>AddBlock(0,1)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 AddBlock(0,1)\n        \u251c\u2500\u2500 KronBlock(0,1)\n        \u2502   \u251c\u2500\u2500 X(0)\n        \u2502   \u2514\u2500\u2500 X(1)\n        \u2514\u2500\u2500 KronBlock(0,1)\n            \u251c\u2500\u2500 Y(0)\n            \u2514\u2500\u2500 Y(1)\n</code></pre> <p>Single-qubit terms can also be added by passing the respective operator directly to the <code>detuning</code> argument. For example, the total magnetization is commonly used as an observable to be measured:</p> <pre><code>total_mag = hamiltonian_factory(n_qubits, detuning = Z)\n</code></pre> <pre><code>AddBlock(0,1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 Z(0)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 Z(1)\n</code></pre> <p>For further customization, arbitrary coefficients can be passed as arrays to the <code>interaction_strength</code> and <code>detuning_strength</code> arguments for the two-qubits and single-qubit terms respectively.</p> <pre><code>n_qubits = 3\n\nhamilt = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.ZZ,\n    detuning=Z,\n    interaction_strength=[0.5, 0.2, 0.1],\n    detuning_strength=[0.1, 0.5, -0.3]\n)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 0.100] \n\u2502   \u2514\u2500\u2500 Z(0)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: -0.30] \n\u2502   \u2514\u2500\u2500 Z(2)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: 0.200] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(2)\n\u2514\u2500\u2500 [mul: 0.100] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 Z(1)\n        \u2514\u2500\u2500 Z(2)\n</code></pre> <p>Ordering interaction strengths matters</p> <p>When passing interaction strengths as an array, the ordering must be identical to the one obtained from the <code>edges</code> property of a Qadence <code>Register</code>:</p> <pre><code>from qadence import Register\n\nprint(Register(n_qubits).edges)\n</code></pre> <pre><code>[(0, 1), (0, 2), (1, 2)]\n</code></pre> <p>For one more example, let's create a transverse-field Ising model,</p> <pre><code>n_qubits = 4\nn_edges = int(0.5 * n_qubits * (n_qubits - 1))\n\nz_terms = [1.0] * n_qubits\nzz_terms = [2.0] * n_edges\n\nzz_ham = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.ZZ,\n    detuning=Z,\n    interaction_strength=zz_terms,\n    detuning_strength=z_terms\n)\n\nx_terms = [-1.0] * n_qubits\nx_ham = hamiltonian_factory(n_qubits, detuning = X, detuning_strength = x_terms)\n\ntransverse_ising = zz_ham + x_ham\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 AddBlock(0,1,2,3)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(0)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(1)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(3)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(1)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(3)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502   \u2502       \u251c\u2500\u2500 Z(1)\n\u2502   \u2502       \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(1,3)\n\u2502   \u2502       \u251c\u2500\u2500 Z(1)\n\u2502   \u2502       \u2514\u2500\u2500 Z(3)\n\u2502   \u2514\u2500\u2500 [mul: 2.000] \n\u2502       \u2514\u2500\u2500 KronBlock(2,3)\n\u2502           \u251c\u2500\u2500 Z(2)\n\u2502           \u2514\u2500\u2500 Z(3)\n\u2514\u2500\u2500 AddBlock(0,1,2,3)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(0)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(1)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(2)\n    \u2514\u2500\u2500 [mul: -1.00] \n        \u2514\u2500\u2500 X(3)\n</code></pre> <p>Random interaction coefficients</p> <p>Random interaction coefficients can be chosen between -1 and 1 by simply passing <code>random_strength = True</code> instead of <code>detuning_strength</code> and <code>interaction_strength</code>.</p>"},{"location":"content/hamiltonians/#arbitrary-hamiltonian-topologies","title":"Arbitrary Hamiltonian topologies","text":"<p>Arbitrary interaction topologies can be created using the Qadence <code>Register</code>. Simply pass the register with the desired topology as the first argument to the <code>hamiltonian_factory</code>:</p> <pre><code>from qadence import Register\n\nreg = Register.square(qubits_side=2)\n\nsquare_hamilt = hamiltonian_factory(reg, interaction=Interaction.NN)\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(3)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(2)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(2,3)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(3)\n</code></pre>"},{"location":"content/hamiltonians/#adding-variational-parameters","title":"Adding variational parameters","text":"<p>Finally, fully parameterized Hamiltonians can be created by passing a string to the strength arguments, and used to prefix the name of the variational parameters.</p> <pre><code>n_qubits = 3\n\nnn_ham = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.NN,\n    detuning=N,\n    interaction_strength=\"c\",\n    detuning_strength=\"d\"\n)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: d_0] \n\u2502   \u2514\u2500\u2500 N(0)\n\u251c\u2500\u2500 [mul: d_1] \n\u2502   \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: d_2] \n\u2502   \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: c_01] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: c_02] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(2)\n\u2514\u2500\u2500 [mul: c_12] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 N(1)\n        \u2514\u2500\u2500 N(2)\n</code></pre> <p>Alternatively, fully customizable sympy functions can be passed in an array using the Qadence parameters. Furthermore, the <code>use_all_node_pairs = True</code> option can be passed so that interactions are created for every single node pair in the register, irrespectively of the topology of the edges. This is useful for creating Hamiltonians that depend on qubit distance.</p> <pre><code>from qadence import VariationalParameter, Register\n\n# Square register of 4 qubits with a dimensionless distance of 8.0\nreg = Register.square(2, spacing = 8.0)\n\n# Get the distances between all pairs of qubits\ndistance_dict = reg.distances\n\n# Create interaction strength with variational parameter and 1/r term\nstrength_list = []\nfor node_pair in reg.all_node_pairs:\n    param = VariationalParameter(\"x\" + f\"_{node_pair[0]}{node_pair[1]}\")\n    dist_factor = reg.distances[node_pair]\n    strength_list.append(param / dist_factor)\n\nnn_ham = hamiltonian_factory(\n    reg,\n    interaction=Interaction.NN,\n    interaction_strength=strength_list,\n    use_all_node_pairs=True,\n)\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 [mul: 0.125*x_01] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: 0.088*x_02] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: 0.125*x_03] \n\u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(3)\n\u251c\u2500\u2500 [mul: 0.125*x_12] \n\u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: 0.088*x_13] \n\u2502   \u2514\u2500\u2500 KronBlock(1,3)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(3)\n\u2514\u2500\u2500 [mul: 0.125*x_23] \n    \u2514\u2500\u2500 KronBlock(2,3)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(3)\n</code></pre>"},{"location":"content/overlap/","title":"Wavefunction overlaps","text":"<p>Qadence offers convenience functions for computing the overlap between the wavefunctions generated by two quantum circuits \\(U\\) and \\(W\\) as:</p> \\[ S = |\\langle \\psi_U | \\psi_W \\rangle|^2 \\quad \\textrm{where} \\quad \\psi_U = U|\\psi_0\\rangle \\] <p>Here is an example on how to compute the overlap between two very simple parametric circuits consisting of a single <code>RX</code> rotation on different qubits. The overlap is expected to be non-zero only when the rotation angle is different from \\(\\pi \\; \\textrm{mod}\\; 2\\pi\\) for both rotations:</p> <pre><code>import numpy as np\nfrom torch import tensor\nfrom qadence import Overlap, OverlapMethod, QuantumCircuit, H, RX, X, FeatureParameter, hea, PI\n\n\n# Create two quantum circuits\n# with a single qubit rotation on two random qubits\nn_qubits = 4\nqubits = np.random.choice(n_qubits, n_qubits, replace=False)\n\nphi = FeatureParameter(\"phi\")\ncircuit_bra = QuantumCircuit(n_qubits, RX(qubits[0], phi))\n\npsi = FeatureParameter(\"psi\")\ncircuit_ket = QuantumCircuit(n_qubits, RX(qubits[1], psi))\n\n# Values for the feature parameters\nvalues_bra = {\"phi\": tensor([PI / 2, PI])}\nvalues_ket = {\"psi\": tensor([PI / 2, PI])}\n\n# Calculate overlap by assigning values to the given bra and ket circuits\novrlp = Overlap(circuit_bra, circuit_ket)\novrlp = ovrlp(bra_param_values=values_bra, ket_param_values=values_ket)\n</code></pre> <pre><code>Overlap with exact method:\n tensor([[2.5000e-01, 1.8747e-33],\n        [1.8747e-33, 1.4058e-65]])\n</code></pre> <p>The <code>Overlap</code> class above inherits from <code>QuantumModel</code> and is executed through its inherited forward method for the given input parameter values. By default, the overlap is computed exactly by performing the dot product of the wavefunction propagated from bra and ket circuits.</p> <p>However, it is possible to choose a different method from the <code>OverlapMethod</code> enumeration to be passed via the <code>overlap_method</code> argument in the <code>Overlap</code> initializer. Currently, one can choose from:</p> <ul> <li><code>EXACT</code>: exact computation using the wavefunction matrix representation. Does not work with real devices since it assumes access to the complete qubit system wavefunction.</li> <li><code>COMPUTE_UNCOMPUTE</code>: exact or sampling-based computation using bra \\(U\\) and ket \\(W^{\\dagger}\\) unitaries.</li> <li><code>SWAP_TEST</code>: exact or sampling-based computation using the SWAP test method.</li> <li><code>HADAMARD_TEST</code>: exact or sampling-based computation using the Hadamard test method.</li> <li><code>JENSEN_SHANNON</code>: compute the overlap using the Jensen-Shannon divergence of the two probability distributions obtained by sampling the propagated circuits. This will yield a different result than the other methods.</li> </ul> <p>All methods (except for the <code>EXACT</code> method) take an optional <code>n_shots</code> argument which can be used to perform shot-based calculations.</p> <p>Warning</p> <p>If you select a finite number of shots, the overlap is not differentiable. Therefore, it cannot be used as output of a quantum model if gradients are required.</p> <pre><code># Calculate overlap with SWAP test\novrlp = Overlap(circuit_bra, circuit_ket, method=OverlapMethod.SWAP_TEST)\novrlp_ha = ovrlp(values_bra, values_ket)\n\n# Calculate overlap with SWAP test\n# using a finite number of shots\novrlp = Overlap(circuit_bra, circuit_ket, method=OverlapMethod.SWAP_TEST)\novrlp_ha = ovrlp(values_bra, values_ket, n_shots=10_000)\n</code></pre> <pre><code>Overlap with SWAP test:\n tensor([[ 2.5000e-01, -3.3307e-16],\n        [-3.3307e-16, -4.4409e-16]])\nOverlap with SWAP test with finite number of shots:\n tensor([[ 0.2490,  0.0026],\n        [-0.0062,  0.0044]])\n</code></pre>"},{"location":"content/parameters/","title":"Parametric programs","text":"<p>Qadence provides a flexible parameter system built on top of Sympy. Parameters can be of different types:</p> <ul> <li>Fixed parameter: a constant with a fixed, non-trainable value (e.g. \\(\\dfrac{\\pi}{2}\\)).</li> <li>Variational parameter: a trainable parameter which will be automatically picked up by the optimizer.</li> <li>Feature parameter: a non-trainable parameter which can be used to pass input values.</li> </ul>"},{"location":"content/parameters/#fixed-parameters","title":"Fixed parameters","text":"<p>Passing fixed parameters to blocks can be done by simply passing a Python numeric type or a <code>torch.Tensor</code>.</p> <pre><code>import torch\nfrom qadence import RX, run, PI\n\nwf = run(RX(0, torch.tensor(PI)))\n\nwf = run(RX(0, PI))\n</code></pre> <pre><code>wf = tensor([[6.1232e-17+0.j, 0.0000e+00-1.j]])\nwf = tensor([[6.1232e-17+0.j, 0.0000e+00-1.j]])\n</code></pre>"},{"location":"content/parameters/#variational-parameters","title":"Variational parameters","text":"<p>To parametrize a block a <code>VariationalParameter</code> instance is required. In most cases Qadence also accepts a Python string, which will be used to automatically initialize a <code>VariationalParameter</code>:</p> <pre><code>from qadence import RX, run, VariationalParameter\n\nblock = RX(0, VariationalParameter(\"theta\"))\nblock = RX(0, \"theta\")  # Equivalent\n\nwf = run(block)\n</code></pre> <pre><code>wf = tensor([[0.8880+0.0000j, 0.0000-0.4598j]])\n</code></pre> <p>By calling <code>run</code>, a random value for <code>\"theta\"</code> is initialized at execution. In a <code>QuantumModel</code>, variational parameters are stored in the underlying model parameter dictionary.</p>"},{"location":"content/parameters/#feature-parameters","title":"Feature parameters","text":"<p>A <code>FeatureParameter</code> type can also be used. It requires an input value or a batch of values. In most cases, Qadence accepts a <code>values</code> dictionary to set the input of feature parameters.</p> <pre><code>from torch import tensor\nfrom qadence import RX, PI, run, FeatureParameter\n\nblock = RX(0, FeatureParameter(\"phi\"))\n\nwf = run(block, values = {\"phi\": tensor([PI, PI/2])})\n</code></pre> <pre><code>wf = tensor([[6.1232e-17+0.0000j, 0.0000e+00-1.0000j],\n        [7.0711e-01+0.0000j, 0.0000e+00-0.7071j]])\n</code></pre> <p>Since a batch of input values was passed, the <code>run</code> function returns a batch of output states. Note that <code>FeatureParameter(\"x\")</code> and <code>VariationalParameter(\"x\")</code> are simply aliases for <code>Parameter(\"x\", trainable = False)</code> and <code>Parameter(\"x\", trainable = True)</code>.</p>"},{"location":"content/parameters/#multiparameter-expressions-and-analog-integration","title":"Multiparameter expressions and analog integration","text":"<p>The integration with Sympy becomes useful when one wishes to write arbitrary parameter compositions. Parameters can also be used as scaling coefficients in the block system, which is essential when defining arbitrary analog operations.</p> <pre><code>from torch import tensor\nfrom qadence import RX, Z, HamEvo, PI\nfrom qadence import VariationalParameter, FeatureParameter, run\nfrom sympy import sin\n\ntheta, phi = VariationalParameter(\"theta\"), FeatureParameter(\"phi\")\n\n# Arbitrary parameter composition\nexpr = PI * sin(theta + phi)\n\n# Use as unitary gate arguments\ngate = RX(0, expr)\n\n# Or as scaling coefficients for Hermitian operators\nh_op = expr * (Z(0) @ Z(1))\n\nwf = run(gate * HamEvo(h_op, 1.0), values = {\"phi\": tensor(PI)})\n</code></pre> <pre><code>wf = tensor([[0.0107+0.7123j, 0.0000+0.0000j, 0.7017+0.0106j, 0.0000+0.0000j]])\n</code></pre>"},{"location":"content/parameters/#parameter-redundancy","title":"Parameter redundancy","text":"<p>Parameters are uniquely defined by their name and redundancy is allowed in composite blocks to assign the same value to different blocks. This is useful, for example, when defining layers of rotation gates typically used as feature maps.</p> <pre><code>from torch import tensor\nfrom qadence import RY, PI, run, kron, FeatureParameter\n\nn_qubits = 3\n\nparam = FeatureParameter(\"phi\")\n\nblock = kron(RY(i, (i+1) * param) for i in range(n_qubits))\n\nwf = run(block, values = {\"phi\": tensor(PI)})\n</code></pre> <pre><code>wf = tensor([[ 1.1248e-32+0.j,  6.1232e-17+0.j, -1.3775e-48+0.j, -7.4988e-33+0.j,\n          1.8370e-16+0.j,  1.0000e+00+0.j, -2.2496e-32+0.j, -1.2246e-16+0.j]])\n</code></pre>"},{"location":"content/parameters/#parametrized-circuits","title":"Parametrized circuits","text":"<p>Let's look at a final example of an arbitrary composition of digital and analog parameterized blocks:</p> <pre><code>import sympy\nfrom qadence import RX, RY, RZ, CNOT, CPHASE, Z, HamEvo\nfrom qadence import run, chain, add, kron, FeatureParameter, VariationalParameter, PI\n\nn_qubits = 3\n\nphi = FeatureParameter(\"\u03a6\")\ntheta = VariationalParameter(\"\u03b8\")\n\nrotation_block = kron(\n    RX(0, phi/theta),\n    RY(1, theta*2),\n    RZ(2, sympy.cos(phi))\n)\ndigital_entangler = CNOT(0, 1) * CPHASE(1, 2, PI)\n\nhamiltonian = add(theta * (Z(i) @ Z(i+1)) for i in range(n_qubits-1))\n\nanalog_evo = HamEvo(hamiltonian, phi)\n\nprogram = chain(rotation_block, digital_entangler, analog_evo)\n</code></pre> %3 cluster_507bbd9edea34c34addb3c2c694af562 1df617777c9c429380c8b00a20108179 0 1702f43a43ea468cb8e177581f11655c RX(\u03a6/\u03b8) 1df617777c9c429380c8b00a20108179--1702f43a43ea468cb8e177581f11655c 0a2bc24578234b8a941ecaa519233984 1 a52af2aba3a645d28a03ccb2ac5364c6 1702f43a43ea468cb8e177581f11655c--a52af2aba3a645d28a03ccb2ac5364c6 bf453715008241aa97d2da12213196ce a52af2aba3a645d28a03ccb2ac5364c6--bf453715008241aa97d2da12213196ce a2e57bb6d89a447392b87af12cb808de HamEvo bf453715008241aa97d2da12213196ce--a2e57bb6d89a447392b87af12cb808de b5a063562c80437680706a94583907ac a2e57bb6d89a447392b87af12cb808de--b5a063562c80437680706a94583907ac 6eb9a7907a2540e1bbb098e475eb410d f7730a631fb14a9cab87984013b3a7cb RY(2*\u03b8) 0a2bc24578234b8a941ecaa519233984--f7730a631fb14a9cab87984013b3a7cb d903264cdf0942409df482a84bcc503c 2 da689d4552a54471abd5076d7e8cc35a X f7730a631fb14a9cab87984013b3a7cb--da689d4552a54471abd5076d7e8cc35a da689d4552a54471abd5076d7e8cc35a--a52af2aba3a645d28a03ccb2ac5364c6 20ec5a87da3844e68272d6f037884c53 da689d4552a54471abd5076d7e8cc35a--20ec5a87da3844e68272d6f037884c53 44ad7057f67048d79c7e9d605c3c9b57 t = \u03a6 20ec5a87da3844e68272d6f037884c53--44ad7057f67048d79c7e9d605c3c9b57 44ad7057f67048d79c7e9d605c3c9b57--6eb9a7907a2540e1bbb098e475eb410d 432bf75e24c2443e859b2d2ed0a5a022 e1124af17930445cb5d9dd45fe4da238 RZ(cos(\u03a6)) d903264cdf0942409df482a84bcc503c--e1124af17930445cb5d9dd45fe4da238 2ff0f67e11094f0baac5d59b5f211d83 e1124af17930445cb5d9dd45fe4da238--2ff0f67e11094f0baac5d59b5f211d83 6f32d66799a04f5f978387f7b5cc6a90 PHASE(3.142) 2ff0f67e11094f0baac5d59b5f211d83--6f32d66799a04f5f978387f7b5cc6a90 6f32d66799a04f5f978387f7b5cc6a90--20ec5a87da3844e68272d6f037884c53 d34165fd9aaf4157b91b971b77c21777 6f32d66799a04f5f978387f7b5cc6a90--d34165fd9aaf4157b91b971b77c21777 d34165fd9aaf4157b91b971b77c21777--432bf75e24c2443e859b2d2ed0a5a022 <p>Please note the different colors for the parametrization with different types. The default palette assigns blue for <code>VariationalParameter</code>, green for <code>FeatureParameter</code>, orange for numeric values, and shaded red for non-parametric gates.</p>"},{"location":"content/qml_constructors/","title":"Quantum machine learning constructors","text":"<p>Besides the arbitrary Hamiltonian constructors, Qadence also provides a complete set of program constructors useful for digital-analog quantum machine learning programs.</p>"},{"location":"content/qml_constructors/#feature-maps","title":"Feature maps","text":"<p>The <code>feature_map</code> function can easily create several types of data-encoding blocks. The two main types of feature maps use a Fourier basis or a Chebyshev basis.</p> <pre><code>from qadence import feature_map, BasisSet, chain\nfrom qadence.draw import display\n\nn_qubits = 3\n\nfourier_fm = feature_map(n_qubits, fm_type=BasisSet.FOURIER)\n\nchebyshev_fm = feature_map(n_qubits, fm_type=BasisSet.CHEBYSHEV)\n\nblock = chain(fourier_fm, chebyshev_fm)\n</code></pre> %3 cluster_2c47cd317eee4ae1b8b5d3c5719d59fe Constant Chebyshev FM cluster_014e40d2e86b45699bae5625bc0fec85 Constant Fourier FM ff4b192ed5944299b78534a53a1ca17d 0 1cbafa9825a7483294af7dc2923fdbb7 RX(phi) ff4b192ed5944299b78534a53a1ca17d--1cbafa9825a7483294af7dc2923fdbb7 994c4856dac14473b28e4806f0ee60de 1 ff285e1f23404145b729d2582104b8ba RX(acos(phi)) 1cbafa9825a7483294af7dc2923fdbb7--ff285e1f23404145b729d2582104b8ba 0809b7248274432c99bf184bc7b2735d ff285e1f23404145b729d2582104b8ba--0809b7248274432c99bf184bc7b2735d f7a4158acf3b464e9538f6dec7977149 43b74820087441148574b06b69f8f640 RX(phi) 994c4856dac14473b28e4806f0ee60de--43b74820087441148574b06b69f8f640 fc5376525df44a398d40a56c5c4ecbea 2 eeb2c80cbd73415798d740d0cde84e6e RX(acos(phi)) 43b74820087441148574b06b69f8f640--eeb2c80cbd73415798d740d0cde84e6e eeb2c80cbd73415798d740d0cde84e6e--f7a4158acf3b464e9538f6dec7977149 c7bfcaeb9a4143b3b9393ccdacd94a57 1af229571b834a06abf0c52aa93af96c RX(phi) fc5376525df44a398d40a56c5c4ecbea--1af229571b834a06abf0c52aa93af96c 6fd2390c877e412f9e6cb7ea313c3547 RX(acos(phi)) 1af229571b834a06abf0c52aa93af96c--6fd2390c877e412f9e6cb7ea313c3547 6fd2390c877e412f9e6cb7ea313c3547--c7bfcaeb9a4143b3b9393ccdacd94a57 <p>A custom encoding function can also be passed with <code>sympy</code></p> <pre><code>from sympy import asin, Function\n\nn_qubits = 3\n\n# Using a pre-defined sympy Function\ncustom_fm_0 = feature_map(n_qubits, fm_type=asin)\n\n# Creating a custom function\ndef custom_fn(x):\n    return asin(x) + x**2\n\ncustom_fm_1 = feature_map(n_qubits, fm_type=custom_fn)\n\nblock = chain(custom_fm_0, custom_fm_1)\n</code></pre> %3 cluster_9e5b3007108d4afb8cfe2e3c28d8fba0 Constant &lt;function custom_fn at 0x7f75c8457250&gt; FM cluster_7d2c30675c324dfea2f31b9313915f5f Constant asin FM a3b7e716577f4d5391d4b1b981f7fac3 0 bf58011e05a643c8ada0c440d2fe0005 RX(asin(phi)) a3b7e716577f4d5391d4b1b981f7fac3--bf58011e05a643c8ada0c440d2fe0005 f1ed5a121c644c3c9681f886f3f5d621 1 67f7d509b666415b9aef8d35df5a87ac RX(phi**2 + asin(phi)) bf58011e05a643c8ada0c440d2fe0005--67f7d509b666415b9aef8d35df5a87ac 26f8784e1e1e48bda76376664fa058e8 67f7d509b666415b9aef8d35df5a87ac--26f8784e1e1e48bda76376664fa058e8 6b6241a385dc42e38ab7a327b65ed2e7 bfde7469e6cf425ea490bfeb7a1209b7 RX(asin(phi)) f1ed5a121c644c3c9681f886f3f5d621--bfde7469e6cf425ea490bfeb7a1209b7 90d4562a6d20428c9653d345d1be7e20 2 6f513c66266f4a438ea55e18665bd953 RX(phi**2 + asin(phi)) bfde7469e6cf425ea490bfeb7a1209b7--6f513c66266f4a438ea55e18665bd953 6f513c66266f4a438ea55e18665bd953--6b6241a385dc42e38ab7a327b65ed2e7 156e78d6b1ac47f0b533a1f22b37dd6f 9f0045b3e1634d9d9917f992201237e4 RX(asin(phi)) 90d4562a6d20428c9653d345d1be7e20--9f0045b3e1634d9d9917f992201237e4 48899293938d493e8eb91571f8bbfffe RX(phi**2 + asin(phi)) 9f0045b3e1634d9d9917f992201237e4--48899293938d493e8eb91571f8bbfffe 48899293938d493e8eb91571f8bbfffe--156e78d6b1ac47f0b533a1f22b37dd6f <p>Furthermore, the <code>reupload_scaling</code> argument can be used to change the scaling applied to each qubit in the support of the feature map. The default scalings can be chosen from the <code>ReuploadScaling</code> enumeration.</p> <pre><code>from qadence import ReuploadScaling\nfrom qadence.draw import display\n\nn_qubits = 5\n\n# Default constant value\nfm_constant = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT)\n\n# Linearly increasing scaling\nfm_tower = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.TOWER)\n\n# Exponentially increasing scaling\nfm_exp = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.EXP)\n\nblock = chain(fm_constant, fm_tower, fm_exp)\n</code></pre> %3 cluster_35374114d99545abb69bdcae303b964c Exponential Fourier FM cluster_37f3c24b841a4a6997e5785fd1d5c383 Constant Fourier FM cluster_762463f8436548a9a1def374f6781551 Tower Fourier FM 7692f1b739324ce0963287bc155663d7 0 f37181ac734d4115801e92bfa653d65e RX(phi) 7692f1b739324ce0963287bc155663d7--f37181ac734d4115801e92bfa653d65e 03a00471506746e0b5c9f8d816741db0 1 8675ef1b41fe4fbaa8164cf1a3180911 RX(1.0*phi) f37181ac734d4115801e92bfa653d65e--8675ef1b41fe4fbaa8164cf1a3180911 8f6468f99dcc4885b60df1e41b3d6803 RX(1.0*phi) 8675ef1b41fe4fbaa8164cf1a3180911--8f6468f99dcc4885b60df1e41b3d6803 0dc997e31c69444483afdad243dfb216 8f6468f99dcc4885b60df1e41b3d6803--0dc997e31c69444483afdad243dfb216 8aa033c087ad4d2096490a729a1a3991 e74fd0a18e454eb193541adbadb0e029 RX(phi) 03a00471506746e0b5c9f8d816741db0--e74fd0a18e454eb193541adbadb0e029 321c9cf4a2d24d6989d4eb918dbbb356 2 cd7086c09c1d4480938fe295e774b317 RX(2.0*phi) e74fd0a18e454eb193541adbadb0e029--cd7086c09c1d4480938fe295e774b317 31b10526f982435a82bfae75fcd6b006 RX(2.0*phi) cd7086c09c1d4480938fe295e774b317--31b10526f982435a82bfae75fcd6b006 31b10526f982435a82bfae75fcd6b006--8aa033c087ad4d2096490a729a1a3991 6c957802e7aa491888a770d8f916b94f 26a7ea3ffc7d4acc9823820a3deb7602 RX(phi) 321c9cf4a2d24d6989d4eb918dbbb356--26a7ea3ffc7d4acc9823820a3deb7602 22f69e25c0ad4cf8be8f93cbcc2203b0 3 fd6ec159555b410a865b369dd78dde49 RX(3.0*phi) 26a7ea3ffc7d4acc9823820a3deb7602--fd6ec159555b410a865b369dd78dde49 12db41068b93410ebff72e1d65ed119b RX(4.0*phi) fd6ec159555b410a865b369dd78dde49--12db41068b93410ebff72e1d65ed119b 12db41068b93410ebff72e1d65ed119b--6c957802e7aa491888a770d8f916b94f ec39f5defd7643b4b912a2d83cc19e24 a258cef667f444ca946e2b3543f503c0 RX(phi) 22f69e25c0ad4cf8be8f93cbcc2203b0--a258cef667f444ca946e2b3543f503c0 03229c17916547b380b9a89d75a9c2dd 4 4f936502b6a445f7a17965f3f34043de RX(4.0*phi) a258cef667f444ca946e2b3543f503c0--4f936502b6a445f7a17965f3f34043de 86f0e6033637466ba3ae7b6a488aec2c RX(8.0*phi) 4f936502b6a445f7a17965f3f34043de--86f0e6033637466ba3ae7b6a488aec2c 86f0e6033637466ba3ae7b6a488aec2c--ec39f5defd7643b4b912a2d83cc19e24 102a7f15e3f543e0a070f832d7fc6d75 8ba26fb073924190b6fee27dbd0e56fe RX(phi) 03229c17916547b380b9a89d75a9c2dd--8ba26fb073924190b6fee27dbd0e56fe 59c3025744b34000b435b686ea47a8ce RX(5.0*phi) 8ba26fb073924190b6fee27dbd0e56fe--59c3025744b34000b435b686ea47a8ce e8cb9b2a88ee4abaabf6d883f16fab20 RX(16.0*phi) 59c3025744b34000b435b686ea47a8ce--e8cb9b2a88ee4abaabf6d883f16fab20 e8cb9b2a88ee4abaabf6d883f16fab20--102a7f15e3f543e0a070f832d7fc6d75 <p>A custom scaling can also be defined with a function with an <code>int</code> input and <code>int</code> or <code>float</code> output.</p> <pre><code>n_qubits = 5\n\ndef custom_scaling(i: int) -&gt; int | float:\n    \"\"\"Sqrt(i+1)\"\"\"\n    return (i+1) ** (0.5)\n\n# Custom scaling function\nfm_custom = feature_map(n_qubits, fm_type=BasisSet.CHEBYSHEV, reupload_scaling=custom_scaling)\n</code></pre> %3 d36c6cf4993c4ad38f7ceddaac7971b9 0 6f8c93b04be544528da81abd89999bf1 RX(1.0*acos(phi)) d36c6cf4993c4ad38f7ceddaac7971b9--6f8c93b04be544528da81abd89999bf1 cf3e8250d5514e289df64d6d422d87a0 1 3e70bd325d2d43009bd3f9a3dc1b7fa7 6f8c93b04be544528da81abd89999bf1--3e70bd325d2d43009bd3f9a3dc1b7fa7 0556e0c99c334428b7419157c7d5fedc 5fb4105dda1740018fa92108a91010ce RX(1.414*acos(phi)) cf3e8250d5514e289df64d6d422d87a0--5fb4105dda1740018fa92108a91010ce dbc57ad632514aa78b56d8d122461d08 2 5fb4105dda1740018fa92108a91010ce--0556e0c99c334428b7419157c7d5fedc 361ebde3baae417f9bdf607d5b0653cd 030a92e3fa3e40a48ba9ef77270c91f4 RX(1.732*acos(phi)) dbc57ad632514aa78b56d8d122461d08--030a92e3fa3e40a48ba9ef77270c91f4 de2ec2bd3e4145e1a6f47833f9199e78 3 030a92e3fa3e40a48ba9ef77270c91f4--361ebde3baae417f9bdf607d5b0653cd 61e0bc4371db448f82969e0b47308271 0214a1a6a6d44ece8df463fc08516533 RX(2.0*acos(phi)) de2ec2bd3e4145e1a6f47833f9199e78--0214a1a6a6d44ece8df463fc08516533 091287cc86c148da85767de7322bece2 4 0214a1a6a6d44ece8df463fc08516533--61e0bc4371db448f82969e0b47308271 b0917777f1ec4dbba619383352f9fbac 64b3ff6531144486b39bb043a3538dbe RX(2.236*acos(phi)) 091287cc86c148da85767de7322bece2--64b3ff6531144486b39bb043a3538dbe 64b3ff6531144486b39bb043a3538dbe--b0917777f1ec4dbba619383352f9fbac <p>To add a trainable parameter that multiplies the feature parameter inside the encoding function, simply pass a <code>param_prefix</code> string:</p> <pre><code>n_qubits = 5\n\nfm_trainable = feature_map(\n    n_qubits,\n    fm_type=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.EXP,\n    param_prefix = \"w\",\n)\n</code></pre> %3 8e2408c141e249938164a34f62c0e7db 0 593d322fe9434519b7f26cc823f59eaa RX(1.0*phi*w\u2080) 8e2408c141e249938164a34f62c0e7db--593d322fe9434519b7f26cc823f59eaa ad02cc05438c44bbac80ab02e43c4fc5 1 8ce70a8a10b6469e8e615630d2c7d4c0 593d322fe9434519b7f26cc823f59eaa--8ce70a8a10b6469e8e615630d2c7d4c0 92cdfef090444e548b5500430d3decb4 b7a94a56357745d2a061d70c875f4c55 RX(2.0*phi*w\u2081) ad02cc05438c44bbac80ab02e43c4fc5--b7a94a56357745d2a061d70c875f4c55 c62f0aaf23e046d7aa9ac3e61e555b6c 2 b7a94a56357745d2a061d70c875f4c55--92cdfef090444e548b5500430d3decb4 bb2009d7363b4ee98c3850004a3d881d 205a29a279424d43af3d1fe3f90e3909 RX(4.0*phi*w\u2082) c62f0aaf23e046d7aa9ac3e61e555b6c--205a29a279424d43af3d1fe3f90e3909 e9d00edb47ad4f03bf52926c2f32be34 3 205a29a279424d43af3d1fe3f90e3909--bb2009d7363b4ee98c3850004a3d881d ba2bb7a9dcd749b0ac3e77409f0011af 99bd3509b72844b1b646587ae9dbead0 RX(8.0*phi*w\u2083) e9d00edb47ad4f03bf52926c2f32be34--99bd3509b72844b1b646587ae9dbead0 60f25ed384c6448da29e36a959aef574 4 99bd3509b72844b1b646587ae9dbead0--ba2bb7a9dcd749b0ac3e77409f0011af c6d136e98a934571a91515e5dd3b9b01 3fa63dd87adf4d74ad616b4cfa977d28 RX(16.0*phi*w\u2084) 60f25ed384c6448da29e36a959aef574--3fa63dd87adf4d74ad616b4cfa977d28 3fa63dd87adf4d74ad616b4cfa977d28--c6d136e98a934571a91515e5dd3b9b01 <p>Note that for the Fourier feature map, the encoding function is simply \\(f(x)=x\\). For other cases, like the Chebyshev <code>acos()</code> encoding, the trainable parameter may cause the feature value to be outside the domain of the encoding function. This will eventually be fixed by adding range constraints to trainable parameters in Qadence.</p> <p>A full description of the remaining arguments can be found in the <code>feature_map</code> API reference. We provide an example below.</p> <pre><code>from qadence import RY\n\nn_qubits = 5\n\n# Custom scaling function\nfm_full = feature_map(\n    n_qubits = n_qubits,\n    support = tuple(reversed(range(n_qubits))), # Reverse the qubit support to run the scaling from bottom to top\n    param = \"x\", # Change the name of the parameter\n    op = RY, # Change the rotation gate between RX, RY, RZ or PHASE\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.EXP,\n    feature_range = (-1.0, 2.0), # Range from which the input data comes from\n    target_range = (1.0, 3.0), # Range the encoder assumes as the natural range\n    multiplier = 5.0, # Extra multiplier, which can also be a Parameter\n    param_prefix = \"w\", # Add trainable parameters\n)\n</code></pre> %3 5320ea9b66a64914a066b3bb302ec391 0 d08ed481770b4aa8839ee4a44d327645 RY(80.0*acos(w\u2084*(0.667*x + 1.667))) 5320ea9b66a64914a066b3bb302ec391--d08ed481770b4aa8839ee4a44d327645 a84ab78de42349ee9e9bbc9ba2eea8a2 1 d5bc546db69d48e2bad92e7bf5c3c5a4 d08ed481770b4aa8839ee4a44d327645--d5bc546db69d48e2bad92e7bf5c3c5a4 1e326e1572bd4e68845690095cf85e42 d0981268c4b741a2b99ccc695fda5523 RY(40.0*acos(w\u2083*(0.667*x + 1.667))) a84ab78de42349ee9e9bbc9ba2eea8a2--d0981268c4b741a2b99ccc695fda5523 8c05768e507a4c1c9fe270fbfafd74d2 2 d0981268c4b741a2b99ccc695fda5523--1e326e1572bd4e68845690095cf85e42 bda5182d0dc145f2a9838abc8b4ce6a7 96cb6ac36e5c4971b82eb5b8fb8d6f73 RY(20.0*acos(w\u2082*(0.667*x + 1.667))) 8c05768e507a4c1c9fe270fbfafd74d2--96cb6ac36e5c4971b82eb5b8fb8d6f73 d2231370eb614ee2bb6d9f11afdbf67b 3 96cb6ac36e5c4971b82eb5b8fb8d6f73--bda5182d0dc145f2a9838abc8b4ce6a7 b051ea0397794e1ab70592778568bb34 24bec948a2a14a459dec64b91df4354a RY(10.0*acos(w\u2081*(0.667*x + 1.667))) d2231370eb614ee2bb6d9f11afdbf67b--24bec948a2a14a459dec64b91df4354a ccb53b56e11c4e80aea53b952355fbcf 4 24bec948a2a14a459dec64b91df4354a--b051ea0397794e1ab70592778568bb34 a7dbc4a9d5144f80b71e4a926e55f44a aaaccd52fc314cf4aae04b5d776a2606 RY(5.0*acos(w\u2080*(0.667*x + 1.667))) ccb53b56e11c4e80aea53b952355fbcf--aaaccd52fc314cf4aae04b5d776a2606 aaaccd52fc314cf4aae04b5d776a2606--a7dbc4a9d5144f80b71e4a926e55f44a"},{"location":"content/qml_constructors/#hardware-efficient-ansatz","title":"Hardware-efficient ansatz","text":"<p>Ansatze blocks for quantum machine-learning are typically built following the Hardware-Efficient Ansatz formalism (HEA). Both fully digital and digital-analog HEAs can easily be built with the <code>hea</code> function. By default, the digital version is returned:</p> <pre><code>from qadence import hea\nfrom qadence.draw import display\n\nn_qubits = 3\ndepth = 2\n\nansatz = hea(n_qubits, depth)\n</code></pre> %3 e8d8c088421e4caaae3d5cb513ae99ec 0 e5a4e0aebfaf44a4b62b91b8fa274295 RX(theta\u2080) e8d8c088421e4caaae3d5cb513ae99ec--e5a4e0aebfaf44a4b62b91b8fa274295 86d78846c1554aac91edb06560c004e3 1 1d3363bdceb54b5594bc40fc4b0b8644 RY(theta\u2083) e5a4e0aebfaf44a4b62b91b8fa274295--1d3363bdceb54b5594bc40fc4b0b8644 30fd45e5856044c0843f2376dbe6aa24 RX(theta\u2086) 1d3363bdceb54b5594bc40fc4b0b8644--30fd45e5856044c0843f2376dbe6aa24 b6b011fc938a412b98f175cb1987ee2b 30fd45e5856044c0843f2376dbe6aa24--b6b011fc938a412b98f175cb1987ee2b 26be9be3a89b457fbbcb101c3d9bbb06 b6b011fc938a412b98f175cb1987ee2b--26be9be3a89b457fbbcb101c3d9bbb06 4353d96fc6db4613acd2af32e31f99e0 RX(theta\u2089) 26be9be3a89b457fbbcb101c3d9bbb06--4353d96fc6db4613acd2af32e31f99e0 1da882fae3c845e4b6456b998e9ddc94 RY(theta\u2081\u2082) 4353d96fc6db4613acd2af32e31f99e0--1da882fae3c845e4b6456b998e9ddc94 0468ddb6958041449ae91bb2424ee5fe RX(theta\u2081\u2085) 1da882fae3c845e4b6456b998e9ddc94--0468ddb6958041449ae91bb2424ee5fe ad046db2e0434252b5f1df4ce8c2fb9f 0468ddb6958041449ae91bb2424ee5fe--ad046db2e0434252b5f1df4ce8c2fb9f bfaf0e121fc5452baa02ae23f871e746 ad046db2e0434252b5f1df4ce8c2fb9f--bfaf0e121fc5452baa02ae23f871e746 1e28063413434f76a0e982f94c398a09 bfaf0e121fc5452baa02ae23f871e746--1e28063413434f76a0e982f94c398a09 60ff9bbc3ec8473b982b836aea76d5ed d3238d7648af4e6c9ff1acbd0cbc1059 RX(theta\u2081) 86d78846c1554aac91edb06560c004e3--d3238d7648af4e6c9ff1acbd0cbc1059 f94e533349874b08bf79a722383472b2 2 f1b4ca28223b47168cb9c37ad5140c2a RY(theta\u2084) d3238d7648af4e6c9ff1acbd0cbc1059--f1b4ca28223b47168cb9c37ad5140c2a 4e0bedf9acd84b91a9674e02885200e7 RX(theta\u2087) f1b4ca28223b47168cb9c37ad5140c2a--4e0bedf9acd84b91a9674e02885200e7 646b5868a62b48bf8406cad6fc12b967 X 4e0bedf9acd84b91a9674e02885200e7--646b5868a62b48bf8406cad6fc12b967 646b5868a62b48bf8406cad6fc12b967--b6b011fc938a412b98f175cb1987ee2b d499a22f8587418b89948f7fa0aade8b 646b5868a62b48bf8406cad6fc12b967--d499a22f8587418b89948f7fa0aade8b 383e01bac17e41bc964a8d8c4f045fa3 RX(theta\u2081\u2080) d499a22f8587418b89948f7fa0aade8b--383e01bac17e41bc964a8d8c4f045fa3 549e8c4952b043e884075a16ba5ac3b9 RY(theta\u2081\u2083) 383e01bac17e41bc964a8d8c4f045fa3--549e8c4952b043e884075a16ba5ac3b9 024f113c561746f5a106a48a80fd7dcc RX(theta\u2081\u2086) 549e8c4952b043e884075a16ba5ac3b9--024f113c561746f5a106a48a80fd7dcc da3bf4c411004829a778079ed856f869 X 024f113c561746f5a106a48a80fd7dcc--da3bf4c411004829a778079ed856f869 da3bf4c411004829a778079ed856f869--ad046db2e0434252b5f1df4ce8c2fb9f 59082e4c0b6f4d5ca18523a74f6d2da6 da3bf4c411004829a778079ed856f869--59082e4c0b6f4d5ca18523a74f6d2da6 59082e4c0b6f4d5ca18523a74f6d2da6--60ff9bbc3ec8473b982b836aea76d5ed 2c5428b5d2b2405d91c4ba0bb9040e50 8538e61bbeb54ff985012d13d5805751 RX(theta\u2082) f94e533349874b08bf79a722383472b2--8538e61bbeb54ff985012d13d5805751 c5b871d5e5c949a1b48a61b1dff05116 RY(theta\u2085) 8538e61bbeb54ff985012d13d5805751--c5b871d5e5c949a1b48a61b1dff05116 74892719f690478c852b44885f3de143 RX(theta\u2088) c5b871d5e5c949a1b48a61b1dff05116--74892719f690478c852b44885f3de143 9b7a201dab1e499ab46e4a42c8ba095c 74892719f690478c852b44885f3de143--9b7a201dab1e499ab46e4a42c8ba095c bbb76f3cbd33425db769b58df3afead4 X 9b7a201dab1e499ab46e4a42c8ba095c--bbb76f3cbd33425db769b58df3afead4 bbb76f3cbd33425db769b58df3afead4--d499a22f8587418b89948f7fa0aade8b 870e3ce967cc4ca8b670cbf6b96cb8e3 RX(theta\u2081\u2081) bbb76f3cbd33425db769b58df3afead4--870e3ce967cc4ca8b670cbf6b96cb8e3 6e7a14ec62f34759848089710dec5e86 RY(theta\u2081\u2084) 870e3ce967cc4ca8b670cbf6b96cb8e3--6e7a14ec62f34759848089710dec5e86 6e767c2e085c4a8280cdda3efbed62d9 RX(theta\u2081\u2087) 6e7a14ec62f34759848089710dec5e86--6e767c2e085c4a8280cdda3efbed62d9 a210e1a9934f475c98cbed99dd48bf0d 6e767c2e085c4a8280cdda3efbed62d9--a210e1a9934f475c98cbed99dd48bf0d 4607d92fa3f54ab28be2d8940363d2ce X a210e1a9934f475c98cbed99dd48bf0d--4607d92fa3f54ab28be2d8940363d2ce 4607d92fa3f54ab28be2d8940363d2ce--59082e4c0b6f4d5ca18523a74f6d2da6 4607d92fa3f54ab28be2d8940363d2ce--2c5428b5d2b2405d91c4ba0bb9040e50 <p>As seen above, the rotation layers are automatically parameterized, and the prefix <code>\"theta\"</code> can be changed with the <code>param_prefix</code> argument.</p> <p>Furthermore, both the single-qubit rotations and the two-qubit entangler can be customized with the <code>operations</code> and <code>entangler</code> argument. The operations can be passed as a list of single-qubit rotations, while the entangler should be either <code>CNOT</code>, <code>CZ</code>, <code>CRX</code>, <code>CRY</code>, <code>CRZ</code> or <code>CPHASE</code>.</p> <pre><code>from qadence import RX, RY, CPHASE\n\nansatz = hea(\n    n_qubits=n_qubits,\n    depth=depth,\n    param_prefix=\"phi\",\n    operations=[RX, RY, RX],\n    entangler=CPHASE\n)\n</code></pre> %3 db8d6fe1b5864b37ba945ba21ec99a5b 0 fdd5a45bfee24b3baad69ea0b9ea1df1 RX(phi\u2080) db8d6fe1b5864b37ba945ba21ec99a5b--fdd5a45bfee24b3baad69ea0b9ea1df1 dc14cbd454694ea2b018321779420e3f 1 18e231e8a4e54e96be5d5e19ff6dc3be RY(phi\u2083) fdd5a45bfee24b3baad69ea0b9ea1df1--18e231e8a4e54e96be5d5e19ff6dc3be c2cab519bdcf4a7f850de3aad122355c RX(phi\u2086) 18e231e8a4e54e96be5d5e19ff6dc3be--c2cab519bdcf4a7f850de3aad122355c b40c7e1889044d4594d1cbc9443b7e47 c2cab519bdcf4a7f850de3aad122355c--b40c7e1889044d4594d1cbc9443b7e47 e54526b67de14551b4dc60fd290c45c2 b40c7e1889044d4594d1cbc9443b7e47--e54526b67de14551b4dc60fd290c45c2 6a0148d64baa4d4e8bc27959e831cf50 RX(phi\u2089) e54526b67de14551b4dc60fd290c45c2--6a0148d64baa4d4e8bc27959e831cf50 d817f1b5507848838519ee3ec0482b8c RY(phi\u2081\u2082) 6a0148d64baa4d4e8bc27959e831cf50--d817f1b5507848838519ee3ec0482b8c 80d3e3d5f92d45bf9578f48f90a4b1fd RX(phi\u2081\u2085) d817f1b5507848838519ee3ec0482b8c--80d3e3d5f92d45bf9578f48f90a4b1fd 9135a26c6f0240e98f6a43b35a150ea1 80d3e3d5f92d45bf9578f48f90a4b1fd--9135a26c6f0240e98f6a43b35a150ea1 3fd49d32e183440bb5ce6ef7b1be19e2 9135a26c6f0240e98f6a43b35a150ea1--3fd49d32e183440bb5ce6ef7b1be19e2 cb09774d98f64fb3b2f4dffc78a6c573 3fd49d32e183440bb5ce6ef7b1be19e2--cb09774d98f64fb3b2f4dffc78a6c573 d9331460cb8742c381758c426949e939 58bb3e0ff13e4f108c399596aa8d4322 RX(phi\u2081) dc14cbd454694ea2b018321779420e3f--58bb3e0ff13e4f108c399596aa8d4322 aac496b68123424cab2a5095de233aa3 2 0592480899fa49039c1a7581e34a609c RY(phi\u2084) 58bb3e0ff13e4f108c399596aa8d4322--0592480899fa49039c1a7581e34a609c cef891a174e94998972349d3054a8ba2 RX(phi\u2087) 0592480899fa49039c1a7581e34a609c--cef891a174e94998972349d3054a8ba2 0d92c0a3eefc4d8989ddbb6df504f1b7 PHASE(phi_ent\u2080) cef891a174e94998972349d3054a8ba2--0d92c0a3eefc4d8989ddbb6df504f1b7 0d92c0a3eefc4d8989ddbb6df504f1b7--b40c7e1889044d4594d1cbc9443b7e47 d84c8a948d4444349029eadbc62f2e63 0d92c0a3eefc4d8989ddbb6df504f1b7--d84c8a948d4444349029eadbc62f2e63 997addaadb5e4afcbce17affeab2bd75 RX(phi\u2081\u2080) d84c8a948d4444349029eadbc62f2e63--997addaadb5e4afcbce17affeab2bd75 8e65064ee02247758807476d856b98ef RY(phi\u2081\u2083) 997addaadb5e4afcbce17affeab2bd75--8e65064ee02247758807476d856b98ef 62323a2a57404c9f9c7bacb9247fe3d4 RX(phi\u2081\u2086) 8e65064ee02247758807476d856b98ef--62323a2a57404c9f9c7bacb9247fe3d4 4419dab5a18146e7bae8b637a2975e09 PHASE(phi_ent\u2082) 62323a2a57404c9f9c7bacb9247fe3d4--4419dab5a18146e7bae8b637a2975e09 4419dab5a18146e7bae8b637a2975e09--9135a26c6f0240e98f6a43b35a150ea1 7cc51e76064341529102b84509e27855 4419dab5a18146e7bae8b637a2975e09--7cc51e76064341529102b84509e27855 7cc51e76064341529102b84509e27855--d9331460cb8742c381758c426949e939 38d8e31261454d79a80c7813149f6ae0 0579f4bf03714c3cb23878d3719fb65b RX(phi\u2082) aac496b68123424cab2a5095de233aa3--0579f4bf03714c3cb23878d3719fb65b 18179b27a7844a9f831d5839e5841ab6 RY(phi\u2085) 0579f4bf03714c3cb23878d3719fb65b--18179b27a7844a9f831d5839e5841ab6 9faff0d774784477ae093411c4379e11 RX(phi\u2088) 18179b27a7844a9f831d5839e5841ab6--9faff0d774784477ae093411c4379e11 6306b73ab9e5424a990942d181130339 9faff0d774784477ae093411c4379e11--6306b73ab9e5424a990942d181130339 0de8a54fe8de49fd82f9bd8d62b66632 PHASE(phi_ent\u2081) 6306b73ab9e5424a990942d181130339--0de8a54fe8de49fd82f9bd8d62b66632 0de8a54fe8de49fd82f9bd8d62b66632--d84c8a948d4444349029eadbc62f2e63 cb3428b5359d47628709e6997f7ce5f1 RX(phi\u2081\u2081) 0de8a54fe8de49fd82f9bd8d62b66632--cb3428b5359d47628709e6997f7ce5f1 e8eac508cc3445ba8e614b9d9106e9de RY(phi\u2081\u2084) cb3428b5359d47628709e6997f7ce5f1--e8eac508cc3445ba8e614b9d9106e9de e1ef9fd7ea2242a19b8ac16f7925e86f RX(phi\u2081\u2087) e8eac508cc3445ba8e614b9d9106e9de--e1ef9fd7ea2242a19b8ac16f7925e86f 79689dcf84a14e6b9ff20ec2e8740c86 e1ef9fd7ea2242a19b8ac16f7925e86f--79689dcf84a14e6b9ff20ec2e8740c86 0d4a2708775547628a2a2684f2d2858e PHASE(phi_ent\u2083) 79689dcf84a14e6b9ff20ec2e8740c86--0d4a2708775547628a2a2684f2d2858e 0d4a2708775547628a2a2684f2d2858e--7cc51e76064341529102b84509e27855 0d4a2708775547628a2a2684f2d2858e--38d8e31261454d79a80c7813149f6ae0 <p>Having a truly hardware-efficient ansatz means that the entangling operation can be chosen according to each device's native interactions. Besides digital operations, in Qadence it is also possible to build digital-analog HEAs with the entanglement produced by the natural evolution of a set of interacting qubits, as natively implemented in neutral atom devices. As with other digital-analog functions, this can be controlled with the <code>strategy</code> argument which can be chosen from the <code>Strategy</code> enum type. Currently, only <code>Strategy.DIGITAL</code> and <code>Strategy.SDAQC</code> are available. By default, calling <code>strategy = Strategy.SDAQC</code> will use a global entangling Hamiltonian with Ising-like \\(NN\\) interactions and constant interaction strength,</p> <pre><code>from qadence import Strategy\n\nansatz = hea(\n    n_qubits,\n    depth=depth,\n    strategy=Strategy.SDAQC\n)\n</code></pre> %3 cluster_8f140863d749468d99ad12dddce80d9c cluster_f974b8963f4d4f20921210e0718485e1 16a0e56a5809484bafbb97a914e2e84b 0 0c9b308ad6c5457eab9ce12fc2fadfb6 RX(theta\u2080) 16a0e56a5809484bafbb97a914e2e84b--0c9b308ad6c5457eab9ce12fc2fadfb6 4de7b68beed445b3a55b713038b550f4 1 cfa5bd50fcc048ef94912a6194d587fd RY(theta\u2083) 0c9b308ad6c5457eab9ce12fc2fadfb6--cfa5bd50fcc048ef94912a6194d587fd 3477f5ff849347e4bf3959ba031036f0 RX(theta\u2086) cfa5bd50fcc048ef94912a6194d587fd--3477f5ff849347e4bf3959ba031036f0 b42c086cc21b4e8ca59c0b89ca837f8e HamEvo 3477f5ff849347e4bf3959ba031036f0--b42c086cc21b4e8ca59c0b89ca837f8e 8f9a66b228ec442f8e53f785cdedce17 RX(theta\u2089) b42c086cc21b4e8ca59c0b89ca837f8e--8f9a66b228ec442f8e53f785cdedce17 7ab709a5a35c4347972a6f5bfc9a740f RY(theta\u2081\u2082) 8f9a66b228ec442f8e53f785cdedce17--7ab709a5a35c4347972a6f5bfc9a740f a5c8152539824f918e93091bef3f76ae RX(theta\u2081\u2085) 7ab709a5a35c4347972a6f5bfc9a740f--a5c8152539824f918e93091bef3f76ae f20b109c8fc744059fe16452ce273c89 HamEvo a5c8152539824f918e93091bef3f76ae--f20b109c8fc744059fe16452ce273c89 f30567b5c7d64e2aa984fec487df8866 f20b109c8fc744059fe16452ce273c89--f30567b5c7d64e2aa984fec487df8866 4ba520d865f14e5aba18c17b35d35e27 ece8d6192e3b44708b81fe521975671e RX(theta\u2081) 4de7b68beed445b3a55b713038b550f4--ece8d6192e3b44708b81fe521975671e 37e8ad8a16ea4ff3bd82161880777dce 2 8312c499e31842f893c4475859bb2725 RY(theta\u2084) ece8d6192e3b44708b81fe521975671e--8312c499e31842f893c4475859bb2725 aa67a0238f524cefbb937d4a15605ca7 RX(theta\u2087) 8312c499e31842f893c4475859bb2725--aa67a0238f524cefbb937d4a15605ca7 e8457aee178046b7ab229939b680972f t = theta_t\u2080 aa67a0238f524cefbb937d4a15605ca7--e8457aee178046b7ab229939b680972f 737c9b58ff1c41e89051596b95d00286 RX(theta\u2081\u2080) e8457aee178046b7ab229939b680972f--737c9b58ff1c41e89051596b95d00286 a61c564c6587491b97afb1e4464b6e8e RY(theta\u2081\u2083) 737c9b58ff1c41e89051596b95d00286--a61c564c6587491b97afb1e4464b6e8e 199fdec29ead43ae94457fab031f2c89 RX(theta\u2081\u2086) a61c564c6587491b97afb1e4464b6e8e--199fdec29ead43ae94457fab031f2c89 4891c81db6a6421cb78846bf9dd94149 t = theta_t\u2081 199fdec29ead43ae94457fab031f2c89--4891c81db6a6421cb78846bf9dd94149 4891c81db6a6421cb78846bf9dd94149--4ba520d865f14e5aba18c17b35d35e27 49c4a631e93348d38285441f63f3e608 7547c92a2af440ac8246a0943fd27697 RX(theta\u2082) 37e8ad8a16ea4ff3bd82161880777dce--7547c92a2af440ac8246a0943fd27697 bd3441a7ca0b43ec8f6e0adc4ba0f874 RY(theta\u2085) 7547c92a2af440ac8246a0943fd27697--bd3441a7ca0b43ec8f6e0adc4ba0f874 6fcd777994304319a805d7eca6513e4c RX(theta\u2088) bd3441a7ca0b43ec8f6e0adc4ba0f874--6fcd777994304319a805d7eca6513e4c fb74a2e045e545148335248f106c18e3 6fcd777994304319a805d7eca6513e4c--fb74a2e045e545148335248f106c18e3 56686b450c104a97ae5984ca45de1c18 RX(theta\u2081\u2081) fb74a2e045e545148335248f106c18e3--56686b450c104a97ae5984ca45de1c18 bc10416dd95249bba50f86fb9d10ad05 RY(theta\u2081\u2084) 56686b450c104a97ae5984ca45de1c18--bc10416dd95249bba50f86fb9d10ad05 d366e73975234444b830016deb82c674 RX(theta\u2081\u2087) bc10416dd95249bba50f86fb9d10ad05--d366e73975234444b830016deb82c674 9b06a55c9aee48c5be0ac2e3258c515f d366e73975234444b830016deb82c674--9b06a55c9aee48c5be0ac2e3258c515f 9b06a55c9aee48c5be0ac2e3258c515f--49c4a631e93348d38285441f63f3e608 <p>Note that, by default, only the time-parameter is automatically parameterized when building a digital-analog HEA. However, as described in the Hamiltonians tutorial, arbitrary interaction Hamiltonians can be easily built with the <code>hamiltonian_factory</code> function, with both customized or fully parameterized interactions, and these can be directly passed as the <code>entangler</code> for a customizable digital-analog HEA.</p> <pre><code>from qadence import hamiltonian_factory, Interaction, N, Register, hea\n\n# Build a parameterized neutral-atom Hamiltonian following a honeycomb_lattice:\nregister = Register.honeycomb_lattice(1, 1)\n\nentangler = hamiltonian_factory(\n    register,\n    interaction=Interaction.NN,\n    detuning=N,\n    interaction_strength=\"e\",\n    detuning_strength=\"n\"\n)\n\n# Build a fully parameterized Digital-Analog HEA:\nn_qubits = register.n_qubits\ndepth = 2\n\nansatz = hea(\n    n_qubits=register.n_qubits,\n    depth=depth,\n    operations=[RX, RY, RX],\n    entangler=entangler,\n    strategy=Strategy.SDAQC\n)\n</code></pre> %3 cluster_927c31e458b3401dbcc652629aa4fa5b cluster_9339c119621b4f398f66c1021642652e 926c71ba04c247f88161d3973e9d591b 0 9cfb8ebb264b43ad89f2ae0068c01ace RX(theta\u2080) 926c71ba04c247f88161d3973e9d591b--9cfb8ebb264b43ad89f2ae0068c01ace e236d4adaa944195ba949fa77306098e 1 e3fe0461b4994f7cb26ba7c82f370371 RY(theta\u2086) 9cfb8ebb264b43ad89f2ae0068c01ace--e3fe0461b4994f7cb26ba7c82f370371 fde9b6c883c24d608226e88094b93bc8 RX(theta\u2081\u2082) e3fe0461b4994f7cb26ba7c82f370371--fde9b6c883c24d608226e88094b93bc8 962d8167d26d4665a0b1faf9c12fbfea fde9b6c883c24d608226e88094b93bc8--962d8167d26d4665a0b1faf9c12fbfea 93a6e5943de44896bb2476962706c78f RX(theta\u2081\u2088) 962d8167d26d4665a0b1faf9c12fbfea--93a6e5943de44896bb2476962706c78f a5ae63398834429e9a08371f099e62b6 RY(theta\u2082\u2084) 93a6e5943de44896bb2476962706c78f--a5ae63398834429e9a08371f099e62b6 b8bfff00d01b464c8cd04e5a795a89a8 RX(theta\u2083\u2080) a5ae63398834429e9a08371f099e62b6--b8bfff00d01b464c8cd04e5a795a89a8 c8151ec395e0496b81b13b70ea74249f b8bfff00d01b464c8cd04e5a795a89a8--c8151ec395e0496b81b13b70ea74249f b79d0aa268f74512b40c2bd43289a471 c8151ec395e0496b81b13b70ea74249f--b79d0aa268f74512b40c2bd43289a471 a8408bbba0014cf2b850f7a67ded78b2 e4eb98bc9d69442f9c70c770566d299a RX(theta\u2081) e236d4adaa944195ba949fa77306098e--e4eb98bc9d69442f9c70c770566d299a 31af0dcc1c874dd38dd83eee7234abe6 2 8bb3bc74da2b4cfb88120059d0aa6eab RY(theta\u2087) e4eb98bc9d69442f9c70c770566d299a--8bb3bc74da2b4cfb88120059d0aa6eab a6911b99f6734b0b98d0b59c7830177c RX(theta\u2081\u2083) 8bb3bc74da2b4cfb88120059d0aa6eab--a6911b99f6734b0b98d0b59c7830177c fa187e6acc4849c1812c46fb6ae781ef a6911b99f6734b0b98d0b59c7830177c--fa187e6acc4849c1812c46fb6ae781ef a651fb0a9b6d46079c0910c553b6f199 RX(theta\u2081\u2089) fa187e6acc4849c1812c46fb6ae781ef--a651fb0a9b6d46079c0910c553b6f199 b8f6682407f242629b13456167b11207 RY(theta\u2082\u2085) a651fb0a9b6d46079c0910c553b6f199--b8f6682407f242629b13456167b11207 db575994ba1e44ae81633e1682e7d206 RX(theta\u2083\u2081) b8f6682407f242629b13456167b11207--db575994ba1e44ae81633e1682e7d206 fa642e78fe1c4ce4b0e89a7317e0be84 db575994ba1e44ae81633e1682e7d206--fa642e78fe1c4ce4b0e89a7317e0be84 fa642e78fe1c4ce4b0e89a7317e0be84--a8408bbba0014cf2b850f7a67ded78b2 c29bc80d718a4d4f82a2b108d81d37a0 130e744d0bad4078837677e49019bebd RX(theta\u2082) 31af0dcc1c874dd38dd83eee7234abe6--130e744d0bad4078837677e49019bebd e3214ad5f73a4436b85c525538907483 3 d1bb17357c614fe69fedabd045248ada RY(theta\u2088) 130e744d0bad4078837677e49019bebd--d1bb17357c614fe69fedabd045248ada bfbb0d41327a4f4aae8810e2cccedbfd RX(theta\u2081\u2084) d1bb17357c614fe69fedabd045248ada--bfbb0d41327a4f4aae8810e2cccedbfd 098706a4d92d49898971e8588038c026 HamEvo bfbb0d41327a4f4aae8810e2cccedbfd--098706a4d92d49898971e8588038c026 48aa579d5f2240749c9247c94bbe2d11 RX(theta\u2082\u2080) 098706a4d92d49898971e8588038c026--48aa579d5f2240749c9247c94bbe2d11 c9c3cb0d01e140b381df68f5b4008b68 RY(theta\u2082\u2086) 48aa579d5f2240749c9247c94bbe2d11--c9c3cb0d01e140b381df68f5b4008b68 4ac4997afe234f76afe2e44301e8b4a0 RX(theta\u2083\u2082) c9c3cb0d01e140b381df68f5b4008b68--4ac4997afe234f76afe2e44301e8b4a0 eb0956d255c9426fa14a170fefa89639 HamEvo 4ac4997afe234f76afe2e44301e8b4a0--eb0956d255c9426fa14a170fefa89639 eb0956d255c9426fa14a170fefa89639--c29bc80d718a4d4f82a2b108d81d37a0 1f6d0eca88754a8a8500400bb7560249 8855ed5a338949aa894c96ce82052560 RX(theta\u2083) e3214ad5f73a4436b85c525538907483--8855ed5a338949aa894c96ce82052560 ebbb1dd76ae84ae3b7058a60e6e19244 4 f18ffa3a99ab45b9938f5c10eec349cb RY(theta\u2089) 8855ed5a338949aa894c96ce82052560--f18ffa3a99ab45b9938f5c10eec349cb c414ef1d0b0c4065be1184b1ff4e15df RX(theta\u2081\u2085) f18ffa3a99ab45b9938f5c10eec349cb--c414ef1d0b0c4065be1184b1ff4e15df 8816c73438dd4b978f20fdd465cabf16 t = theta_t\u2080 c414ef1d0b0c4065be1184b1ff4e15df--8816c73438dd4b978f20fdd465cabf16 5621ec502f7e4f918290ad7733d9bbca RX(theta\u2082\u2081) 8816c73438dd4b978f20fdd465cabf16--5621ec502f7e4f918290ad7733d9bbca ba8d26fa49cf4e8b867184fe0c483508 RY(theta\u2082\u2087) 5621ec502f7e4f918290ad7733d9bbca--ba8d26fa49cf4e8b867184fe0c483508 bcdcf2839281458eaf8598c75c52e7e3 RX(theta\u2083\u2083) ba8d26fa49cf4e8b867184fe0c483508--bcdcf2839281458eaf8598c75c52e7e3 2424e4efc0d9404fa1eb6d41ef3275f8 t = theta_t\u2081 bcdcf2839281458eaf8598c75c52e7e3--2424e4efc0d9404fa1eb6d41ef3275f8 2424e4efc0d9404fa1eb6d41ef3275f8--1f6d0eca88754a8a8500400bb7560249 a42c62328bea45c5adf3e29a13fa18d7 fb670ece919d4e0e9b81517a543a1dc4 RX(theta\u2084) ebbb1dd76ae84ae3b7058a60e6e19244--fb670ece919d4e0e9b81517a543a1dc4 b560ddeab036438fa54ff3187bed1948 5 91648c03347046628cfff4db02e4d58e RY(theta\u2081\u2080) fb670ece919d4e0e9b81517a543a1dc4--91648c03347046628cfff4db02e4d58e bbb037e63dcb4ee799a3fe018789db24 RX(theta\u2081\u2086) 91648c03347046628cfff4db02e4d58e--bbb037e63dcb4ee799a3fe018789db24 83ec7a5f0ef340b89e377f00fc897a84 bbb037e63dcb4ee799a3fe018789db24--83ec7a5f0ef340b89e377f00fc897a84 86b828d5c3e04434969ad8c1427d6731 RX(theta\u2082\u2082) 83ec7a5f0ef340b89e377f00fc897a84--86b828d5c3e04434969ad8c1427d6731 33cb8db354d04fcf8e1c5fbe83bfc3f6 RY(theta\u2082\u2088) 86b828d5c3e04434969ad8c1427d6731--33cb8db354d04fcf8e1c5fbe83bfc3f6 b538917978ac43499e7348d6706e97ef RX(theta\u2083\u2084) 33cb8db354d04fcf8e1c5fbe83bfc3f6--b538917978ac43499e7348d6706e97ef b4634fbed008472a98f18c4a0260b803 b538917978ac43499e7348d6706e97ef--b4634fbed008472a98f18c4a0260b803 b4634fbed008472a98f18c4a0260b803--a42c62328bea45c5adf3e29a13fa18d7 9648049e78794183bc431ac6e282e0e7 96602eb3332a4b859975328a22770f8a RX(theta\u2085) b560ddeab036438fa54ff3187bed1948--96602eb3332a4b859975328a22770f8a 117205a003c841ff8b7ba029546bb94e RY(theta\u2081\u2081) 96602eb3332a4b859975328a22770f8a--117205a003c841ff8b7ba029546bb94e c7c2bbbb183e4a098f58e64c3e7cc839 RX(theta\u2081\u2087) 117205a003c841ff8b7ba029546bb94e--c7c2bbbb183e4a098f58e64c3e7cc839 45eca65c6ce64928b086703e8fd282c0 c7c2bbbb183e4a098f58e64c3e7cc839--45eca65c6ce64928b086703e8fd282c0 6cf95e24cdc847d3a4e8842bd3ec3eff RX(theta\u2082\u2083) 45eca65c6ce64928b086703e8fd282c0--6cf95e24cdc847d3a4e8842bd3ec3eff 34bf40a3bded4a3b90f25ed92584598e RY(theta\u2082\u2089) 6cf95e24cdc847d3a4e8842bd3ec3eff--34bf40a3bded4a3b90f25ed92584598e 4d80ab4562fd4997b24e09f490bcd6a6 RX(theta\u2083\u2085) 34bf40a3bded4a3b90f25ed92584598e--4d80ab4562fd4997b24e09f490bcd6a6 36c291682dbd47b88632c2d6cd33e9ab 4d80ab4562fd4997b24e09f490bcd6a6--36c291682dbd47b88632c2d6cd33e9ab 36c291682dbd47b88632c2d6cd33e9ab--9648049e78794183bc431ac6e282e0e7"},{"location":"content/qml_constructors/#identity-initialized-ansatz","title":"Identity-initialized ansatz","text":"<p>It is widely known that parametrized quantum circuits are characterized by barren plateaus, where the gradient becomes exponentially small in the number of qubits. Here we include one of many techniques that have been proposed in recent years to mitigate this effect and facilitate <code>QNN</code>s training: Grant et al. showed that initializing the weights of a <code>QNN</code> so that each block of the circuit evaluates to identity reduces the effect of barren plateaus in the initial stage of training. In a similar fashion to <code>hea</code>, such circuit can be created via calling the associated function, <code>identity_initialized_ansatz</code>:</p> <pre><code>from qadence.constructors import identity_initialized_ansatz\nfrom qadence.draw import display\n\nn_qubits = 3\ndepth = 2\n\nansatz = identity_initialized_ansatz(n_qubits, depth)\n</code></pre> %3 cluster_53df082dd2f147289f60f1358af22664 BPMA-1 cluster_e73d1c238abe467d99d77e95394d0cad BPMA-0 6a72eb002d1243b9bc643e42eafccc62 0 a8ca81fb379a449b96f52f9a25d5fa1c RX(iia_\u03b1\u2080\u2080) 6a72eb002d1243b9bc643e42eafccc62--a8ca81fb379a449b96f52f9a25d5fa1c 2c5d9626a61547128151f8bc0aab037b 1 fa84b500853846218d25deae2ab54f7a RY(iia_\u03b1\u2080\u2083) a8ca81fb379a449b96f52f9a25d5fa1c--fa84b500853846218d25deae2ab54f7a af64d626dce049b0b63b27a4408d9df4 fa84b500853846218d25deae2ab54f7a--af64d626dce049b0b63b27a4408d9df4 4415be447292426e8a44091d8ff6bca0 af64d626dce049b0b63b27a4408d9df4--4415be447292426e8a44091d8ff6bca0 cba8d8659d484d9d945795c83c2dc843 RX(iia_\u03b3\u2080\u2080) 4415be447292426e8a44091d8ff6bca0--cba8d8659d484d9d945795c83c2dc843 f1422dfb858c4459996837f0d73657b7 cba8d8659d484d9d945795c83c2dc843--f1422dfb858c4459996837f0d73657b7 71e2dab0d3d84185993dabd5887da974 f1422dfb858c4459996837f0d73657b7--71e2dab0d3d84185993dabd5887da974 9690e7def98b49b6aab24b5e46b6ca51 RY(iia_\u03b2\u2080\u2083) 71e2dab0d3d84185993dabd5887da974--9690e7def98b49b6aab24b5e46b6ca51 96d69ebe41d5410dbff47b71fbc0d950 RX(iia_\u03b2\u2080\u2080) 9690e7def98b49b6aab24b5e46b6ca51--96d69ebe41d5410dbff47b71fbc0d950 eb1998129b14462eb34db547a5ee8528 RX(iia_\u03b1\u2081\u2080) 96d69ebe41d5410dbff47b71fbc0d950--eb1998129b14462eb34db547a5ee8528 59ae720eeec249b49dc6cc22c63118ca RY(iia_\u03b1\u2081\u2083) eb1998129b14462eb34db547a5ee8528--59ae720eeec249b49dc6cc22c63118ca 1324cd5e4ade4c0cb02eefb9d5215b69 59ae720eeec249b49dc6cc22c63118ca--1324cd5e4ade4c0cb02eefb9d5215b69 72b7c66ec96549228a69c8e9f8365b9d 1324cd5e4ade4c0cb02eefb9d5215b69--72b7c66ec96549228a69c8e9f8365b9d 8e69b128235f47bbb0524f3d88d3b9cf RX(iia_\u03b3\u2081\u2080) 72b7c66ec96549228a69c8e9f8365b9d--8e69b128235f47bbb0524f3d88d3b9cf 61118e48182e4967b66ebae95e7b69c1 8e69b128235f47bbb0524f3d88d3b9cf--61118e48182e4967b66ebae95e7b69c1 2842810380714c0b9e5013a68468605a 61118e48182e4967b66ebae95e7b69c1--2842810380714c0b9e5013a68468605a 37cdd980ef00466693fc3e8ed04d3298 RY(iia_\u03b2\u2081\u2083) 2842810380714c0b9e5013a68468605a--37cdd980ef00466693fc3e8ed04d3298 49cb094fbf4f4c609c8e21bd41d74f71 RX(iia_\u03b2\u2081\u2080) 37cdd980ef00466693fc3e8ed04d3298--49cb094fbf4f4c609c8e21bd41d74f71 1d8caf4ee25d42fab7030e675275696a 49cb094fbf4f4c609c8e21bd41d74f71--1d8caf4ee25d42fab7030e675275696a 0225fcdb798846889a4afea7577f9409 4599f7abb87e47eeb180e3cb0bceaa3a RX(iia_\u03b1\u2080\u2081) 2c5d9626a61547128151f8bc0aab037b--4599f7abb87e47eeb180e3cb0bceaa3a 56ed15dee5634196a03df8bc8eb24e45 2 37526502d7fc4d4bb63820d6b973f2dc RY(iia_\u03b1\u2080\u2084) 4599f7abb87e47eeb180e3cb0bceaa3a--37526502d7fc4d4bb63820d6b973f2dc 7dcadeee445445a289ab70deac87dfa4 X 37526502d7fc4d4bb63820d6b973f2dc--7dcadeee445445a289ab70deac87dfa4 7dcadeee445445a289ab70deac87dfa4--af64d626dce049b0b63b27a4408d9df4 c1d21d485ba1442f913b0b9c6f833a31 7dcadeee445445a289ab70deac87dfa4--c1d21d485ba1442f913b0b9c6f833a31 489a6512268c47198bc24df39a12a4bb RX(iia_\u03b3\u2080\u2081) c1d21d485ba1442f913b0b9c6f833a31--489a6512268c47198bc24df39a12a4bb 8406f47073314613be0d334cc7151e09 489a6512268c47198bc24df39a12a4bb--8406f47073314613be0d334cc7151e09 eba67a8ccf4d4d4faa632ab25cbb09d8 X 8406f47073314613be0d334cc7151e09--eba67a8ccf4d4d4faa632ab25cbb09d8 eba67a8ccf4d4d4faa632ab25cbb09d8--71e2dab0d3d84185993dabd5887da974 283adebff0024b2e92f9775efe5e7c7f RY(iia_\u03b2\u2080\u2084) eba67a8ccf4d4d4faa632ab25cbb09d8--283adebff0024b2e92f9775efe5e7c7f a392c17db8054f64ac08ae784a258ef5 RX(iia_\u03b2\u2080\u2081) 283adebff0024b2e92f9775efe5e7c7f--a392c17db8054f64ac08ae784a258ef5 fdb6f20bd36e4f1a81d180970360b06f RX(iia_\u03b1\u2081\u2081) a392c17db8054f64ac08ae784a258ef5--fdb6f20bd36e4f1a81d180970360b06f 2e4edcf7e41f4e4b8970fda19adb71e2 RY(iia_\u03b1\u2081\u2084) fdb6f20bd36e4f1a81d180970360b06f--2e4edcf7e41f4e4b8970fda19adb71e2 93b5b7fcc3644cac8ba491fb4a7ecadc X 2e4edcf7e41f4e4b8970fda19adb71e2--93b5b7fcc3644cac8ba491fb4a7ecadc 93b5b7fcc3644cac8ba491fb4a7ecadc--1324cd5e4ade4c0cb02eefb9d5215b69 76e2b56b489b4fa985a276eb4de8db8f 93b5b7fcc3644cac8ba491fb4a7ecadc--76e2b56b489b4fa985a276eb4de8db8f d19c01df267440aabcd7ab43ab4210d3 RX(iia_\u03b3\u2081\u2081) 76e2b56b489b4fa985a276eb4de8db8f--d19c01df267440aabcd7ab43ab4210d3 9cd62f19b9e5478883d6127f9e0ad729 d19c01df267440aabcd7ab43ab4210d3--9cd62f19b9e5478883d6127f9e0ad729 6bfaee789d1c4c43a4581da01091f23f X 9cd62f19b9e5478883d6127f9e0ad729--6bfaee789d1c4c43a4581da01091f23f 6bfaee789d1c4c43a4581da01091f23f--2842810380714c0b9e5013a68468605a 6a65fde176734bed9e760f81c63ed984 RY(iia_\u03b2\u2081\u2084) 6bfaee789d1c4c43a4581da01091f23f--6a65fde176734bed9e760f81c63ed984 2f3e9369c3cf47ac8c8e71135f230394 RX(iia_\u03b2\u2081\u2081) 6a65fde176734bed9e760f81c63ed984--2f3e9369c3cf47ac8c8e71135f230394 2f3e9369c3cf47ac8c8e71135f230394--0225fcdb798846889a4afea7577f9409 bf5efaf3a5844fc29f06f5685021952c 1b599e5bd69f48bf833d93c02c6a3e35 RX(iia_\u03b1\u2080\u2082) 56ed15dee5634196a03df8bc8eb24e45--1b599e5bd69f48bf833d93c02c6a3e35 314cce38f65a4f918c0395aec901d583 RY(iia_\u03b1\u2080\u2085) 1b599e5bd69f48bf833d93c02c6a3e35--314cce38f65a4f918c0395aec901d583 ca3544f5ccdb4b5783498708d2a680c3 314cce38f65a4f918c0395aec901d583--ca3544f5ccdb4b5783498708d2a680c3 7f42222bd9d64f34a73e7f55c9942166 X ca3544f5ccdb4b5783498708d2a680c3--7f42222bd9d64f34a73e7f55c9942166 7f42222bd9d64f34a73e7f55c9942166--c1d21d485ba1442f913b0b9c6f833a31 5333558974894e2fb1bdb0999a56284b RX(iia_\u03b3\u2080\u2082) 7f42222bd9d64f34a73e7f55c9942166--5333558974894e2fb1bdb0999a56284b 0b1c20c01a894b3c9df2d3534890573a X 5333558974894e2fb1bdb0999a56284b--0b1c20c01a894b3c9df2d3534890573a 0b1c20c01a894b3c9df2d3534890573a--8406f47073314613be0d334cc7151e09 ec8766d3170247f0bee31d1919c3b82f 0b1c20c01a894b3c9df2d3534890573a--ec8766d3170247f0bee31d1919c3b82f ffaaf2c2b1524a9cb7a3652cb630d63b RY(iia_\u03b2\u2080\u2085) ec8766d3170247f0bee31d1919c3b82f--ffaaf2c2b1524a9cb7a3652cb630d63b 46aafdee45c842c7859cea54bfc35779 RX(iia_\u03b2\u2080\u2082) ffaaf2c2b1524a9cb7a3652cb630d63b--46aafdee45c842c7859cea54bfc35779 e322b444429d484d8e3caabe74de6ff0 RX(iia_\u03b1\u2081\u2082) 46aafdee45c842c7859cea54bfc35779--e322b444429d484d8e3caabe74de6ff0 a399b26442634e69933cd0087de399b9 RY(iia_\u03b1\u2081\u2085) e322b444429d484d8e3caabe74de6ff0--a399b26442634e69933cd0087de399b9 bcaa8fb33e8c4ba1aee6f44c67def59d a399b26442634e69933cd0087de399b9--bcaa8fb33e8c4ba1aee6f44c67def59d 9dbf086bc55542339d54e3804c50c002 X bcaa8fb33e8c4ba1aee6f44c67def59d--9dbf086bc55542339d54e3804c50c002 9dbf086bc55542339d54e3804c50c002--76e2b56b489b4fa985a276eb4de8db8f d263c373b32944d2820e67182ac1b1c0 RX(iia_\u03b3\u2081\u2082) 9dbf086bc55542339d54e3804c50c002--d263c373b32944d2820e67182ac1b1c0 32ea094a8fbc456fbd5005f3ef004efa X d263c373b32944d2820e67182ac1b1c0--32ea094a8fbc456fbd5005f3ef004efa 32ea094a8fbc456fbd5005f3ef004efa--9cd62f19b9e5478883d6127f9e0ad729 38e792078f0a4127af78b7b0dc2aaa11 32ea094a8fbc456fbd5005f3ef004efa--38e792078f0a4127af78b7b0dc2aaa11 8f6175bfac7a465984dcbc26c29cc489 RY(iia_\u03b2\u2081\u2085) 38e792078f0a4127af78b7b0dc2aaa11--8f6175bfac7a465984dcbc26c29cc489 321406b860c34b919cdac1e74ce36380 RX(iia_\u03b2\u2081\u2082) 8f6175bfac7a465984dcbc26c29cc489--321406b860c34b919cdac1e74ce36380 321406b860c34b919cdac1e74ce36380--bf5efaf3a5844fc29f06f5685021952c"},{"location":"content/quantummodels/","title":"Quantum models","text":"<p>A quantum program can be expressed and executed using the <code>QuantumModel</code> type. It serves three primary purposes:</p> <p>Parameter handling: by conveniently handling and embedding the two parameter types that Qadence supports: feature and variational (see more details in the previous section).</p> <p>Differentiability: by enabling a differentiable backend that supports two differentiable modes: automatic differentiation (AD) and parameter shift rules (PSR). The former is used general differentiation in statevector simulators based on PyTorch and JAX. The latter is a quantum specific method used to differentiate gate parameters, and is enabled for all backends.</p> <p>Execution: by defining which backend the program is expected to be executed on. Qadence supports circuit compilation to the native backend representation.</p> <p>Backends</p> <p>The goal is for quantum models to be executed seemlessly on a number of different purpose backends: simulators, emulators or real hardware. By default, Qadence executes on the PyQTorch backend which implements a state vector simulator. Currently, this is the most feature rich backend. The Pulser backend is being developed, and currently supports a more limited set of functionalities (pulse sequences on programmable neutral atom arrays). The Horqrux backend, built on JAX, is also available, but currently not supported with the <code>QuantumModel</code> interface. For more information see the backend section.</p> <p>The base <code>QuantumModel</code> exposes the following methods:</p> <ul> <li><code>QuantumModel.run()</code>: To extract the wavefunction after circuit execution. Not supported by all backends.</li> <li><code>QuantumModel.sample()</code>: Sample a bitstring from the resulting quantum state after circuit execution. Supported by all backends.</li> <li><code>QuantumModel.expectation()</code>: Compute the expectation value of an observable.</li> </ul> <p>Every <code>QuantumModel</code> is an instance of a <code>torch.nn.Module</code> that enables differentiability for its <code>expectation</code> method. For statevector simulators, AD also works for the statevector itself.</p> <p>To construct a <code>QuantumModel</code>, the program block must first be initialized into a <code>QuantumCircuit</code> instance by combining it with a <code>Register</code>. An integer number can also be passed for the total number of qubits, which instantiates a <code>Register</code> automatically. The qubit register also includes topological information on the qubit layout, essential for digital-analog computations. However, we will explore that in a later tutorial. For now, let's construct a simple parametrized quantum circuit.</p> <pre><code>from qadence import QuantumCircuit, RX, RY, chain, kron\nfrom qadence import FeatureParameter, VariationalParameter\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, theta), RY(1, theta)),\n    kron(RX(0, phi), RY(1, phi)),\n)\n\ncircuit = QuantumCircuit(2, block)\nunique_params = circuit.unique_parameters\n</code></pre> <pre><code>unique_params = [theta, phi]\n</code></pre> <p>The model can then be instantiated. Similarly to the direct execution functions shown in the previous tutorial, the <code>run</code>, <code>sample</code> and <code>expectation</code> methods are available directly from the model.</p> <pre><code>import torch\nfrom qadence import QuantumModel, PI, Z\n\nobservable = Z(0) + Z(1)\n\nmodel = QuantumModel(circuit, observable)\n\nvalues = {\"phi\": torch.tensor([PI, PI/2])}\n\nwf = model.run(values)\nxs = model.sample(values, n_shots=100)\nex = model.expectation(values)\n</code></pre> <pre><code>wf = tensor([[ 0.0023+0.0000j, -0.0482+0.0000j,  0.0000+0.0482j,  0.0000-0.9977j],\n        [ 0.4518+0.0000j,  0.4977+0.0000j,  0.0000-0.4977j,  0.0000-0.5482j]])\nxs = [OrderedCounter({'11': 99, '01': 1}), OrderedCounter({'11': 36, '10': 27, '01': 22, '00': 15})]\nex = tensor([[-1.9907],\n        [-0.1926]])\n</code></pre> <p>By default, the <code>forward</code> method of <code>QuantumModel</code> calls <code>model.run()</code>. To define custom quantum models, the best way is to inherit from <code>QuantumModel</code> and override the <code>forward</code> method, as typically done with custom PyTorch Modules.</p> <p>The <code>QuantumModel</code> class provides convenience methods to manipulate parameters. Being a <code>torch.nn.Module</code>, all torch methods are also available.</p> <pre><code># To pass onto a torch optimizer\nparameter_generator = model.parameters()\n\n# Number of variational parameters\nnum_vparams = model.num_vparams\n\n# Dictionary to easily inspect variational parameter values\nvparams_values = model.vparams\n</code></pre> <pre><code>vparams_values = OrderedDict([('theta', tensor([0.0965]))])\n</code></pre>"},{"location":"content/quantummodels/#model-output","title":"Model output","text":"<p>The output of a quantum model is typically encoded in the measurement of an expectation value. In Qadence, one way to customize the number of outputs is by batching the number of observables at model creation by passing a list of blocks.</p> <pre><code>from torch import tensor\nfrom qadence import chain, kron, VariationalParameter, FeatureParameter\nfrom qadence import QuantumModel, QuantumCircuit, PI, Z, RX, CNOT\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, phi), RX(1, phi)),\n    CNOT(0, 1)\n)\n\ncircuit = QuantumCircuit(2, block)\n\nmodel = QuantumModel(circuit, [Z(0), Z(0) + Z(1)])\n\nvalues = {\"phi\": tensor(PI)}\n\nex = model.expectation(values)\n</code></pre> <pre><code>ex = tensor([[-1.0000e+00, -7.4988e-33]])\n</code></pre> <p>As mentioned in the previous tutorial, blocks can also be arbitrarily parameterized through multiplication, which allows the inclusion of trainable parameters in the definition of the observable.</p> <pre><code>from qadence import I, Z\n\na = VariationalParameter(\"a\")\nb = VariationalParameter(\"b\")\n\n# Magnetization with a trainable shift and scale\nobservable = a * I(0) + b * Z(0)\n\nmodel = QuantumModel(circuit, observable)\n</code></pre>"},{"location":"content/quantummodels/#quantum-neural-network-qnn","title":"Quantum Neural Network (QNN)","text":"<p>The <code>QNN</code> is a subclass of the <code>QuantumModel</code> geared towards quantum machine learning and parameter optimisation. See the quantum machine learning section section or the <code>QNN</code> API reference for more detailed information. There are three main differences in interface when compared with the <code>QuantumModel</code>:</p> <ul> <li>It is initialized with a list of the input parameter names, and then supports direct <code>torch.Tensor</code> inputs instead of the values dictionary shown above. The ordering of the input values should respect the order given in the input names.</li> <li>Passing an observable is mandatory.</li> <li>The <code>forward</code> method calls <code>model.expectation()</code>.</li> </ul> <pre><code>from torch import tensor\nfrom qadence import chain, kron, VariationalParameter, FeatureParameter\nfrom qadence import QNN, QuantumCircuit, PI, Z, RX, RY, CNOT\n\ntheta = FeatureParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, phi), RX(1, phi)),\n    kron(RY(0, theta), RY(1, theta)),\n    CNOT(0, 1)\n)\n\ncircuit = QuantumCircuit(2, block)\nobservable = Z(0) + Z(1)\n\nmodel = QNN(circuit, observable, inputs = [\"phi\", \"theta\"])\n\n# \"phi\" = PI, PI/2, \"theta\" = 0.0, 1.0\nvalues = tensor([[PI, 0.0], [PI/2, 1.0]])\n\nex = model(values)\n</code></pre> <pre><code>ex = tensor([[-7.4988e-33],\n        [ 1.1102e-16]])\n</code></pre>"},{"location":"content/register/","title":"Quantum registers","text":"<p>In Qadence, quantum programs can be executed by specifying the layout of a register of resources as a lattice. Built-in <code>Register</code> types can be used or constructed for arbitrary topologies. Common register topologies are available and illustrated in the plot below.</p> 2025-02-07T10:21:00.975313 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"content/register/#building-and-drawing-registers","title":"Building and drawing registers","text":"<p>Built-in topologies are directly accessible in the <code>Register</code> methods:</p> <pre><code>from qadence import Register\n\nreg = Register.all_to_all(n_qubits = 4)\nreg_line = Register.line(n_qubits = 4)\nreg_circle = Register.circle(n_qubits = 4)\nreg_squre = Register.square(qubits_side = 2)\nreg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\nreg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\nreg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n</code></pre> <p>The <code>Register</code> class builds on top of the NetworkX <code>Graph</code>, and the graphs can be visualized with the <code>reg.draw()</code> method</p> <p>Qubit coordinates are saved as node properties in the underlying NetworkX graph, but can be accessed directly with the <code>coords</code> property.</p> <p><pre><code>reg = Register.square(2)\nprint(reg.coords)\n</code></pre> <pre><code>{0: (0.5, -0.5), 1: (0.5, 0.5), 2: (-0.5, 0.5), 3: (-0.5, -0.5)}\n</code></pre>  By default, the coords are scaled such that the minimum distance between any two qubits is 1, unless the register is created directly from specific coordinates as shown below. The <code>spacing</code> argument can be used to set the minimum spacing. The <code>rescale_coords</code> method can be used to create a new register by rescaling the coordinates of an already created register.</p> <pre><code>scaled_reg_1 = Register.square(2, spacing = 4.0)\nscaled_reg_2 = reg.rescale_coords(scaling = 4.0)\nprint(scaled_reg_1.coords)\nprint(scaled_reg_2.coords)\n</code></pre> <pre><code>{0: (2.0, -2.0), 1: (2.0, 2.0), 2: (-2.0, 2.0), 3: (-2.0, -2.0)}\n{0: (2.0, -2.0), 1: (2.0, 2.0), 2: (-2.0, 2.0), 3: (-2.0, -2.0)}\n</code></pre> <p>The distance between qubits can also be directly accessed with the <code>distances</code> and <code>edge_distances</code> properties.</p> <pre><code>print(reg.distances)\nprint(reg.edge_distances)\n</code></pre> <pre><code>Distance between all qubit pairs:\n{(0, 1): 1.0, (0, 2): 1.4142135623730951, (0, 3): 1.0, (1, 2): 1.0, (1, 3): 1.4142135623730951, (2, 3): 1.0}\nDistance between qubits connect by an edge in the graph\n{(0, 1): 1.0, (0, 3): 1.0, (1, 2): 1.0, (2, 3): 1.0}\n</code></pre> <p>By calling the <code>Register</code> directly, either the number of nodes or a specific graph can be given as input. If passing a custom graph directly, the node positions will not be defined automatically, and should be previously saved in the <code>\"pos\"</code> node property. If not, <code>reg.coords</code> will return empty tuples and all distances will be 0.</p> <pre><code>import networkx as nx\n\n# Same as Register.all_to_all(n_qubits = 2):\nreg = Register(2)\n\n# Register from a custom graph:\ngraph = nx.complete_graph(3)\n\n# Set node positions, in this case a simple line:\nfor i, node in enumerate(graph.nodes):\n    graph.nodes[node][\"pos\"] = (1.0 * i, 0.0)\n\nreg = Register(graph)\n\nprint(reg.distances)\n</code></pre> <pre><code>{(0, 1): 1.0, (0, 2): 2.0, (1, 2): 1.0}\n</code></pre> <p>Alternatively, arbitrarily shaped registers can also be constructed by providing the node coordinates. In this case, there will be no edges automatically created in the connectivity graph.</p> <pre><code>import numpy as np\nfrom qadence import Register, PI\n\nreg = Register.from_coordinates(\n    [(x, np.sin(x)) for x in np.linspace(0, 2*PI, 10)]\n)\n\nreg.draw(show=False)\n</code></pre> 2025-02-07T10:21:01.140071 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>Units for qubit coordinates</p> <p>In general, Qadence makes no assumption about the units for qubit coordinates and distances. However, if used in the context of a Hamiltonian coefficient, care should be taken by the user to guarantee the quantity \\(H.t\\) is dimensionless for exponentiation in the PyQTorch backend, where it is assumed that \\(\\hbar = 1\\). For registers passed to the Pulser backend, coordinates are in \\(\\mu \\textrm{m}\\).</p>"},{"location":"content/register/#connectivity-graphs","title":"Connectivity graphs","text":"<p>Register topology is often assumed in digital simulations to be an all-to-all qubit connectivity. When running on real devices that enable the digital-analog computing paradigm, qubit interactions must be specified either by specifying distances between qubits, or by defining edges in the register connectivity graph.</p> <p>The abstract graph nodes and edges are accessible for direct usage.</p> <pre><code>from qadence import Register\n\nreg = Register.rectangular_lattice(2,3)\n</code></pre> <pre><code>reg.nodes = NodeView((0, 1, 2, 3, 4, 5))\nreg.edges = EdgeView([(0, 2), (0, 1), (1, 3), (2, 4), (2, 3), (3, 5), (4, 5)])\n</code></pre> <p>There is also an <code>all_node_pairs</code> property for convenience:</p> <pre><code>print(reg.all_node_pairs)\n</code></pre> <pre><code>[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n</code></pre> <p>More details about the usage of <code>Register</code> types in the digital-analog paradigm can be found in the digital-analog basics section.</p>"},{"location":"content/serializ_and_prep/","title":"Serialization","text":"<p>Qadence offers convenience functions for serializing and deserializing any quantum program. This is useful for storing quantum programs and sending them for execution over the network via an API.</p> <p>Note</p> <p>Qadence currently uses a custom JSON serialization as interchange format. Support for QASM format for digital quantum programs is currently under consideration.</p> <ul> <li><code>serialize/deserialize</code>: serialize and deserialize a Qadence object into a dictionary</li> <li><code>save/load</code>: save and load a Qadence object to a file with one of the supported   formats. Currently, these are <code>.json</code> and the PyTorch-compatible <code>.pt</code> format.</li> </ul> <p>Let's start with serialization into a dictionary.</p> <pre><code>import torch\nfrom qadence import QuantumCircuit, QuantumModel, DiffMode\nfrom qadence import chain, hamiltonian_factory, feature_map, hea, Z\nfrom qadence.serialization import serialize, deserialize\n\nn_qubits = 4\n\nmy_block = chain(feature_map(n_qubits, param=\"x\"), hea(n_qubits, depth=2))\nobs = hamiltonian_factory(n_qubits, detuning=Z)\n\n# Use the block defined above to create a quantum circuit\n# serialize/deserialize it\nqc = QuantumCircuit(n_qubits, my_block)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n# Let's wrap it in a QuantumModel\n# and serialize it\nqm = QuantumModel(qc, obs, diff_mode=DiffMode.AD)\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n\n# Check if the loaded QuantumModel returns the same expectation\nvalues = {\"x\": torch.rand(10)}\nassert torch.allclose(qm.expectation(values=values), qm_deserialized.expectation(values=values))\n</code></pre> <p>Finally, we can save the quantum circuit and the model with the two supported formats.</p> <pre><code>from qadence.serialization import serialize, deserialize, save, load, SerializationFormat\n\nqc_fname = \"circuit\"\nsave(qc, folder=\".\", file_name=qc_fname, format=SerializationFormat.PT)\nloaded_qc = load(f\"{qc_fname}.pt\")\nassert qc == loaded_qc\n\nqm_fname = \"model\"\nsave(qm, folder=\".\", file_name=qm_fname, format=SerializationFormat.JSON)\nmodel = load(f\"{qm_fname}.json\")\nassert isinstance(model, QuantumModel)\n</code></pre>"},{"location":"content/state_conventions/","title":"State Conventions","text":"<p>Here is an overview of the state conventions used in Qadence together with practical examples.</p>"},{"location":"content/state_conventions/#qubit-register-order","title":"Qubit register order","text":"<p>Qubit registers in quantum computing are often indexed in increasing or decreasing order from left to right. In Qadence, the convention is qubit indexation in increasing order. For example, a register of four qubits in bra-ket notation reads:</p> \\[|q_0, q_1, q_2, q_3\\rangle\\] <p>Furthermore, when displaying a quantum circuit, qubits are ordered from top to bottom.</p>"},{"location":"content/state_conventions/#basis-state-order","title":"Basis state order","text":"<p>Basis state ordering refers to how basis states are ordered when considering the conversion from bra-ket notation to the standard linear algebra basis. In Qadence, basis states are ordered in the following manner:</p> \\[ \\begin{align} |00\\rangle = [1, 0, 0, 0]^T\\\\ |01\\rangle = [0, 1, 0, 0]^T\\\\ |10\\rangle = [0, 0, 1, 0]^T\\\\ |11\\rangle = [0, 0, 0, 1]^T \\end{align} \\]"},{"location":"content/state_conventions/#endianness","title":"Endianness","text":"<p>Endianness refers to the storage convention for binary information (in bytes) in a classical memory register. In quantum computing, information is either stored in bits or in qubits. The most commonly used conventions are:</p> <ul> <li>A big-endian system stores the most significant bit of a binary word at the smallest memory address.</li> <li>A little-endian system stores the least significant bit of a binary word at the smallest memory address.</li> </ul> <p>Given the register convention in Qadence, the integer \\(2\\) written in binary big-endian as \\(10\\) can be encoded in a qubit register in both big-endian as \\(|10\\rangle\\) or little-endian as \\(|01\\rangle\\).</p> <p>The convention for Qadence is big-endian.</p>"},{"location":"content/state_conventions/#quantum-states","title":"Quantum states","text":"<p>In practical scenarios, conventions regarding register order, basis state order and endianness are very much intertwined, and identical results can be obtained by fixing or varying any of them. In Qadence, we assume that qubit ordering and basis state ordering is fixed, and allow an <code>endianness</code> argument that can be passed to control the expected result. Here are a few examples:</p> <p>A simple and direct way to exemplify the endianness convention is using convenience functions for state preparation.</p> <p>Bitstring convention as inputs</p> <p>When a bitstring is passed as input to a function for state preparation, it has to be understood in big-endian convention.</p> <pre><code>from qadence import Endianness, product_state\n\n# The state |10&gt;, the 3rd basis state.\nstate_big = product_state(\"10\", endianness=Endianness.BIG) # or just \"Big\"\n\n# The state |01&gt;, the 2nd basis state.\nstate_little = product_state(\"10\", endianness=Endianness.LITTLE) # or just \"Little\"\n</code></pre> <pre><code>State in big endian = tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]])\nState in little endian = tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> <p>Here, a bitword expressed as a Python string to encode the integer 2 in big-endian is used to create the respective basis state in both conventions. However, note that the same results can be obtained by fixing the endianness convention as big-endian (thus creating the state \\(|10\\rangle\\) in both cases), and changing the basis state ordering. A similar argument holds for fixing both endianness and basis state ordering and simply changing the qubit index order.</p> <p>Another example where endianness directly comes into play is when measuring a register. A big- or little-endian measurement will choose the first or the last qubit, respectively, as the most significant bit. Let's see this in an example:</p> <pre><code>from qadence import I, H, sample\n\n# Create superposition state: |00&gt; + |01&gt; (normalized)\nblock = I(0) @ H(1)  # Identity on qubit 0, Hadamard on qubit 1\n\n# Generate bitword samples following both conventions\n# Samples \"00\" and \"01\"\nresult_big = sample(block, endianness=Endianness.BIG)\n# Samples \"00\" and \"10\"\nresult_little = sample(block, endianness=Endianness.LITTLE)\n</code></pre> <pre><code>Sample in big endian = [OrderedCounter({'01': 57, '00': 43})]\nSample in little endian = [Counter({'00': 56, '10': 44})]\n</code></pre> <p>In Qadence, endianness can be flipped for many relevant objects:</p> <pre><code>from qadence import invert_endianness\n\n# Equivalent to sampling in little-endian.\nflip_big_sample = invert_endianness(result_big)\n\n# Equivalent to a state created in little-endian.\nflip_big_state = invert_endianness(state_big)\n</code></pre> <pre><code>Flipped sample = [Counter({'10': 57, '00': 43})]\nFlipped state = tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre>"},{"location":"content/state_conventions/#quantum-operations","title":"Quantum operations","text":"<p>When looking at the matricial form of quantum operations, the usage of the term endianness becomes slightly abusive. To exemplify, we may consider the <code>CNOT</code> operation with <code>control = 0</code> and <code>target = 1</code>. This operation is often described with two different matrices:</p> \\[ \\text{CNOT(0, 1)} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\qquad \\text{or} \\qquad \\text{CNOT(0, 1)} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] <p>The difference can be easily explained either by considering a different ordering of the qubit indices, or a different ordering of the basis states. In Qadence, both can be retrieved through the <code>endianness</code> argument:</p> <pre><code>from qadence import block_to_tensor, CNOT\n\nmatrix_big = block_to_tensor(CNOT(0, 1), endianness=Endianness.BIG)\nmatrix_little = block_to_tensor(CNOT(0, 1), endianness=Endianness.LITTLE)\n</code></pre> <pre><code>CNOT matrix in big endian =\n\ntensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]])\n\nCNOT matrix in little endian =\n\ntensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]]])\n</code></pre>"},{"location":"content/state_conventions/#backends","title":"Backends","text":"<p>An important part of having clear state conventions is that we need to make sure our results are consistent accross different computational backends, which may have their own conventions. In Qadence, this is taken care of automatically: by calling operations for different backends, the result is expected to be equivalent up to qubit ordering.</p> <pre><code>from qadence import BackendName, RX, run, sample, PI\n\n# RX(PI/4) on qubit 1\nn_qubits = 2\nop = RX(1, PI/4)\n</code></pre> <pre><code>Same sampling order in big endian:\n\nOn PyQTorch = [OrderedCounter({'00': 88, '01': 12})]\nOn Pulser = [Counter({'00': 88, '01': 12})]\n\nSame wavefunction order:\n\nOn PyQTorch = tensor([[0.9239+0.0000j, 0.0000-0.3827j, 0.0000+0.0000j, 0.0000+0.0000j]])\nOn Pulser = tensor([[0.9239+0.0000j, 0.0000-0.3826j, 0.0000+0.0000j, 0.0000+0.0000j]])\n</code></pre>"},{"location":"content/state_init/","title":"State initialization","text":"<p>Qadence offers convenience routines for preparing initial quantum states. These routines are divided into two approaches:</p> <ul> <li>As a dense matrix.</li> <li>From a suitable quantum circuit. This is available for every backend and it should be added in front of the desired quantum circuit to simulate.</li> </ul> <p>Let's illustrate the usage of the state preparation routine.</p> <pre><code>from qadence import random_state, product_state, is_normalized, StateGeneratorType\n\n# Random initial state.\n# the default `type` is StateGeneratorType.HaarMeasureFast\nstate = random_state(n_qubits=2, type=StateGeneratorType.RANDOM_ROTATIONS)\n\n# Check the normalization.\nassert is_normalized(state)\n\n# Product state from a given bitstring.\n# NB: Qadence follows the big endian convention.\nstate = product_state(\"01\")\n</code></pre> <pre><code>Random initial state generated with rotations:\n\nstate = [0.38529803+0.j         0.        +0.55576108j 0.        -0.41971478j\n 0.60540445+0.j        ]\n\nProduct state corresponding to bitstring '01':\n\nstate = [0.+0.j 1.+0.j 0.+0.j 0.+0.j]\n</code></pre> <p>Now we see how to generate the product state corresponding to the one above with a suitable quantum circuit.</p> <p><pre><code>from qadence import product_block, tag, hea, QuantumCircuit\nfrom qadence.draw import display\n\nstate_prep_block = product_block(\"01\")\n# display(state_prep_block)\n\n# Let's now prepare a circuit.\nn_qubits = 4\n\nstate_prep_block = product_block(\"0001\")\ntag(state_prep_block, \"Prep block\")\n\ncircuit_block = tag(hea(n_qubits, depth = 2), \"Circuit block\")\n\nqc_with_state_prep = QuantumCircuit(n_qubits, state_prep_block, circuit_block)\n</code></pre> %3 cluster_ee379cb5733d4200ae1e60c4b20dc17c Circuit block cluster_8b733295bd564580ab16fe6972f5e045 Prep block 2ec9b767a42f42b4a51a31967ed75205 0 7b67e8f4ff3c4d4499c5383ecc9c3423 2ec9b767a42f42b4a51a31967ed75205--7b67e8f4ff3c4d4499c5383ecc9c3423 594b117fe6074d19ab12bb5375632189 1 718459294d2a4956b48da8eebb00c24e RX(theta\u2080) 7b67e8f4ff3c4d4499c5383ecc9c3423--718459294d2a4956b48da8eebb00c24e 328d31a2e5e344c68a1640f2cb749d2f RY(theta\u2084) 718459294d2a4956b48da8eebb00c24e--328d31a2e5e344c68a1640f2cb749d2f 90cc7d6f278f48af951bf8c5154a20e0 RX(theta\u2088) 328d31a2e5e344c68a1640f2cb749d2f--90cc7d6f278f48af951bf8c5154a20e0 bf5c98ab274042f7986732e0e5f9617c 90cc7d6f278f48af951bf8c5154a20e0--bf5c98ab274042f7986732e0e5f9617c 66169f65a39a4cfb878d1631ef5f7b20 bf5c98ab274042f7986732e0e5f9617c--66169f65a39a4cfb878d1631ef5f7b20 8f16f816d8204b348cac61c46d364def RX(theta\u2081\u2082) 66169f65a39a4cfb878d1631ef5f7b20--8f16f816d8204b348cac61c46d364def 9ce536e9f96a45248395b55779c2de38 RY(theta\u2081\u2086) 8f16f816d8204b348cac61c46d364def--9ce536e9f96a45248395b55779c2de38 c968fc1e51004703909dd2d79ba5218b RX(theta\u2082\u2080) 9ce536e9f96a45248395b55779c2de38--c968fc1e51004703909dd2d79ba5218b db4e640eebcd4396a89f68f184404400 c968fc1e51004703909dd2d79ba5218b--db4e640eebcd4396a89f68f184404400 87197e4f52e7427ebf5369a8837dd2e7 db4e640eebcd4396a89f68f184404400--87197e4f52e7427ebf5369a8837dd2e7 65ad8852f02e46bbbc54977b05ffc08d 87197e4f52e7427ebf5369a8837dd2e7--65ad8852f02e46bbbc54977b05ffc08d df95f63070cb4bc18d268a7e7ef0770c 1931cd29ce3b47758e9866fb50f6424f 594b117fe6074d19ab12bb5375632189--1931cd29ce3b47758e9866fb50f6424f 296e7583e1db4c5589b8277cd5716f02 2 1a97db961c6f41f29347eed8526c567e RX(theta\u2081) 1931cd29ce3b47758e9866fb50f6424f--1a97db961c6f41f29347eed8526c567e fc5e23d44db74da2a53fd7b5974c832f RY(theta\u2085) 1a97db961c6f41f29347eed8526c567e--fc5e23d44db74da2a53fd7b5974c832f 6bf6c8bc81ce410bb085987eb554f38c RX(theta\u2089) fc5e23d44db74da2a53fd7b5974c832f--6bf6c8bc81ce410bb085987eb554f38c 5e24d3ee0ffc47e6a45c214cbfa6eb94 X 6bf6c8bc81ce410bb085987eb554f38c--5e24d3ee0ffc47e6a45c214cbfa6eb94 5e24d3ee0ffc47e6a45c214cbfa6eb94--bf5c98ab274042f7986732e0e5f9617c 7534d3732c1440079a9d60e499e5b830 5e24d3ee0ffc47e6a45c214cbfa6eb94--7534d3732c1440079a9d60e499e5b830 0853dee4bf584a0cb4c2b07ba5b24f88 RX(theta\u2081\u2083) 7534d3732c1440079a9d60e499e5b830--0853dee4bf584a0cb4c2b07ba5b24f88 aae1d121579342469865202e4cd7a1c2 RY(theta\u2081\u2087) 0853dee4bf584a0cb4c2b07ba5b24f88--aae1d121579342469865202e4cd7a1c2 b3f955a854064f6882eb8b14bfc1148d RX(theta\u2082\u2081) aae1d121579342469865202e4cd7a1c2--b3f955a854064f6882eb8b14bfc1148d 976aba2a74b34284ac6e7b3ccb1cb5fc X b3f955a854064f6882eb8b14bfc1148d--976aba2a74b34284ac6e7b3ccb1cb5fc 976aba2a74b34284ac6e7b3ccb1cb5fc--db4e640eebcd4396a89f68f184404400 fd7730faa9194d98a353cf1ffccb8c31 976aba2a74b34284ac6e7b3ccb1cb5fc--fd7730faa9194d98a353cf1ffccb8c31 fd7730faa9194d98a353cf1ffccb8c31--df95f63070cb4bc18d268a7e7ef0770c f316814019674fcc93676b209bdaa8bc 52ee22c4133e40728fbf9c563d3d0d95 296e7583e1db4c5589b8277cd5716f02--52ee22c4133e40728fbf9c563d3d0d95 49e3a7b0cb114820a57c5985dd7e112d 3 72c86e4518254623b81ae24563d228cc RX(theta\u2082) 52ee22c4133e40728fbf9c563d3d0d95--72c86e4518254623b81ae24563d228cc 983e74d8f3654f1ebb4f046f2d061ff4 RY(theta\u2086) 72c86e4518254623b81ae24563d228cc--983e74d8f3654f1ebb4f046f2d061ff4 63ac253c50934ee291dba47078da84f6 RX(theta\u2081\u2080) 983e74d8f3654f1ebb4f046f2d061ff4--63ac253c50934ee291dba47078da84f6 bac982c1bd7f42458cc15d2ea3dc4dd5 63ac253c50934ee291dba47078da84f6--bac982c1bd7f42458cc15d2ea3dc4dd5 b19c8a5ae1724f03976145b0c4cfac00 X bac982c1bd7f42458cc15d2ea3dc4dd5--b19c8a5ae1724f03976145b0c4cfac00 b19c8a5ae1724f03976145b0c4cfac00--7534d3732c1440079a9d60e499e5b830 ca29d780506c45f082e8efd32ac189c1 RX(theta\u2081\u2084) b19c8a5ae1724f03976145b0c4cfac00--ca29d780506c45f082e8efd32ac189c1 722b89887586420e8fe38df72a87ded6 RY(theta\u2081\u2088) ca29d780506c45f082e8efd32ac189c1--722b89887586420e8fe38df72a87ded6 044ba763bccf457f9fd7804cd68e80e3 RX(theta\u2082\u2082) 722b89887586420e8fe38df72a87ded6--044ba763bccf457f9fd7804cd68e80e3 7c4066648a2443c7ae9ba7091c7b7642 044ba763bccf457f9fd7804cd68e80e3--7c4066648a2443c7ae9ba7091c7b7642 1eefff33bcde451d8af53608157d4571 X 7c4066648a2443c7ae9ba7091c7b7642--1eefff33bcde451d8af53608157d4571 1eefff33bcde451d8af53608157d4571--fd7730faa9194d98a353cf1ffccb8c31 1eefff33bcde451d8af53608157d4571--f316814019674fcc93676b209bdaa8bc d78ce26eb9464ff9a1f6a24a3dc7704a 3c3446e57dd049e3bd193dc90738942d X 49e3a7b0cb114820a57c5985dd7e112d--3c3446e57dd049e3bd193dc90738942d cea4aceacbe349228afa624de4ad1337 RX(theta\u2083) 3c3446e57dd049e3bd193dc90738942d--cea4aceacbe349228afa624de4ad1337 640bf636f8624b649ee39da384d1b665 RY(theta\u2087) cea4aceacbe349228afa624de4ad1337--640bf636f8624b649ee39da384d1b665 266dab4134124cf7bf8b4eff1cec3ed4 RX(theta\u2081\u2081) 640bf636f8624b649ee39da384d1b665--266dab4134124cf7bf8b4eff1cec3ed4 f021d5d76d4c4f60985ccfd831bf095d X 266dab4134124cf7bf8b4eff1cec3ed4--f021d5d76d4c4f60985ccfd831bf095d f021d5d76d4c4f60985ccfd831bf095d--bac982c1bd7f42458cc15d2ea3dc4dd5 17af3ab9e2b94db88ed32b9ed1c0a1ba f021d5d76d4c4f60985ccfd831bf095d--17af3ab9e2b94db88ed32b9ed1c0a1ba 2b9a3e6787314c978fc53f71cdb819b6 RX(theta\u2081\u2085) 17af3ab9e2b94db88ed32b9ed1c0a1ba--2b9a3e6787314c978fc53f71cdb819b6 39a05013a8ba468bb64d4e8afe67c509 RY(theta\u2081\u2089) 2b9a3e6787314c978fc53f71cdb819b6--39a05013a8ba468bb64d4e8afe67c509 d66e0b09ebdf4cb9980e34e12fc88dde RX(theta\u2082\u2083) 39a05013a8ba468bb64d4e8afe67c509--d66e0b09ebdf4cb9980e34e12fc88dde 74a9d5a236a84b838baeee0d5f25d02c X d66e0b09ebdf4cb9980e34e12fc88dde--74a9d5a236a84b838baeee0d5f25d02c 74a9d5a236a84b838baeee0d5f25d02c--7c4066648a2443c7ae9ba7091c7b7642 6406ec8a65a24fd785564006716b234d 74a9d5a236a84b838baeee0d5f25d02c--6406ec8a65a24fd785564006716b234d 6406ec8a65a24fd785564006716b234d--d78ce26eb9464ff9a1f6a24a3dc7704a  Several standard quantum states can be conveniently initialized in Qadence, both in statevector form as well as in block form as shown in following.</p>"},{"location":"content/state_init/#state-vector-initialization","title":"State vector initialization","text":"<p>Qadence offers a number of constructor functions for state vector preparation.</p> <pre><code>from qadence import uniform_state, zero_state, one_state\n\nn_qubits = 3\nbatch_size = 2\n\nuniform_state = uniform_state(n_qubits, batch_size)\nzero_state = zero_state(n_qubits, batch_size)\none_state = one_state(n_qubits, batch_size)\n</code></pre> <pre><code>Uniform state = \n\ntensor([[0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,\n         0.3536+0.j],\n        [0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,\n         0.3536+0.j]])\nZero state = \n\ntensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\nOne state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> <p>As already seen, product states can be easily created, even in batches:</p> <pre><code>from qadence import product_state, rand_product_state\n\n# From a bitsring \"100\"\nprod_state = product_state(\"100\", batch_size)\n\n# Or a random product state\nrand_state = rand_product_state(n_qubits, batch_size)\n</code></pre> <pre><code>Product state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n\nRandom state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> <p>Creating a GHZ state:</p> <pre><code>from qadence import ghz_state\n\nghz = ghz_state(n_qubits, batch_size)\n</code></pre> <pre><code>GHZ state = \n\ntensor([[0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j,\n         0.7071+0.j],\n        [0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j,\n         0.7071+0.j]])\n</code></pre> <p>Creating a random state uniformly sampled from a Haar measure:</p> <pre><code>from qadence import random_state\n\nrand_haar_state = random_state(n_qubits, batch_size)\n</code></pre> <pre><code>Random state from Haar = \n\ntensor([[-0.0418+0.2231j,  0.0432-0.1901j, -0.0964+0.0036j, -0.0887+0.2639j,\n         -0.7089-0.0307j, -0.2299-0.2473j,  0.0158+0.3053j,  0.1789+0.2841j],\n        [-0.0417+0.1488j, -0.1586+0.4486j,  0.0976+0.3513j, -0.1487-0.2545j,\n          0.1258-0.2726j, -0.4855-0.1732j, -0.2351+0.0458j,  0.1482-0.3078j]])\n</code></pre> <p>Custom initial states can then be passed to either <code>run</code>, <code>sample</code> and <code>expectation</code> through the <code>state</code> argument</p> <pre><code>from qadence import random_state, product_state, CNOT, run\n\ninit_state = product_state(\"10\")\nfinal_state = run(CNOT(0, 1), state=init_state)\n</code></pre> <pre><code>Final state = tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre>"},{"location":"content/state_init/#density-matrices-conversion","title":"Density matrices conversion","text":"<p>It is also possible to obtain density matrices from statevectors. They can be passed as inputs to quantum programs performing density matrix based operations such as noisy simulations, when the backend allows such as PyQTorch.</p> <pre><code>from qadence import product_state, density_mat\n\ninit_state = product_state(\"10\")\ninit_density_matrix = density_mat(init_state)\n\nfinal_density_matrix = run(CNOT(0, 1), state=init_density_matrix)\n</code></pre> <pre><code>Initial = DensityMatrix([[[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]]])\nFinal = DensityMatrix([[[0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 1.-0.j]]])\n</code></pre>"},{"location":"content/state_init/#block-initialization","title":"Block initialization","text":"<p>Not all backends support custom statevector initialization, however previous utility functions have their counterparts to initialize the respective blocks:</p> <pre><code>from qadence import uniform_block, one_block\n\nn_qubits = 3\n\nuniform_block = uniform_block(n_qubits)\n\none_block = one_block(n_qubits)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 H(0)\n\u251c\u2500\u2500 H(1)\n\u2514\u2500\u2500 H(2)\nKronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> <p>Similarly, for product states:</p> <pre><code>from qadence import product_block, rand_product_block\n\nproduct_block = product_block(\"100\")\n\nrand_product_block = rand_product_block(n_qubits)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 I(1)\n\u2514\u2500\u2500 I(2)\nKronBlock(0,1,2)\n\u251c\u2500\u2500 I(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> <p>And GHZ states:</p> <pre><code>from qadence import ghz_block\n\nghz_block = ghz_block(n_qubits)\n</code></pre> <pre><code>ChainBlock(0,1,2)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 ChainBlock(0,1,2)\n    \u251c\u2500\u2500 CNOT(0, 1)\n    \u2514\u2500\u2500 CNOT(1, 2)\n</code></pre> <p>Initial state blocks can simply be chained at the start of a given circuit.</p>"},{"location":"content/state_init/#utility-functions","title":"Utility functions","text":"<p>Some state vector utility functions are also available. We can easily create the probability mass function of a given statevector using <code>torch.distributions.Categorical</code></p> <pre><code>from qadence import random_state, pmf\n\nn_qubits = 3\n\nstate = random_state(n_qubits)\ndistribution = pmf(state)\n</code></pre> <pre><code>Categorical(probs: torch.Size([1, 8]))\n</code></pre> <p>We can also check if a state is normalized:</p> <pre><code>from qadence import random_state, is_normalized\n\nstate = random_state(n_qubits)\nprint(is_normalized(state))\n</code></pre> <pre><code>True\n</code></pre> <p>Or normalize a state:</p> <pre><code>import torch\nfrom qadence import normalize, is_normalized\n\nstate = torch.tensor([[1, 1, 1, 1]], dtype = torch.cdouble)\nprint(normalize(state))\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre>"},{"location":"content/time_dependent/","title":"Time-dependent generators","text":"<p>For use cases when the Hamiltonian of the system is time-dependent, Qadence provides a special parameter <code>TimeParameter(\"t\")</code> that denotes the explicit time dependence. Using this time parameter, one can define a parameterized block acting as the generator passed to <code>HamEvo</code> that encapsulates the required time dependence function.</p>"},{"location":"content/time_dependent/#noiseless-time-dependent-hamiltonian-evolution","title":"Noiseless time-dependent Hamiltonian evolution","text":"<pre><code>from qadence import X, Y, HamEvo, TimeParameter, FeatureParameter, run\nfrom pyqtorch.utils import SolverType\nimport torch\n\n# Simulation parameters\node_solver = SolverType.DP5_SE  # time-dependent Schrodinger equation solver method\nn_steps_hevo = 500  # integration time steps used by solver\n\n# Define block parameters\nt = TimeParameter(\"t\")\nomega_param = FeatureParameter(\"omega\")\n\n# Arbitrarily compose a time-dependent generator\ntd_generator = omega_param * (t * X(0) + t**2 * Y(1))\n\n# Create parameterized HamEvo block\nhamevo = HamEvo(td_generator, t)\n</code></pre> <p>Note that when using <code>HamEvo</code> with a time-dependent generator, the actual time parameter that was used to construct the generator must be passed for the second argument <code>parameter</code>.</p> <p>By default, the code above will initialize an internal parameter <code>FeatureParameter(\"duration\")</code> in the <code>HamEvo</code>. Alternatively, the <code>duration</code> argument can be used to rename this parameter, or to pass a fixed value directly. If no fixed value is passed, it must then be set in the <code>values</code> dictionary at runtime.</p> <p>Future improvements</p> <p>Currently it is only possible to pass a single value for the duration, and the only result obtained will be the one corresponding to the state at end of the integration. In the future we will change the interface to allow directly passing some array of save values to obtain expectation values or statevectors at intermediate steps during the evolution.</p> <pre><code>values = {\"omega\": torch.tensor(10.0), \"duration\": torch.tensor(1.0)}\n\nconfig = {\"ode_solver\": ode_solver, \"n_steps_hevo\": n_steps_hevo}\n\nout_state = run(hamevo, values = values, configuration = config)\n\nprint(out_state)\n</code></pre> <pre><code>tensor([[-0.2785+0.0000j, -0.0541+0.0000j,  0.0000-0.9414j,  0.0000-0.1827j]])\n</code></pre> <p>Note that Qadence makes no assumption on units. The unit of passed duration value \\(\\tau\\) must be aligned with the units of other parameters in the time-dependent generator so that the integral of generator \\(\\overset{\\tau}{\\underset{0}{\\int}}\\mathcal{\\hat{H}}(t){\\rm d}t\\) is dimensionless.</p>"},{"location":"content/time_dependent/#noisy-time-dependent-hamiltonian-evolution","title":"Noisy time-dependent Hamiltonian evolution","text":"<p>To perform noisy time-dependent Hamiltonian evolution, one needs to pass a list of noise operators to the <code>noise_operators</code> argument in <code>HamEvo</code>. They correspond to the jump operators used within the time-dependent Schrodinger equation solver method <code>SolverType.DP5_ME</code>:</p> <pre><code>from qadence import X, Y, HamEvo, TimeParameter, FeatureParameter, run\nfrom pyqtorch.utils import SolverType\nimport torch\n\n# Simulation parameters\node_solver = SolverType.DP5_ME  # time-dependent Schrodinger equation solver method\nn_steps_hevo = 500  # integration time steps used by solver\n\n# Define block parameters\nt = TimeParameter(\"t\")\nomega_param = FeatureParameter(\"omega\")\n\n# Arbitrarily compose a time-dependent generator\ntd_generator = omega_param * (t * X(0) + t**2 * Y(1))\n\n# Create parameterized HamEvo block\nnoise_operators = [X(i) for i in td_generator.qubit_support]\nhamevo = HamEvo(td_generator, t, noise_operators = noise_operators)\n\nvalues = {\"omega\": torch.tensor(10.0), \"duration\": torch.tensor(1.0)}\n\nconfig = {\"ode_solver\": ode_solver, \"n_steps_hevo\": n_steps_hevo}\n\nout_state = run(hamevo, values = values, configuration = config)\n\nprint(out_state)\n</code></pre>   DensityMatrix([[[0.2734+0.0000j, 0.0297+0.0000j, 0.0000-0.0227j, 0.0000-0.0025j],                 [0.0297+0.0000j, 0.1698+0.0000j, 0.0000-0.0025j, 0.0000-0.0141j],                 [0.0000+0.0227j, 0.0000+0.0025j, 0.3435+0.0000j, 0.0373+0.0000j],                 [0.0000+0.0025j, 0.0000+0.0141j, 0.0373+0.0000j, 0.2133+0.0000j]]])    <p>Noise operators definition</p> <p>Note it is not possible to define <code>noise_operators</code> with parametric operators. If you want to do so, we recommend obtaining the tensors via run and set <code>noise_operators</code> using <code>MatrixBlock</code>. Also, <code>noise_operators</code> should have the same or a subset of the qubit support of the <code>HamEvo</code> instance.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"getting_started/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"getting_started/CONTRIBUTING/","title":"How to contribute","text":"<p>We're grateful for your interest in participating in Qadence. Please follow our guidelines to ensure a smooth contribution process.</p>"},{"location":"getting_started/CONTRIBUTING/#reporting-an-issue-or-proposing-a-feature","title":"Reporting an issue or proposing a feature","text":"<p>Your course of action will depend on your objective, but generally, you should start by creating an issue. If you've discovered a bug or have a feature you'd like to see added to qadence, feel free to create an issue on qadence's GitHub issue tracker. Here are some steps to take:</p> <ol> <li>Quickly search the existing issues using relevant keywords to ensure your issue hasn't been addressed already.</li> <li> <p>If your issue is not listed, create a new one. Try to be as detailed and clear as possible in your description.</p> </li> <li> <p>If you're merely suggesting an improvement or reporting a bug, that's already excellent! We thank you for it. Your issue will be listed and, hopefully, addressed at some point.</p> </li> <li>However, if you're willing to be the one solving the issue, that would be even better! In such instances, you would proceed by preparing a Pull Request.</li> </ol>"},{"location":"getting_started/CONTRIBUTING/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>We're excited that you're eager to contribute to Qadence. To contribute, fork the <code>main</code> branch of qadence repository and once you are satisfied with your feature and all the tests pass create a Pull Request.</p> <p>Here's the process for making a contribution:</p> <p>Click the \"Fork\" button at the upper right corner of the repo page to create a new GitHub repo at <code>https://github.com/USERNAME/qadence</code>, where <code>USERNAME</code> is your GitHub ID. Then, <code>cd</code> into the directory where you want to place your new fork and clone it:</p> <pre><code>git clone https://github.com/USERNAME/qadence.git\n</code></pre> <p>Next, navigate to your new qadence fork directory and mark the main qadence repository as the <code>upstream</code>:</p> <pre><code>git remote add upstream https://github.com/pasqal-io/qadence.git\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>We recommended to use <code>hatch</code> for managing environments:</p> <p>To develop within qadence, use: <pre><code>pip install hatch\nhatch -v shell\n</code></pre></p> <p>To run qadence tests, use:</p> <pre><code>hatch -e tests run test\n</code></pre> <p>If you don't want to use <code>hatch</code>, you can use the environment manager of your choice (e.g. Conda) and execute the following:</p> <pre><code>pip install pytest\npip install -e .\npytest\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#useful-things-for-your-workflow-linting-and-testing","title":"Useful things for your workflow: linting and testing","text":"<p>Use <code>pre-commit</code> to lint your code and run the unit tests before pushing a new commit.</p> <p>Using <code>hatch</code>, it's simply:</p> <pre><code>hatch -e tests run pre-commit run --all-files\nhatch -e tests run test\n</code></pre> <p>Our CI/CD pipeline will also test if the documentation can be built correctly. To test it locally, please run:</p> <pre><code>hatch -e docs run mkdocs build --clean --strict\n</code></pre> <p>Without <code>hatch</code>, <code>pip</code> install those libraries first: \"mkdocs\", \"mkdocs-material\", \"mkdocstrings\", \"mkdocstrings-python\", \"mkdocs-section-index\", \"mkdocs-jupyter\", \"mkdocs-exclude\", \"markdown-exec\"</p> <p>And then:</p> <pre><code> mkdocs build --clean --strict\n</code></pre>"},{"location":"getting_started/LICENSE/","title":"Apache License","text":"<p>Version 2.0, January 2004</p> <p>http://www.apache.org/licenses/</p>"},{"location":"getting_started/LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"getting_started/LICENSE/#1-definitions","title":"1. Definitions:","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"getting_started/LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"getting_started/LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"getting_started/LICENSE/#4-redistribution","title":"4. Redistribution.","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ul> <li> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> </li> <li> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> </li> <li> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> </li> <li> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> </li> </ul> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"getting_started/LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions.","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"getting_started/LICENSE/#6-trademarks","title":"6. Trademarks.","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"getting_started/LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty.","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"getting_started/LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability.","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"getting_started/LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability.","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"getting_started/LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":""},{"location":"getting_started/LICENSE/#appendix-how-to-apply-the-apache-license-to-your-work","title":"APPENDIX: How to apply the Apache License to your work.","text":"<p>To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!)  The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.</p> <p>Copyright [yyyy] Pasqal</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>Qadence is fully tested on Linux/MacOS operating systems. For Windows users, we recommend using WSL2 to install a Linux distribution of choice.</p>"},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>Qadence can be installed from PyPI with <code>pip</code> as follows:</p> <pre><code>pip install qadence\n</code></pre> <p>By default, this will also install PyQTorch, a differentiable state vector simulator which serves as the main numerical backend for Qadence.</p> <p>It is possible to install additional backends and the circuit visualization library using the following extras:</p> <ul> <li><code>visualization</code>: to display quantum circuits.</li> <li><code>pulser</code>: the Pulser backend for composing, simulating and executing pulse sequences for neutral-atom quantum devices (in development).</li> </ul> <p>To install other backends or the visualization tool, please use:</p> <pre><code>pip install \"qadence[pulser, visualization]\"\n</code></pre> <p>Note</p> <p>In order to correctly install the <code>visualization</code> extra, the <code>graphviz</code> package needs to be installed in your system:</p> <pre><code># on Ubuntu\nsudo apt install graphviz\n\n# on MacOS\nbrew install graphviz\n\n# via conda\nconda install python-graphviz\n</code></pre>"},{"location":"getting_started/installation/#install-from-source","title":"Install from source","text":"<p>We recommend to use the <code>hatch</code> environment manager to install <code>qadence</code> from source:</p> <pre><code>python -m pip install hatch\n\n# get into a shell with all the dependencies\npython -m hatch shell\n\n# run a command within the virtual environment with all the dependencies\npython -m hatch run python my_script.py\n</code></pre> <p>Warning</p> <p><code>hatch</code> will not combine nicely with other environment managers such Conda. If you want to use Conda, install it from source using <code>pip</code>:</p> <pre><code># within the Conda environment\npython -m pip install -e .\n</code></pre>"},{"location":"getting_started/installation/#citation","title":"Citation","text":"<p>If you use Qadence for a publication, we kindly ask you to cite our work using the following BibTex entry:</p> <pre><code>@article{qadence2024pasqal,\n  title = {Qadence: a differentiable interface for digital-analog programs.},\n  author={Dominik Seitz and Niklas Heim and Jo\u00e3o P. Moutinho and Roland Guichard and Vytautas Abramavicius and Aleksander Wennersteen and Gert-Jan Both and Anton Quelle and Caroline de Groot and Gergana V. Velikova and Vincent E. Elfving and Mario Dagrada},\n  journal={arXiv:2401.09915},\n  url = {https://github.com/pasqal-io/qadence},\n  year = {2024}\n}\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section is undergoing changes and most of the information here will be reorganized.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/advanced_tutorials/","title":"Advanced Tutorials","text":"<p>In this section, advanced programming concepts and implementations in Qadence are examplified.</p>"},{"location":"tutorials/advanced_tutorials/cloud-submission/","title":"Submission of Qadence Jobs to Pasqal Cloud","text":"<p>It is possible to submit quantum computational jobs to execute remotely on Pasqal's cloud platform from Qadence. This feature can only be used if you have an account on the cloud platform, which has access to the Qadence workload. The qadence module <code>qadence.pasqal_cloud_connection</code> offers functionality to specify the computation easily, upload the specification and retrieve the result when the computation has finished execution on the cloud platform. In this tutorial, a simple quantum circuit will be defined as an example to showcase the submission process for remote computations. The same process can be applied to run more complex quantum circuits on the cloud platform.</p> <p>Let's first define a very simple quantum circuit that creates a Bell state.</p> <pre><code>from qadence import CNOT, H, QuantumCircuit\n\ncircuit = QuantumCircuit(2, H(0), CNOT(0, 1))\n</code></pre> <p>If we want to upload this circuit to the cloud platform we need to follow 4 steps: - Authentication and connection to cloud - Defining workload specification - Submission - Retrieval of results</p>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#authentication-and-connection","title":"Authentication and connection","text":"<p>To setup a connection the cloud platform, use the <code>SDK</code> object present in <code>qadence.pasqal_cloud_connection</code>. The email and password are the ones used to login to the webportal. The project-id can be found in the webportal under \"Projects\".</p> <pre><code>from qadence.pasqal_cloud_connection import SDK\n\nconnection = SDK(\"john.doe@email.com\", \"my-password\", project_id=\"proj1\")\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#defining-workload-specification","title":"Defining workload specification","text":"<p>A workload is a quantum calculation to execute on a Pasqal cloud backend. To create a workload specification, we need some extra information on top the circuit itself. We need to specify the backend, chosen here to be PyQTorch. The cloud platform currently only supports PyQTorch. Moreover, the requested result type needs to be defined. Based on the workload specification, the appropriate run methods (<code>run</code>, <code>sample</code> or <code>expectation</code>) will be called by the <code>QuantumModel</code> by passing them through the enum value <code>ResultTypes</code> argument. Moreover, the requested result type needs to be defined. These are provided in a list, so that multiple result types can be requested in a single submission.</p> <pre><code>from qadence import BackendName\nfrom qadence.pasqal_cloud_connection import WorkloadSpec, ResultType\n\nworkload = WorkloadSpec(circuit, BackendName.PYQTORCH, [ResultType.SAMPLE, ResultType.RUN])\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#using-a-quantum-model","title":"Using a Quantum Model","text":"<p>If you already have your quantum computation defined as a <code>QuantumModel</code>, it is possible to create a workload specification directly from the model using <code>get_workload_spec</code>. Then, the circuit and backend specifications will be extracted from the model, the other values need to be provided as extra arguments.</p> <pre><code>from qadence import QuantumModel\nfrom qadence.pasqal_cloud_connection import get_workload_spec\n\nmodel = QuantumModel(circuit)\nworkload = get_workload_spec(model, [ResultType.SAMPLE])\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#observable-expectation-value","title":"Observable Expectation Value","text":"<p>For the result type <code>ResultType.EXPECTATION</code> it is mandatory to provide an observable to the workload specification. In the example below we use the trivial identity observable <code>I(0) @ I(1)</code>.</p> <pre><code>from qadence import I\n\nworkload = WorkloadSpec(circuit, BackendName.PYQTORCH, [ResultType.EXPECTATION], observable=I(0)*I(1))\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#parametric-circuits","title":"Parametric Circuits","text":"<p>In the case of a parametric circuit, i.e. a circuit that contains feature parameters or variational parameters, values for these parameters need to be provided. The parameter values are defined in a dictionary, where keys are the parameter name and values are parameter value passed as torch tensors. The parameter values are defined in a dictionary, where keys are the parameter name and values are parameter value passed as torch tensors. It is possible to set multiple values by using a 1-D tensor, to the parameters, in that case the computation is executed for each value in the tensor. A mix of 0-D and 1-D tensors can be provided to keep some parameters constant and others changed during this process. However, all 1-D tensors need to have the same length. An exception will be raised if the dimensions and lengths are invalid.</p> <pre><code>from torch import tensor\n\nparametric_circuit = ...\nparameter_values = {\"param1\": tensor(0), \"param2\": tensor([0, 1, 2]), \"param3\": tensor([5, 6, 7])}\nworkload = WorkloadSpec(parametric_circuit, BackendName.PYQTORCH, [ResultType.SAMPLE], parameter_values=parameter_values)\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#submission","title":"Submission","text":"<p>Submission to the cloud platform is done very easily using the <code>submit_workload</code> function. The workload id will be provided by executing the function. This id is needed later, to request the status of the given workload.</p> <pre><code>from qadence.pasqal_cloud_connection import submit_workload\n\nworkload_id = submit_workload(connection, workload)\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#check-workload-status","title":"Check Workload Status","text":"<p>The status of a workload can be: done, pending, running, paused, canceled, timed out or error. The <code>check_status</code> function can be used to see if the workload is finished already. If so, the results of the computation will be provided in a <code>WorkloadResult</code> object. The result of the computation itself can be found in the <code>result</code> attribute of this object. If the workload has not finished yet, or resulted in an error, <code>check_status</code> will raise an exception, either a <code>WorkloadStoppedError</code> or <code>WorkloadNotDoneError</code>.</p> <pre><code>from qadence.pasqal_cloud_connection import check_status\n\nworkload_result = check_status(connection, workload_id)\nprint(workload_result.result)\n</code></pre>"},{"location":"tutorials/advanced_tutorials/cloud-submission/#retrieval-of-results","title":"Retrieval of Results","text":"<p>If you wish to wait for the workload to be finished, before moving further with your code, you can use the <code>get_result</code> function. This function checks in set intervals the status of the workload until the workload is finished or the function has timed out. The polling rate as well as the time out duration can be set optionally.</p> <pre><code>from qadence.pasqal_cloud_connection import get_result\n\nworkload_result = get_result(connection, workload_id, timeout=60, refresh_time=1)\nprint(workload_result.result)\n</code></pre>"},{"location":"tutorials/advanced_tutorials/custom-models/","title":"Custom quantum models","text":"<p>In <code>qadence</code>, the <code>QuantumModel</code> is the central class point for executing <code>QuantumCircuit</code>s.  The idea of a <code>QuantumModel</code> is to decouple the backend execution from the management of circuit parameters and desired quantum computation output.</p> <p>In the following, we create a custom <code>QuantumModel</code> instance which introduces some additional optimizable parameters: *  an adjustable scaling factor in front of the observable to measured *  adjustable scale and shift factors to be applied to the model output before returning the result</p> <p>This can be easily done using PyTorch flexible model definition, and it will automatically work with the rest of <code>qadence</code> infrastructure.</p> <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit\n\n\nclass CustomQuantumModel(QuantumModel):\n\n    def __init__(self, circuit: QuantumCircuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\"):\n        super().__init__(circuit, observable=observable, backend=backend, diff_mode=diff_mode)\n\n        self.n_qubits = circuit.n_qubits\n\n        # define some additional parameters which will scale and shift (variationally) the\n        # output of the QuantumModel\n        # you can use all torch machinery for building those\n        self.scale_out = torch.nn.Parameter(torch.ones(1))\n        self.shift_out = torch.nn.Parameter(torch.ones(1))\n\n    # override the forward pass of the model\n    # the forward pass is the output of your QuantumModel and in this case\n    # it's the (scaled) expectation value of the total magnetization with\n    # a variable coefficient in front\n    def forward(self, values: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n\n        # scale the observable\n        res = self.expectation(values)\n\n        # scale and shift the result before returning\n        return self.shift_out + res * self.scale_out\n</code></pre> <p>The custom model can be used like any other <code>QuantumModel</code>: <pre><code>from qadence import Parameter, RX, CNOT, QuantumCircuit\nfrom qadence import chain, kron, hamiltonian_factory, Z\nfrom sympy import acos\n\ndef quantum_circuit(n_qubits):\n\n    x = Parameter(\"x\", trainable=False)\n    fm = kron(RX(i, acos(x) * (i+1)) for i in range(n_qubits))\n\n    ansatz = kron(RX(i, f\"theta{i}\") for i in range(n_qubits))\n    ansatz = chain(ansatz, CNOT(0, n_qubits-1))\n\n    block = chain(fm, ansatz)\n    block.tag = \"circuit\"\n    return QuantumCircuit(n_qubits, block)\n\nn_qubits = 4\nbatch_size = 10\ncircuit = quantum_circuit(n_qubits)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)  # Total magnetization\n\nmodel = CustomQuantumModel(circuit, observable, backend=\"pyqtorch\")\n\nvalues = {\"x\": torch.rand(batch_size)}\nres = model(values)\nprint(\"Model output: \", res)\nassert len(res) == batch_size\n</code></pre> <pre><code>Model output:  tensor([[-0.6788],\n        [-0.7390],\n        [-1.0357],\n        [-1.0159],\n        [ 0.5652],\n        [-0.4907],\n        [-0.6884],\n        [-0.9594],\n        [-1.0453],\n        [-0.9532]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> </p>"},{"location":"tutorials/advanced_tutorials/custom-models/#quantum-model-with-wavefunction-overlaps","title":"Quantum model with wavefunction overlaps","text":"<p><code>QuantumModel</code>'s can also use different quantum operations in their forward pass, such as wavefunction overlaps described here. Beware that the resulting overlap tensor has to be differentiable to apply gradient-based optimization. This is only applicable to the <code>\"EXACT\"</code> overlap method.</p> <p>Here we show how to use overlap calculation when fitting a parameterized quantum circuit to act as a standard Hadamard gate.</p> <pre><code>from qadence import RY, RX, H, Overlap\n\n# create a quantum model which acts as an Hadamard gate after training\nclass LearnHadamard(QuantumModel):\n    def __init__(\n        self,\n        train_circuit: QuantumCircuit,\n        target_circuit: QuantumCircuit,\n        backend=\"pyqtorch\",\n    ):\n        super().__init__(circuit=train_circuit, backend=backend)\n        self.overlap_fn = Overlap(train_circuit, target_circuit, backend=backend, method=\"exact\", diff_mode='ad')\n\n    def forward(self):\n        return self.overlap_fn()\n\n    # compute the wavefunction of the associated train circuit\n    def wavefunction(self):\n        return model.overlap_fn.run({})\n\n\ntrain_circuit = QuantumCircuit(1, chain(RX(0, \"phi\"), RY(0, \"theta\")))\ntarget_circuit = QuantumCircuit(1, H(0))\n\nmodel = LearnHadamard(train_circuit, target_circuit)\n\n# get the overlap between model and target circuit wavefunctions\nprint(model())\n</code></pre> <pre><code>tensor([[0.5791]], grad_fn=&lt;UnsqueezeBackward0&gt;)\n</code></pre> <p>This model can then be trained with the standard Qadence helper functions.</p> <pre><code>from qadence import run\nfrom qadence.ml_tools import Trainer, TrainConfig\nTrainer.set_use_grad(True)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n\ndef loss_fn(model: LearnHadamard, _unused) -&gt; tuple[torch.Tensor, dict]:\n    loss = criterion(torch.tensor([[1.0]]), model())\n    return loss, {}\n\nconfig = TrainConfig(max_iter=2500)\ntrainer = Trainer(\n    model, optimizer, config, loss_fn\n)\nmodel, optimizer = trainer.fit()\n\nwf_target = run(target_circuit)\nassert torch.allclose(wf_target, model.wavefunction(), atol=1e-2)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/advanced_tutorials/differentiability/","title":"Differentiability","text":"<p>Many application in quantum computing and quantum machine learning more specifically requires the differentiation of a quantum circuit with respect to its parameters.</p> <p>In Qadence, we perform quantum computations via the <code>QuantumModel</code> interface. The derivative of the outputs of quantum models with respect to feature and variational parameters in the quantum circuit can be implemented in Qadence with two different modes:</p> <ul> <li>Automatic differentiation (AD) mode <sup>1</sup>. This mode allows to differentiation both <code>run()</code> and <code>expectation()</code> methods of the <code>QuantumModel</code> and it is the fastest available differentiation method. Under the hood, it is based on the PyTorch autograd engine wrapped by the <code>DifferentiableBackend</code> class. This mode is not working on quantum devices.</li> <li>Generalized parameter shift rule (GPSR) mode. This is general implementation of the well known parameter  shift rule algorithm <sup>2</sup> which works for arbitrary quantum operations <sup>3</sup>. This mode is only applicable to  the <code>expectation()</code> method of <code>QuantumModel</code> but it is compatible with execution or quantum devices.</li> </ul>"},{"location":"tutorials/advanced_tutorials/differentiability/#automatic-differentiation","title":"Automatic differentiation","text":"<p>Automatic differentiation <sup>1</sup> is a procedure to derive a complex function defined as a sequence of elementary mathematical operations in the form of a computer program. Automatic differentiation is a cornerstone of modern machine learning and a crucial ingredient of its recent successes. In its so-called reverse mode, it follows this sequence of operations in reverse order by systematically applying the chain rule to recover the exact value of derivative. Reverse mode automatic differentiation is implemented in Qadence leveraging the PyTorch <code>autograd</code> engine.</p> <p>Only available via the PyQTorch or Horqrux backends</p> <p>Currently, automatic differentiation mode is only available when the <code>pyqtorch</code> or <code>horqrux</code> backends are selected.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#generalized-parameter-shift-rule","title":"Generalized parameter shift rule","text":"<p>The generalized parameter shift rule implementation in Qadence was introduced in <sup>3</sup>. Here the standard parameter shift rules, which only works for quantum operations whose generator has a single gap in its eigenvalue spectrum, was generalized to work with arbitrary generators of quantum operations.</p> <p>For this, we define the differentiable function as quantum expectation value</p> \\[ f(x) = \\left\\langle 0\\right|\\hat{U}^{\\dagger}(x)\\hat{C}\\hat{U}(x)\\left|0\\right\\rangle \\] <p>where \\(\\hat{U}(x)={\\rm exp}{\\left( -i\\frac{x}{2}\\hat{G}\\right)}\\) is the quantum evolution operator with generator \\(\\hat{G}\\) representing the structure of the underlying quantum circuit and \\(\\hat{C}\\) is the cost operator. Then using the eigenvalue spectrum \\(\\left\\{ \\lambda_n\\right\\}\\) of the generator \\(\\hat{G}\\) we calculate the full set of corresponding unique non-zero spectral gaps \\(\\left\\{ \\Delta_s\\right\\}\\) (differences between eigenvalues). It can be shown that the final expression of derivative of \\(f(x)\\) is then given by the following expression:</p> <p>\\(\\begin{equation} \\frac{{\\rm d}f\\left(x\\right)}{{\\rm d}x}=\\overset{S}{\\underset{s=1}{\\sum}}\\Delta_{s}R_{s}, \\end{equation}\\)</p> <p>where \\(S\\) is the number of unique non-zero spectral gaps and \\(R_s\\) are real quantities that are solutions of a system of linear equations</p> <p>\\(\\begin{equation} \\begin{cases} F_{1} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{1}\\Delta_{s}}{2}\\right)R_{s},\\\\ F_{2} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{2}\\Delta_{s}}{2}\\right)R_{s},\\\\  &amp; ...\\\\ F_{S} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{M}\\Delta_{s}}{2}\\right)R_{s}. \\end{cases} \\end{equation}\\)</p> <p>Here \\(F_s=f(x+\\delta_s)-f(x-\\delta_s)\\) denotes the difference between values of functions evaluated at shifted arguments \\(x\\pm\\delta_s\\).</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#adjoint-differentiation","title":"Adjoint Differentiation","text":"<p>Qadence also offers a memory-efficient, non-device compatible alternative to automatic differentation, called 'Adjoint Differentiation' <sup>4</sup> and allows for precisely calculating the gradients of variational parameters in O(P) time and using O(1) state-vectors. Adjoint Differentation is currently only supported by the Torch Engine and allows for first-order derivatives only.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#usage","title":"Usage","text":""},{"location":"tutorials/advanced_tutorials/differentiability/#basics","title":"Basics","text":"<p>In Qadence, the differentiation modes can be selected via the <code>diff_mode</code> argument of the QuantumModel class. It either accepts a <code>DiffMode</code>(<code>DiffMode.GSPR</code>, <code>DiffMode.AD</code> or <code>DiffMode.ADJOINT</code>) or a string (<code>\"gpsr\"\"</code>, <code>\"ad\"</code> or <code>\"adjoint\"</code>). The code in the box below shows how to create <code>QuantumModel</code> instances with all available differentiation modes.</p> <pre><code>from qadence import (FeatureParameter, RX, Z, hea, chain,\n                    hamiltonian_factory, QuantumCircuit,\n                    QuantumModel, BackendName, DiffMode)\nimport torch\n\nn_qubits = 2\n\n# Define a symbolic parameter to differentiate with respect to\nx = FeatureParameter(\"x\")\n\nblock = chain(hea(n_qubits, 1), RX(0, x))\n\n# create quantum circuit\ncircuit = QuantumCircuit(n_qubits, block)\n\n# create total magnetization cost operator\nobs = hamiltonian_factory(n_qubits, detuning=Z)\n\n# create models with AD, ADJOINT and GPSR differentiation engines\nmodel_ad = QuantumModel(circuit, obs,\n                        backend=BackendName.PYQTORCH,\n                        diff_mode=DiffMode.AD)\nmodel_adjoint = QuantumModel(circuit, obs,\n                        backend=BackendName.PYQTORCH,\n                        diff_mode=DiffMode.ADJOINT)\nmodel_gpsr = QuantumModel(circuit, obs,\n                          backend=BackendName.PYQTORCH,\n                          diff_mode=DiffMode.GPSR)\n\n# Create concrete values for the parameter we want to differentiate with respect to\nxs = torch.linspace(0, 2*torch.pi, 100, requires_grad=True)\nvalues = {\"x\": xs}\n\n# calculate function f(x)\nexp_val_ad = model_ad.expectation(values)\nexp_val_adjoint = model_adjoint.expectation(values)\nexp_val_gpsr = model_gpsr.expectation(values)\n\n# calculate derivative df/dx using the PyTorch\n# autograd engine\ndexpval_x_ad = torch.autograd.grad(\n    exp_val_ad, values[\"x\"], torch.ones_like(exp_val_ad), create_graph=True\n)[0]\ndexpval_x_adjoint = torch.autograd.grad(\n    exp_val_adjoint, values[\"x\"], torch.ones_like(exp_val_ad), create_graph=True\n)[0]\ndexpval_x_gpsr = torch.autograd.grad(\n    exp_val_gpsr, values[\"x\"], torch.ones_like(exp_val_gpsr), create_graph=True\n)[0]\n</code></pre> <p>We can plot the resulting derivatives and see that in both cases they coincide.</p> <pre><code>import matplotlib.pyplot as plt\n\n# plot f(x) and df/dx derivatives calculated using AD ,ADJOINT and GPSR\n# differentiation engines\nfig, ax = plt.subplots()\nax.scatter(xs.detach().numpy(),\n           exp_val_ad.detach().numpy(),\n           label=\"f(x)\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_ad.detach().numpy(),\n           label=\"df/dx AD\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_adjoint.detach().numpy(),\n           label=\"df/dx ADJOINT\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_gpsr.detach().numpy(),\n           s=5,\n           label=\"df/dx GPSR\")\nplt.legend()\n</code></pre> 2025-02-07T10:21:18.531668 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/advanced_tutorials/differentiability/#low-level-control-on-the-shift-values","title":"Low-level control on the shift values","text":"<p>In order to get a finer control over the GPSR differentiation engine we can use the low-level Qadence API to define a <code>DifferentiableBackend</code>.</p> <pre><code>from qadence.engines.torch import DifferentiableBackend\nfrom qadence.backends.pyqtorch import Backend as PyQBackend\n\n# define differentiable quantum backend\nquantum_backend = PyQBackend()\nconv = quantum_backend.convert(circuit, obs)\npyq_circ, pyq_obs, embedding_fn, params = conv\ndiff_backend = DifferentiableBackend(quantum_backend, diff_mode=DiffMode.GPSR, shift_prefac=0.2)\n\n# calculate function f(x)\nexpval = diff_backend.expectation(pyq_circ, pyq_obs, embedding_fn(params, values))\n</code></pre> <p>Here we passed an additional argument <code>shift_prefac</code> to the <code>DifferentiableBackend</code> instance that governs the magnitude of shifts \\(\\delta\\equiv\\alpha\\delta^\\prime\\) shown in equation (2) above. In this relation \\(\\delta^\\prime\\) is set internally and \\(\\alpha\\) is the value passed by <code>shift_prefac</code> and the resulting shift value \\(\\delta\\) is then used in all the following GPSR calculations.</p> <p>Tuning parameter \\(\\alpha\\) is useful to improve results when the generator \\(\\hat{G}\\) or the quantum operation is a dense matrix, for example a complex <code>HamEvo</code> operation; if many entries of this matrix are sufficiently larger than 0 the operation is equivalent to a strongly interacting system. In such case parameter \\(\\alpha\\) should be gradually lowered in order to achieve exact derivative values.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#low-level-differentiation-of-qadence-circuits-using-jax","title":"Low-level differentiation of qadence circuits using JAX","text":"<p>For users interested in using the <code>JAX</code> engine instead, we show how to run and differentiate qadence programs using the <code>horqrux</code> backend under qadence examples.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#references","title":"References","text":"<ol> <li> <p>A. G. Baydin et al., Automatic Differentiation in Machine Learning: a Survey \u21a9\u21a9</p> </li> <li> <p>Schuld et al., Evaluating analytic gradients on quantum hardware (2018). \u21a9</p> </li> <li> <p>Kyriienko et al., General quantum circuit differentiation rules \u21a9\u21a9</p> </li> <li> <p>Tyson et al., Efficient calculation of gradients in classical simulations of variational quantum algorithms \u21a9</p> </li> </ol>"},{"location":"tutorials/advanced_tutorials/profiling-and-debugging/","title":"Profiling and debugging on CUDA devices","text":"<p>For this to work, you'll have to have the right to access perf counters on your machine (for example with sudo or inside a Docker container). See: https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/index.html.</p> <pre><code>$ pip install nvidia-pyindex\n$ pip install nvidia-dlprof[pytorch]\n</code></pre> <p>Make sure that your entrypoint is an executable script. That means that it must start with a she-bang on the top line, e.g. <pre><code>#!/bin/python\n</code></pre> and have execution rights <pre><code>$ chmod +x your_script.py\n</code></pre></p> <p>Lastly it's recommended to add <pre><code>import nvidia_dlprof_pytorch_nvtx\nnvidia_dlprof_pytorch_nvtx.init()\n</code></pre> To your script in the beginning enable extra annotations of PyTorch functions.</p> <p>You can then use dlprof to profile. <pre><code>dlprof --mode=pytorch your_script.py\n</code></pre></p> <pre><code>PYQ_LOG_LEVEL=info QADENCE_LOG_LEVEL=debug dlprof --mode=pytorch examples/backends/differentiable_backend.py\n</code></pre> <p>For example to achieve this through Docker we can start a session in the shell, also mounting our local Qadence version and PyQTorch (Both optional, but you probably need to mount your script at least) <pre><code>$ docker run --rm --gpus=1 --shm-size=1g --ulimit memlock=-1 \\\n --ulimit stack=67108864 -it -p8000:8000 -v./:/opt/qadence -v ../PyQ:/opt/pyqtorch pytorch:24.02-py3 bash\n</code></pre> (You may need to jump through extra hoops to make Docker access the GPUs if you have error messages like <code>docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].</code>)</p> <p>After this you should have a shell inside the container and you can <pre><code>root@2a85826c4e7b:/workspace# cd /opt/qadence/\nroot@2a85826c4e7b:/opt/qadence# pip3 install -e .\nroot@2a85826c4e7b:/opt/qadence# pip3 install -e ../pyqtorch/\nroot@2a85826c4e7b:/opt/qadence# pip3 install nvidia-dlprof[pytorch]\nroot@2a85826c4e7b:/opt/qadence# PYQ_LOG_LEVEL=debug QADENCE_LOG_LEVEL=debug dlprof --mode=pytorch --nsys_opts=\"-t cuda,nvtx,cublas,cusparse,cusparse-verbose,cublas-verbose --force-overwrite true\" examples/models/quantum_model.py\n</code></pre></p> <p>Where we have <code>--force-overwrite true</code> to always store the latest profiling result (hence you must rename the file) if you wish to keep several. We add <code>cublas,cusparse,cusparse-verbose,cublas-verbose</code> do get more details about the numerical backend pacakges being used.</p>"},{"location":"tutorials/advanced_tutorials/projectors/","title":"Projector blocks","text":"<p>This section introduces the <code>ProjectorBlock</code> as an implementation for the quantum mechanical projection operation onto the subspace spanned by \\(|a\\rangle\\): \\(\\mathbb{\\hat{P}}=|a\\rangle \\langle a|\\). It evaluates the outer product for bras and kets expressed as bitstrings for a given qubit support. They have to possess matching lengths.</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence.operations import Projector  # Projector as an operation.\n\n# Define a projector for |1&gt; onto the qubit labelled 0.\nprojector_block = Projector(ket=\"1\", bra=\"1\", qubit_support=0)\n\n# As any block, the matrix representation can be retrieved.\nprojector_matrix = block_to_tensor(projector_block)\n</code></pre> <pre><code>projector matrix = tensor([[[0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j]]])\n</code></pre> <p>Other standard operations are expressed as projectors in Qadence. For instance, the number operator is the projector onto the 1-subspace, \\(N=|1\\rangle\\langle 1|\\).</p> <p>In fact, projectors can be used to compose any arbitrary operator. For example, the <code>CNOT</code> can be defined as \\(\\textrm{CNOT}(i,j)=|0\\rangle\\langle 0|_i\\otimes \\mathbb{I}_j+|1\\rangle\\langle 1|_i\\otimes X_j\\) and we can compare its matrix representation with the native one in Qadence:</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence import kron, I, X, CNOT\n\n# Define a projector for |0&gt; onto the qubit labelled 0.\nprojector0 = Projector(ket=\"0\", bra=\"0\", qubit_support=0)\n\n# Define a projector for |1&gt; onto the qubit labelled 0.\nprojector1 = Projector(ket=\"1\", bra=\"1\", qubit_support=0)\n\n# Construct the projector controlled CNOT.\nprojector_cnot = kron(projector0, I(1)) + kron(projector1, X(1))\n\n# Get the underlying unitary.\nprojector_cnot_matrix = block_to_tensor(projector_cnot)\n\n# Qadence CNOT unitary.\nqadence_cnot_matrix = block_to_tensor(CNOT(0,1))\n</code></pre> <pre><code>projector cnot matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]])\nqadence cnot matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Another example is the canonical SWAP unitary that can be defined as \\(SWAP=|00\\rangle\\langle 00|+|01\\rangle\\langle 10|+|10\\rangle\\langle 01|+|11\\rangle\\langle 11|\\). Indeed, it can be shown that their matricial representations are again identical:</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence import SWAP\n\n# Define all projectors.\nprojector00 = Projector(ket=\"00\", bra=\"00\", qubit_support=(0, 1))\nprojector01 = Projector(ket=\"01\", bra=\"10\", qubit_support=(0, 1))\nprojector10 = Projector(ket=\"10\", bra=\"01\", qubit_support=(0, 1))\nprojector11 = Projector(ket=\"11\", bra=\"11\", qubit_support=(0, 1))\n\n# Construct the SWAP gate.\nprojector_swap = projector00 + projector10 + projector01 + projector11\n\n# Get the underlying unitary.\nprojector_swap_matrix = block_to_tensor(projector_swap)\n\n# Qadence SWAP unitary.\nqadence_swap_matrix = block_to_tensor(SWAP(0,1))\n</code></pre> <pre><code>projector swap matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]])\nqadence swap matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]], grad_fn=&lt;UnsafeViewBackward0&gt;)\n</code></pre> <p>Warning</p> <p>Projectors are non-unitary operators, only supported by the PyQTorch backend.</p> <p>To examplify this point, let's run some non-unitary computation involving projectors.</p> <pre><code>from qadence import chain, run\nfrom qadence.operations import H, CNOT\n\n# Define a projector for |1&gt; onto the qubit labelled 1.\nprojector_block = Projector(ket=\"1\", bra=\"1\", qubit_support=1)\n\n# Some non-unitary computation.\nnon_unitary_block = chain(H(0), CNOT(0,1), projector_block)\n\n# Projected wavefunction becomes unnormalized\nprojected_wf = run(non_unitary_block)  # Run on PyQTorch.\n</code></pre> <pre><code>projected_wf = tensor([[0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j]])\n</code></pre>"},{"location":"tutorials/development/architecture/","title":"Architecture and sharp bits","text":"<p>Qadence as a software library mixes functional and object-oriented programming. We do that by maintaining core objects and operating on them with functions.</p> <p>Furthermore, Qadence strives at keeping the lower level abstraction layers for automatic differentiation and quantum computation fully stateless while only the frontend layer which is the main user-facing interface is stateful.</p> <p>Code design philosopy</p> <p>Functional, stateless core with object-oriented, stateful user interface.</p>"},{"location":"tutorials/development/architecture/#abstraction-layers","title":"Abstraction layers","text":"<p>In Qadence there are 4 main objects spread across 3 different levels of abstraction:</p> <ul> <li> <p>Frontend layer: The user facing layer and encompasses two objects:</p> <ul> <li><code>QuantumCircuit</code>: A class representing an abstract quantum   circuit not tight not any particular framework. Parameters are represented symbolically using   <code>sympy</code> expressions.</li> <li><code>QuantumModel</code>: The models are higher-level abstraction   providing an interface for executing different kinds of common quantum computing models such   quantum neural networks (QNNs), quantum kernels etc.</li> </ul> </li> <li> <p>Differentiation layer: Intermediate layer has the purpose of integrating quantum   computation with a given automatic differentiation engine. It is meant to be purely stateless and   contains one object:</p> <ul> <li><code>DifferentiableBackend</code>:   An abstract class whose concrete implementation wraps a quantum backend and make it   automatically differentiable using different engines (e.g. PyTorch or Jax).   Note, that today only PyTorch is supported but there is plan to add also a Jax   differentiable backend which will require some changes in the base class implementation.</li> </ul> </li> <li> <p>Quantum layer: The lower-level layer which directly interfaces with quantum emulators   and processing units. It is meant to be purely stateless and it contains one base object which is   specialized for each supported backend:</p> <ul> <li><code>Backend</code>: An abstract class whose concrete implementation   enables the execution of quantum circuit with a variety of quantum backends (normally non   automatically differentiable by default) such as PyQTorch, or Pulser.</li> </ul> </li> </ul>"},{"location":"tutorials/development/architecture/#main-components","title":"Main components","text":""},{"location":"tutorials/development/architecture/#quantumcircuit","title":"<code>QuantumCircuit</code>","text":"<p>We consider <code>QuantumCircuit</code> to be an abstract object, i.e. it is not tied to any backend. However, it blocks are even more abstract. This is because we consider <code>QuantumCircuit</code>s \"real\", whereas the blocks are largely considered just syntax.</p> <p>Unitary <code>QuantumCircuits</code> (this encompasses digital, or gate-based, circuits as well as analog circuits) are constructed by [<code>PrimitiveBlocks</code>] using a syntax that allows you to execute them in sequence, dubbed <code>ChainBlock</code> in the code, or in parallel (i.e. at the same time) where applicable, dubbed <code>KronBlock</code> in the code. Notice that this differs from other packages by providing more control of the layout of the circuit than conventional packages like Qiskit, and from Yao where the blocks are the primary type.</p>"},{"location":"tutorials/development/architecture/#quantummodel","title":"<code>QuantumModel</code>","text":"<p><code>QuantumModel</code>s are meant to be the main entry point for quantum computations in <code>qadence</code>. In general, they take one or more quantum circuit as input and they wrap all the necessary boiler plate code to make the circuit executable and differentiable on the chosen backend.</p> <p>Models are meant to be specific for a certain kind of quantum problem or algorithm and you can easily create new ones starting from the base class <code>QuantumModel</code>, as explained in the custom model tutorial. Currently, Qadence offers a <code>QNN</code> model class which provides convenient methods to work with quantum neural networks with multi-dimensional inputs and outputs.</p>"},{"location":"tutorials/development/architecture/#differentiablebackend","title":"<code>DifferentiableBackend</code>","text":"<p>The differentiable backend is a thin wrapper which takes as input a <code>QuantumCircuit</code> instance and a chosen quantum backend and make the circuit execution routines (expectation value, overalap, etc.) differentiable. Qadence offers both a PyTorch and Jax differentiation engine.</p>"},{"location":"tutorials/development/architecture/#quantum-backend","title":"Quantum <code>Backend</code>","text":"<p>For execution the primary object is the <code>Backend</code>. Backends maintain the same user-facing interface, and internally connects to other libraries to execute circuits. Those other libraries can execute the code on QPUs and local or cloud-based emulators. The <code>Backends</code> use PyTorch tensors to represent data and leverages PyTorchs autograd to help compute derivatives of circuits.</p>"},{"location":"tutorials/development/architecture/#symbolic-parameters","title":"Symbolic parameters","text":"<p>To illustrate how parameters work in Qadence, let's consider the following simple block composed of just two rotations:</p> <pre><code>import sympy\nfrom qadence import Parameter, RX\n\nparam = Parameter(\"phi\", trainable=False)\nblock = RX(0, param) * RX(1, sympy.acos(param))\n</code></pre> <p>The rotation angles assigned to <code>RX</code> (and to any Qadence quantum operation) are defined as arbitrary expressions of <code>Parameter</code>'s. <code>Parameter</code> is a subclass of <code>sympy.Symbol</code>, thus fully interoperable with it.</p> <p>To assign values of the parameter <code>phi</code> in a quantum model, one should use a dictionary containing the a key with parameter name and the corresponding values values:</p> <pre><code>import torch\nfrom qadence import run\n\nvalues = {\"phi\": torch.rand(10)}\nwf = run(block, values=values)\n</code></pre> <p>This is the only interface for parameter assignment exposed to the user. Under the hood, parameters applied to every quantum operation are identified in different ways:</p> <ul> <li> <p>By default, with a stringified version of the <code>sympy</code> expression supplied to the quantum operation. Notice that multiple operations can have the same expression.</p> </li> <li> <p>In certain case, e.g. for constructing parameter shift rules, one must access a unique identifier of the parameter for each quantum operation. Therefore, Qadence also creates unique identifiers for each parametrized operation (see the <code>ParamMap</code> class).</p> </li> </ul> <p>By default, when one constructs a new backend, the parameter identifiers are the <code>sympy</code> expressions which are used when converting an abstract block into a native circuit for the chosen backend. However, one can use the unique identifiers as parameter names by setting the private flag <code>_use_gate_params</code> to <code>True</code> in the backend configuration <code>BackendConfiguration</code>. This is automatically set when PSR differentiation is selected (see next section for more details).</p> <p>You can see the logic for choosing the parameter identifier in <code>get_param_name</code>.</p>"},{"location":"tutorials/development/architecture/#differentiation-with-parameter-shift-rules-psr","title":"Differentiation with parameter shift rules (PSR)","text":"<p>In Qadence, parameter shift rules are applied by implementing a custom <code>torch.autograd.Function</code> class for PyTorch and the <code>custom_vjp</code> in the Jax Engine, respectively.</p> <p>A custom PyTorch <code>Function</code> looks like this:</p> <pre><code>import torch\nfrom torch.autograd import Function\n\nclass CustomFunction(Function):\n\n    # forward pass implementation giving the output of the module\n    @staticmethod\n    def forward(ctx, inputs: torch.Tensor, params: torch.Tensor):\n        ctx.save_for_backward(inputs, params)\n        ...\n\n    # backward pass implementation giving the derivative of the module\n    # with respect to the parameters. This must return the whole vector-jacobian\n    # product to integrate within the autograd engine\n    @staticmethod\n    def backward(ctx, grad_output: torch.Tensor):\n        inputs, params = ctx.saved_tensors\n        ...\n</code></pre> <p>The class <code>PSRExpectation</code> under <code>qadence.engines.torch.differentiable_expectation</code> implements parameter shift rules for all parameters using a custom function as the one above. There are a few implementation details to keep in mind if you want to modify the PSR code:</p> <ul> <li> <p>PyTorch <code>Function</code> only works with tensor arguments. Parameters in Qadence are passed around as   dictionaries with parameter names as keys and current parameter values (tensors)   as values. This works for both variational and feature parameters. However, the <code>Function</code> class   only work with PyTorch tensors as input, not dictionaries. Therefore, the forward pass of   <code>PSRExpectation</code> accepts one argument <code>param_keys</code> with the   parameter keys and a variadic positional argument <code>param_values</code> with the parameter values one by   one. The dictionary is reconstructed within the <code>forward()</code> pass body.</p> </li> <li> <p>Higher-order derivatives with PSR. Higher-order PSR derivatives can be tricky. Parameter shift   rules calls, under the hood, the <code>QuantumBackend</code> expectation value routine that usually yield a   non-differentiable output. Therefore, a second call to the backward pass would not work. However,   Qadence employs a very simple trick to make higher-order derivatives work: instead of using   directly the expectation value of the quantum backend, the PSR backward pass uses the PSR forward   pass itself as expectation value function (see the code below). In this way, multiple calls to the   backward pass are allowed since the <code>expectation_fn</code> routine is always differentiable by   definition. Notice that this implementation is simple but suboptimal since, in some corner cases,   higher-order derivates might include some repeated terms that, with this implementation, are   always recomputed.</p> </li> </ul> <pre><code># expectation value used in the PSR backward pass\ndef expectation_fn(params: dict[str, Tensor]) -&gt; Tensor:\n    return PSRExpectation.apply(\n        ctx.expectation_fn,\n        ctx.param_psrs,\n        params.keys(),\n        *params.values(),\n    )\n</code></pre> <ul> <li> <p>Operation parameters must be uniquely identified for PSR to work. Parameter shift rules work at the level of individual quantum operations. This means that, given a parameter <code>x</code>, one needs to sum the contributions from shifting the parameter values of all the operation where the parameter <code>x</code> appears. When constructing the PSR rules, one must access a unique parameter identifier for each operation even if the corresponding user-facing parameter is the same. Therefore, when PSR differentiation is selected, the flag <code>_use_gate_params</code> is automatically set to <code>True</code> in the backend configuration <code>BackendConfiguration</code> (see previous section).</p> </li> <li> <p>PSR must not be applied to observable parameters. In Qadence, Pauli observables can also be parametrized. However, the tunable parameters of observables are purely classical and should not be included in the differentiation with PSRs. However, the quantum expectation value depends on them, thus they still need to enter into the PSR evaluation. To solve this issue, the code sets the <code>requires_grad</code> attribute of all observable parameters to <code>False</code> when constructing the PSRs for the circuit as in the snippet below:</p> </li> </ul> <pre><code>for obs in observable:\n    for param_id, _ in uuid_to_eigen(obs).items():\n        param_to_psr[param_id] = lambda x: torch.tensor([0.0], requires_grad=False)\n</code></pre>"},{"location":"tutorials/development/draw/","title":"<code>qadence.draw</code> example plots","text":"<p>Mostly for quick, manual checking of correct plotting output.</p> <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\n</code></pre> %3 55ab67f2ba5742c3a5e57fa0c08b4ff6 0 ec8f2cc89e674f96a1b65bc0cc63ca91 X 55ab67f2ba5742c3a5e57fa0c08b4ff6--ec8f2cc89e674f96a1b65bc0cc63ca91 538e66aa9df94a06b8cd583f52553df9 1 855b9231e9b14275b9bb4ab2df419cb9 ec8f2cc89e674f96a1b65bc0cc63ca91--855b9231e9b14275b9bb4ab2df419cb9 b640788fe14648128dcdccdd1945edd2 2a7ca5099d7d45a7baac9c256295e2a9 Y 538e66aa9df94a06b8cd583f52553df9--2a7ca5099d7d45a7baac9c256295e2a9 2a7ca5099d7d45a7baac9c256295e2a9--b640788fe14648128dcdccdd1945edd2 <pre><code>from qadence import X, Y, chain\nfrom qadence.draw import display\n\nb = chain(X(0), Y(0))\n</code></pre> %3 cc7f318986c6479b9cbb5a400970b7c1 0 20c1f178ab7941e0842930e620160a7e X cc7f318986c6479b9cbb5a400970b7c1--20c1f178ab7941e0842930e620160a7e 90d46862606246ddbbbf2cc536e36b4b Y 20c1f178ab7941e0842930e620160a7e--90d46862606246ddbbbf2cc536e36b4b 2aaceca52eaf4c4fbc8dc6c5988c2582 90d46862606246ddbbbf2cc536e36b4b--2aaceca52eaf4c4fbc8dc6c5988c2582 <pre><code>from qadence import X, Y, chain\nfrom qadence.draw import display\n\nb = chain(X(0), Y(1))\n</code></pre> %3 e3460f33e6cc480aaa2ea619a43ff5e8 0 19326ede2cbd4545919b78c7513c8879 X e3460f33e6cc480aaa2ea619a43ff5e8--19326ede2cbd4545919b78c7513c8879 d8be1baa75db41a79992a3c07960234d 1 722915c69215463ca570e650d9e65747 19326ede2cbd4545919b78c7513c8879--722915c69215463ca570e650d9e65747 f8bb0f2b8e65414caa1af8e662ece507 722915c69215463ca570e650d9e65747--f8bb0f2b8e65414caa1af8e662ece507 001e83111fea4f52826ae78c4e5274ec ca8e4f7229f0496b89aa8124d9123d69 d8be1baa75db41a79992a3c07960234d--ca8e4f7229f0496b89aa8124d9123d69 703ea568622442e9ac249e1a675ba566 Y ca8e4f7229f0496b89aa8124d9123d69--703ea568622442e9ac249e1a675ba566 703ea568622442e9ac249e1a675ba566--001e83111fea4f52826ae78c4e5274ec <pre><code>from qadence import X, Y, add\nfrom qadence.draw import display\n\nb = add(X(0), Y(1), X(2))\n</code></pre> %3 cluster_1080f6cf75574ca3abe1babc5050d5bf 39daabd24e4f496aa443d279d9e0ca78 0 b433d6ac80ff4d9489e2677e966bc0ba 39daabd24e4f496aa443d279d9e0ca78--b433d6ac80ff4d9489e2677e966bc0ba 664cb7a644094e8bbda5b47249e52fe8 1 2b3d8e8b6390449795c3633210c9af60 b433d6ac80ff4d9489e2677e966bc0ba--2b3d8e8b6390449795c3633210c9af60 bbca78046969415d91b562fb54713aaa 1d5b76cb9a3646aeb7d25f494a41450f AddBlock 664cb7a644094e8bbda5b47249e52fe8--1d5b76cb9a3646aeb7d25f494a41450f 37bc240deae94793a40e7c1b7e8df47d 2 1d5b76cb9a3646aeb7d25f494a41450f--bbca78046969415d91b562fb54713aaa b2b62401d83842e2abafd73ce8a48a4c e1542f5758c04c4db58eccc9f2c5b939 37bc240deae94793a40e7c1b7e8df47d--e1542f5758c04c4db58eccc9f2c5b939 e1542f5758c04c4db58eccc9f2c5b939--b2b62401d83842e2abafd73ce8a48a4c <pre><code>from qadence import CNOT, RX, HamEvo, X, Y, Z, chain, kron\n\nrx = kron(RX(3,0.5), RX(2, \"x\"))\nrx.tag = \"rx\"\ngen = chain(Z(i) for i in range(4))\n\n# `chain` puts things in sequence\nblock = chain(\n    kron(X(0), Y(1), rx),\n    CNOT(2,3),\n    HamEvo(gen, 10)\n)\n</code></pre> %3 cluster_488dbf7ef870461eae93cba966b27c54 cluster_7bc726e8e2fb4379997dfc8da5198439 rx 63cc4ef044f64667b58dc72539d2cc53 0 1e8b3b3684a94d78b1f7aceb76cf4b2b X 63cc4ef044f64667b58dc72539d2cc53--1e8b3b3684a94d78b1f7aceb76cf4b2b 31b0a55fdd534fdabb242f6aefc2fbdf 1 1f4347e8bb5a4780aa0023ffddc56f8a 1e8b3b3684a94d78b1f7aceb76cf4b2b--1f4347e8bb5a4780aa0023ffddc56f8a 15c46ceda1f147dfb8c32d6a36b252ae 1f4347e8bb5a4780aa0023ffddc56f8a--15c46ceda1f147dfb8c32d6a36b252ae e895aa4eb2de44049f2987ac950f3fac 15c46ceda1f147dfb8c32d6a36b252ae--e895aa4eb2de44049f2987ac950f3fac 146f0c33717f452fbc4bb5448ca858b0 3b727ac6cdae433997a3aaff73bd6755 Y 31b0a55fdd534fdabb242f6aefc2fbdf--3b727ac6cdae433997a3aaff73bd6755 785fbfe755794c8ea9ad44a7f6ccfa19 2 969d90b0bd314e6c828dc0b951b6e57b 3b727ac6cdae433997a3aaff73bd6755--969d90b0bd314e6c828dc0b951b6e57b 25fa1074dfce40b4b87420b2785a6b21 HamEvo 969d90b0bd314e6c828dc0b951b6e57b--25fa1074dfce40b4b87420b2785a6b21 25fa1074dfce40b4b87420b2785a6b21--146f0c33717f452fbc4bb5448ca858b0 9404ac7f0e824174bea8e426e1334a60 706413c948b241bb904d777f9cb41d89 RX(x) 785fbfe755794c8ea9ad44a7f6ccfa19--706413c948b241bb904d777f9cb41d89 bc82852d5b684d729e37e18a8bf3e4db 3 d9c9d0c359d240d1a88c33c89a548151 706413c948b241bb904d777f9cb41d89--d9c9d0c359d240d1a88c33c89a548151 50ff58947b4e44aaa18db0fba19bc456 t = 10 d9c9d0c359d240d1a88c33c89a548151--50ff58947b4e44aaa18db0fba19bc456 50ff58947b4e44aaa18db0fba19bc456--9404ac7f0e824174bea8e426e1334a60 2fcaed10fa144b3da04452fcfc3261b6 355e1e28c9eb46d78751e1cb507a670a RX(0.5) bc82852d5b684d729e37e18a8bf3e4db--355e1e28c9eb46d78751e1cb507a670a c5373357734747b09f60686e4c6c3093 X 355e1e28c9eb46d78751e1cb507a670a--c5373357734747b09f60686e4c6c3093 c5373357734747b09f60686e4c6c3093--d9c9d0c359d240d1a88c33c89a548151 34c20c2c6aa14aa2ab4814dd1b66e6e7 c5373357734747b09f60686e4c6c3093--34c20c2c6aa14aa2ab4814dd1b66e6e7 34c20c2c6aa14aa2ab4814dd1b66e6e7--2fcaed10fa144b3da04452fcfc3261b6 <pre><code>from qadence import feature_map, hea, chain\n\nblock = chain(feature_map(4, reupload_scaling=\"Tower\"), hea(4,2))\n</code></pre> %3 cluster_e6ace07af1ba4c78872a354f7e499445 HEA cluster_ef7836e7d5754f6d9e047e37677b6da5 Tower Fourier FM 03b7a02610f0480d832c9a9a8db48df9 0 b377d20e504b40a89625787eb7ced080 RX(1.0*phi) 03b7a02610f0480d832c9a9a8db48df9--b377d20e504b40a89625787eb7ced080 d627602177af43e6a3a179e5f831c5a6 1 f18c68d7a18b4995a46c75f4b09cc044 RX(theta\u2080) b377d20e504b40a89625787eb7ced080--f18c68d7a18b4995a46c75f4b09cc044 2ec4f9850c1943a78d770ce05fc1d8da RY(theta\u2084) f18c68d7a18b4995a46c75f4b09cc044--2ec4f9850c1943a78d770ce05fc1d8da b11737dc50a74dedbb242490730703c8 RX(theta\u2088) 2ec4f9850c1943a78d770ce05fc1d8da--b11737dc50a74dedbb242490730703c8 36c510ab40b74964aaf1a564ee8f5eaa b11737dc50a74dedbb242490730703c8--36c510ab40b74964aaf1a564ee8f5eaa 39de27fe1e0d46d69ed3637babd58ae2 36c510ab40b74964aaf1a564ee8f5eaa--39de27fe1e0d46d69ed3637babd58ae2 966e43092f534bab8379aecc5b514ff7 RX(theta\u2081\u2082) 39de27fe1e0d46d69ed3637babd58ae2--966e43092f534bab8379aecc5b514ff7 a684007249a641b896b345adca954537 RY(theta\u2081\u2086) 966e43092f534bab8379aecc5b514ff7--a684007249a641b896b345adca954537 d6f3dfbda40f40e4b7cd7b7f78a0c5dc RX(theta\u2082\u2080) a684007249a641b896b345adca954537--d6f3dfbda40f40e4b7cd7b7f78a0c5dc d6d04dc7fa434be5b64bf4080813564f d6f3dfbda40f40e4b7cd7b7f78a0c5dc--d6d04dc7fa434be5b64bf4080813564f 45ad0e752a0a4679878a2eddfe8789b6 d6d04dc7fa434be5b64bf4080813564f--45ad0e752a0a4679878a2eddfe8789b6 dd28e573986c4af4a2132c48529ea33b 45ad0e752a0a4679878a2eddfe8789b6--dd28e573986c4af4a2132c48529ea33b e050fc96c8fc45f9a27af464b8396f80 690abb8d24ae4054ada07a03edc44b9c RX(2.0*phi) d627602177af43e6a3a179e5f831c5a6--690abb8d24ae4054ada07a03edc44b9c a4d337972d3e4e82b443ad1b9df808c7 2 559fa5f900204dcf8d3681b0e330e18f RX(theta\u2081) 690abb8d24ae4054ada07a03edc44b9c--559fa5f900204dcf8d3681b0e330e18f 9ba551442dfe4b2ab3138372872ff798 RY(theta\u2085) 559fa5f900204dcf8d3681b0e330e18f--9ba551442dfe4b2ab3138372872ff798 3843037c6b0842ac89e13e6a2d00cb6b RX(theta\u2089) 9ba551442dfe4b2ab3138372872ff798--3843037c6b0842ac89e13e6a2d00cb6b 3a7de11347e548fe8b63f1dd3f88579a X 3843037c6b0842ac89e13e6a2d00cb6b--3a7de11347e548fe8b63f1dd3f88579a 3a7de11347e548fe8b63f1dd3f88579a--36c510ab40b74964aaf1a564ee8f5eaa a21b64156d734222a58a84b7ad251f68 3a7de11347e548fe8b63f1dd3f88579a--a21b64156d734222a58a84b7ad251f68 15b636d1b7d245dd83de3b6138d288ed RX(theta\u2081\u2083) a21b64156d734222a58a84b7ad251f68--15b636d1b7d245dd83de3b6138d288ed 8a9bb31e47b64ceea40d5fe9d2be0d83 RY(theta\u2081\u2087) 15b636d1b7d245dd83de3b6138d288ed--8a9bb31e47b64ceea40d5fe9d2be0d83 88433ade2c8c4a18a4b453162ba5cb00 RX(theta\u2082\u2081) 8a9bb31e47b64ceea40d5fe9d2be0d83--88433ade2c8c4a18a4b453162ba5cb00 69e21226edd64ade8bd5527687478ac6 X 88433ade2c8c4a18a4b453162ba5cb00--69e21226edd64ade8bd5527687478ac6 69e21226edd64ade8bd5527687478ac6--d6d04dc7fa434be5b64bf4080813564f d3098d0461fe42aba2e89519cec66deb 69e21226edd64ade8bd5527687478ac6--d3098d0461fe42aba2e89519cec66deb d3098d0461fe42aba2e89519cec66deb--e050fc96c8fc45f9a27af464b8396f80 8d9254cbf0ba4329acb4d2f9509c0577 b5403762dd6c44a69ef62d223f51579b RX(3.0*phi) a4d337972d3e4e82b443ad1b9df808c7--b5403762dd6c44a69ef62d223f51579b ff0ea313008a4abc8eed19e30cc524e7 3 7767cd6a079942b798a5fe1d7d4b7e88 RX(theta\u2082) b5403762dd6c44a69ef62d223f51579b--7767cd6a079942b798a5fe1d7d4b7e88 3d8a8c6c95a34b699a43cd9bc1205913 RY(theta\u2086) 7767cd6a079942b798a5fe1d7d4b7e88--3d8a8c6c95a34b699a43cd9bc1205913 db077b85ccff432fb0ce9b6f2a198c83 RX(theta\u2081\u2080) 3d8a8c6c95a34b699a43cd9bc1205913--db077b85ccff432fb0ce9b6f2a198c83 f15bb383ac0f4a2db06f195c2c4cdf06 db077b85ccff432fb0ce9b6f2a198c83--f15bb383ac0f4a2db06f195c2c4cdf06 c694909112004032a467592b2f35d981 X f15bb383ac0f4a2db06f195c2c4cdf06--c694909112004032a467592b2f35d981 c694909112004032a467592b2f35d981--a21b64156d734222a58a84b7ad251f68 7c72366ba8af4c18806587c59ed4bb86 RX(theta\u2081\u2084) c694909112004032a467592b2f35d981--7c72366ba8af4c18806587c59ed4bb86 4ec30092b9af47f39d6120332b1c3590 RY(theta\u2081\u2088) 7c72366ba8af4c18806587c59ed4bb86--4ec30092b9af47f39d6120332b1c3590 7c00359de62a4d909c92970bc1060b02 RX(theta\u2082\u2082) 4ec30092b9af47f39d6120332b1c3590--7c00359de62a4d909c92970bc1060b02 d5fb2b382e3b4a29b507d010e6108fb4 7c00359de62a4d909c92970bc1060b02--d5fb2b382e3b4a29b507d010e6108fb4 bd4ae7f5414147afa50b78d6969541d4 X d5fb2b382e3b4a29b507d010e6108fb4--bd4ae7f5414147afa50b78d6969541d4 bd4ae7f5414147afa50b78d6969541d4--d3098d0461fe42aba2e89519cec66deb bd4ae7f5414147afa50b78d6969541d4--8d9254cbf0ba4329acb4d2f9509c0577 c6fe4b6b233f4445a437d83b15ea49a1 addad6bdeaa74902bb830255d8d46bc8 RX(4.0*phi) ff0ea313008a4abc8eed19e30cc524e7--addad6bdeaa74902bb830255d8d46bc8 0a98a20d97264409a7be4e0f4c231b75 RX(theta\u2083) addad6bdeaa74902bb830255d8d46bc8--0a98a20d97264409a7be4e0f4c231b75 f1b4a2b274a742bcabfdeac92ae4a89f RY(theta\u2087) 0a98a20d97264409a7be4e0f4c231b75--f1b4a2b274a742bcabfdeac92ae4a89f 2415450653f34d6793aac297942493f7 RX(theta\u2081\u2081) f1b4a2b274a742bcabfdeac92ae4a89f--2415450653f34d6793aac297942493f7 e4b44c25e2f84b6483462a72b171b60d X 2415450653f34d6793aac297942493f7--e4b44c25e2f84b6483462a72b171b60d e4b44c25e2f84b6483462a72b171b60d--f15bb383ac0f4a2db06f195c2c4cdf06 13dd78520e56463f9a3e4e4d4636b941 e4b44c25e2f84b6483462a72b171b60d--13dd78520e56463f9a3e4e4d4636b941 777088eb8b2849ea834d1888493faf59 RX(theta\u2081\u2085) 13dd78520e56463f9a3e4e4d4636b941--777088eb8b2849ea834d1888493faf59 9c816ad983bb4557bf19aac57d3e51a0 RY(theta\u2081\u2089) 777088eb8b2849ea834d1888493faf59--9c816ad983bb4557bf19aac57d3e51a0 f58d39d24f0e4e96b343f1a692f0f564 RX(theta\u2082\u2083) 9c816ad983bb4557bf19aac57d3e51a0--f58d39d24f0e4e96b343f1a692f0f564 f7d087f877c64614a6b18eaadbfa109a X f58d39d24f0e4e96b343f1a692f0f564--f7d087f877c64614a6b18eaadbfa109a f7d087f877c64614a6b18eaadbfa109a--d5fb2b382e3b4a29b507d010e6108fb4 80ed6eaddda543ce9724b81c8cdd4033 f7d087f877c64614a6b18eaadbfa109a--80ed6eaddda543ce9724b81c8cdd4033 80ed6eaddda543ce9724b81c8cdd4033--c6fe4b6b233f4445a437d83b15ea49a1 <pre><code>from qadence import QuantumModel, QuantumCircuit, total_magnetization, hea\n\nmodel = QuantumModel(QuantumCircuit(3, hea(3,2)), total_magnetization(3))\n</code></pre> %3 cluster_84708c533bdc48edbbf3ad2b521bf27f Obs. cluster_ecc0be093a874eada1d7f7a12a64357d cluster_25b07d0065994613a133544b0659418c HEA 3a92b539f7c4425e8803a98f647e7957 0 bebe9dc526574eeab38f12b2675f50a8 RX(theta\u2080) 3a92b539f7c4425e8803a98f647e7957--bebe9dc526574eeab38f12b2675f50a8 c54f245afe5c4493a85e91b74efce215 1 db1b940baf724d2b9e5a3e396cd720c3 RY(theta\u2083) bebe9dc526574eeab38f12b2675f50a8--db1b940baf724d2b9e5a3e396cd720c3 247c5c9318af4a0b8feedeea409e47eb RX(theta\u2086) db1b940baf724d2b9e5a3e396cd720c3--247c5c9318af4a0b8feedeea409e47eb 7f84b7581cec4a82938250898c2cac8b 247c5c9318af4a0b8feedeea409e47eb--7f84b7581cec4a82938250898c2cac8b 9222101ad748493d9fdd883a089e3872 7f84b7581cec4a82938250898c2cac8b--9222101ad748493d9fdd883a089e3872 e805fac4dac745829e8181d7128c6ea5 RX(theta\u2089) 9222101ad748493d9fdd883a089e3872--e805fac4dac745829e8181d7128c6ea5 015f6db13bb742aca5aa0c8069fb9e19 RY(theta\u2081\u2082) e805fac4dac745829e8181d7128c6ea5--015f6db13bb742aca5aa0c8069fb9e19 86454cc5a5c74cf7ab80a85ef1ca4ea3 RX(theta\u2081\u2085) 015f6db13bb742aca5aa0c8069fb9e19--86454cc5a5c74cf7ab80a85ef1ca4ea3 b41828e154b34d4b9f9b87a7a0573c9e 86454cc5a5c74cf7ab80a85ef1ca4ea3--b41828e154b34d4b9f9b87a7a0573c9e 9904dc7247964e11af6713d5e76a15b2 b41828e154b34d4b9f9b87a7a0573c9e--9904dc7247964e11af6713d5e76a15b2 5b584f54ceae48c69521974259b3de67 9904dc7247964e11af6713d5e76a15b2--5b584f54ceae48c69521974259b3de67 ed14bf419d33418c9a1a1f1594e57da2 5b584f54ceae48c69521974259b3de67--ed14bf419d33418c9a1a1f1594e57da2 82536e352ac14ba99f89bffab58d379e 66e6e2327745452eaf0da949b3ae9ebf RX(theta\u2081) c54f245afe5c4493a85e91b74efce215--66e6e2327745452eaf0da949b3ae9ebf 15f4a9a15d204a6680dccdbfd768cf3f 2 073e83029191411dafbdc478d4b6ce10 RY(theta\u2084) 66e6e2327745452eaf0da949b3ae9ebf--073e83029191411dafbdc478d4b6ce10 8cd27a2404be44bead879ee3d59e2f87 RX(theta\u2087) 073e83029191411dafbdc478d4b6ce10--8cd27a2404be44bead879ee3d59e2f87 adb29651250a4d49bb71dcd79cd5e21f X 8cd27a2404be44bead879ee3d59e2f87--adb29651250a4d49bb71dcd79cd5e21f adb29651250a4d49bb71dcd79cd5e21f--7f84b7581cec4a82938250898c2cac8b 58670a7e03ab41c48b82a166bbe571ab adb29651250a4d49bb71dcd79cd5e21f--58670a7e03ab41c48b82a166bbe571ab 856af072ab0443ac817816dff80db65b RX(theta\u2081\u2080) 58670a7e03ab41c48b82a166bbe571ab--856af072ab0443ac817816dff80db65b 598b4648f54a4992a2507ef78c60a183 RY(theta\u2081\u2083) 856af072ab0443ac817816dff80db65b--598b4648f54a4992a2507ef78c60a183 04641162e88540dd8bc002e8738313f2 RX(theta\u2081\u2086) 598b4648f54a4992a2507ef78c60a183--04641162e88540dd8bc002e8738313f2 b4b65c6df125459ca048a49a99cf7dd5 X 04641162e88540dd8bc002e8738313f2--b4b65c6df125459ca048a49a99cf7dd5 b4b65c6df125459ca048a49a99cf7dd5--b41828e154b34d4b9f9b87a7a0573c9e 4378223df9404475a8aff45db923d352 b4b65c6df125459ca048a49a99cf7dd5--4378223df9404475a8aff45db923d352 6335289d9b8d404682dd1529e58594e5 AddBlock 4378223df9404475a8aff45db923d352--6335289d9b8d404682dd1529e58594e5 6335289d9b8d404682dd1529e58594e5--82536e352ac14ba99f89bffab58d379e 888e37bfd53343acae63da50aa0a75c7 08225354d5e94e12a0c6250a3a13605d RX(theta\u2082) 15f4a9a15d204a6680dccdbfd768cf3f--08225354d5e94e12a0c6250a3a13605d b85979ab27f448a0afea298bce3793a7 RY(theta\u2085) 08225354d5e94e12a0c6250a3a13605d--b85979ab27f448a0afea298bce3793a7 b0fc1c85ca654426a794c4b4dc61b491 RX(theta\u2088) b85979ab27f448a0afea298bce3793a7--b0fc1c85ca654426a794c4b4dc61b491 8d98eb3e554b484da73722aee93768c2 b0fc1c85ca654426a794c4b4dc61b491--8d98eb3e554b484da73722aee93768c2 7d20d4b590004d44937ddde0d0386385 X 8d98eb3e554b484da73722aee93768c2--7d20d4b590004d44937ddde0d0386385 7d20d4b590004d44937ddde0d0386385--58670a7e03ab41c48b82a166bbe571ab ff56febca827401d80e1fa486010a4ea RX(theta\u2081\u2081) 7d20d4b590004d44937ddde0d0386385--ff56febca827401d80e1fa486010a4ea 3d857e53ec834453a073a5aceeb00254 RY(theta\u2081\u2084) ff56febca827401d80e1fa486010a4ea--3d857e53ec834453a073a5aceeb00254 e7027717e0344e5ba76ae7f76fd10267 RX(theta\u2081\u2087) 3d857e53ec834453a073a5aceeb00254--e7027717e0344e5ba76ae7f76fd10267 02c9c80108244506b946bd6115d9a9ff e7027717e0344e5ba76ae7f76fd10267--02c9c80108244506b946bd6115d9a9ff dbea29b6d6bb46dead7c4f2b94b61b5f X 02c9c80108244506b946bd6115d9a9ff--dbea29b6d6bb46dead7c4f2b94b61b5f dbea29b6d6bb46dead7c4f2b94b61b5f--4378223df9404475a8aff45db923d352 60f130cdc12b4955a5f5bd084c08c414 dbea29b6d6bb46dead7c4f2b94b61b5f--60f130cdc12b4955a5f5bd084c08c414 60f130cdc12b4955a5f5bd084c08c414--888e37bfd53343acae63da50aa0a75c7 <pre><code>from qadence import *\n\nb = chain(SWAP(0,1), SWAP(0,3))\n</code></pre> %3 fde8579bc30245a4a58982fd62c344c1 0 85f2133b80aa47e18b977420e08a7a8e fde8579bc30245a4a58982fd62c344c1--85f2133b80aa47e18b977420e08a7a8e b5da44f1a1e14439aabb669f9e71de01 1 84ae80d681324285bc7cc38cbb186272 063de572ecc540f4936e565807df3986 85f2133b80aa47e18b977420e08a7a8e--063de572ecc540f4936e565807df3986 1563db82439a4b0d900e572ae4036560 84ae80d681324285bc7cc38cbb186272--1563db82439a4b0d900e572ae4036560 a1077a7b020b45d699c65d6c5e7543e8 81266a3d7233437190fbb7e066e15043 1563db82439a4b0d900e572ae4036560--81266a3d7233437190fbb7e066e15043 61b8a4f2d72f478b95d41cdbb781cfa9 a1077a7b020b45d699c65d6c5e7543e8--61b8a4f2d72f478b95d41cdbb781cfa9 6a24a9dab9bd403e956ff3a385eb68f6 710d0b59b6b0400f8c2fcb8b88d020fa b5da44f1a1e14439aabb669f9e71de01--710d0b59b6b0400f8c2fcb8b88d020fa db5a5509205e4173bbc8a7acc323a124 2 710d0b59b6b0400f8c2fcb8b88d020fa--84ae80d681324285bc7cc38cbb186272 cc5754ea69814d1a936385ebcabdd6fb 063de572ecc540f4936e565807df3986--cc5754ea69814d1a936385ebcabdd6fb 9e2955d13ac946bfbfb1db1272668bce cc5754ea69814d1a936385ebcabdd6fb--9e2955d13ac946bfbfb1db1272668bce 9e2955d13ac946bfbfb1db1272668bce--6a24a9dab9bd403e956ff3a385eb68f6 584a78280ae34d2f91b55c37fb296b01 11191d4d21574002a0446be9dd64067d db5a5509205e4173bbc8a7acc323a124--11191d4d21574002a0446be9dd64067d e260f702843e41a29e8fe86211d47574 3 bb0c1c982d214d2d9af453fb1535143e 11191d4d21574002a0446be9dd64067d--bb0c1c982d214d2d9af453fb1535143e a228c96cd2ed49f98b5d6436d5d99b7f bb0c1c982d214d2d9af453fb1535143e--a228c96cd2ed49f98b5d6436d5d99b7f 88799891b78b4b219df974db200f3afb a228c96cd2ed49f98b5d6436d5d99b7f--88799891b78b4b219df974db200f3afb 88799891b78b4b219df974db200f3afb--584a78280ae34d2f91b55c37fb296b01 d0dd428db45a400297f88ff5fdddc879 c6e287a5e7e94bf7a9a7afe2350ad06a e260f702843e41a29e8fe86211d47574--c6e287a5e7e94bf7a9a7afe2350ad06a 99d23c44194843cd80a6a3c055da7b78 c6e287a5e7e94bf7a9a7afe2350ad06a--99d23c44194843cd80a6a3c055da7b78 128af50ae65840ca9d216d8ac24b44cc 99d23c44194843cd80a6a3c055da7b78--128af50ae65840ca9d216d8ac24b44cc 128af50ae65840ca9d216d8ac24b44cc--a1077a7b020b45d699c65d6c5e7543e8 81266a3d7233437190fbb7e066e15043--d0dd428db45a400297f88ff5fdddc879 <pre><code>from qadence import *\n\nb = chain(CPHASE(0, 1, 0.5), CPHASE(0, 2, 0.5), CPHASE(0, 3, 0.5))\n</code></pre> %3 7e8abf68d2e4496fb2df4e0d9cf536dc 0 7c3104625c85499d8dcd88f05a185880 7e8abf68d2e4496fb2df4e0d9cf536dc--7c3104625c85499d8dcd88f05a185880 c293b5de65c34b1d9844967a815fb90e 1 dcc6f422fb5642e389d6e527a794493e 7c3104625c85499d8dcd88f05a185880--dcc6f422fb5642e389d6e527a794493e 1ee350e06f044988a10e791d74ffc3d1 dcc6f422fb5642e389d6e527a794493e--1ee350e06f044988a10e791d74ffc3d1 840774a502ff47a1ab314b7a449894d5 1ee350e06f044988a10e791d74ffc3d1--840774a502ff47a1ab314b7a449894d5 c8fa5b9654db4e1da14299d2f6456888 dd2fdbde3b1c40d8960beabe23d20d49 PHASE(0.5) c293b5de65c34b1d9844967a815fb90e--dd2fdbde3b1c40d8960beabe23d20d49 850002b06bd54ca28b8b6d5dc1915ee3 2 dd2fdbde3b1c40d8960beabe23d20d49--7c3104625c85499d8dcd88f05a185880 b07204ae39294c20a418b31b0b378cbd dd2fdbde3b1c40d8960beabe23d20d49--b07204ae39294c20a418b31b0b378cbd d694acb208904434afc96c261570ead9 b07204ae39294c20a418b31b0b378cbd--d694acb208904434afc96c261570ead9 d694acb208904434afc96c261570ead9--c8fa5b9654db4e1da14299d2f6456888 a9ede4bf98d2457c8a0dd49b74beeeb5 1f7d62c777fd47d08c92615ffe138561 850002b06bd54ca28b8b6d5dc1915ee3--1f7d62c777fd47d08c92615ffe138561 9427d4f39b5d4baebbe99f9aa16c32ec 3 f1d57645f4544651b84b90e1676e32e7 PHASE(0.5) 1f7d62c777fd47d08c92615ffe138561--f1d57645f4544651b84b90e1676e32e7 f1d57645f4544651b84b90e1676e32e7--dcc6f422fb5642e389d6e527a794493e f7c4abb9709141a0ba66026de2280816 f1d57645f4544651b84b90e1676e32e7--f7c4abb9709141a0ba66026de2280816 f7c4abb9709141a0ba66026de2280816--a9ede4bf98d2457c8a0dd49b74beeeb5 f723803f0ab141a197500be36ac9021a eae2bbea0adb4446b7536b91e5b81771 9427d4f39b5d4baebbe99f9aa16c32ec--eae2bbea0adb4446b7536b91e5b81771 7e1916efa04f410fb3a598b728303daf eae2bbea0adb4446b7536b91e5b81771--7e1916efa04f410fb3a598b728303daf ff7ce3b4fea34a238c71f7105e2e1ef5 PHASE(0.5) 7e1916efa04f410fb3a598b728303daf--ff7ce3b4fea34a238c71f7105e2e1ef5 ff7ce3b4fea34a238c71f7105e2e1ef5--1ee350e06f044988a10e791d74ffc3d1 ff7ce3b4fea34a238c71f7105e2e1ef5--f723803f0ab141a197500be36ac9021a"},{"location":"tutorials/development/draw/#developer-documentation","title":"Developer documentation","text":"<p>This section contains examples in pure graphviz that can be used to understand roughly what is done in the actual drawing backend.</p> <pre><code>import graphviz\n\nfont_name = \"Sans-Serif\"\nfont_size = \"8\"\n\ngraph_attr = {\n    \"rankdir\": \"LR\",  # LR = left to right, TB = top to bottom\n    \"nodesep\": \"0.1\",  # In inches, tells distance between nodes without edges\n    \"compound\": \"true\",  # Needed to draw properly edges in hamevo when content is hidden\n    \"splines\": \"false\",  # Needed to draw control gates vertical lines one over the other\n}  # These are the default values for graphs\n\nnode_attr = {\n    \"shape\": \"box\",  # 'box' for normal nodes, 'point' for control gates or 'plaintext' for starting nodes (the qubit label).\n    \"style\": \"rounded\",  # Unfortunately we can't specify the radius of the rounded, at least for this version\n    \"fontname\": font_name,\n    \"fontsize\": font_size,\n    \"width\": \"0.1\",  # In inches, it doesn't get tinier than the label font.\n    \"height\": \"0.1\"  # In inches, it doesn't get tinier than the label font.\n}  # These are the defaults values that can be overridden at node declaration.\n\ndefault_cluster_attr = {\n    \"fontname\": font_name,\n    \"fontsize\": font_size,\n    \"labelloc\": \"b\",  # location of cluster label. b as bottom, t as top\n    \"style\": \"rounded\"\n} # These are the defaults values that can be overridden at sub graph declaration\n\nhamevo_cluster_attr = {\n    \"label\": \"HamEvo(t=10)\"\n}\nhamevo_cluster_attr.update(default_cluster_attr)\n\nh = graphviz.Graph(graph_attr=graph_attr, node_attr=node_attr)\nh.node(\"Hello World!\")\nh\n</code></pre> <pre><code>\n</code></pre> <pre><code># Define graph\nh = graphviz.Graph(node_attr=node_attr, graph_attr=graph_attr)\n\n# Add start and end nodes\nfor i in range(4):\n    h.node(f's{i}', shape=\"plaintext\", label=f'{i}', group=f\"{i}\")\n    h.node(f'e{i}', style='invis', group=f\"{i}\")\n\n# Add nodes\nh.node('X', group=\"0\")\nh.node('Y', group=\"1\")\n\n# Add hamevo and its nodes\nhamevo = graphviz.Graph(name='cluster_hamevo', graph_attr=hamevo_cluster_attr)\nfor i in range(4):\n    hamevo.node(f'z{i}', shape=\"box\", style=\"invis\", label=f'{i}', group=f\"{i}\")\nh.subgraph(hamevo)\n\n# Add rx gates cluster and its nodes\ncluster_attr = {\"label\": \"RX gates\"}\ncluster_attr.update(default_cluster_attr)\ncluster = graphviz.Graph(name=\"cluster_0\", graph_attr=cluster_attr)\ncluster.node('RX(x)', group=\"2\")\ncluster.node('RX(0.5)', group=\"3\")\nh.subgraph(cluster)\n\nh.node('cnot0', label='', shape='point', width='0.1', group='0')\nh.node('cnot1', label='X', group='1')\nh.node('cnot2', label='', shape='point', width='0.1', group='2')\nh.node('cnot3', label='', shape='point', width='0.1', group='3')\n\n# Add edges\nh.edge('s0', 'X')\nh.edge('X', 'cnot0')\nh.edge('cnot0', 'z0', lhead='cluster_hamevo')\nh.edge('z0', 'e0', ltail='cluster_hamevo')\nh.edge('s1', 'Y')\nh.edge('Y', 'cnot1')\nh.edge('cnot1', 'z1', lhead='cluster_hamevo')\nh.edge('z1', 'e1', ltail='cluster_hamevo')\nh.edge('s2', 'RX(x)')\nh.edge('RX(x)', 'cnot2')\nh.edge('cnot2', 'z2', lhead='cluster_hamevo')\nh.edge('z2', 'e2', ltail='cluster_hamevo')\nh.edge('s3', 'RX(0.5)')\nh.edge('RX(0.5)', 'cnot3')\nh.edge('cnot3', 'z3', lhead='cluster_hamevo')\nh.edge('z3', 'e3', ltail='cluster_hamevo')\nh.edge('cnot1', 'cnot0', constraint='false')  # constraint: false is needed to draw vertical edges\nh.edge('cnot1', 'cnot2', constraint='false')  # constraint: false is needed to draw vertical edges\nh.edge('cnot1', 'cnot3', constraint='false')  # constraint: false is needed to draw vertical edges\nh\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/development/draw/#example-of-cluster-of-clusters","title":"Example of cluster of clusters","text":"<pre><code># Define graph\nh = graphviz.Graph(node_attr=node_attr, graph_attr=graph_attr)\n\n# Define start and end nodes\nfor i in range(4):\n    h.node(f's{i}', shape=\"plaintext\", label=f'{i}', group=f\"{i}\")\n    h.node(f'e{i}', style='invis', group=f\"{i}\")\n\n# Define outer cluster\ncluster_attr = {\"label\": \"Outer cluster\"}\ncluster_attr.update(default_cluster_attr)\nouter_cluster = graphviz.Graph(name=\"cluster_outer\", graph_attr=cluster_attr)\n\n# Define inner cluster 1 and its nodes\ncluster_attr = {\"label\": \"Inner cluster 1\"}\ncluster_attr.update(default_cluster_attr)\ninner1_cluster = graphviz.Graph(name=\"cluster_inner1\", graph_attr=cluster_attr)\ninner1_cluster.node(\"a0\", group=\"0\")\ninner1_cluster.node(\"a1\", group=\"1\")\nouter_cluster.subgraph(inner1_cluster)\n\n# Define inner cluster 2 and its nodes\ncluster_attr = {\"label\": \"Inner cluster 2\"}\ncluster_attr.update(default_cluster_attr)\ninner2_cluster = graphviz.Graph(name=\"cluster_inner2\", graph_attr=cluster_attr)\ninner2_cluster.node(\"a2\", group=\"2\")\ninner2_cluster.node(\"a3\", group=\"3\")\nouter_cluster.subgraph(inner2_cluster)\n\n# This has to be done here, after inner clusters definitions\nh.subgraph(outer_cluster)\n\n# Define more nodes\nfor i in range(4):\n    h.node(f\"b{i}\", group=f\"{i}\")\n\nfor i in range(4):\n    h.edge(f's{i}', f'a{i}')\n    h.edge(f'a{i}', f'b{i}')\n    h.edge(f'b{i}', f'e{i}')\n\nh\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/digital_analog_qc/","title":"Digital-Analog Quantum Computation","text":"<p>Digital-analog quantum computation (DAQC) is a universal quantum computing paradigm<sup>1</sup>, based on two primary computations:</p> <ul> <li>Fast single-qubit operations (digital).</li> <li>Multi-partite entangling operations acting on all qubits (analog).</li> </ul> <p>A promising quantum computing platform for the implementation of the DAQC paradigm is neutral-atoms, where both these computations are realizable.</p>"},{"location":"tutorials/digital_analog_qc/#digital-analog-emulation","title":"Digital-analog emulation","text":"<p>Qadence simplifies the execution of DAQC programs on either emulated or real devices by providing a simplified interface for customizing interactions and interfacing with pulse-level programming in <code>Pulser</code><sup>3</sup>.</p>"},{"location":"tutorials/digital_analog_qc/#digital-analog-transformation","title":"Digital-analog transformation","text":"<p>Furthermore, the essence of digital-analog computation is the ability to represent any analog operation, i.e. any arbitrary Hamiltonian, using an auxiliary device-amenable Hamiltonian, such as the ubiquitous Ising model<sup>2</sup>. This is at the core of the DAQC implementation in Qadence.</p>"},{"location":"tutorials/digital_analog_qc/#execution-on-rydberg-atom-arrays-with-restriced-addressability","title":"Execution on Rydberg atom arrays with restriced addressability","text":"<p>Finally, Qadence offers some convenience constructors and interfaces to execute programs compatible with a DAQC flavor featuring only a restricted access to individual qubit addressability with always-on interaction. This regime is common in currently available neutral atom quantum computers.</p>"},{"location":"tutorials/digital_analog_qc/#references","title":"References","text":"<ol> <li> <p>Dodd et al., Universal quantum computation and simulation using any entangling Hamiltonian and local unitaries, PRA 65, 040301 (2002). \u21a9</p> </li> <li> <p>Pulser: An open-source package for the design of pulse sequences in programmable neutral-atom arrays \u21a9</p> </li> <li> <p>Parra-Rodriguez et al., Digital-Analog Quantum Computation, PRA 101, 022305 (2020). \u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/analog-basics/","title":"Basic operations on neutral-atoms","text":"<p>Warning</p> <p>The digital-analog emulation framework is under construction and more changes to the interface may still occur.</p> <p>Qadence includes primitives for the construction of programs implemented on a set of interacting qubits. The goal is to build digital-analog programs that better represent the reality of interacting qubit platforms, such as neutral-atoms, while maintaining a simplified interface for users coming from a digital quantum computing background that may not be as familiar with pulse-level programming.</p> <p>To build the intuition for the interface in Qadence, it is important to go over some of the underlying physics. We can write a general Hamiltonian for a set of \\(n\\) interacting qubits as</p> \\[ \\mathcal{H} = \\sum_{i=0}^{n-1}\\left(\\mathcal{H}^\\text{d}_{i}(t) + \\sum_{j&lt;i}\\mathcal{H}^\\text{int}_{ij}\\right), \\] <p>where the driving Hamiltonian \\(\\mathcal{H}^\\text{d}_{i}\\) describes the pulses used to control single-qubit rotations, and the interaction Hamiltonian \\(\\mathcal{H}^\\text{int}_{ij}\\) describes the natural interaction between qubits.</p>"},{"location":"tutorials/digital_analog_qc/analog-basics/#rydberg-atoms","title":"Rydberg atoms","text":"<p>For the purpose of digital-analog emulation of neutral-atom systems in Qadence, we now consider a simplified time-independent global driving Hamiltonian, written as</p> \\[ \\mathcal{H}^\\text{d}_{i} = \\frac{\\Omega}{2}\\left(\\cos(\\phi) X_i - \\sin(\\phi) Y_i \\right) - \\delta N_i \\] <p>where \\(\\Omega\\) is the Rabi frequency, \\(\\delta\\) is the detuning, \\(\\phi\\) is the phase, \\(X_i\\) and \\(Y_i\\) are the standard Pauli operators, and \\(N_i=\\frac{1}{2}(I_i-Z_i)\\) is the number operator. This Hamiltonian allows arbitrary global single-qubit rotations to be written, meaning that the values set for \\((\\Omega,\\phi,\\delta)\\) are the same accross the qubit support.</p> <p>For the interaction term, Rydberg atoms typically allow both an Ising and an XY mode of operation. For now, we focus on the Ising interaction, where the Hamiltonian is written as</p> \\[ \\mathcal{H}^\\text{int}_{ij} = \\frac{C_6}{r_{ij}^6}N_iN_j \\] <p>where \\(r_{ij}\\) is the distance between atoms \\(i\\) and \\(j\\), and \\(C_6\\) is a coefficient depending on the specific Rydberg level of the excited state used in the computational logic states. A typical value for rydberg level of 60 is \\(C_6\\approx 866~[\\text{rad} . \\mu \\text{m}^6 / \\text{ns}]\\).</p> <p>For a given register of atoms prepared in some spatial coordinates, the Hamiltonians described will generate the dynamics of some unitary operation as</p> \\[ U(t, \\Omega, \\delta, \\phi) = \\exp(-i\\mathcal{H}t) \\] <p>where we specify the final parameter \\(t\\), the duration of the operation.</p> <p>Qadence uses the following units for user-specified parameters:</p> <ul> <li>Rabi frequency and detuning \\(\\Omega\\), \\(\\delta\\): \\([\\text{rad}/\\mu \\text{s}]\\)</li> <li>Phase \\(\\phi\\): \\([\\text{rad}]\\)</li> <li>Duration \\(t\\): \\([\\text{ns}]\\)</li> <li>Atom coordinates: \\([\\mu \\text{m}]\\)</li> </ul>"},{"location":"tutorials/digital_analog_qc/analog-basics/#in-practice","title":"In practice","text":"<p>Given the Hamiltonian description in the previous section, we will now go over a few examples of the standard operations available in Qadence.</p>"},{"location":"tutorials/digital_analog_qc/analog-basics/#arbitrary-rotation","title":"Arbitrary rotation","text":"<p>To start, we will exemplify the a general rotation on a set of atoms. To create an arbitrary register of atoms, we refer the user to the register creation tutorial. Below, we create a line register of three qubits with a separation of \\(8~\\mu\\text{m}\\). This is a typical value used in combination with a standard experimental setup of neutral atoms such that the interaction term in the Hamiltonian can effectively be used for computations.</p> <pre><code>from qadence import Register\n\nreg = Register.line(3, spacing=8.0)  # Atom spacing in \u03bcm\n</code></pre> <p>Currently, the most general rotation operation uses the <code>AnalogRot</code> operation, which essentially implements \\(U(t, \\Omega, \\delta, \\phi)\\) defined above.</p> <pre><code>from qadence import AnalogRot, PI\n\nrot_op = AnalogRot(\n    duration = 500., # [ns]\n    omega = PI, # [rad/\u03bcs]\n    delta = PI, # [rad/\u03bcs]\n    phase = PI, # [rad]\n)\n</code></pre> <p>Note that in the code above a specific qubit support is not defined. By default this operation applies a global rotation on all qubits. We can define a circuit using the 3-qubit register and run it in the pyqtorch backend:</p> <pre><code>from qadence import BackendName, run\n\nwf = run(reg, rot_op, backend = BackendName.PYQTORCH)\n\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4248-0.2411j, -0.1687+0.3156j, -0.1696+0.2676j, -0.2040-0.2671j,\n         -0.1687+0.3156j,  0.0014-0.2721j, -0.2040-0.2671j,  0.3034-0.1130j]])\n</code></pre> Under the hood of AnalogRot      To be fully explicit about what goes on under the hood of `AnalogRot`, we can look at the example     code below.      <pre><code>from qadence import BackendName, HamEvo, X, Y, N, add, run, PI\nfrom qadence.analog.constants import C6_DICT\nfrom math import cos, sin\n\n# Following the 3-qubit register above\nn_qubits = 3\ndx = 8.0\n\n# Parameters used in the AnalogRot\nduration = 500.\nomega = PI\ndelta = PI\nphase = PI\n\n# Building the terms in the driving Hamiltonian\nh_x = (omega / 2) * cos(phase) * add(X(i) for i in range(n_qubits))\nh_y = (-1.0 * omega / 2) * sin(phase) * add(Y(i) for i in range(n_qubits))\nh_n = -1.0 * delta * add(N(i) for i in range(n_qubits))\n\n# Building the interaction Hamiltonian\n\n# Dictionary of coefficient values for each Rydberg level, which is 60 by default\nc_6 = C6_DICT[60]\n\nh_int = c_6 * (\n    1/(dx**6) * (N(0)@N(1)) +\n    1/(dx**6) * (N(1)@N(2)) +\n    1/((2*dx)**6) * (N(0)@N(2))\n)\n\nhamiltonian = h_x + h_y + h_n + h_int\n\n# Convert duration to \u00b5s due to the units of the Hamiltonian\nexplicit_rot = HamEvo(hamiltonian, duration / 1000)\n\nwf = run(n_qubits, explicit_rot, backend = BackendName.PYQTORCH)\n\n# We get the same final wavefunction\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4248-0.2411j, -0.1687+0.3156j, -0.1696+0.2676j, -0.2040-0.2671j,\n         -0.1687+0.3156j,  0.0014-0.2721j, -0.2040-0.2671j,  0.3034-0.1130j]])\n</code></pre> <p>When sending the <code>AnalogRot</code> operation to the pyqtorch backend, Qadence automatically builds the correct Hamiltonian and the corresponding <code>HamEvo</code> operation with the added qubit interactions, as shown explicitly in the minimized section above. However, this operation is also supported in the Pulser backend, where the correct pulses are automatically created.</p> <pre><code>wf = run(\n    reg,\n    rot_op,\n    backend = BackendName.PULSER,\n)\n\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4253-0.2408j, -0.1688+0.3157j, -0.1698+0.2678j, -0.2044-0.2667j,\n         -0.1688+0.3157j,  0.0011-0.2721j, -0.2044-0.2667j,  0.3026-0.1137j]])\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#rx-ry-rz-rotations","title":"RX / RY / RZ rotations","text":"<p>The <code>AnalogRot</code> provides full control over the parameters of \\(\\mathcal{H}^\\text{d}\\), but users coming from a digital quantum computing background may be more familiar with the standard <code>RX</code>, <code>RY</code> and <code>RZ</code> rotations, also available in Qadence. For the emulated analog interface, Qadence provides alternative <code>AnalogRX</code>, <code>AnalogRY</code> and <code>AnalogRZ</code> operations which call <code>AnalogRot</code> under the hood to represent the rotations accross the respective axis.</p> <p>For a given angle of rotation \\(\\theta\\) provided to each of these operations, currently a set of hardcoded assumptions are made on the tunable Hamiltonian parameters:</p> \\[ \\begin{aligned} \\text{RX}:&amp; \\quad \\Omega = \\pi, \\quad \\delta = 0, \\quad \\phi = 0, \\quad t = (\\theta/\\Omega)\\times 10^3 \\\\ \\text{RY}:&amp; \\quad \\Omega = \\pi, \\quad \\delta = 0, \\quad \\phi = -\\pi/2, \\quad t = (\\theta/\\Omega)\\times 10^3 \\\\ \\text{RZ}:&amp; \\quad \\Omega = 0, \\quad \\delta = \\pi, \\quad \\phi = 0, \\quad t = (\\theta/\\delta)\\times 10^3 \\\\ \\end{aligned} \\] <p>Note that the \\(\\text{RZ}\\) operation as defined above includes a global phase compared to the standard \\(\\text{RZ}\\) rotation since it evolves \\(\\exp\\left(-i\\frac{\\theta}{2}\\frac{I-Z}{2}\\right)\\) instead of \\(\\exp\\left(-i\\frac{\\theta}{2}Z\\right)\\) given the detuning operator in \\(\\mathcal{H}^\\text{d}\\).</p> <p>Warning</p> <p>As shown above, the values of \\(\\Omega\\) and \\(\\delta\\) are currently hardcoded in these operators, and the effective angle of rotation is controlled by varying the duration of the evolution. Currently, the best way to overcome this is to use <code>AnalogRot</code> directly, but more general and convenient options will be provided soon in an improved interface.</p> <p>Below we exemplify the usage of <code>AnalogRX</code>:</p> <pre><code>from qadence import Register, BackendName\nfrom qadence import RX, AnalogRX, random_state, equivalent_state, kron, run, PI\n\nn_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\n# Rotation angle\ntheta = PI\n\n# Analog rotation using the Rydberg Hamiltonian\nrot_analog = AnalogRX(angle = theta)\n\n# Equivalent full-digital global rotation\nrot_digital = kron(RX(i, theta) for i in range(n_qubits))\n\n# Some random initial state\ninit_state = random_state(n_qubits)\n\n# Compare the final state using the full digital and the AnalogRX\nwf_analog_pyq = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\n\nwf_digital_pyq = run(\n    reg,\n    rot_digital,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_digital_pyq, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  False\n</code></pre> <p>As we can see, running a global <code>RX</code> or the <code>AnalogRX</code> does not result in equivalent states at the end, given that the digital <code>RX</code> operation does not include the interaction between the qubits. By setting <code>dx</code> very high in the code above the interaction will be less significant and the results will match.</p> <p>However, if we compare with the Pulser backend, we see that the results for <code>AnalogRX</code> are consistent with the expected results from a real device:</p> <pre><code>wf_analog_pulser = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PULSER,\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_analog_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#evolving-the-interaction-term","title":"Evolving the interaction term","text":"<p>Finally, besides applying specific qubit rotations, we can also choose to evolve only the interaction term \\(\\mathcal{H}^\\text{int}\\), equivalent to setting \\(\\Omega = \\delta = \\phi = 0\\). To do so, Qadence provides the function <code>AnalogInteraction</code> which does exactly this.</p> <pre><code>from qadence import Register, BackendName, random_state, equivalent_state, AnalogInteraction, run\n\nn_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\nduration = 1000.\nop = AnalogInteraction(duration = duration)\n\ninit_state = random_state(n_qubits)\n\nwf_pyq = run(reg, op, state = init_state, backend = BackendName.PYQTORCH)\nwf_pulser = run(reg, op, state = init_state, backend = BackendName.PULSER)\n\nbool_equiv = equivalent_state(wf_pyq, wf_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#device-specifications-in-qadence","title":"Device specifications in Qadence","text":"<p>As a way to control other specifications of the interacting Rydberg atoms, Qadence provides a <code>RydbergDevice</code> class, which is currently used for both the pyqtorch and the pulser backends. Below we initialize a Rydberg device showcasing all the possible options.</p> <pre><code>from qadence import RydbergDevice, DeviceType, Interaction, PI\n\ndevice_specs = RydbergDevice(\n    interaction=Interaction.NN, # Or Interaction.XY, supported only for pyqtorch\n    rydberg_level=60, # Integer value affecting the C_6 coefficient\n    coeff_xy=3700.00, # C_3 coefficient for the XY interaction\n    max_detuning=2 * PI * 4, # Max value for delta, currently only used in pulser\n    max_amp=2 * PI * 3, # Max value for omega, currently only used in pulser\n    pattern=None, # Semi-local addressing pattern, see the relevant tutorial\n    type=DeviceType.IDEALIZED, # Pulser device to which the qadence device is converted in that backend\n)\n</code></pre> <p>The values above are the defaults when simply running <code>device_specs = RydbergDevice()</code>. The convenience wrappers <code>IdealDevice()</code> or <code>RealisticDevice()</code> can also be used which simply change the <code>type</code> for the Pulser backend, but also allow an <code>AddressingPattern</code> passed in the <code>pattern</code> argument (see the relevant tutorial here).</p> <p>Warning</p> <p>Currently, the options above are not fully integrated in both backends and this class should mostly be used if a user wishes to experiment with a different <code>rydberg_level</code>, or to change the device type for the pulser backend.</p> <p>Planned features to add to the RydbergDevice include the definition of custom interaction functions, the control of other drive Hamiltonian parameters so that \\(\\Omega\\), \\(\\delta\\) and \\(\\phi\\) are not hardcoded when doing analog rotations, and the usage of the <code>max_detuning</code> and <code>max_amp</code> to control those respective parameters when training models in the pyqtorch backend.</p> <p>Finally, to change a given simulation, the device specifications are integrated in the Qadence <code>Register</code>. By default, all registers initialize an <code>IdealDevice()</code> under the hood. Below we run a quick test for a different rydberg level.</p> <pre><code>from qadence import Register, BackendName, random_state, equivalent_state, run\nfrom qadence import AnalogRX, RydbergDevice, PI\n\ndevice_specs = RydbergDevice(rydberg_level = 70)\n\nn_qubits_side = 2\nreg = Register.square(\n    n_qubits_side,\n    spacing = 8.0,\n    device_specs = device_specs\n)\n\nrot_analog = AnalogRX(angle = PI)\n\ninit_state = random_state(n_qubits = 4)\n\nwf_analog_pyq = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\nwf_analog_pulser = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PULSER\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_analog_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#technical-details","title":"Technical details","text":"<p>Warning</p> <p>The details described here are relevant in the current version but will be lifted soon for the next version of the emulated analog interface.</p> <p>In the previous section we have exemplified the main ingredients of the current user-facing functionalities of the emulated analog interface, and in the next tutorial on Quantum Circuit Learning we will exmplify its usage in a simple QML example. Here we specify some extra details of this interface.</p> <p>In the block system, all analog rotation operators initialize a <code>ConstantAnalogRotation</code> block, while the <code>AnalogInteraction</code> operation initializes an <code>InteractionBlock</code>. As we have shown, by default, these blocks use a global qubit support, which can be passed explicitly by setting <code>qubit_support = QubitSupportType.GLOBAL</code>. However, composing blocks using <code>kron</code> with local qubit supports and different durations is not allowed.</p> <pre><code>from qadence import AnalogRX, AnalogRY, Register, kron\n\ndx = 8.0\nreg = Register.from_coordinates([(0, 0), (dx, 0)])\n\n# Does not work (the angle affects the duration, as seen above):\nrot_0 = AnalogRX(angle = 1.0, qubit_support = (0,))\nrot_1 = AnalogRY(angle = 2.0, qubit_support = (1,))\n\ntry:\n    block = kron(rot_0, rot_1)\nexcept ValueError as error:\n    print(\"Error:\", error)\n\n# Works:\nrot_0 = AnalogRX(angle = 1.0, qubit_support = (0,))\nrot_1 = AnalogRY(angle = 1.0, qubit_support = (1,))\n\nblock = kron(rot_0, rot_1)\n</code></pre> <pre><code>Error: Kron'ed blocks have to have same duration.\n</code></pre> <p>Using <code>chain</code> is only supported between analog blocks with global qubit support:</p> <pre><code>from qadence import chain\n\nrot_0 = AnalogRX(angle = 1.0, qubit_support = \"global\")\nrot_1 = AnalogRY(angle = 2.0, qubit_support = \"global\")\n\nblock = chain(rot_0, rot_1)\n</code></pre> <p>The restrictions above only apply to the analog blocks, and analog and digital blocks can currently be composed.</p> <pre><code>from qadence import RX\n\nrot_0 = AnalogRX(angle = 1.0, qubit_support = \"global\")\nrot_1 = AnalogRY(angle = 2.0, qubit_support = (0,))\nrot_digital = RX(1, 1.0)\n\nblock_0 = chain(rot_0, rot_digital)\nblock_1 = kron(rot_1, rot_digital)\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-blocks-qcl/","title":"Fitting a function with analog blocks","text":"<p>Analog blocks can be parametrized in the usual Qadence manner. Like any other parameters, they can be optimized. The next snippet exemplifies the creation of an analog and parameterized ansatz to fit a simple function. First, define a register and feature map block. We again use a default spacing of \\(8~\\mu\\text{m}\\) as done in the basic tutorial.</p> <pre><code>from qadence import Register, FeatureParameter, chain\nfrom qadence import AnalogRX, AnalogRY, AnalogRZ, AnalogInteraction\nfrom sympy import acos\n\n# Line register\nn_qubits = 2\nregister = Register.line(n_qubits, spacing = 8.0)\n\n# The input feature x for the circuit to learn f(x)\nx = FeatureParameter(\"x\")\n\n# Feature map with a few global analog rotations\nfm = chain(\n    AnalogRX(x),\n    AnalogRY(2*x),\n    AnalogRZ(3*x),\n)\n</code></pre> <p>Next, we define the ansatz with parameterized rotations.</p> <pre><code>from qadence import hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel, BackendName, DiffMode\nfrom qadence import VariationalParameter\n\nt_0 = 1000. * VariationalParameter(\"t_0\")\nt_1 = 1000. * VariationalParameter(\"t_1\")\nt_2 = 1000. * VariationalParameter(\"t_2\")\n\n# Creating the ansatz with parameterized rotations and wait time\nansatz = chain(\n    AnalogRX(\"tht_0\"),\n    AnalogRY(\"tht_1\"),\n    AnalogRZ(\"tht_2\"),\n    AnalogInteraction(t_0),\n    AnalogRX(\"tht_3\"),\n    AnalogRY(\"tht_4\"),\n    AnalogRZ(\"tht_5\"),\n    AnalogInteraction(t_1),\n    AnalogRX(\"tht_6\"),\n    AnalogRY(\"tht_7\"),\n    AnalogRZ(\"tht_8\"),\n    AnalogInteraction(t_2),\n)\n</code></pre> <p>We define the measured observable as the total magnetization, and build the <code>QuantumModel</code>.</p> <pre><code># Total magnetization observable\nobservable = hamiltonian_factory(n_qubits, detuning = Z)\n\n# Defining the circuit and observable\ncircuit = QuantumCircuit(register, fm, ansatz)\n\nmodel = QuantumModel(\n    circuit,\n    observable = observable,\n    backend = BackendName.PYQTORCH,\n    diff_mode = DiffMode.AD\n)\n</code></pre> <p>Now we can define the function to fit as well as our training and test data.</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Function to fit:\ndef f(x):\n    return x**2\n\nx_test = torch.linspace(-1.0, 1.0, steps=100)\ny_test = f(x_test)\n\nx_train = torch.linspace(-1.0, 1.0, steps=10)\ny_train = f(x_train)\n\n# Initial prediction from the model, to be visualized later\ny_pred_initial = model.expectation({\"x\": x_test}).detach()\n</code></pre> <p>Finally we define a simple loss function and training loop.</p> <pre><code>mse_loss = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(x_train, y_train):\n    out = model.expectation({\"x\": x_train})\n    loss = mse_loss(out.squeeze(), y_train)\n    return loss\n\nn_epochs = 200\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_fn(x_train, y_train)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>And with the model trained we can plot the final results.</p> <pre><code>y_pred_final = model.expectation({\"x\": x_test}).detach()\n\nplt.plot(x_test, y_pred_initial, label = \"Initial prediction\")\nplt.plot(x_test, y_pred_final, label = \"Final prediction\")\nplt.scatter(x_train, y_train, label = \"Training points\")\n</code></pre> 2025-02-07T10:21:40.616103 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/analog-qubo/","title":"Solve a QUBO problem","text":"<p>In this notebook, we solve a quadratic unconstrained binary optimization (QUBO) problem with Qadence. QUBOs are very popular combinatorial optimization problems with a wide range of applications. Here, we solve the problem using the QAOA <sup>1</sup> variational algorithm by embedding the QUBO problem weights onto a register as standard for neutral atom quantum devices.</p> <p>Additional background information on QUBOs can be found here, directly solved using the pulse-level interface Pulser.</p>"},{"location":"tutorials/digital_analog_qc/analog-qubo/#define-and-solve-qubo","title":"Define and solve QUBO","text":"Pre-requisite: optimal register coordinates for embedding the QUBO problem <p>A basic ingredient for solving a QUBO problem with a neutral atom device is to embed the problem onto the atomic register. In short, embedding algorithms cast the problem onto a graph mapped onto the register by optimally finding atomic coordinates. A discussion on the embedding algorithms is beyond the scope of this tutorial and a simplified version taken from here is added below.</p> <pre><code>import numpy as np\nimport numpy.typing as npt\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist, squareform\nfrom qadence import RydbergDevice\n\ndef qubo_register_coords(Q: np.ndarray, device: RydbergDevice) -&gt; list:\n    \"\"\"Compute coordinates for register.\"\"\"\n\n    def evaluate_mapping(new_coords, *args):\n        \"\"\"Cost function to minimize. Ideally, the pairwise\n        distances are conserved\"\"\"\n        Q, shape = args\n        new_coords = np.reshape(new_coords, shape)\n        interaction_coeff = device.coeff_ising\n        new_Q = squareform(interaction_coeff / pdist(new_coords) ** 6)\n        return np.linalg.norm(new_Q - Q)\n\n    shape = (len(Q), 2)\n    np.random.seed(0)\n    x0 = np.random.random(shape).flatten()\n    res = minimize(\n        evaluate_mapping,\n        x0,\n        args=(Q, shape),\n        method=\"Nelder-Mead\",\n        tol=1e-6,\n        options={\"maxiter\": 200000, \"maxfev\": None},\n    )\n    return [(x, y) for (x, y) in np.reshape(res.x, (len(Q), 2))]\n</code></pre> <p>With the embedding routine define above, we can translate a matrix defining a QUBO problem to a set of atom coordinates for the register. The QUBO problem is initially defined by a graph of weighted edges and a cost function to be optimized. The weighted edges are represented by a real-valued symmetric matrix <code>Q</code> which is used throughout the tutorial.</p> <pre><code>import torch\nfrom qadence import QuantumModel\n\nseed = 0\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# QUBO problem weights (real-value symmetric matrix)\nQ = np.array(\n    [\n        [-10.0, 19.7365809, 19.7365809, 5.42015853, 5.42015853],\n        [19.7365809, -10.0, 20.67626392, 0.17675796, 0.85604541],\n        [19.7365809, 20.67626392, -10.0, 0.85604541, 0.17675796],\n        [5.42015853, 0.17675796, 0.85604541, -10.0, 0.32306662],\n        [5.42015853, 0.85604541, 0.17675796, 0.32306662, -10.0],\n    ]\n)\n\n# Loss function to guide the optimization routine\ndef loss(model: QuantumModel, *args) -&gt; tuple[torch.Tensor, dict]:\n    to_arr_fn = lambda bitstring: np.array(list(bitstring), dtype=int)\n    cost_fn = lambda arr: arr.T @ Q @ arr\n    samples = model.sample({}, n_shots=1000)[0]\n    cost_fn = sum(samples[key] * cost_fn(to_arr_fn(key)) for key in samples)\n    return torch.tensor(cost_fn / sum(samples.values())), {}\n</code></pre> <p>The QAOA algorithm needs a variational quantum circuit with optimizable parameters. For that purpose, we use a fully analog circuit composed of two global rotations per layer on different axes of the Bloch sphere. The first rotation corresponds to the mixing Hamiltonian and the second one to the embedding Hamiltonian <sup>1</sup>. In this setting, the embedding is realized by the appropriate register coordinates and the resulting qubit interaction. Details on the analog blocks used here can be found in the analog basics tutorial.</p> Rydberg level <p>The Rydberg level is set to 70. We initialize the weighted register graph from the QUBO definition similarly to what is done in the original tutorial, and set the device specifications with the updated Rydberg level.</p> <pre><code>from qadence import QuantumCircuit, Register, RydbergDevice\nfrom qadence import chain, AnalogRX, AnalogRZ\n\n# Device specification and atomic register\ndevice = RydbergDevice(rydberg_level=70)\n\nreg = Register.from_coordinates(\n    qubo_register_coords(Q, device), device_specs=device\n)\n\n# Analog variational quantum circuit\nlayers = 2\nblock = chain(*[AnalogRX(f\"t{i}\") * AnalogRZ(f\"s{i}\") for i in range(layers)])\ncircuit = QuantumCircuit(reg, block)\n</code></pre> <p>By initializing the <code>QuantumModel</code> with this circuit we can check the initial counts where no clear solution can be found.</p> <pre><code>model = QuantumModel(circuit)\ninitial_counts = model.sample({}, n_shots=1000)[0]\n</code></pre> <pre><code>initial_counts = OrderedCounter({'00000': 101, '10000': 87, '01000': 75, '00110': 72, '00100': 70, '01010': 64, '01001': 62, '00101': 53, '00010': 51, '00011': 48, '01011': 46, '00001': 45, '10010': 45, '00111': 38, '10001': 34, '11000': 29, '10100': 18, '01100': 14, '01110': 11, '01111': 10, '10110': 7, '11001': 7, '11010': 6, '01101': 4, '10101': 2, '10011': 1})\n</code></pre> <p>Finally, we can proceed with the variational optimization. The cost function defined above is derived from bitstring computations and therefore non differentiable. We use Qadence ML facilities to run gradient-free optimizations using the <code>nevergrad</code> library.</p> <pre><code>from qadence.ml_tools import Trainer, TrainConfig, num_parameters\nimport nevergrad as ng\n\nTrainer.set_use_grad(False)\n\nconfig = TrainConfig(max_iter=100)\n\noptimizer = ng.optimizers.NGOpt(\n    budget=config.max_iter, parametrization=num_parameters(model)\n)\n\ntrainer = Trainer(model, optimizer, config, loss)\n\ntrainer.fit()\n\noptimal_counts = model.sample({}, n_shots=1000)[0]\n</code></pre> <p>Finally, let's plot the solution. The expected bitstrings are marked in red.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Known solutions to the QUBO problem.\nsolution_bitstrings = [\"01011\", \"00111\"]\n\ndef plot_distribution(C, ax, title):\n    C = dict(sorted(C.items(), key=lambda item: item[1], reverse=True))\n    color_dict = {key: \"r\" if key in solution_bitstrings else \"b\" for key in C}\n    ax.set_xlabel(\"bitstrings\")\n    ax.set_ylabel(\"counts\")\n    ax.set_xticks([i for i in range(len(C.keys()))], C.keys(), rotation=90)\n    ax.bar(C.keys(), C.values(), color=color_dict.values())\n    ax.set_title(title)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nplot_distribution(initial_counts, axs[0], \"Initial counts\")\nplot_distribution(optimal_counts, axs[1], \"Optimal counts\")\n</code></pre> 2025-02-07T10:21:47.342447 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/analog-qubo/#references","title":"References","text":"<ol> <li> <p>Edward Farhi, Jeffrey Goldstone, Sam Gutmann, A Quantum Approximate Optimization Algorithm, arXiv:1411.4028 (2014) \u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/","title":"<code>CNOT</code> with interacting qubits","text":"<p>Digital-analog quantum computing focuses on using single qubit digital gates combined with more complex and device-dependent analog interactions to represent quantum programs. This paradigm has been shown to be universal for quantum computation<sup>1</sup>. However, while this approach may have advantages when adapting quantum programs to real devices, known quantum algorithms are very often expressed in a fully digital paradigm. As such, it is also important to have concrete ways to transform from one paradigm to another.</p> <p>This tutorial will exemplify the DAQC transformation starting with the representation of a simple digital <code>CNOT</code> using the universality of the Ising Hamiltonian<sup>2</sup>.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#cnot-with-cphase","title":"<code>CNOT</code> with <code>CPHASE</code>","text":"<p>Let's look at a single example of how the digital-analog transformation can be used to perform a <code>CNOT</code> on two qubits inside a register of globally interacting qubits.</p> <p>First, note that the <code>CNOT</code> can be decomposed with two Hadamard and a <code>CPHASE</code> gate with \\(\\phi=\\pi\\):</p> <pre><code>import torch\nfrom qadence import chain, sample, product_state\n\nfrom qadence.draw import display\nfrom qadence import X, I, Z, H, N, CPHASE, CNOT, HamEvo, PI\n\nn_qubits = 2\n\n# CNOT gate\ncnot_gate = CNOT(0, 1)\n\n# CNOT decomposed\nphi = PI\ncnot_decomp = chain(H(1), CPHASE(0, 1, phi), H(1))\n\ninit_state = product_state(\"10\")\n</code></pre> <pre><code>sample from CNOT gate and 100 shots = [OrderedCounter({'11': 100})]\nsample from decomposed CNOT gate and 100 shots = [OrderedCounter({'11': 100})]\n</code></pre> <p>The <code>CPHASE</code> matrix is diagonal, and can be implemented by exponentiating an Ising-like Hamiltonian, or generator,</p> \\[\\text{CPHASE}(i,j,\\phi)=\\text{exp}\\left(-i\\phi \\mathcal{H}_\\text{CP}(i, j)\\right)\\] \\[\\begin{aligned} \\mathcal{H}_\\text{CP}&amp;=-\\frac{1}{4}(I_i-Z_i)(I_j-Z_j)\\\\ &amp;=-N_iN_j \\end{aligned}\\] <p>where the number operator \\(N_i = \\frac{1}{2}(I_i-Z_i)=\\hat{n}_i\\) is used, leading to an Ising-like interaction \\(\\hat{n}_i\\hat{n}_j\\) realisable in neutral-atom systems. Let's rebuild the <code>CNOT</code> using this evolution.</p> <pre><code>from qadence import kron, block_to_tensor\n\n# Hamiltonian for the CPHASE gate\nh_cphase = (-1.0) * kron(N(0), N(1))\n\n# Exponentiating and time-evolving the Hamiltonian until t=phi.\ncphase_evo = HamEvo(h_cphase, phi)\n\n# Check that we have the CPHASE gate:\ncphase_matrix = block_to_tensor(CPHASE(0, 1, phi))\ncphase_evo_matrix = block_to_tensor(cphase_evo)\n</code></pre> <pre><code>cphase_matrix == cphase_evo_matrix: True\n</code></pre> <p>Now that the <code>CPHASE</code> generator is checked, it can be applied to the <code>CNOT</code>:</p> <pre><code># CNOT with Hamiltonian Evolution\ncnot_evo = chain(\n    H(1),\n    cphase_evo,\n    H(1)\n)\n\n# Initialize state to check CNOTs sample outcomes.\ninit_state = product_state(\"10\")\n</code></pre> <pre><code>sample cnot_gate = [OrderedCounter({'11': 100})]\nsample cnot_evo = [OrderedCounter({'11': 100})]\n</code></pre> <p>Thus, a <code>CNOT</code> gate can be created by combining a few single-qubit gates together with a two-qubit Ising interaction between the control and the target qubit which is the essence of the Ising transform proposed in the seminal DAQC paper<sup>2</sup> for \\(ZZ\\) interactions. In Qadence, both \\(ZZ\\) and \\(NN\\) interactions are supported.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#cnot-in-an-interacting-system-of-three-qubits","title":"<code>CNOT</code> in an interacting system of three qubits","text":"<p>Consider a simple experimental setup with \\(n=3\\) interacting qubits laid out in a triangular grid. For the sake of simplicity, all qubits interact with each other with an \\(NN\\)-Ising interaction of constant strength \\(g_\\text{int}\\). The Hamiltonian for the system can be written by summing interaction terms over all pairs:</p> \\[\\mathcal{H}_\\text{sys}=\\sum_{i=0}^{n}\\sum_{j=0}^{i-1}g_\\text{int}N_iN_j,\\] <p>which in this case leads to only three interaction terms,</p> \\[\\mathcal{H}_\\text{sys}=g_\\text{int}(N_0N_1+N_1N_2+N_0N_2)\\] <p>This generator can be easily built in Qadence:</p> <pre><code>from qadence import add, kron\nn_qubits = 3\n\n# Interaction strength.\ng_int = 1.0\n\n# Build a list of interactions.\ninteraction_list = []\nfor i in range(n_qubits):\n    for j in range(i):\n        interaction_list.append(g_int * kron(N(i), N(j)))\n\nh_sys = add(*interaction_list)\n</code></pre> <pre><code>h_sys = AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(0)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(2)\n\u2502       \u2514\u2500\u2500 N(0)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(1)\n</code></pre> <p>Now let's consider that the experimental system is fixed, and qubits can not be isolated one from another. The options are:</p> <ul> <li>Turn on or off the global system Hamiltonian.</li> <li>Perform local single-qubit rotations.</li> </ul> <p>To perform a fully digital <code>CNOT(0,1)</code>, the interacting control on qubit 0 and target on qubit 1 must be isolated from the third one to implement the gate directly. While this can be achieved for a three-qubit system, it becomes experimentally untractable when scaling the qubit count.</p> <p>However, this is not the case within the digital-analog paradigm. In fact, the two qubit Ising interaction required for the <code>CNOT</code> can be represented with a combination of the global system Hamiltonian and a specific set of single-qubit rotations. Full details about this transformation are to be found in the DAQC paper<sup>2</sup> but a more succint yet in-depth description takes place in the next section. It is conveniently available in Qadence by calling the <code>daqc_transform</code> function.</p> <p>In the most general sense, the <code>daqc_transform</code> function will return a circuit that represents the evolution of a target Hamiltonian \\(\\mathcal{H}_\\text{target}\\) (here the unitary of the gate) until a specified time \\(t_f\\) by using only the evolution of a build Hamiltonian \\(\\mathcal{H}_\\text{build}\\) (here \\(\\mathcal{H}_\\text{sys}\\)) together with local \\(X\\)-gates. In Qadence, <code>daqc_transform</code> is applicable for \\(\\mathcal{H}_\\text{target}\\) and \\(\\mathcal{H}_\\text{build}\\) composed only of \\(ZZ\\)- or \\(NN\\)-interactions. These generators are parsed by the <code>daqc_transform</code> function and the appropriate type is automatically determined together with the appropriate single-qubit detunings and global phases.</p> <p>Let's apply it for the <code>CNOT</code> implementation:</p> <pre><code>from qadence import daqc_transform, Strategy\n\n# Settings for the target CNOT operation\ni = 0  # Control qubit\nj = 1  # Target qubit\nk = 2  # The extra qubit\n\n# Define the target CNOT operation\n# by composing with identity on the extra qubit.\ncnot_target = kron(CNOT(i, j), I(k))\n\n# The two-qubit NN-Ising interaction term for the CPHASE\nh_int = (-1.0) * kron(N(i), N(j))\n\n# Transforming the two-qubit Ising interaction using only our system Hamiltonian\ntransformed_ising = daqc_transform(\n    n_qubits=3,        # Total number of qubits in the transformation\n    gen_target=h_int,  # The target Ising generator\n    t_f=PI,            # The target evolution time\n    gen_build=h_sys,   # The building block Ising generator to be used\n    strategy=Strategy.SDAQC,   # Currently only sDAQC is implemented\n    ignore_global_phases=False  # Global phases from mapping between Z and N\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_ee8fb21163704af79fc02422b8d857dd cluster_f7705633aa654d078ba967cad7827589 cluster_a054b01c9cbe40e6af7386fc2ed58cc6 cluster_ff71c2cf896a4a9c928ff8b03d4bf007 cluster_e8c7aacc6db44d11b2b37f6e93f3209c cluster_f8093b42bd9f4deca9afcff0151336f4 cluster_5e258da09e56477ca4c411048bf43168 3be62304adb24d7ca59e96dfeb6140cf 0 bd75fb6cb95e4f33b946c09ab82586ec HamEvo 3be62304adb24d7ca59e96dfeb6140cf--bd75fb6cb95e4f33b946c09ab82586ec f22acf0d2c0f4ecdac877f1e57534449 1 1fe6b78e048b47c6b65ad13618e0cf99 HamEvo bd75fb6cb95e4f33b946c09ab82586ec--1fe6b78e048b47c6b65ad13618e0cf99 4050f9f64d2542c99c0184095d001bb1 HamEvo 1fe6b78e048b47c6b65ad13618e0cf99--4050f9f64d2542c99c0184095d001bb1 2fb26e7d1d6a4f3d85adbb391bc59628 X 4050f9f64d2542c99c0184095d001bb1--2fb26e7d1d6a4f3d85adbb391bc59628 fbef780e4cf54103897b114fb71c9237 HamEvo 2fb26e7d1d6a4f3d85adbb391bc59628--fbef780e4cf54103897b114fb71c9237 73c92985b61d41798c10f719cf736cf6 HamEvo fbef780e4cf54103897b114fb71c9237--73c92985b61d41798c10f719cf736cf6 bc6e3d9cf38d49cea4a1cb4e91ffbd7a X 73c92985b61d41798c10f719cf736cf6--bc6e3d9cf38d49cea4a1cb4e91ffbd7a 82d8028eea4a4acc87297ac23cd98701 bc6e3d9cf38d49cea4a1cb4e91ffbd7a--82d8028eea4a4acc87297ac23cd98701 1fdcd437f91f45c69a543fd1725c1248 HamEvo 82d8028eea4a4acc87297ac23cd98701--1fdcd437f91f45c69a543fd1725c1248 2df71af8a57647408348e292e530eb26 HamEvo 1fdcd437f91f45c69a543fd1725c1248--2df71af8a57647408348e292e530eb26 b58690f5cb234d608bde07a30b937e42 2df71af8a57647408348e292e530eb26--b58690f5cb234d608bde07a30b937e42 eacd2eab2f69423e9779a898cbc65dec b58690f5cb234d608bde07a30b937e42--eacd2eab2f69423e9779a898cbc65dec e086fb89a11f4ccbbee401fe0666eb2b 58e6c55215664c389a3733a334478c9f t = -3.14 f22acf0d2c0f4ecdac877f1e57534449--58e6c55215664c389a3733a334478c9f 2391bc86a7d94388b4c0ceec1f3f49e7 2 6e5abec2055b48cfafec793663ef808d t = 3.142 58e6c55215664c389a3733a334478c9f--6e5abec2055b48cfafec793663ef808d 15d07820c25a4ce289175d592db04844 t = -3.14 6e5abec2055b48cfafec793663ef808d--15d07820c25a4ce289175d592db04844 bc15019791b1498b954b4d2b6dd467cd 15d07820c25a4ce289175d592db04844--bc15019791b1498b954b4d2b6dd467cd 7e63fbebb2424206857bef6449a59c96 t = 1.571 bc15019791b1498b954b4d2b6dd467cd--7e63fbebb2424206857bef6449a59c96 78d38147e46f45bd8444557a7055fba6 t = 1.571 7e63fbebb2424206857bef6449a59c96--78d38147e46f45bd8444557a7055fba6 bfcef536dda044ea9a2c8fdac93c23f4 78d38147e46f45bd8444557a7055fba6--bfcef536dda044ea9a2c8fdac93c23f4 657b2331235747ae8fbda6216baf3396 X bfcef536dda044ea9a2c8fdac93c23f4--657b2331235747ae8fbda6216baf3396 c3e373f329c0474d9e8a5edd8b2b2d24 t = 1.571 657b2331235747ae8fbda6216baf3396--c3e373f329c0474d9e8a5edd8b2b2d24 0110307b50384af1a0a56e04cfa16b0d t = 1.571 c3e373f329c0474d9e8a5edd8b2b2d24--0110307b50384af1a0a56e04cfa16b0d f954fb8c8c084d20a9e443c5f9305d0d X 0110307b50384af1a0a56e04cfa16b0d--f954fb8c8c084d20a9e443c5f9305d0d f954fb8c8c084d20a9e443c5f9305d0d--e086fb89a11f4ccbbee401fe0666eb2b 8771816bca194eb4a8f86f44d93e4f31 40288fcd11124690b94f39b6752d5e6e 2391bc86a7d94388b4c0ceec1f3f49e7--40288fcd11124690b94f39b6752d5e6e 203d15177500439d93bf178f6ca3737a 40288fcd11124690b94f39b6752d5e6e--203d15177500439d93bf178f6ca3737a eef3de83447a4f7c956bce0fa2619f28 203d15177500439d93bf178f6ca3737a--eef3de83447a4f7c956bce0fa2619f28 b0b0ce4d020c4f10b4e4d174ff060ecc X eef3de83447a4f7c956bce0fa2619f28--b0b0ce4d020c4f10b4e4d174ff060ecc 695c040226224458acad27365ac0a809 b0b0ce4d020c4f10b4e4d174ff060ecc--695c040226224458acad27365ac0a809 fe4da700b1de4712986455cc7c42f814 695c040226224458acad27365ac0a809--fe4da700b1de4712986455cc7c42f814 6fa1297117834054925fbc8850ac0f5a X fe4da700b1de4712986455cc7c42f814--6fa1297117834054925fbc8850ac0f5a 326ece75a3e940ea8c5ff30bb3ce124c X 6fa1297117834054925fbc8850ac0f5a--326ece75a3e940ea8c5ff30bb3ce124c 8abe0b7adf14410ca2aa39c9f80ab589 326ece75a3e940ea8c5ff30bb3ce124c--8abe0b7adf14410ca2aa39c9f80ab589 e7e116a6ed41473082904f8989b2e25c 8abe0b7adf14410ca2aa39c9f80ab589--e7e116a6ed41473082904f8989b2e25c 42329f6fef304838bbd675ddf545baf8 X e7e116a6ed41473082904f8989b2e25c--42329f6fef304838bbd675ddf545baf8 42329f6fef304838bbd675ddf545baf8--8771816bca194eb4a8f86f44d93e4f31 <p>The output circuit displays three groups of system Hamiltonian evolutions which account for global-phases and single-qubit detunings related to the mapping between the \\(Z\\) and \\(N\\) operators. Optionally, global phases can be ignored.</p> <p>In general, the mapping of a \\(n\\)-qubit Ising Hamiltonian to another will require at most \\(n(n-1)\\) evolutions. The transformed circuit performs these evolutions for specific times that are computed from the solution of a linear system of equations involving the set of interactions in the target and build Hamiltonians.</p> <p>In this case, the mapping is exact when using the step-wise DAQC strategy (<code>Strategy.SDAQC</code>) available in Qadence. In banged DAQC (<code>Strategy.BDAQC</code>) the mapping is approximate, but easier to implement on a physical device with always-on interactions such as neutral-atom systems.</p> <p>Just as before, the transformed Ising circuit can be checked to exactly recover the <code>CPHASE</code> gate:</p> <pre><code># CPHASE on (i, j), Identity on third qubit:\ncphase_matrix = block_to_tensor(kron(CPHASE(i, j, phi), I(k)))\n\n# CPHASE using the transformed circuit:\ncphase_evo_matrix = block_to_tensor(transformed_ising)\n\n# Check that it implements the CPHASE.\n# Will fail if global phases are ignored.\n</code></pre> <pre><code>cphase_matrix == cphase_evo_matrix : True\n</code></pre> <p>The <code>CNOT</code> gate can now finally be built:</p> <pre><code>from qadence import equivalent_state, run, sample\n\ncnot_daqc = chain(\n    H(j),\n    transformed_ising,\n    H(j)\n)\n\n# And finally apply the CNOT on a specific 3-qubit initial state:\ninit_state = product_state(\"101\")\n\n# Check we get an equivalent wavefunction\nwf_cnot = run(n_qubits, block=cnot_target, state=init_state)\nwf_daqc = run(n_qubits, block=cnot_daqc, state=init_state)\n\n# Visualize the CNOT bit-flip in samples.\n</code></pre> <pre><code>wf_cnot == wf_dacq : True\nsample cnot_target = [OrderedCounter({'111': 100})]\nsample cnot_dacq = [OrderedCounter({'111': 100})]\n</code></pre> <p>As one can see, a <code>CNOT</code> operation has been succesfully implemented on the desired target qubits by using only the global system as the building block Hamiltonian and single-qubit rotations. Decomposing a single digital gate into an Ising Hamiltonian serves as a proof of principle for the potential of this technique to represent universal quantum computation.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#technical-details-on-the-daqc-transformation","title":"Technical details on the DAQC transformation","text":"<ul> <li>The mapping between target generator and final circuit is performed by solving a linear system of size \\(n(n-1)\\) where \\(n\\) is the number of qubits, so it can be computed efficiently (i.e., with a polynomial cost in the number of qubits).</li> <li>The linear system to be solved is actually not invertible for \\(n=4\\) qubits. This is very specific edge case requiring a workaround, that is currently not yet implemented.</li> <li>As mentioned, the final circuit has at most \\(n(n-1)\\) slices, so there is at most a quadratic overhead in circuit depth.</li> </ul> <p>Finally, and most important to its usage:</p> <ul> <li>The target Hamiltonian should be sufficiently represented in the building block Hamiltonian.</li> </ul> <p>To illustrate this point, consider the following target and build Hamiltonians:</p> <pre><code># Interaction between qubits 0 and 1\ngen_target = 1.0 * (Z(0) @ Z(1))\n\n# Fixed interaction between qubits 1 and 2, and customizable between 0 and 1\ndef gen_build(g_int):\n    return g_int * (Z(0) @ Z(1)) + 1.0 * (Z(1) @ Z(2))\n</code></pre> <p>And now we perform the DAQC transform by setting <code>g_int=1.0</code>, exactly matching the target Hamiltonian:</p> <pre><code>transformed_ising = daqc_transform(\n    n_qubits=3,\n    gen_target=gen_target,\n    t_f=1.0,\n    gen_build=gen_build(g_int=1.0),\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_20ad55ad6fe34c1ca453f750469edd04 cluster_10a5cafd164e40788addd038d45c22d2 59e7f231fced4e58b922ebfc8bdb91f8 0 4545cee4aaf04f9689e139a62aa6dccc X 59e7f231fced4e58b922ebfc8bdb91f8--4545cee4aaf04f9689e139a62aa6dccc eba796732f8749049698650d8c4ef8fe 1 e913619795114ab3a5e24d9d620f1852 HamEvo 4545cee4aaf04f9689e139a62aa6dccc--e913619795114ab3a5e24d9d620f1852 e9bd81372dca4a33b9c702dad9093f3f X e913619795114ab3a5e24d9d620f1852--e9bd81372dca4a33b9c702dad9093f3f b244ad02444e442cb9fa6010cf7cf800 e9bd81372dca4a33b9c702dad9093f3f--b244ad02444e442cb9fa6010cf7cf800 20c37d5cddd545dea231f255d37d2595 HamEvo b244ad02444e442cb9fa6010cf7cf800--20c37d5cddd545dea231f255d37d2595 814f63c7dfb8476fa4442fbdaa3a6eb5 20c37d5cddd545dea231f255d37d2595--814f63c7dfb8476fa4442fbdaa3a6eb5 2cef06843d58473f9a4f0db1e1840683 814f63c7dfb8476fa4442fbdaa3a6eb5--2cef06843d58473f9a4f0db1e1840683 7abd9660ba984103a72b647d962d83fc 2b59f4d72afb45448f811b50b7cd0861 eba796732f8749049698650d8c4ef8fe--2b59f4d72afb45448f811b50b7cd0861 2545a16fdb3447aa8b98b550b8b33e7f 2 e0d0578e3509474c90072d7022729075 t = -0.50 2b59f4d72afb45448f811b50b7cd0861--e0d0578e3509474c90072d7022729075 84fdacc831434ab88566441ba058de3c e0d0578e3509474c90072d7022729075--84fdacc831434ab88566441ba058de3c 8dc01d44fbfa4f339380c05dfaa9d4a8 X 84fdacc831434ab88566441ba058de3c--8dc01d44fbfa4f339380c05dfaa9d4a8 d008a1d123d149979d90fa3b5cb59255 t = -0.50 8dc01d44fbfa4f339380c05dfaa9d4a8--d008a1d123d149979d90fa3b5cb59255 4715b2975dea415db810d63ff603c057 X d008a1d123d149979d90fa3b5cb59255--4715b2975dea415db810d63ff603c057 4715b2975dea415db810d63ff603c057--7abd9660ba984103a72b647d962d83fc de8f5026c14e4375b5b05be7b89c636e a75004532d424d4d8e944be14847a7b2 X 2545a16fdb3447aa8b98b550b8b33e7f--a75004532d424d4d8e944be14847a7b2 2f6e75da1bd84d029801d4601f1fe384 a75004532d424d4d8e944be14847a7b2--2f6e75da1bd84d029801d4601f1fe384 b3258ec1843b4f0ca88584281bd008af X 2f6e75da1bd84d029801d4601f1fe384--b3258ec1843b4f0ca88584281bd008af 0e90e95b876f430db4f41e7d90c43893 X b3258ec1843b4f0ca88584281bd008af--0e90e95b876f430db4f41e7d90c43893 2bc9261fd407431eba973da3bf9eb795 0e90e95b876f430db4f41e7d90c43893--2bc9261fd407431eba973da3bf9eb795 a820709ebecf442081e224b46391450b X 2bc9261fd407431eba973da3bf9eb795--a820709ebecf442081e224b46391450b a820709ebecf442081e224b46391450b--de8f5026c14e4375b5b05be7b89c636e <p>Now, if the interaction between qubits 0 and 1 is weakened in the build Hamiltonian:</p> <pre><code>transformed_ising = daqc_transform(\n    n_qubits=3,\n    gen_target=gen_target,\n    t_f=1.0,\n    gen_build=gen_build(g_int=0.001),\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_85c7e2aa710b4520877ddbff797bf437 cluster_fb239d3a7b7f44989a6e35917e3752bd cfad7cb2b4e24018b64a5b87f8beba3c 0 aeb287239f924ee2ad377b181255fa40 X cfad7cb2b4e24018b64a5b87f8beba3c--aeb287239f924ee2ad377b181255fa40 09826fe7fd8a445eb0bfb89c136939a7 1 e50fa9e19c934d6f9172b7d0c7b8d6f6 HamEvo aeb287239f924ee2ad377b181255fa40--e50fa9e19c934d6f9172b7d0c7b8d6f6 163df0e2c1944f89842b9a07745f6482 X e50fa9e19c934d6f9172b7d0c7b8d6f6--163df0e2c1944f89842b9a07745f6482 5abf1114ffb4414786bff5cb374722f5 163df0e2c1944f89842b9a07745f6482--5abf1114ffb4414786bff5cb374722f5 721e0458ebd14a70a318b1791d474de5 HamEvo 5abf1114ffb4414786bff5cb374722f5--721e0458ebd14a70a318b1791d474de5 ff566da8412e4453af56c2247a3eff3e 721e0458ebd14a70a318b1791d474de5--ff566da8412e4453af56c2247a3eff3e e7163e26cd7d4550a2b4152f91272f62 ff566da8412e4453af56c2247a3eff3e--e7163e26cd7d4550a2b4152f91272f62 2d69fa29d23d4356bdf10191eb57dbe0 d0116b4055a246a68c7b134b57a03907 09826fe7fd8a445eb0bfb89c136939a7--d0116b4055a246a68c7b134b57a03907 cb7397f8d648420196bccbfac4ec65ec 2 98858f3745b2436d8f269ae7d27196f7 t = -500. d0116b4055a246a68c7b134b57a03907--98858f3745b2436d8f269ae7d27196f7 af8010337098413f9d9df8c2452c29a1 98858f3745b2436d8f269ae7d27196f7--af8010337098413f9d9df8c2452c29a1 8f2b5264eb1e491bafd4d6657027ddb9 X af8010337098413f9d9df8c2452c29a1--8f2b5264eb1e491bafd4d6657027ddb9 ff99791aa83b4b5c8624a78cc63b33a2 t = -500. 8f2b5264eb1e491bafd4d6657027ddb9--ff99791aa83b4b5c8624a78cc63b33a2 2af59a7caa7140018d346cddc2600090 X ff99791aa83b4b5c8624a78cc63b33a2--2af59a7caa7140018d346cddc2600090 2af59a7caa7140018d346cddc2600090--2d69fa29d23d4356bdf10191eb57dbe0 312d47724a7e41cf9d47a1e06c295a03 bd13ff677f364f0cace09cbc736a19e8 X cb7397f8d648420196bccbfac4ec65ec--bd13ff677f364f0cace09cbc736a19e8 639888c0b60041b9a73656a45e22c4c9 bd13ff677f364f0cace09cbc736a19e8--639888c0b60041b9a73656a45e22c4c9 71634379bfa2488582a414bec913ea29 X 639888c0b60041b9a73656a45e22c4c9--71634379bfa2488582a414bec913ea29 2b93d0eee15c4e6998e672f766d80d71 X 71634379bfa2488582a414bec913ea29--2b93d0eee15c4e6998e672f766d80d71 d36a0ab33a0c4731970bb8cdc33291c0 2b93d0eee15c4e6998e672f766d80d71--d36a0ab33a0c4731970bb8cdc33291c0 64b3235a40a44398b8451aef5e7c4ba7 X d36a0ab33a0c4731970bb8cdc33291c0--64b3235a40a44398b8451aef5e7c4ba7 64b3235a40a44398b8451aef5e7c4ba7--312d47724a7e41cf9d47a1e06c295a03 <p>The times slices using the build Hamiltonian need now to evolve for much longer to represent the same interaction since it is not sufficiently represented in the building block Hamiltonian.</p> <p>In the limit where that interaction is not present, the transform will not work:</p> <pre><code>try:\n    transformed_ising = daqc_transform(\n        n_qubits=3,\n        gen_target=gen_target,\n        t_f=1.0,\n        gen_build=gen_build(g_int = 0.0),\n    )\nexcept ValueError as error:\n    print(\"Error:\", error)\n</code></pre> <pre><code>Error: Incompatible interactions between target and build Hamiltonians.\n</code></pre>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#references","title":"References","text":"<ol> <li> <p>Dodd et al., Universal quantum computation and simulation using any entangling Hamiltonian and local unitaries, PRA 65, 040301 (2002). \u21a9</p> </li> <li> <p>Parra-Rodriguez et al., Digital-Analog Quantum Computation, PRA 101, 022305 (2020). \u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/","title":"Fitting a function with a Hamiltonian ansatz","text":"<p>In the analog QCL tutorial we used analog blocks to learn a function of interest. The analog blocks are a direct abstraction of device execution with global addressing. However, we may want to directly program an Hamiltonian-level ansatz to have a finer control on our model. In Qadence this can easily be done through digital-analog programs. In this tutorial we will solve a simple QCL problem with this approach.</p>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#setting-up-the-problem","title":"Setting up the problem","text":"<p>The example problem considered is to fit a function of interest in a specified range. Below we define and plot the function \\(f(x)=x^5\\).</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Function to fit:\ndef f(x):\n    return x**5\n\nxmin = -1.0\nxmax = 1.0\nn_test = 100\n\nx_test = torch.linspace(xmin, xmax, steps = n_test)\ny_test = f(x_test)\n\nplt.plot(x_test, y_test)\nplt.xlim((-1.1, 1.1))\nplt.ylim((-1.1, 1.1))\n</code></pre> 2025-02-07T10:21:47.715847 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#digital-analog-ansatz","title":"Digital-Analog Ansatz","text":"<p>We start by defining the register of qubits. The topology we use now will define the interactions in the entangling Hamiltonian. As an example, we can define a rectangular lattice with 6 qubits.</p> <pre><code>from qadence import Register\n\nreg = Register.rectangular_lattice(\n    qubits_row = 3,\n    qubits_col = 2,\n)\n</code></pre> <p>Inspired by the Ising interaction mode of Rydberg atoms, we can now define an interaction Hamiltonian as \\(\\mathcal{H}_{ij}=\\frac{1}{r_{ij}^6}N_iN_j\\), where \\(N_i=(1/2)(I_i-Z_i)\\) is the number operator and and \\(r_{ij}\\) is the distance between qubits \\(i\\) and \\(j\\). We can easily instatiate this interaction Hamiltonian from the register information:</p> <pre><code>from qadence import N, add\n\ndef h_ij(i: int, j: int):\n    return N(i)@N(j)\n\nh_int = add(h_ij(*edge)/r**6 for edge, r in reg.edge_distances.items())\n</code></pre> <p>To build the digital-analog ansatz we can make use of the standard <code>hea</code> function by specifying we want to use the <code>Strategy.SDAQC</code> and passing the Hamiltonian we created as the entangler, as see in the QML constructors tutorial. The entangling operation will be replaced by the evolution of this Hamiltonian <code>HamEvo(h_int, t)</code>, where the time parameter <code>t</code> is considered to be a variational parameter at each layer.</p> <pre><code>from qadence import hea, Strategy, RX, RY\n\ndepth = 2\n\nda_ansatz = hea(\n    n_qubits = reg.n_qubits,\n    depth = depth,\n    operations = [RX, RY, RX],\n    entangler = h_int,\n    strategy = Strategy.SDAQC,\n)\n\nprint(html_string(da_ansatz))\n</code></pre> %3 cluster_4e1955791c2e4ecab764e2df2c660721 cluster_c3bb3021a8aa4a0b9fa967b69641a2cf ed4923b8c7b245e2aa21d11c6af5db25 0 c5b3ca2a685b420fb6dd739d37fec8f8 RX(theta\u2080) ed4923b8c7b245e2aa21d11c6af5db25--c5b3ca2a685b420fb6dd739d37fec8f8 ab9b301546f54b15ad43ff54a073931f 1 52e7ad4215584f8bae59b681583e21d7 RY(theta\u2086) c5b3ca2a685b420fb6dd739d37fec8f8--52e7ad4215584f8bae59b681583e21d7 051261e6fca3428d8c96c2fb117294cb RX(theta\u2081\u2082) 52e7ad4215584f8bae59b681583e21d7--051261e6fca3428d8c96c2fb117294cb 8374faba32c44e21984b8b3fe46c262f 051261e6fca3428d8c96c2fb117294cb--8374faba32c44e21984b8b3fe46c262f b1b505782bea474ab02cc4748c0698e8 RX(theta\u2081\u2088) 8374faba32c44e21984b8b3fe46c262f--b1b505782bea474ab02cc4748c0698e8 23eab612bc6b46e3b8eaa003744534e3 RY(theta\u2082\u2084) b1b505782bea474ab02cc4748c0698e8--23eab612bc6b46e3b8eaa003744534e3 ed7e708ad71b482fa38a9ac126103932 RX(theta\u2083\u2080) 23eab612bc6b46e3b8eaa003744534e3--ed7e708ad71b482fa38a9ac126103932 5a95c25e583e46b88296b4ff30485df7 ed7e708ad71b482fa38a9ac126103932--5a95c25e583e46b88296b4ff30485df7 b97f6a4bccf048b79a03f7e205500f27 5a95c25e583e46b88296b4ff30485df7--b97f6a4bccf048b79a03f7e205500f27 4125f449864d4cc992ada7e772913c7c 2c3affee762a45df90a9704dc028bc1e RX(theta\u2081) ab9b301546f54b15ad43ff54a073931f--2c3affee762a45df90a9704dc028bc1e 292fb694f76c4a4dab56daadb3526452 2 a336855c09f844efaf67b50720ebdc8a RY(theta\u2087) 2c3affee762a45df90a9704dc028bc1e--a336855c09f844efaf67b50720ebdc8a 9968f591d8f04222931b6c39395535ab RX(theta\u2081\u2083) a336855c09f844efaf67b50720ebdc8a--9968f591d8f04222931b6c39395535ab ee4d8daf087c40dc9ee015129995cddc 9968f591d8f04222931b6c39395535ab--ee4d8daf087c40dc9ee015129995cddc 9bbe120e5ca24de29c2cec987275dc67 RX(theta\u2081\u2089) ee4d8daf087c40dc9ee015129995cddc--9bbe120e5ca24de29c2cec987275dc67 74e28c052987486db281606ede9a7230 RY(theta\u2082\u2085) 9bbe120e5ca24de29c2cec987275dc67--74e28c052987486db281606ede9a7230 1869ec288ea644718ab2a452051124a1 RX(theta\u2083\u2081) 74e28c052987486db281606ede9a7230--1869ec288ea644718ab2a452051124a1 b2f9801b47bc4f1db21095ebbd7d35ee 1869ec288ea644718ab2a452051124a1--b2f9801b47bc4f1db21095ebbd7d35ee b2f9801b47bc4f1db21095ebbd7d35ee--4125f449864d4cc992ada7e772913c7c 6e53a4dc8872470f902ed521cbe6c346 66d8145de6f74c1eaca98142815d43f6 RX(theta\u2082) 292fb694f76c4a4dab56daadb3526452--66d8145de6f74c1eaca98142815d43f6 5c3e1f6d367e4437af0391b5ee72172d 3 899ceb4dc7554543a92450b0ca079503 RY(theta\u2088) 66d8145de6f74c1eaca98142815d43f6--899ceb4dc7554543a92450b0ca079503 dc29fe3a446c4267acb9742b20f18f31 RX(theta\u2081\u2084) 899ceb4dc7554543a92450b0ca079503--dc29fe3a446c4267acb9742b20f18f31 5b8fee679f2444dfb1bf5452c14443d7 HamEvo dc29fe3a446c4267acb9742b20f18f31--5b8fee679f2444dfb1bf5452c14443d7 00686cefd6544ebf8503afc52fc7b927 RX(theta\u2082\u2080) 5b8fee679f2444dfb1bf5452c14443d7--00686cefd6544ebf8503afc52fc7b927 e3c5c13373f548f9ba3dc9b1a5c29bc2 RY(theta\u2082\u2086) 00686cefd6544ebf8503afc52fc7b927--e3c5c13373f548f9ba3dc9b1a5c29bc2 9fb7a36024d545e2b2e00513bea649ca RX(theta\u2083\u2082) e3c5c13373f548f9ba3dc9b1a5c29bc2--9fb7a36024d545e2b2e00513bea649ca b2db5f9c59834a6082bfa828cd738cd8 HamEvo 9fb7a36024d545e2b2e00513bea649ca--b2db5f9c59834a6082bfa828cd738cd8 b2db5f9c59834a6082bfa828cd738cd8--6e53a4dc8872470f902ed521cbe6c346 77ff0fe3a9494eccb833221ec6072ff4 73feb4d3fc31465fbe3670f35815dc9b RX(theta\u2083) 5c3e1f6d367e4437af0391b5ee72172d--73feb4d3fc31465fbe3670f35815dc9b a45c392a41aa471ab62a9b29a129395a 4 a0017147def0465f942c866b1956f554 RY(theta\u2089) 73feb4d3fc31465fbe3670f35815dc9b--a0017147def0465f942c866b1956f554 bebed5c325e1478b9a7cacc7b55be1a9 RX(theta\u2081\u2085) a0017147def0465f942c866b1956f554--bebed5c325e1478b9a7cacc7b55be1a9 4d90b53e98c043fe8f79c029a390a8c5 t = theta_t\u2080 bebed5c325e1478b9a7cacc7b55be1a9--4d90b53e98c043fe8f79c029a390a8c5 ab5d9d154f604262a046c3700722f7e5 RX(theta\u2082\u2081) 4d90b53e98c043fe8f79c029a390a8c5--ab5d9d154f604262a046c3700722f7e5 4eaab2f0b1a8439ebe24b7591d6ddb43 RY(theta\u2082\u2087) ab5d9d154f604262a046c3700722f7e5--4eaab2f0b1a8439ebe24b7591d6ddb43 79e3b8bfa4b343508d9ef733d5fd8ee3 RX(theta\u2083\u2083) 4eaab2f0b1a8439ebe24b7591d6ddb43--79e3b8bfa4b343508d9ef733d5fd8ee3 7381ffc9c3a84471a2efa98fdaa91e64 t = theta_t\u2081 79e3b8bfa4b343508d9ef733d5fd8ee3--7381ffc9c3a84471a2efa98fdaa91e64 7381ffc9c3a84471a2efa98fdaa91e64--77ff0fe3a9494eccb833221ec6072ff4 c3319e81c1864feea0ff05817b845ff6 d08df58bc96b4f7aaf740d75212e2e57 RX(theta\u2084) a45c392a41aa471ab62a9b29a129395a--d08df58bc96b4f7aaf740d75212e2e57 146d9efda42c467c99a0363517c744c7 5 a03d765c0e24403a85543b46e6003b2e RY(theta\u2081\u2080) d08df58bc96b4f7aaf740d75212e2e57--a03d765c0e24403a85543b46e6003b2e 2622a704f8cd4dee8b224c7f9b0a5b29 RX(theta\u2081\u2086) a03d765c0e24403a85543b46e6003b2e--2622a704f8cd4dee8b224c7f9b0a5b29 6e83965d8bd94b81b59b3afa6df49716 2622a704f8cd4dee8b224c7f9b0a5b29--6e83965d8bd94b81b59b3afa6df49716 02425ccc50024c03ad3a2b72867b65fb RX(theta\u2082\u2082) 6e83965d8bd94b81b59b3afa6df49716--02425ccc50024c03ad3a2b72867b65fb d18dfaebcf534276be343f5401d72010 RY(theta\u2082\u2088) 02425ccc50024c03ad3a2b72867b65fb--d18dfaebcf534276be343f5401d72010 93cc1a50cbfe4046af5c5d24d77d3992 RX(theta\u2083\u2084) d18dfaebcf534276be343f5401d72010--93cc1a50cbfe4046af5c5d24d77d3992 922f9f9be2ae424fa986138d1d1465fc 93cc1a50cbfe4046af5c5d24d77d3992--922f9f9be2ae424fa986138d1d1465fc 922f9f9be2ae424fa986138d1d1465fc--c3319e81c1864feea0ff05817b845ff6 a081165e0c2c459695294da8a67a16c6 0748188fb6ae4ad7abc7788aaaed6985 RX(theta\u2085) 146d9efda42c467c99a0363517c744c7--0748188fb6ae4ad7abc7788aaaed6985 b04550e3d97d437e82ed7f17c02f2f15 RY(theta\u2081\u2081) 0748188fb6ae4ad7abc7788aaaed6985--b04550e3d97d437e82ed7f17c02f2f15 fbb6be216b4b49c1a8b36489084d427c RX(theta\u2081\u2087) b04550e3d97d437e82ed7f17c02f2f15--fbb6be216b4b49c1a8b36489084d427c 08b461fde5644b9b8edd95d7e924044c fbb6be216b4b49c1a8b36489084d427c--08b461fde5644b9b8edd95d7e924044c 9ee5e94135c448bda769e5d82b2ba398 RX(theta\u2082\u2083) 08b461fde5644b9b8edd95d7e924044c--9ee5e94135c448bda769e5d82b2ba398 aa7f8d58d31d42f88607d8b9cd60952b RY(theta\u2082\u2089) 9ee5e94135c448bda769e5d82b2ba398--aa7f8d58d31d42f88607d8b9cd60952b bb20bf6d842048f4ba292405dbcc0f5e RX(theta\u2083\u2085) aa7f8d58d31d42f88607d8b9cd60952b--bb20bf6d842048f4ba292405dbcc0f5e a48300115e6742e3a6472a2c6c49ba0e bb20bf6d842048f4ba292405dbcc0f5e--a48300115e6742e3a6472a2c6c49ba0e a48300115e6742e3a6472a2c6c49ba0e--a081165e0c2c459695294da8a67a16c6"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#creating-the-quantummodel","title":"Creating the QuantumModel","text":"<p>The rest of the procedure is the same as any other Qadence workflow. We start by defining a feature map for input encoding and an observable for output decoding.</p> <pre><code>from qadence import feature_map, BasisSet, ReuploadScaling\nfrom qadence import Z, I\n\nfm = feature_map(\n    n_qubits = reg.n_qubits,\n    param = \"x\",\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.TOWER,\n)\n\n# Total magnetization\nobservable = add(Z(i) for i in range(reg.n_qubits))\n</code></pre> <p>And we have all the ingredients to initialize the <code>QuantumModel</code>:</p> <pre><code>from qadence import QuantumCircuit, QuantumModel\n\ncircuit = QuantumCircuit(reg, fm, da_ansatz)\n\nmodel = QuantumModel(circuit, observable = observable)\n</code></pre>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#training-the-model","title":"Training the model","text":"<p>We can now train the model. We use a set of 20 equally spaced training points.</p> <pre><code># Chebyshev FM does not accept x = -1, 1\nxmin = -0.99\nxmax = 0.99\nn_train = 20\n\nx_train = torch.linspace(xmin, xmax, steps = n_train)\ny_train = f(x_train)\n\n# Initial model prediction\ny_pred_initial = model.expectation({\"x\": x_test}).detach()\n</code></pre> <p>And we use a simple custom training loop.</p> <pre><code>criterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nn_epochs = 200\n\ndef loss_fn(x_train, y_train):\n    out = model.expectation({\"x\": x_train})\n    loss = criterion(out.squeeze(), y_train)\n    return loss\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_fn(x_train, y_train)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#results","title":"Results","text":"<p>Finally we can plot the resulting trained model.</p> <pre><code>y_pred_final = model.expectation({\"x\": x_test}).detach()\n\nplt.plot(x_test, y_pred_initial, label = \"Initial prediction\")\nplt.plot(x_test, y_pred_final, label = \"Final prediction\")\nplt.scatter(x_train, y_train, label = \"Training points\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.xlim((-1.1, 1.1))\nplt.ylim((-1.1, 1.1))\n</code></pre> 2025-02-07T10:21:55.934714 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/","title":"Pulse-level programming with Pulser","text":"<p>Qadence offers a direct interface with Pulser<sup>1</sup>, an open-source pulse-level interface written in Python and specifically designed for programming neutral atom quantum computers.</p> <p>Using directly Pulser requires advanced knowledge on pulse-level programming and on how neutral atom devices work. Qadence abstracts this complexity out by using the familiar block-based interface for building pulse sequences in Pulser while leaving the possibility to directly manipulate them if required by, for instance, optimal pulse shaping.</p> <p>Note</p> <p>The Pulser backend is still experimental and the interface might change in the future. Please note that it does not support <code>DiffMode.AD</code>.</p> <p>Note</p> <p>With the Pulser backend, <code>qadence</code> simulations can be executed on the cloud emulators available on the PASQAL cloud platform. In order to do so, make to have valid credentials for the PASQAL cloud platform and use the following configuration for the Pulser backend:</p> <pre><code>config = {\n    \"cloud_configuration\": {\n        \"username\": \"&lt;changeme&gt;\",\n        \"password\": \"&lt;changeme&gt;\",\n        \"project_id\": \"&lt;changeme&gt;\",  # the project should have access to emulators\n        \"platform\": \"EMU_FREE\"  # choose between `EMU_TN` and `EMU_FREE`\n    }\n}\n</code></pre> <p>For inquiries and more details on the cloud credentials, please contact info@pasqal.com.</p>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#default-qubit-interaction","title":"Default qubit interaction","text":"<p>When simulating pulse sequences written using Pulser, the underlying constructed Hamiltonian is equivalent to a digital-analog quantum computing program (see digital-analog emulation for more details) with the following interaction term:</p> \\[ \\mathcal{H}_{\\textrm{int}} = \\sum_{i&lt;j} \\frac{C_6}{|R_i - R_j|^6} \\hat{n}_i \\hat{n}_j \\] <p>where \\(C_6\\) is an interaction strength coefficient dependent on the principal quantum number of chosen the neutral atom system, \\(R_i\\) are atomic positions in Cartesian coordinates and \\(\\hat{n} = \\frac{1-\\sigma^z_i}{2}\\) the number operator.</p> <p>Note</p> <p>The Ising interaction is always-on for all computations performed with the Pulser backend. It cannot be switched off.</p>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#available-quantum-operations","title":"Available quantum operations","text":"<p>Currently, the Pulser backend supports the following operations:</p> gate description trainable parameter <code>RX</code>, <code>RY</code> Single qubit rotations. Notice that the interaction is on and this affects the resulting gate fidelity. rotation angle <code>AnalogRX</code>, <code>AnalogRY</code>, <code>AnalogRZ</code> Span a single qubit rotation among the entire register. rotation angle <code>entangle</code> Fully entangle the register. interaction time <code>AnalogInteraction</code> An idle block to to free-evolve for a duration according to the interaction. free evolution time"},{"location":"tutorials/digital_analog_qc/pulser-basic/#sequence-the-bell-state-on-a-two-qubit-register","title":"Sequence the Bell state on a two qubit register","text":"<p>The next example illustrates how to create a pulse sequence to prepare a Bell state. This is a sequence of an entanglement operation, represented as an <code>entangle</code> gate (using <code>CZ</code> interactions) in the \\(X\\)-basis and a \\(Y\\) rotation for readout in the \\(Z\\)-basis:</p> <pre><code>from qadence import chain, entangle, RY\n\nbell_state = chain(\n   entangle(\"t\", qubit_support=(0,1)),\n   RY(0, \"y\"),\n)\n</code></pre> <pre><code>bell_state = ChainBlock(0,1)\n\u251c\u2500\u2500 AnalogEntanglement(t=0.25022735767019744, support=(0, 1))\n\u2514\u2500\u2500 RY(0) [params: ['y']]\n</code></pre> <p>Next, a <code>Register</code> with two qubits is combined with the resulting <code>ChainBlock</code> to form a circuit. Then, the <code>QuantumModel</code> converts the circuit into a proper parametrized pulse sequence with the Pulser backend. Supplying the parameter values allows to sample the pulse sequence outcome:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom qadence import Register, QuantumCircuit, QuantumModel, PI\n\nregister = Register.line(2, spacing = 8.0)  # Two qubits with a distance of 8\u00b5m\ncircuit = QuantumCircuit(register, bell_state)\nmodel = QuantumModel(circuit, backend=\"pulser\", diff_mode=\"gpsr\")\n\nparams = {\n    \"t\": torch.tensor([1000]),  # ns\n    \"y\": torch.tensor([3*PI/2]),\n}\n\n# Return the final state vector\nfinal_vector = model.run(params)\n\n# Sample from the result state vector\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> <pre><code>final_vector = tensor([[-0.7114-0.0169j, -0.0338+0.0155j,  0.0110-0.0457j,  0.6631-0.2245j]])\nsample = Counter({'11': 26, '00': 23, '10': 1})\n</code></pre> <p>Plot the distribution:</p> <p><pre><code>\n</code></pre> 2025-02-07T10:21:56.143177 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/  One can visualise the pulse sequence with different parameters using the <code>assign_paramters</code> method.</p> <pre><code>model.assign_parameters(params).draw(show=False)\n</code></pre> 2025-02-07T10:21:56.232856 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#change-device-specifications","title":"Change device specifications","text":"<p>At variance with other backends, Pulser provides the concept of <code>Device</code>. A <code>Device</code> instance encapsulates all the properties for the definition of a real neutral atoms processor, including but not limited to the maximum laser amplitude for pulses, the maximum distance between two qubits and the maximum duration of the pulse. For more information, please check this tutorial.</p> <p>Qadence offers a simplified interface with only two devices which are detailed here:</p> <ul> <li><code>IDEALIZED</code> (default): ideal device which should be used only for testing purposes. It does not restrict the simulation of pulse sequences.</li> <li><code>REALISTIC</code>: device specification close to real neutral atom quantum processors.</li> </ul> <p>Note</p> <p>If you want to perform simulations closer to the specifications of real neutral atom machines, always select the <code>REALISTIC</code> device.</p> <p>One can use the <code>Configuration</code> of the Pulser backend to select the appropriate device:</p> <pre><code>from qadence import BackendName, DiffMode\nfrom qadence import RealisticDevice\n\n# Choose a realistic device\nregister = Register.line(2, spacing = 8.0, device_specs = RealisticDevice())\n\ncircuit = QuantumCircuit(register, bell_state)\n\nmodel = QuantumModel(\n    circuit,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR,\n)\n\nparams = {\n    \"t\": torch.tensor([1000]),  # ns\n    \"y\": torch.tensor([3*PI/2]),\n}\n\n# Sample from the result state vector\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> <pre><code>sample = Counter({'00': 27, '11': 22, '10': 1})\n</code></pre>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#create-a-custom-gate","title":"Create a custom gate","text":"<p>A major advantage of the block-based interface in Qadence is the ease to compose complex operations from a restricted set of primitive ones. In the following, a custom entanglement operation is used as an example.</p> <p>The operation consists of moving all the qubits to the \\(X\\)-basis. This is realized when the atomic interaction performs a controlled-\\(Z\\) operation during the free evolution. As seen before, this is implemented with the <code>AnalogInteraction</code> and <code>AnalogRY</code> blocks together with appropriate parameters.</p> <pre><code>from qadence import AnalogRY, chain, AnalogInteraction\n\n# Custom entanglement operation.\ndef my_entanglement(duration):\n    return chain(\n        AnalogRY(-PI / 2),\n        AnalogInteraction(duration)\n    )\n\nprotocol = chain(\n   my_entanglement(\"t\"),\n   RY(0, \"y\"),\n)\n\nregister = Register.line(2, spacing = 8.0)\ncircuit = QuantumCircuit(register, protocol)\nmodel = QuantumModel(circuit, backend=BackendName.PULSER, diff_mode=DiffMode.GPSR)\n\nparams = {\n    \"t\": torch.tensor([500]),  # ns\n    \"y\": torch.tensor([PI / 2]),\n}\n\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> 2025-02-07T10:21:56.568783 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#digital-analog-qnn-circuit","title":"Digital-analog QNN circuit","text":"<p>Finally, let's put all together by constructing a digital-analog version of a quantum neural network circuit with feature map and variational ansatz.</p> <pre><code>from qadence import kron, feature_map, BasisSet\nfrom qadence.operations import RX, RY, AnalogRX\n\nhea_one_layer = chain(\n    kron(RY(0, \"th00\"), RY(1, \"th01\")),\n    kron(RX(0, \"th10\"), RX(1, \"th11\")),\n    kron(RY(0, \"th20\"), RY(1, \"th21\")),\n    entangle(\"t\", qubit_support=(0,1)),\n)\n\nprotocol = chain(\n    feature_map(1, param=\"x\", fm_type=BasisSet.FOURIER),\n    hea_one_layer,\n    AnalogRX(PI/4)\n)\n\nregister = Register.line(2, spacing=8.0)\ncircuit = QuantumCircuit(register, protocol)\nmodel = QuantumModel(circuit, backend=BackendName.PULSER, diff_mode=DiffMode.GPSR)\n\nparams = {\n    \"x\": torch.tensor([0.8]), # rad\n    \"t\": torch.tensor([900]), # ns\n    \"th00\":  torch.rand(1), # rad\n    \"th01\":  torch.rand(1), # rad\n    \"th10\":  torch.rand(1), # rad\n    \"th11\":  torch.rand(1), # rad\n    \"th20\":  torch.rand(1), # rad\n    \"th21\":  torch.rand(1), # rad\n}\n\nmodel.assign_parameters(params).draw(draw_phase_area=True, show=False)\n</code></pre> 2025-02-07T10:21:56.687410 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#references","title":"References","text":"<ol> <li> <p>Pulser: An open-source package for the design of pulse sequences in programmable neutral-atom arrays \u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/","title":"Restricted local addressability","text":""},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#physics-behind-semi-local-addressing-patterns","title":"Physics behind semi-local addressing patterns","text":"<p>Recall that in Qadence the general neutral-atom Hamiltonian for a set of \\(n\\) interacting qubits is given by expression</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm drive} + \\mathcal{H}_{\\rm int} = \\sum_{i=0}^{n-1}\\left(\\mathcal{H}^\\text{d}_{i}(t) + \\sum_{j&lt;i}\\mathcal{H}^\\text{int}_{ij}\\right) \\] <p>as is described in detail in the analog interface basics documentation.</p> <p>The driving Hamiltonian term in priciple can model any local single-qubit rotation by addressing each qubit individually. However, some neutral-atom devices offer restricted local addressability using devices called spatial light modulators (SLMs).</p> <p>We refer to this regime as semi-local addressability. In this regime, the individual qubit addressing is restricted to a pattern of targeted qubits which is kept fixed during the execution of the quantum circuit. More formally, the addressing pattern appears as an additional term in the neutral-atom Hamiltonian:</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm drive} + \\mathcal{H}_{\\rm int} + \\mathcal{H}_{\\rm local} \\] <p>where \\(\\mathcal{H}_{\\rm pattern}\\) is given by</p> \\[ \\mathcal{H}_{\\rm local} = \\sum_{i=0}^{n-1}\\left(-\\Delta w_i^{\\rm det} \\hat{n}_i + \\Gamma w_i^{\\rm drive} \\hat{\\sigma}^x_i\\right). \\] <p>Here \\(\\Delta\\) specifies the maximal negative detuning that each qubit in the register can be exposed to. The weight \\(w_i^{\\rm det}\\in [0, 1]\\) determines the actual value of detuning that \\(i\\)-th qubit feels and this way the detuning pattern is emulated. Similarly, for the amplitude pattern \\(\\Gamma\\) determines the maximal additional positive drive that acts on qubits. In this case the corresponding weights \\(w_i^{\\rm drive}\\) can vary in the interval \\([0, 1]\\).</p> <p>Using the detuning and amplitude patterns described above one can modify the behavior of a selected set of qubits, thus achieving semi-local addressing.</p> <p>Qadence implements semi-local addressing in two different flavors of increasing complexity: either as a circuit constructor or directly as a pattern added to the general evolution Hamiltonian described by the circuit.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#using-circuit-constructors","title":"Using circuit constructors","text":"<p>The <code>rydberg_hea</code> constructor routine allows to build a circuit instance implementing a basic version of the Hamiltonian evolution described above where both \\(\\Delta\\) and \\(\\tilde{\\Omega}\\) coefficients are considered constants. Furthemore, no global drive and detuning are explicitly added to the Hamiltonian. Therefore, the final Hamiltonian generator of the circuit reads as follows:</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm local}(w^{\\rm drive}, w^{\\rm det}) + \\mathcal{H}_{\\textrm{int}} \\] <p>This implementation does not perform any checks on the weights normalization, thus making it not realistic. This implies that global drive and detuning can be retrieved by appropriately choosing the weights.</p> <p>You can easily create a Rydberg hardware efficient ansatz implementing multiple layers of the evolution generated by the local addressing Hamiltonian:</p> \\[ \\mathcal{H}_{\\rm evo} = \\sum_j \\mathcal{H}_{\\textrm{local}}(w_{j}^{\\rm drive}, w_{j}^{\\rm det}) \\] <p>Notice that in real-device implementation, one layer only is usually achievable.</p> <pre><code>import qadence as qd\nfrom qadence import rydberg_hea, rydberg_hea_layer\n\nn_qubits = 4\nn_layers = 2\nregister = qd.Register.line(n_qubits)\n\n# ansatz constructor\n# the evolution time is parametrized for each layer of the evolution\nansatz = rydberg_hea(\n    register,\n    n_layers=n_layers,  # number of subsequent layers of Hamiltonian evolution\n    addressable_detuning=True,  # make the local detuning weights w_i^{det} as variational parameters\n    addressable_drive=True, # make the local drive weights w_i^{drv} as variational parameters\n    tunable_phase=True, # make the phase \\phi as a variational parameter\n)\n\n# alternatively, a single ansatz layer can also be created for\n# better flexibility\n\n# these can be variational parameters\ntevo_drive = 1.0  # evolution time for the locally addressed drive term\ntevo_det = 1.0 # evolution time for the locally addressed detuning term\ntevo_int = 1.0  # evolution time for the interaction term\n\n# these can be list of variational parameters\nweights_drive = [0.0, 0.25, 0.5, 0.25]\nweights_det = [0.0, 0.0, 0.5, 0.5]\n\nansatz_layer = rydberg_hea_layer(\n    register,\n    tevo_det,\n    tevo_drive,\n    tevo_int,\n    detunings=weights_det,\n    drives=weights_drive,\n)\n</code></pre> <pre><code>\n</code></pre> <p>This circuit constructor is meant to be used with fully differentiable backends such as <code>pyqtorch</code> and mainly for quick experimentation with neutral atom compatible ansatze.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#using-addressing-patterns","title":"Using addressing patterns","text":"<p>In Qadence semi-local addressing patterns can be created by either specifying fixed values for the weights of the qubits being addressed or defining them as trainable parameters that can be optimized later in some training loop. Semi-local addressing patterns can be defined with the <code>AddressingPattern</code> dataclass.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#fixed-weights","title":"Fixed weights","text":"<p>With fixed weights, detuning/amplitude addressing patterns can be defined in the following way:</p> <pre><code>import torch\nfrom qadence.analog import AddressingPattern\n\nn_qubits = 3\n\nw_det = {0: 0.9, 1: 0.5, 2: 1.0}\nw_amp = {0: 0.1, 1: 0.4, 2: 0.8}\ndet = 9.0\namp = 6.5\npattern = AddressingPattern(\n    n_qubits=n_qubits,\n    det=det,\n    amp=amp,\n    weights_det=w_det,\n    weights_amp=w_amp,\n)\n</code></pre> <p>If only detuning or amplitude pattern is needed - the corresponding weights for all qubits can be set to 0.</p> <p>The created addressing pattern can now be passed as an argument to any Qadence device class, or to the <code>IdealDevice</code> or <code>RealisticDevice</code> to make use of the pre-defined options in those devices,</p> <pre><code>import torch\nfrom qadence import (\n    AnalogRX,\n    AnalogRY,\n    BackendName,\n    DiffMode,\n    Parameter,\n    QuantumCircuit,\n    QuantumModel,\n    Register,\n    chain,\n    total_magnetization,\n    IdealDevice,\n    PI\n)\n\n# define register and circuit\nspacing = 8.0\nx = Parameter(\"x\")\nblock = chain(AnalogRX(3 * x), AnalogRY(0.5 * x))\n\ndevice_specs = IdealDevice(pattern = pattern)\n\nreg = Register.line(\n    n_qubits,\n    spacing=spacing,\n    device_specs=device_specs,\n)\n\ncirc = QuantumCircuit(reg, block)\n\nobs = total_magnetization(n_qubits)\n\nmodel_pyq = QuantumModel(\n    circuit=circ, observable=obs, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD\n)\n\n# calculate expectation value of the circuit for random input value\nvalue = {\"x\": 1.0 + torch.rand(1)}\nexpval_pyq = model_pyq.expectation(values = value)\n</code></pre>   Expectation value on PyQ:  tensor([2.1555])     <p>The same configuration can also be seamlessly used to create a model with the Pulser backend.</p> <pre><code>model_pulser = QuantumModel(\n    circuit=circ,\n    observable=obs,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR\n)\n\n# calculate expectation value of the circuit for same random input value\nexpval_pulser = model_pulser.expectation(values = value)\n</code></pre>   Expectation value on Pulser:  tensor([2.1534])     <p>Note that by default the addressing pattern terms are added to every analog operation in the circuit. However, it is possible to turn the addressing pattern off for specific operations by passing <code>add_pattern=False</code> in the operation. For example <code>AnalogRX(pi)</code> will get the extra addressing pattern term, but <code>AnalogRX(pi, add_pattern=False)</code> will not. This is currently only implemented for the PyQTorch backend. If an addressing pattern is specified for the Pulser backend, it will be added to all the blocks.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#trainable-weights","title":"Trainable weights","text":"<p>Note</p> <p>Trainable parameters currently are supported only by <code>pyqtorch</code> backend.</p> <p>Since both the maximum detuning/amplitude value of the addressing pattern and the corresponding weights can be user specified, they can be variationally used in some QML setting. This can be achieved by defining pattern weights as trainable <code>Parameter</code> instances or strings specifying weight names.</p> <pre><code>n_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\n# some random target function value\nf_value = torch.rand(1)\n\n# define trainable addressing pattern\nw_amp = {i: f\"w_amp{i}\" for i in range(n_qubits)}\nw_det = {i: f\"w_det{i}\" for i in range(n_qubits)}\namp = \"max_amp\"\ndet = \"max_det\"\n\npattern = AddressingPattern(\n    n_qubits=n_qubits,\n    det=det,\n    amp=amp,\n    weights_det=w_det,\n    weights_amp=w_amp,\n)\n\n# some fixed analog operation\nblock = AnalogRX(PI)\n\ndevice_specs = IdealDevice(pattern = pattern)\n\nreg = Register.line(\n    n_qubits,\n    spacing=spacing,\n    device_specs=device_specs,\n)\n\ncirc = QuantumCircuit(reg, block)\n\n# define quantum model\nobs = total_magnetization(n_qubits)\nmodel = QuantumModel(circuit=circ, observable=obs, backend=BackendName.PYQTORCH)\n\n# prepare for training\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_criterion = torch.nn.MSELoss()\nn_epochs = 200\nloss_save = []\n\n# train model\nfor _ in range(n_epochs):\n    optimizer.zero_grad()\n    out = model.expectation()\n    loss = loss_criterion(f_value, out)\n    loss.backward()\n    optimizer.step()\n    loss_save.append(loss.item())\n\n# get final results\nf_value_model = model.expectation().detach()\n\nassert torch.isclose(f_value, f_value_model, atol=0.01)\n</code></pre>   The target function value:  tensor([0.5492]) The trained function value:  tensor([[0.5492]])    <p>Here, the expectation value of the circuit is fitted by varying the parameters of the addressing pattern.</p>"},{"location":"tutorials/qml/","title":"Variational quantum algorithms","text":"<p>Variational algorithms on noisy devices and quantum machine learning (QML)[^1] in particular are one of the main target applications for Qadence. For this purpose, the library offers both flexible symbolic expressions for the quantum circuit parameters via <code>sympy</code> (see here for more details) and native automatic differentiation via integration with PyTorch deep learning framework.</p> <p>Furthermore, Qadence offers a wide range of utilities for helping building and researching quantum machine learning algorithms, including:</p> <ul> <li>a set of constructors for circuits commonly used in quantum machine learning such as feature maps and ansatze</li> <li>a set of tools for training and optimizing quantum neural networks and loading classical data into a QML algorithm</li> </ul>"},{"location":"tutorials/qml/#some-simple-examples","title":"Some simple examples","text":"<p>Qadence symbolic parameter interface allows to create arbitrary feature maps to encode classical data into quantum circuits with an arbitrary non-linear function embedding for the input values:</p> <pre><code>import qadence as qd\nfrom qadence.operations import *\nimport torch\nfrom sympy import acos\n\nn_qubits = 4\n\n# Example feature map, also directly available with the `feature_map` function\nfp = qd.FeatureParameter(\"phi\")\nfm = qd.kron(RX(i, acos(fp)) for i in range(n_qubits))\n\n# the key in the dictionary must correspond to\n# the name of the assigned to the feature parameter\ninputs = {\"phi\": torch.rand(3)}\nsamples = qd.sample(fm, values=inputs)\n</code></pre> <pre><code>samples = OrderedCounter({'0000': 99, '0010': 1})\n</code></pre> <p>The <code>constructors.feature_map</code> module provides convenience functions to build commonly used feature maps where the input parameter is encoded in the single-qubit gates rotation angle. This function will be further demonstrated in the QML constructors tutorial.</p> <p>Furthermore, Qadence is natively integrated with PyTorch automatic differentiation engine thus Qadence quantum models can be used seamlessly in a PyTorch workflow.</p> <p>Let's create a quantum neural network model using the feature map just defined, a digital-analog variational ansatz (also explained here) and a simple observable \\(X(0) \\otimes X(1)\\). We use the convenience <code>QNN</code> quantum model abstraction.</p> <pre><code>ansatz = qd.hea(n_qubits, strategy=\"sDAQC\")\ncircuit = qd.QuantumCircuit(n_qubits, fm, ansatz)\nobservable = qd.kron(X(0), X(1))\n\nmodel = qd.QNN(circuit, observable)\n\n# NOTE: the `QNN` is a torch.nn.Module\nassert isinstance(model, torch.nn.Module)\n</code></pre> <pre><code>True\n</code></pre> <p>Differentiation works the same way as any other PyTorch module:</p> <pre><code>values = {\"phi\": torch.rand(10, requires_grad=True)}\n\n# the forward pass of the quantum model returns the expectation\n# value of the input observable\nout = model(values)\n\n# you can compute the gradient with respect to inputs using\n# PyTorch autograd differentiation engine\ndout = torch.autograd.grad(out, values[\"phi\"], torch.ones_like(out), create_graph=True)[0]\nprint(f\"First-order derivative w.r.t. the feature parameter: \\n{dout}\")\n\n# you can also call directly a backward pass to compute derivatives with respect\n# to the variational parameters and use it for implementing variational\n# optimization\nout.sum().backward()\n</code></pre> <pre><code>Quantum model output: \ntensor([[ 0.0231],\n        [-0.0133],\n        [ 0.3699],\n        [ 0.4348],\n        [ 0.3164],\n        [ 0.2681],\n        [-0.0471],\n        [ 0.1624],\n        [ 0.2670],\n        [ 0.3393]], grad_fn=&lt;CatBackward0&gt;)\n\nFirst-order derivative w.r.t. the feature parameter: \ntensor([ 0.6922,  0.6359,  0.5828, -0.0646, -2.0352,  0.7593,  0.5677,  0.7926,\n         0.7603,  0.6604], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>To run QML on real devices, Qadence offers generalized parameter shift rules (GPSR) <sup>1</sup> for arbitrary quantum operations which can be selected when constructing the <code>QNN</code> model:</p> <pre><code>model = qd.QNN(circuit, observable, diff_mode=\"gpsr\")\nout = model(values)\n\ndout = torch.autograd.grad(out, values[\"phi\"], torch.ones_like(out), create_graph=True)[0]\nprint(f\"First-order derivative w.r.t. the feature parameter: \\n{dout}\")\n</code></pre> <pre><code>First-order derivative w.r.t. the feature parameter: \ntensor([ 0.6922,  0.6359,  0.5828, -0.0646, -2.0352,  0.7593,  0.5677,  0.7926,\n         0.7603,  0.6604], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>See here for more details on how the parameter shift rules implementation works in Qadence.</p>"},{"location":"tutorials/qml/#references","title":"References","text":"<p>[^1] Schuld, Petruccione, Machine learning on Quantum Computers, Springer Nature (2021)</p> <ol> <li> <p>Kyriienko et al., General quantum circuit differentiation rules \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/config_qnn/","title":"Configuring a QNN","text":"<p>In <code>qadence</code>, the <code>QNN</code> is a variational quantum model that can potentially take multi-dimensional input.</p> <p>The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>The circuit has two parts, the feature map and the ansatz. The feature map is responsible for encoding the input data into the quantum state, while the ansatz is responsible for the variational part of the model. In addition, a third part of the QNN is the observables, which is (a list of) operators that are measured at the end of the circuit.</p> <p>In QML Constructors we have seen how to construct the feature map and the ansatz. In this tutorial, we will see how to do the same using configs.</p> <p>One convenient way to construct these three parts of the model is to use the config classes, namely, <code>ObservableConfig</code>, <code>FeatureMapConfig</code>, <code>AnsatzConfig</code>. These classes allow you to specify the type of circuit and the parameters of the circuit in a structured way.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-observable","title":"Defining the Observable","text":"<p>The model output is the expectation value of the defined observable(s). We use the <code>ObservableConfig</code> class to specify the observable.</p> <p>We can specify any Hamiltonian that we want to measure at the end of the circuit. Let us say we want to measure the \\(Z\\) operator.</p> <pre><code>from qadence import observable_from_config, ObservableConfig, Z\n\nobservable_config = ObservableConfig(\n    detuning=Z,\n    scale=3.0,\n    shift=-1.0,\n)\n\nobservable = observable_from_config(register=4, config=observable_config)\n</code></pre> %3 cluster_f036bbd891f24e76bc700dca2a4da889 17886c9d71ad4b4f8360e686e33ca2a1 0 d1d666861805478f9354f5d49ea9f59d 17886c9d71ad4b4f8360e686e33ca2a1--d1d666861805478f9354f5d49ea9f59d 362ff571f41d4c6d801aea948f5d707e 1 4aa3e6fce6c747a29c37e77b193bb8d0 d1d666861805478f9354f5d49ea9f59d--4aa3e6fce6c747a29c37e77b193bb8d0 9db8f55058e14f3da6f9cbc74d7d5d12 290381f3b8634b47bda9062bb7e959a1 AddBlock 362ff571f41d4c6d801aea948f5d707e--290381f3b8634b47bda9062bb7e959a1 f2501eac036a467e8e289c56a9f2f73e 2 290381f3b8634b47bda9062bb7e959a1--9db8f55058e14f3da6f9cbc74d7d5d12 5ff0d18c617c43d88018e588e98db5d8 1544a4aa5ae549d68eddad11b19b4dd5 f2501eac036a467e8e289c56a9f2f73e--1544a4aa5ae549d68eddad11b19b4dd5 7a14a90b4bdf4fa8be87e51587708911 3 1544a4aa5ae549d68eddad11b19b4dd5--5ff0d18c617c43d88018e588e98db5d8 88f79c2ac4e847ed96b75cbe8c0f57fb 94244d68572f49e3b928c5358ee6899f 7a14a90b4bdf4fa8be87e51587708911--94244d68572f49e3b928c5358ee6899f 94244d68572f49e3b928c5358ee6899f--88f79c2ac4e847ed96b75cbe8c0f57fb <p>We have specified the observable Hamiltonian to be one with \\(Z\\)-detuning. The result is linearly scaled by 3.0 and shifted by -1.0. These parameters can optionally also be FeatureParameter or VariationalParameter</p> <p>One can also specify the observable as a list of observables, in which case the QNN will output a list of values.</p> <p>For full details on the <code>ObservableConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-feature-map","title":"Defining the Feature Map","text":"<p>Let us say we want to build a 4-qubit QNN that takes two inputs, namely, the \\(x\\) and the \\(y\\) coordinates of a point in the plane. We can use the <code>FeatureMapConfig</code> class to specify the feature map.</p> <pre><code>from qadence import BasisSet, chain, create_fm_blocks, FeatureMapConfig, ReuploadScaling\n\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs = [\"x\", \"y\"],\n    basis_set=BasisSet.CHEBYSHEV,\n    reupload_scaling=ReuploadScaling.TOWER,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\n\nfm_blocks = create_fm_blocks(register=4, config=fm_config)\nfeature_map = chain(*fm_blocks)\n</code></pre> %3 cluster_19625a5c7e33487d9d46f2ed93f3a221 Tower Chebyshev FM cluster_aabad077262d41829e220e860f88b504 Tower Chebyshev FM d11f94aef4aa4d97a42a8b791e663630 0 9032a1ecb5a84a4681c02853ba52d95c RX(1.0*acos(x)) d11f94aef4aa4d97a42a8b791e663630--9032a1ecb5a84a4681c02853ba52d95c 0545dfb5c76342f0a77792fe65bec13e 1 830798f0709f47acb54829e174e58423 9032a1ecb5a84a4681c02853ba52d95c--830798f0709f47acb54829e174e58423 e9e968c3d9cc495ab72c767b0fb3937c 6f6df8b240824a6c967b15c163b301be RX(2.0*acos(x)) 0545dfb5c76342f0a77792fe65bec13e--6f6df8b240824a6c967b15c163b301be 07fdc1036fe8471882f1be8baf3de4f1 2 6f6df8b240824a6c967b15c163b301be--e9e968c3d9cc495ab72c767b0fb3937c 0e7658ea01564488ac5689b5788210cf 395822e68a034fecb8c9ececb823cb70 RX(1.0*acos(2.0*y - 1.0)) 07fdc1036fe8471882f1be8baf3de4f1--395822e68a034fecb8c9ececb823cb70 7dca2e277274416fb3b6ac27f42c87fa 3 395822e68a034fecb8c9ececb823cb70--0e7658ea01564488ac5689b5788210cf cbafb2e431f145ba8bfebf77833fe21a b1fb877188574fbcbd9761c927241bda RX(2.0*acos(2.0*y - 1.0)) 7dca2e277274416fb3b6ac27f42c87fa--b1fb877188574fbcbd9761c927241bda b1fb877188574fbcbd9761c927241bda--cbafb2e431f145ba8bfebf77833fe21a <p>We have specified that the feature map should take two features, and have named the <code>FeatureParameter</code> \"x\" and \"y\" respectively. Both these parameters are encoded using the Chebyshev basis set, and the reupload scaling is set to <code>ReuploadScaling.TOWER</code>. One can optionally add the basis and the reupload scaling for each parameter separately.</p> <p>The <code>feature_range</code> parameter is a dictionary that specifies the range of values that each feature comes from. This is useful for scaling the input data to the range that the encoding function can handle. In default case, this range is mapped to the target range of the Chebyshev basis set which is \\([-1, 1]\\). One can also specify the target range for each feature separately.</p> <p>For full details on the <code>FeatureMapConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-ansatz","title":"Defining the Ansatz","text":"<p>The next part of the QNN is the ansatz. We use <code>AnsatzConfig</code> class to specify the type of ansatz.</p> <p>Let us say, we want to follow this feature map with 2 layers of hardware efficient ansatz.</p> <pre><code>from qadence import AnsatzConfig, AnsatzType, create_ansatz, Strategy\n\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nansatz = create_ansatz(register=4, config=ansatz_config)\n</code></pre> %3 938d3b3fa8074079ad80838e9a45e32e 0 5721207a3dcf4fd98b343d08aeb53c0c RX(theta\u2080) 938d3b3fa8074079ad80838e9a45e32e--5721207a3dcf4fd98b343d08aeb53c0c e8e147d1af6e42e690315dd1674ce396 1 7a722bebe4c44253a8f9a43a58547878 RY(theta\u2084) 5721207a3dcf4fd98b343d08aeb53c0c--7a722bebe4c44253a8f9a43a58547878 df596b8e3f264eada7ec718231ba0df4 RX(theta\u2088) 7a722bebe4c44253a8f9a43a58547878--df596b8e3f264eada7ec718231ba0df4 1e0678b2651b4055bda4074a95916287 df596b8e3f264eada7ec718231ba0df4--1e0678b2651b4055bda4074a95916287 2b953dac1f75450b9acbf6c2840f0383 1e0678b2651b4055bda4074a95916287--2b953dac1f75450b9acbf6c2840f0383 f3a69376ba3a4f6e876c2e6eaef2f6e9 RX(theta\u2081\u2082) 2b953dac1f75450b9acbf6c2840f0383--f3a69376ba3a4f6e876c2e6eaef2f6e9 6bf79112a98e42029dad701285eba9ba RY(theta\u2081\u2086) f3a69376ba3a4f6e876c2e6eaef2f6e9--6bf79112a98e42029dad701285eba9ba 6bbe4b28da724a71850bac4db2b840c9 RX(theta\u2082\u2080) 6bf79112a98e42029dad701285eba9ba--6bbe4b28da724a71850bac4db2b840c9 0b9f2b51713b404ca9771bb2c2b09e19 6bbe4b28da724a71850bac4db2b840c9--0b9f2b51713b404ca9771bb2c2b09e19 eefaa2d499044fc7ba33037dd878b789 0b9f2b51713b404ca9771bb2c2b09e19--eefaa2d499044fc7ba33037dd878b789 182079cd6d2748678b26bbf6f5529563 eefaa2d499044fc7ba33037dd878b789--182079cd6d2748678b26bbf6f5529563 fc8062fc0fe847048f2ed468e9b9471c 71483b3125cd4a7d8a86801f91b1f1c4 RX(theta\u2081) e8e147d1af6e42e690315dd1674ce396--71483b3125cd4a7d8a86801f91b1f1c4 d4b9409f42c641bc81bd079c56da8e4e 2 f56a1eedea6042df92cc81a0dc75b02c RY(theta\u2085) 71483b3125cd4a7d8a86801f91b1f1c4--f56a1eedea6042df92cc81a0dc75b02c 58d0f077298740788a3e7eccce1a44d9 RX(theta\u2089) f56a1eedea6042df92cc81a0dc75b02c--58d0f077298740788a3e7eccce1a44d9 378938f143de443591b465c2c5bddb56 X 58d0f077298740788a3e7eccce1a44d9--378938f143de443591b465c2c5bddb56 378938f143de443591b465c2c5bddb56--1e0678b2651b4055bda4074a95916287 d3608fb11d3148c79c8a80e136035d76 378938f143de443591b465c2c5bddb56--d3608fb11d3148c79c8a80e136035d76 bc831ee53cad482397a5f9690d052d2b RX(theta\u2081\u2083) d3608fb11d3148c79c8a80e136035d76--bc831ee53cad482397a5f9690d052d2b 37b6fe9f5b934f25b3a75306a92c4d53 RY(theta\u2081\u2087) bc831ee53cad482397a5f9690d052d2b--37b6fe9f5b934f25b3a75306a92c4d53 ca2e2169a5f04d4099aa261088434f48 RX(theta\u2082\u2081) 37b6fe9f5b934f25b3a75306a92c4d53--ca2e2169a5f04d4099aa261088434f48 02066c02d80046e8b8ec0e8c056548be X ca2e2169a5f04d4099aa261088434f48--02066c02d80046e8b8ec0e8c056548be 02066c02d80046e8b8ec0e8c056548be--0b9f2b51713b404ca9771bb2c2b09e19 9955b08eb56a4f2bafb275897ad6e8a2 02066c02d80046e8b8ec0e8c056548be--9955b08eb56a4f2bafb275897ad6e8a2 9955b08eb56a4f2bafb275897ad6e8a2--fc8062fc0fe847048f2ed468e9b9471c f31208117c8b4f7ca36ce7d277c333e5 58dcdcf9695d47058fbbd89236b4ac42 RX(theta\u2082) d4b9409f42c641bc81bd079c56da8e4e--58dcdcf9695d47058fbbd89236b4ac42 6408d46062f04dafa674f1bf5aafdd3f 3 8b6b310aa1fa4a10a2ddf188e03cb43a RY(theta\u2086) 58dcdcf9695d47058fbbd89236b4ac42--8b6b310aa1fa4a10a2ddf188e03cb43a a94fd7911a1e4c7d9f1aab72194f7d0e RX(theta\u2081\u2080) 8b6b310aa1fa4a10a2ddf188e03cb43a--a94fd7911a1e4c7d9f1aab72194f7d0e 83dbc742b9914f478a2269abb78402aa a94fd7911a1e4c7d9f1aab72194f7d0e--83dbc742b9914f478a2269abb78402aa 5d58b8ff7113420a9b4d6bb889fef7ae X 83dbc742b9914f478a2269abb78402aa--5d58b8ff7113420a9b4d6bb889fef7ae 5d58b8ff7113420a9b4d6bb889fef7ae--d3608fb11d3148c79c8a80e136035d76 03429b2b5a2349fbb59e4d60aff5d292 RX(theta\u2081\u2084) 5d58b8ff7113420a9b4d6bb889fef7ae--03429b2b5a2349fbb59e4d60aff5d292 f5487ca644c647fdad73694057d1d914 RY(theta\u2081\u2088) 03429b2b5a2349fbb59e4d60aff5d292--f5487ca644c647fdad73694057d1d914 5d7f4e90f7744de3a7841cb2e4ef8800 RX(theta\u2082\u2082) f5487ca644c647fdad73694057d1d914--5d7f4e90f7744de3a7841cb2e4ef8800 600910915b43400faed69833963896c7 5d7f4e90f7744de3a7841cb2e4ef8800--600910915b43400faed69833963896c7 a5bc2260994d4ee09e40fc902b0ea7f4 X 600910915b43400faed69833963896c7--a5bc2260994d4ee09e40fc902b0ea7f4 a5bc2260994d4ee09e40fc902b0ea7f4--9955b08eb56a4f2bafb275897ad6e8a2 a5bc2260994d4ee09e40fc902b0ea7f4--f31208117c8b4f7ca36ce7d277c333e5 c3201d1d6c0d4dc2aed8fa2faba4e25d f6f44180b8ec4186b686da60c8578201 RX(theta\u2083) 6408d46062f04dafa674f1bf5aafdd3f--f6f44180b8ec4186b686da60c8578201 f439b54bf47c43a5bf38e3a0173dadc4 RY(theta\u2087) f6f44180b8ec4186b686da60c8578201--f439b54bf47c43a5bf38e3a0173dadc4 fe9fd2eb68c9452dbb0af602ed979fd9 RX(theta\u2081\u2081) f439b54bf47c43a5bf38e3a0173dadc4--fe9fd2eb68c9452dbb0af602ed979fd9 1c9b8903887c46f2a6b68d29a8041b98 X fe9fd2eb68c9452dbb0af602ed979fd9--1c9b8903887c46f2a6b68d29a8041b98 1c9b8903887c46f2a6b68d29a8041b98--83dbc742b9914f478a2269abb78402aa 01f47e3d43064cd68f47cb44e1d34bae 1c9b8903887c46f2a6b68d29a8041b98--01f47e3d43064cd68f47cb44e1d34bae de632ca8b7584411adb15a7e9373f1c4 RX(theta\u2081\u2085) 01f47e3d43064cd68f47cb44e1d34bae--de632ca8b7584411adb15a7e9373f1c4 171e932477734f8796d6808196b756fb RY(theta\u2081\u2089) de632ca8b7584411adb15a7e9373f1c4--171e932477734f8796d6808196b756fb e8692e5ad36242ffa03cd2e77a12e9d5 RX(theta\u2082\u2083) 171e932477734f8796d6808196b756fb--e8692e5ad36242ffa03cd2e77a12e9d5 f74025292c534c60af9fd143d861d87e X e8692e5ad36242ffa03cd2e77a12e9d5--f74025292c534c60af9fd143d861d87e f74025292c534c60af9fd143d861d87e--600910915b43400faed69833963896c7 17d5c6935d9f4b8ea8124325cf8f1dfd f74025292c534c60af9fd143d861d87e--17d5c6935d9f4b8ea8124325cf8f1dfd 17d5c6935d9f4b8ea8124325cf8f1dfd--c3201d1d6c0d4dc2aed8fa2faba4e25d <p>We have specified that the ansatz should have a depth of 2, and the ansatz type is \"hea\" (Hardware Efficient Ansatz). The ansatz strategy is set to \"digital\", which means digital gates are being used. One could alternatively use \"analog\" or \"rydberg\" as the ansatz strategy.</p> <p>For full details on the <code>AnsatzConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-qnn-from-the-configs","title":"Defining the QNN from the Configs","text":"<p>To build the QNN, we can now use the <code>QNN</code> class as a <code>QuantumModel</code> subtype. In addition to the feature map, ansatz and the observable configs, we can also specify options such as the <code>backend</code>, <code>diff_mode</code>, etc. For full details on the <code>QNN</code> class, see the API documentation or the documentation on the config constructor here.</p> <pre><code>from qadence import BackendName, DiffMode, QNN\n\nqnn = QNN.from_configs(\n    register=4,\n    obs_config=observable_config,\n    fm_config=fm_config,\n    ansatz_config=ansatz_config,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.AD,\n)\n</code></pre> %3 cluster_ae0863235b764a8fa0621318ce181b44 Obs. cluster_a9a54e75f9cc4c3687d1942a53e86cd1 cluster_2595efde5bf64c88a0a392680a036380 Tower Chebyshev FM cluster_8c98903353f546ed81d89431f0095256 Tower Chebyshev FM cluster_dc191d3e77da48cdad6dfffc5fa7db16 HEA dbc1db3458604e9aa1321bbc72811597 0 c62e98cbd2dc47f590498288148ae651 RX(1.0*acos(x)) dbc1db3458604e9aa1321bbc72811597--c62e98cbd2dc47f590498288148ae651 4801113b6b5b4ce7b36c45f0166c7877 1 d1a87a13d5214636a10fc8053883fc2c RX(theta\u2080) c62e98cbd2dc47f590498288148ae651--d1a87a13d5214636a10fc8053883fc2c ce5c0a352def41799eac0f6f91b1f737 RY(theta\u2084) d1a87a13d5214636a10fc8053883fc2c--ce5c0a352def41799eac0f6f91b1f737 af62282b648c409f9010201950af0872 RX(theta\u2088) ce5c0a352def41799eac0f6f91b1f737--af62282b648c409f9010201950af0872 9f3fa769b6f94128b1bbea5e109abd20 af62282b648c409f9010201950af0872--9f3fa769b6f94128b1bbea5e109abd20 badbcadfcbca46ab88bd5c368e888de2 9f3fa769b6f94128b1bbea5e109abd20--badbcadfcbca46ab88bd5c368e888de2 0884e2630a33447490304027fbb4b1e3 RX(theta\u2081\u2082) badbcadfcbca46ab88bd5c368e888de2--0884e2630a33447490304027fbb4b1e3 8c770d8f4b174f6fa68373b43356c40c RY(theta\u2081\u2086) 0884e2630a33447490304027fbb4b1e3--8c770d8f4b174f6fa68373b43356c40c 786b7b0bab7e44c7ad1345d45e9f8e9e RX(theta\u2082\u2080) 8c770d8f4b174f6fa68373b43356c40c--786b7b0bab7e44c7ad1345d45e9f8e9e 6a4dea36fc194a97a9164e26b828c876 786b7b0bab7e44c7ad1345d45e9f8e9e--6a4dea36fc194a97a9164e26b828c876 00e1cd3274d74dd6acef75b38ac0bce1 6a4dea36fc194a97a9164e26b828c876--00e1cd3274d74dd6acef75b38ac0bce1 26913dba9c194996b7438b0153b5d83c 00e1cd3274d74dd6acef75b38ac0bce1--26913dba9c194996b7438b0153b5d83c aa30a8df5a444a198f3bfa88abb6ba49 26913dba9c194996b7438b0153b5d83c--aa30a8df5a444a198f3bfa88abb6ba49 d7f060ab77b446c88780cd0d72e08ded 78200720e53a46c5bd6b67b41e6ffc2e RX(2.0*acos(x)) 4801113b6b5b4ce7b36c45f0166c7877--78200720e53a46c5bd6b67b41e6ffc2e 6d8d1451182e45cb9d81f6ae50f39d5c 2 ef5491c60fd0450d8d75f91c784382de RX(theta\u2081) 78200720e53a46c5bd6b67b41e6ffc2e--ef5491c60fd0450d8d75f91c784382de c93de726542340f485125031980f780c RY(theta\u2085) ef5491c60fd0450d8d75f91c784382de--c93de726542340f485125031980f780c 095e5a623f6a46978c57fa86df9035e6 RX(theta\u2089) c93de726542340f485125031980f780c--095e5a623f6a46978c57fa86df9035e6 4a00be41a26948c587f16aaa1d5d6fd6 X 095e5a623f6a46978c57fa86df9035e6--4a00be41a26948c587f16aaa1d5d6fd6 4a00be41a26948c587f16aaa1d5d6fd6--9f3fa769b6f94128b1bbea5e109abd20 68a1bc20032e47309cd9975cb8c5eb09 4a00be41a26948c587f16aaa1d5d6fd6--68a1bc20032e47309cd9975cb8c5eb09 8fecdc2b565a4686a3ca9051a02e15bf RX(theta\u2081\u2083) 68a1bc20032e47309cd9975cb8c5eb09--8fecdc2b565a4686a3ca9051a02e15bf 2683aab6df0f4bd6926aee447abca760 RY(theta\u2081\u2087) 8fecdc2b565a4686a3ca9051a02e15bf--2683aab6df0f4bd6926aee447abca760 8da345f80b434d278f7a42bd035b4b66 RX(theta\u2082\u2081) 2683aab6df0f4bd6926aee447abca760--8da345f80b434d278f7a42bd035b4b66 047512bcdc4d43bcaf023c7071f0737d X 8da345f80b434d278f7a42bd035b4b66--047512bcdc4d43bcaf023c7071f0737d 047512bcdc4d43bcaf023c7071f0737d--6a4dea36fc194a97a9164e26b828c876 0b6ff8a68a5f4585b554523774640ce3 047512bcdc4d43bcaf023c7071f0737d--0b6ff8a68a5f4585b554523774640ce3 6dcb6091e2b24eb984706e1ba4cc0156 AddBlock 0b6ff8a68a5f4585b554523774640ce3--6dcb6091e2b24eb984706e1ba4cc0156 6dcb6091e2b24eb984706e1ba4cc0156--d7f060ab77b446c88780cd0d72e08ded cb040cde599f4ffd8ebea96b33fc5029 73228e1036954116a3edcbd38ccadb6b RX(1.0*acos(2.0*y - 1.0)) 6d8d1451182e45cb9d81f6ae50f39d5c--73228e1036954116a3edcbd38ccadb6b 51896b4cfea94cae926295ac5aea61ea 3 9bb7aac32b05446584e125cec05bf1c3 RX(theta\u2082) 73228e1036954116a3edcbd38ccadb6b--9bb7aac32b05446584e125cec05bf1c3 9b8e244a7e8e407e95445706ff58e0f3 RY(theta\u2086) 9bb7aac32b05446584e125cec05bf1c3--9b8e244a7e8e407e95445706ff58e0f3 06d08f93dbed43b39905f21eaa606a86 RX(theta\u2081\u2080) 9b8e244a7e8e407e95445706ff58e0f3--06d08f93dbed43b39905f21eaa606a86 01a12e96312d426fb30d20abd6273cc9 06d08f93dbed43b39905f21eaa606a86--01a12e96312d426fb30d20abd6273cc9 e92866735fd74badbe15a3fa59c839e5 X 01a12e96312d426fb30d20abd6273cc9--e92866735fd74badbe15a3fa59c839e5 e92866735fd74badbe15a3fa59c839e5--68a1bc20032e47309cd9975cb8c5eb09 dbeeb118945b414ebb4da3c3bdf3f175 RX(theta\u2081\u2084) e92866735fd74badbe15a3fa59c839e5--dbeeb118945b414ebb4da3c3bdf3f175 e86a4fd5d9694e70b219184ea1119e3d RY(theta\u2081\u2088) dbeeb118945b414ebb4da3c3bdf3f175--e86a4fd5d9694e70b219184ea1119e3d 1d65cef339be40668aed292ac8598d4f RX(theta\u2082\u2082) e86a4fd5d9694e70b219184ea1119e3d--1d65cef339be40668aed292ac8598d4f 2f60f28836fc48a0891a0a7c95e50644 1d65cef339be40668aed292ac8598d4f--2f60f28836fc48a0891a0a7c95e50644 e8da2b07804843cc92deb523af64a7bd X 2f60f28836fc48a0891a0a7c95e50644--e8da2b07804843cc92deb523af64a7bd e8da2b07804843cc92deb523af64a7bd--0b6ff8a68a5f4585b554523774640ce3 df45b3c277b948cd828d8d2d5a479669 e8da2b07804843cc92deb523af64a7bd--df45b3c277b948cd828d8d2d5a479669 df45b3c277b948cd828d8d2d5a479669--cb040cde599f4ffd8ebea96b33fc5029 fb3c763db0f9466197b2b3f42bc928a0 b007e78f83e24d37a3082779ed648a27 RX(2.0*acos(2.0*y - 1.0)) 51896b4cfea94cae926295ac5aea61ea--b007e78f83e24d37a3082779ed648a27 bd54d43890284fae971623b7b94ac34b RX(theta\u2083) b007e78f83e24d37a3082779ed648a27--bd54d43890284fae971623b7b94ac34b 264de10a9ca7429694c38799d28fa94f RY(theta\u2087) bd54d43890284fae971623b7b94ac34b--264de10a9ca7429694c38799d28fa94f 3970ad7064c04bcb968c3a95cf5dd4c3 RX(theta\u2081\u2081) 264de10a9ca7429694c38799d28fa94f--3970ad7064c04bcb968c3a95cf5dd4c3 38ccd1062902498f98c8aed2985dd5a2 X 3970ad7064c04bcb968c3a95cf5dd4c3--38ccd1062902498f98c8aed2985dd5a2 38ccd1062902498f98c8aed2985dd5a2--01a12e96312d426fb30d20abd6273cc9 f60f30274f6743829fa7249f57a95200 38ccd1062902498f98c8aed2985dd5a2--f60f30274f6743829fa7249f57a95200 1c92c477b43d4761824d1abad104ab8d RX(theta\u2081\u2085) f60f30274f6743829fa7249f57a95200--1c92c477b43d4761824d1abad104ab8d bfc5375aae274d7a8d443575b19c55ea RY(theta\u2081\u2089) 1c92c477b43d4761824d1abad104ab8d--bfc5375aae274d7a8d443575b19c55ea f42efe456f7d4227b3222aa7503a2a4c RX(theta\u2082\u2083) bfc5375aae274d7a8d443575b19c55ea--f42efe456f7d4227b3222aa7503a2a4c dbecf236706542eab34ccdf2392cd3c2 X f42efe456f7d4227b3222aa7503a2a4c--dbecf236706542eab34ccdf2392cd3c2 dbecf236706542eab34ccdf2392cd3c2--2f60f28836fc48a0891a0a7c95e50644 c6cbaae2830143baa86e36b0af47e8ac dbecf236706542eab34ccdf2392cd3c2--c6cbaae2830143baa86e36b0af47e8ac 0df55ac84b5d422892d7870dd97c5927 c6cbaae2830143baa86e36b0af47e8ac--0df55ac84b5d422892d7870dd97c5927 0df55ac84b5d422892d7870dd97c5927--fb3c763db0f9466197b2b3f42bc928a0"},{"location":"tutorials/qml/dqc_1d/","title":"Solving a 1D ODE","text":"<p>In this tutorial we will show how to use Qadence to solve a basic 1D Ordinary Differential Equation (ODE) with a QNN using Differentiable Quantum Circuits (DQC) <sup>1</sup>.</p> <p>Consider the following non-linear ODE and boundary condition:</p> \\[ \\frac{df}{dx}= 5\\times(4x^3+x^2-2x-\\frac12), \\qquad f(0)=0 \\] <p>It admits an exact solution:</p> \\[ f(x)=5\\times(x^4+\\frac13x^3-x^2-\\frac12x) \\] <p>Our goal will be to find this solution for \\(x\\in[-1, 1]\\).</p> <pre><code>import torch\n\ndef dfdx_equation(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Derivative as per the equation.\"\"\"\n    return 5*(4*x**3 + x**2 - 2*x - 0.5)\n</code></pre> <p>For the purpose of this tutorial, we will compute the derivative of the circuit using <code>torch.autograd</code>. The point of the DQC algorithm is to use differentiable circuits with parameter shift rules. In Qadence, PSR is implemented directly as custom overrides of the derivative function in the autograd engine, and thus we can later change the derivative method for the model itself if we wish.</p> <pre><code>def calc_deriv(outputs: torch.Tensor, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute a derivative of model that learns f(x), computes df/dx using torch.autograd.\"\"\"\n    grad = torch.autograd.grad(\n        outputs=outputs,\n        inputs=inputs,\n        grad_outputs = torch.ones_like(inputs),\n        create_graph = True,\n        retain_graph = True,\n    )[0]\n    return grad\n</code></pre>"},{"location":"tutorials/qml/dqc_1d/#defining-the-loss-function","title":"Defining the loss function","text":"<p>The essential part of solving this problem is to define the right loss function to represent our goal. In this case, we want to define a model that has the capacity to learn the target solution, and we want to minimize: - The derivative of this model in comparison with the exact derivative in the equation; - The output of the model at the boundary in comparison with the value for the boundary condition;</p> <p>We can write it like so:</p> <pre><code># Mean-squared error as the comparison criterion\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Loss function encoding the problem to solve.\"\"\"\n    # Equation loss\n    model_output = model(inputs)\n    deriv_model = calc_deriv(model_output, inputs)\n    deriv_exact = dfdx_equation(inputs)\n    ode_loss = criterion(deriv_model, deriv_exact)\n\n    # Boundary loss, f(0) = 0\n    boundary_model = model(torch.tensor([[0.0]]))\n    boundary_exact = torch.tensor([[0.0]])\n    boundary_loss = criterion(boundary_model, boundary_exact)\n\n    return ode_loss + boundary_loss\n</code></pre> <p>Different loss criterions could be considered, and we could also play with the balance between the sum of the two loss terms. For now, let's proceed with the definition above.</p> <p>Note that so far we have not used any quantum specific assumption, and we could in principle use the same loss function with a classical neural network.</p>"},{"location":"tutorials/qml/dqc_1d/#defining-a-qnn-with-qadence","title":"Defining a QNN with Qadence","text":"<p>Now, we can finally use Qadence to write a QNN. We will use a feature map to encode the input values, a trainable ansatz circuit, and an observable to measure as the output.</p> <pre><code>from qadence import feature_map, hea, chain\nfrom qadence import QNN, QuantumCircuit, Z\nfrom qadence.types import BasisSet, ReuploadScaling\n\nn_qubits = 3\ndepth = 3\n\n# Feature map\nfm = feature_map(\n    n_qubits = n_qubits,\n    param = \"x\",\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.TOWER,\n)\n\n# Ansatz\nansatz = hea(n_qubits = n_qubits, depth = depth)\n\n# Observable\nobservable = Z(0)\n\ncircuit = QuantumCircuit(n_qubits, chain(fm, ansatz))\nmodel = QNN(circuit = circuit, observable = observable, inputs = [\"x\"])\n</code></pre> <p>We used a Chebyshev feature map with a tower-like scaling of the input reupload, and a standard hardware-efficient ansatz. You can check the qml constructors tutorial to see how you can customize these components. In the observable, for now we consider the simple case of measuring the magnetization of the first qubit.</p> <pre><code>from qadence.draw import display\n\n# display(circuit)\n</code></pre> %3 cluster_dd2e03b31f484bd88d2945483c7c255f HEA cluster_c7d78a7aec114d4e9864125c69a0e0e5 Tower Chebyshev FM f372f0a2d93547958ddd891d8083e905 0 b5393bbe62dc42958d14a0f616570247 RX(1.0*acos(x)) f372f0a2d93547958ddd891d8083e905--b5393bbe62dc42958d14a0f616570247 72ac38690ac44fb3b3bd378d62778096 1 79d5580601084afeb492a03cd5de4e4a RX(theta\u2080) b5393bbe62dc42958d14a0f616570247--79d5580601084afeb492a03cd5de4e4a 8dbf57d9b31f4585aedb9b2f1814829a RY(theta\u2083) 79d5580601084afeb492a03cd5de4e4a--8dbf57d9b31f4585aedb9b2f1814829a 3cb54502bf574d1ebaa0be89d4398dc0 RX(theta\u2086) 8dbf57d9b31f4585aedb9b2f1814829a--3cb54502bf574d1ebaa0be89d4398dc0 8fa4ee6b3c224bf798326e843da323b1 3cb54502bf574d1ebaa0be89d4398dc0--8fa4ee6b3c224bf798326e843da323b1 6c2e6abf1c554174a02bbd13353f8b91 8fa4ee6b3c224bf798326e843da323b1--6c2e6abf1c554174a02bbd13353f8b91 078dfba5674143acaf82dec9c9c884b8 RX(theta\u2089) 6c2e6abf1c554174a02bbd13353f8b91--078dfba5674143acaf82dec9c9c884b8 5a68c9d4ec6d4798b629279ea47792a2 RY(theta\u2081\u2082) 078dfba5674143acaf82dec9c9c884b8--5a68c9d4ec6d4798b629279ea47792a2 314d3e5e54b14c8887540fd894df4088 RX(theta\u2081\u2085) 5a68c9d4ec6d4798b629279ea47792a2--314d3e5e54b14c8887540fd894df4088 6b74b6c780bd4bcbaededf682785debf 314d3e5e54b14c8887540fd894df4088--6b74b6c780bd4bcbaededf682785debf e8ccb70ec45c4a6d825f5a4b82a9a474 6b74b6c780bd4bcbaededf682785debf--e8ccb70ec45c4a6d825f5a4b82a9a474 2eb3c4b856544feeb7cd249a818c84d2 RX(theta\u2081\u2088) e8ccb70ec45c4a6d825f5a4b82a9a474--2eb3c4b856544feeb7cd249a818c84d2 7234c573737743c9ad5136631854b6f8 RY(theta\u2082\u2081) 2eb3c4b856544feeb7cd249a818c84d2--7234c573737743c9ad5136631854b6f8 17f88ebf3dcf448bb2af71caa3613c41 RX(theta\u2082\u2084) 7234c573737743c9ad5136631854b6f8--17f88ebf3dcf448bb2af71caa3613c41 a975f1b05d2046da945c160477f900a9 17f88ebf3dcf448bb2af71caa3613c41--a975f1b05d2046da945c160477f900a9 ad6e5451a0764897ae006ba6e4f62107 a975f1b05d2046da945c160477f900a9--ad6e5451a0764897ae006ba6e4f62107 cd68e724aa15432bb4d6d15a12beceec ad6e5451a0764897ae006ba6e4f62107--cd68e724aa15432bb4d6d15a12beceec d357656bbaf648bf9799b165c867e078 29b4f32f2d424fc1adb3b180f75d39a6 RX(2.0*acos(x)) 72ac38690ac44fb3b3bd378d62778096--29b4f32f2d424fc1adb3b180f75d39a6 475eb7bf44de4e2dacc428abf350e605 2 2b36018459d844f089b05a771d24f119 RX(theta\u2081) 29b4f32f2d424fc1adb3b180f75d39a6--2b36018459d844f089b05a771d24f119 a26761b75ba548d693b3f571ff80685e RY(theta\u2084) 2b36018459d844f089b05a771d24f119--a26761b75ba548d693b3f571ff80685e d5acc24b4b6a44928c98a48b15127fb7 RX(theta\u2087) a26761b75ba548d693b3f571ff80685e--d5acc24b4b6a44928c98a48b15127fb7 668022c2beb3492a809423f3f87dd90e X d5acc24b4b6a44928c98a48b15127fb7--668022c2beb3492a809423f3f87dd90e 668022c2beb3492a809423f3f87dd90e--8fa4ee6b3c224bf798326e843da323b1 160033b2e0a649aab0ff8061283006c3 668022c2beb3492a809423f3f87dd90e--160033b2e0a649aab0ff8061283006c3 6c90c668ac4745ba9d4f1bfbdbc7612e RX(theta\u2081\u2080) 160033b2e0a649aab0ff8061283006c3--6c90c668ac4745ba9d4f1bfbdbc7612e 1a9524895f0b4059bcc621fd7ab6a739 RY(theta\u2081\u2083) 6c90c668ac4745ba9d4f1bfbdbc7612e--1a9524895f0b4059bcc621fd7ab6a739 0a1103e622a04733894f4b0316afe72f RX(theta\u2081\u2086) 1a9524895f0b4059bcc621fd7ab6a739--0a1103e622a04733894f4b0316afe72f 890eddff96a04969846eb0519e0b019e X 0a1103e622a04733894f4b0316afe72f--890eddff96a04969846eb0519e0b019e 890eddff96a04969846eb0519e0b019e--6b74b6c780bd4bcbaededf682785debf 7651ac33686f4915a795f0585c162279 890eddff96a04969846eb0519e0b019e--7651ac33686f4915a795f0585c162279 39d0b3b1a184409fb6a6cf82489d5092 RX(theta\u2081\u2089) 7651ac33686f4915a795f0585c162279--39d0b3b1a184409fb6a6cf82489d5092 5ce57207af7943aca7a4395383fb52f3 RY(theta\u2082\u2082) 39d0b3b1a184409fb6a6cf82489d5092--5ce57207af7943aca7a4395383fb52f3 6d26c9ddb3c74740bd3c3c1f91d08a40 RX(theta\u2082\u2085) 5ce57207af7943aca7a4395383fb52f3--6d26c9ddb3c74740bd3c3c1f91d08a40 6083a5ba0b0a4f6cbb6b2c8a6dab6abd X 6d26c9ddb3c74740bd3c3c1f91d08a40--6083a5ba0b0a4f6cbb6b2c8a6dab6abd 6083a5ba0b0a4f6cbb6b2c8a6dab6abd--a975f1b05d2046da945c160477f900a9 87a59577da6a4236adff5016031d94e1 6083a5ba0b0a4f6cbb6b2c8a6dab6abd--87a59577da6a4236adff5016031d94e1 87a59577da6a4236adff5016031d94e1--d357656bbaf648bf9799b165c867e078 c5ace7dce6374ab1a690a5ed3d9f6475 6c2921fd585c4ac79c6a7134f3ebfa8e RX(3.0*acos(x)) 475eb7bf44de4e2dacc428abf350e605--6c2921fd585c4ac79c6a7134f3ebfa8e 6058447a7cf64914b38f5716cea5ce93 RX(theta\u2082) 6c2921fd585c4ac79c6a7134f3ebfa8e--6058447a7cf64914b38f5716cea5ce93 2512a674a40a4657a52c0c3950baf809 RY(theta\u2085) 6058447a7cf64914b38f5716cea5ce93--2512a674a40a4657a52c0c3950baf809 acec831f2227420da0280784ea44d0e3 RX(theta\u2088) 2512a674a40a4657a52c0c3950baf809--acec831f2227420da0280784ea44d0e3 268720586e744bd9afccf50f9f1660a1 acec831f2227420da0280784ea44d0e3--268720586e744bd9afccf50f9f1660a1 2915c52fb2e64b999bfe931f205dab26 X 268720586e744bd9afccf50f9f1660a1--2915c52fb2e64b999bfe931f205dab26 2915c52fb2e64b999bfe931f205dab26--160033b2e0a649aab0ff8061283006c3 8f72ab67246e42778fb990f02e55cc88 RX(theta\u2081\u2081) 2915c52fb2e64b999bfe931f205dab26--8f72ab67246e42778fb990f02e55cc88 178b784af4f74c8c8061baf8b7eea4f4 RY(theta\u2081\u2084) 8f72ab67246e42778fb990f02e55cc88--178b784af4f74c8c8061baf8b7eea4f4 3207f1ebc7f2448082f034bafaa50e54 RX(theta\u2081\u2087) 178b784af4f74c8c8061baf8b7eea4f4--3207f1ebc7f2448082f034bafaa50e54 4a077326e9f74330a441dee3972cbedb 3207f1ebc7f2448082f034bafaa50e54--4a077326e9f74330a441dee3972cbedb dca9c6a71b1545b9bb3532597f1bd715 X 4a077326e9f74330a441dee3972cbedb--dca9c6a71b1545b9bb3532597f1bd715 dca9c6a71b1545b9bb3532597f1bd715--7651ac33686f4915a795f0585c162279 1258e130921541e6a726eeb460fdf304 RX(theta\u2082\u2080) dca9c6a71b1545b9bb3532597f1bd715--1258e130921541e6a726eeb460fdf304 f0307b50ea66414f8e752335aaad77c6 RY(theta\u2082\u2083) 1258e130921541e6a726eeb460fdf304--f0307b50ea66414f8e752335aaad77c6 75a5537e1d454dccb007da086728f630 RX(theta\u2082\u2086) f0307b50ea66414f8e752335aaad77c6--75a5537e1d454dccb007da086728f630 3dc6b3a296ed4b05b11d0bad847eaf13 75a5537e1d454dccb007da086728f630--3dc6b3a296ed4b05b11d0bad847eaf13 2e5902330ceb4eeaa10009586fbe469a X 3dc6b3a296ed4b05b11d0bad847eaf13--2e5902330ceb4eeaa10009586fbe469a 2e5902330ceb4eeaa10009586fbe469a--87a59577da6a4236adff5016031d94e1 2e5902330ceb4eeaa10009586fbe469a--c5ace7dce6374ab1a690a5ed3d9f6475"},{"location":"tutorials/qml/dqc_1d/#training-the-model","title":"Training the model","text":"<p>Now that the model is defined we can proceed with the training. the <code>QNN</code> class can be used like any other <code>torch.nn.Module</code>. Here we write a simple training loop, but you can also look at the ml tools tutorial to use the convenience training functions that Qadence provides.</p> <p>To train the model, we will select a random set of collocation points uniformly distributed within \\(-1.0&lt; x &lt;1.0\\) and compute the loss function for those points.</p> <pre><code>n_epochs = 200\nn_points = 10\n\nxmin = -0.99\nxmax = 0.99\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Training data. We unsqueeze essentially making each batch have a single x value.\n    x_train = (xmin + (xmax-xmin)*torch.rand(n_points, requires_grad = True)).unsqueeze(1)\n\n    loss = loss_fn(inputs = x_train, model = model)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Note the values of \\(x\\) are only picked from \\(x\\in[-0.99, 0.99]\\) since we are using a Chebyshev feature map, and derivative of \\(\\text{acos}(x)\\) diverges at \\(-1\\) and \\(1\\).</p>"},{"location":"tutorials/qml/dqc_1d/#plotting-the-results","title":"Plotting the results","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef f_exact(x: torch.Tensor) -&gt; torch.Tensor:\n    return 5*(x**4 + (1/3)*x**3 - x**2 - 0.5*x)\n\nx_test = torch.arange(xmin, xmax, step = 0.01).unsqueeze(1)\n\nresult_exact = f_exact(x_test).flatten()\n\nresult_model = model(x_test).flatten().detach()\n\nplt.plot(x_test, result_exact, label = \"Exact solution\")\nplt.plot(x_test, result_model, label = \" Trained model\")\n</code></pre> 2025-02-07T10:22:21.012028 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>Clearly, the result is not optimal.</p>"},{"location":"tutorials/qml/dqc_1d/#improving-the-solution","title":"Improving the solution","text":"<p>One point to consider when defining the QNN is the possible output range, which is bounded by the spectrum of the chosen observable. For the magnetization of a single qubit, this means that the output is bounded between -1 and 1, which we can clearly see in the plot.</p> <p>One option would be to define the observable as the total magnetization over all qubits, which would allow a range of -3 to 3.</p> <pre><code>from qadence import add\n\nobservable = add(Z(i) for i in range(n_qubits))\n\nmodel = QNN(circuit = circuit, observable = observable, inputs = [\"x\"])\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Training data\n    x_train = (xmin + (xmax-xmin)*torch.rand(n_points, requires_grad = True)).unsqueeze(1)\n\n    loss = loss_fn(inputs = x_train, model = model)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>And we again plot the result:</p> <pre><code>x_test = torch.arange(xmin, xmax, step = 0.01).unsqueeze(1)\n\nresult_exact = f_exact(x_test).flatten()\n\nresult_model = model(x_test).flatten().detach()\n\nplt.plot(x_test, result_exact, label = \"Exact solution\")\nplt.plot(x_test, result_model, label = \"Trained model\")\n</code></pre> 2025-02-07T10:22:27.887638 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/dqc_1d/#references","title":"References","text":"<ol> <li> <p>Kyriienko et al., Solving nonlinear differential equations with differentiable quantum circuits. \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/qaoa/","title":"Solving MaxCut with QAOA","text":"<p>This tutorial shows how to solve the maximum cut (MaxCut) combinatorial optimization problem on a graph using the Quantum Approximate Optimization Algorithm (QAOA), first introduced by Farhi et al. in 2014 <sup>1</sup>.</p> <p>Given an arbitrary graph, the MaxCut problem consists in finding a graph cut which partitions the nodes into two disjoint sets, such that the number of edges in the cut is maximized. This is a very common combinatorial optimization problem known to be computationally hard (NP-hard).</p> <p>The graph used for this tutorial is an unweighted graph randomly generated using the <code>networkx</code> library with a certain probability \\(p\\) of having an edge between two arbitrary nodes (known as Erd\u0151s\u2013R\u00e9nyi graph).</p> <pre><code>import numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport random\n\n# ensure reproducibility\nseed = 42\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Create random graph\nn_nodes = 4\nedge_prob = 0.8\ngraph = nx.gnp_random_graph(n_nodes, edge_prob)\n\nnx.draw(graph)\n</code></pre> 2025-02-07T10:22:27.962143 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>The goal of the MaxCut algorithm is to maximize the following cost function:</p> \\[\\mathcal{C}(p) = \\sum_{\\alpha}^m \\mathcal{C}_{\\alpha}(p)\\] <p>where \\(p\\) is a given cut of the graph, \\(\\alpha\\) is an index over the edges and \\(\\mathcal{C}_{\\alpha}(p)\\) is written such that if the nodes connected by the \\(\\alpha\\) edge are in the same set, it returns \\(0\\), otherwise it returns \\(1\\). We will represent a cut \\(p\\) as a bitstring of length \\(N\\), where \\(N\\) is the number of nodes, and where the bit in position \\(i\\) shows to which partition node \\(i\\) belongs. We assign value 0 to one of the partitions defined by the cut and 1 to the other. Since this choice is arbitrary, every cut is represented by two bitstrings, e.g. \"0011\" and \"1100\" are equivalent.</p> <p>Since in this tutorial we are only dealing with small graphs, we can find the maximum cut by brute force to make sure QAOA works as intended. <pre><code># Function to calculate the cost associated with a cut\ndef calculate_cost(cut: str, graph: nx.graph) -&gt; float:\n    \"\"\"Returns the cost of a given cut (represented by a bitstring)\"\"\"\n    cost = 0\n    for edge in graph.edges():\n        (i, j) = edge\n        if cut[i] != cut[j]:\n            cost += 1\n    return cost\n\n\n# Function to get a binary representation of an int\nget_binary = lambda x, n: format(x, \"b\").zfill(n)\n\n# List of all possible cuts\nall_possible_cuts = [bin(k)[2:].rjust(n_nodes, \"0\") for k in range(2**n_nodes)]\n\n# List with the costs associated to each cut\nall_costs = [calculate_cost(cut, graph) for cut in all_possible_cuts]\n\n# Get the maximum cost\nmaxcost = max(all_costs)\n\n# Get all cuts that correspond to the maximum cost\nmaxcuts = [get_binary(i, n_nodes) for i, j in enumerate(all_costs) if j == maxcost]\nprint(f\"The maximum cut is represented by the bitstrings {maxcuts}, with a cost of {maxcost}\")\n</code></pre> <pre><code>The maximum cut is represented by the bitstrings ['0011', '0101', '0110', '1001', '1010', '1100'], with a cost of 4\n</code></pre> </p>"},{"location":"tutorials/qml/qaoa/#the-qaoa-quantum-circuit","title":"The QAOA quantum circuit","text":"<p>The Max-Cut problem can be solved by using the QAOA algorithm. QAOA belongs to the class of Variational Quantum Algorithms (VQAs), which means that its quantum circuit contains a certain number of parametrized quantum gates that need to be optimized with a classical optimizer. The QAOA circuit is composed of two operators:</p> <ul> <li>The cost operator \\(U_c\\): a circuit generated by the cost Hamiltonian which encodes the cost function described above into a quantum circuit. The solution to the optimization problem is encoded in the ground state of the cost Hamiltonian \\(H_c\\). The cost operator  is simply the evolution of the cost Hamiltonian parametrized by a variational parameter \\(\\gamma\\) so that \\(U_c = e^{i\\gamma H_c}.\\)</li> <li>The mixing operator \\(U_b\\): a simple set of single-qubit rotations with adjustable   angles which are tuned during the classical optimization loop to minimize the cost</li> </ul> <p>The cost Hamiltonian of the MaxCut problem can be written as:</p> \\[H_c = \\frac12 \\sum_{\\langle i,j\\rangle} (\\mathbb{1} - Z_iZ_j)\\] <p>where \\(\\langle i,j\\rangle\\) represents the edge between nodes \\(i\\) and \\(j\\). The solution of the MaxCut problem is encoded in the ground state of the above Hamiltonian.</p> <p>The QAOA quantum circuit consists of a number of layers, each layer containing a cost and a mixing operator. Below, the QAOA quantum circuit is defined using <code>qadence</code> operations. First, a layer of Hadamard gates is applied to all qubits to prepare the initial state \\(|+\\rangle ^{\\otimes n}\\). The cost operator of each layer can be built \"manually\", implementing the \\(e^{iZZ\\gamma}\\) terms with CNOTs and a \\(\\rm{RZ}(2\\gamma)\\) rotation, or it can also be automatically decomposed into digital single and two-qubits operations via the <code>.digital_decomposition()</code> method. The decomposition is exact since the Hamiltonian generator is diagonal.</p> <pre><code>from qadence import tag, kron, chain, RX, RZ, Z, H, CNOT, I, add\nfrom qadence import HamEvo, QuantumCircuit, Parameter\n\nn_qubits = graph.number_of_nodes()\nn_edges = graph.number_of_edges()\nn_layers = 6\n\n# Generate the cost Hamiltonian\nzz_ops = add(Z(edge[0]) @ Z(edge[1]) for edge in graph.edges)\ncost_ham = 0.5 * (n_edges * kron(I(i) for i in range(n_qubits)) - zz_ops)\n\n\n# QAOA circuit\ndef build_qaoa_circuit(n_qubits, n_layers, graph):\n    layers = []\n    # Layer of Hadamards\n    initial_layer = kron(H(i) for i in range(n_qubits))\n    layers.append(initial_layer)\n    for layer in range(n_layers):\n\n        # cost layer with digital decomposition\n        # cost_layer = HamEvo(cost_ham, f\"g{layer}\").digital_decomposition(approximation=\"basic\")\n        cost_layer = []\n        for edge in graph.edges():\n            (q0, q1) = edge\n            zz_term = chain(\n                CNOT(q0, q1),\n                RZ(q1, Parameter(f\"g{layer}\")),\n                CNOT(q0, q1),\n            )\n            cost_layer.append(zz_term)\n        cost_layer = chain(*cost_layer)\n        cost_layer = tag(cost_layer, \"cost\")\n\n        # mixing layer with single qubit rotations\n        mixing_layer = kron(RX(i, f\"b{layer}\") for i in range(n_qubits))\n        mixing_layer = tag(mixing_layer, \"mixing\")\n\n        # putting all together in a single ChainBlock\n        layers.append(chain(cost_layer, mixing_layer))\n\n    final_b = chain(*layers)\n    return QuantumCircuit(n_qubits, final_b)\n\n\ncircuit = build_qaoa_circuit(n_qubits, n_layers, graph)\n\n# Print a single layer of the circuit\n</code></pre> %3 cluster_6640f9c973cc4e9886a1d81a71eea50a mixing cluster_c5c0d3c20953417e9963178210134adb cost 06ca380186aa41a0b78b8dad9131ab72 0 30bf0a421a4d426eab3577e2f3c3b34d H 06ca380186aa41a0b78b8dad9131ab72--30bf0a421a4d426eab3577e2f3c3b34d 29b782df7c41461e8ae841ea94389b7e 1 49b9c4197e064847b0fb66b42b9e5131 30bf0a421a4d426eab3577e2f3c3b34d--49b9c4197e064847b0fb66b42b9e5131 1e0d4f80fd4c4838850be060773bb807 49b9c4197e064847b0fb66b42b9e5131--1e0d4f80fd4c4838850be060773bb807 1dd1f0c803454fd79b790231aef25b08 1e0d4f80fd4c4838850be060773bb807--1dd1f0c803454fd79b790231aef25b08 86227fcd81cc4db7b97055495a3480b6 1dd1f0c803454fd79b790231aef25b08--86227fcd81cc4db7b97055495a3480b6 401ddb69f4f74400a6cb0ba928d667e9 86227fcd81cc4db7b97055495a3480b6--401ddb69f4f74400a6cb0ba928d667e9 38e9778d34014599a2b7a9fbd2d8926a 401ddb69f4f74400a6cb0ba928d667e9--38e9778d34014599a2b7a9fbd2d8926a eb3f189773fb4a15ab9ad7f447f6ef48 38e9778d34014599a2b7a9fbd2d8926a--eb3f189773fb4a15ab9ad7f447f6ef48 59b57c5f6fc845a98eca671d2db549a2 eb3f189773fb4a15ab9ad7f447f6ef48--59b57c5f6fc845a98eca671d2db549a2 a232a757eb0b4f919c55d65fda3875fa 59b57c5f6fc845a98eca671d2db549a2--a232a757eb0b4f919c55d65fda3875fa 4ca16117f4a64f6c931dc670c15a74bb a232a757eb0b4f919c55d65fda3875fa--4ca16117f4a64f6c931dc670c15a74bb c1ac6ffa4f0b4a85b89390b25c75f5b4 4ca16117f4a64f6c931dc670c15a74bb--c1ac6ffa4f0b4a85b89390b25c75f5b4 6d3c9a6e713f4a83bd75c63fea6e0b41 c1ac6ffa4f0b4a85b89390b25c75f5b4--6d3c9a6e713f4a83bd75c63fea6e0b41 16cb8dd8fe2f49579913f339032e0ca4 6d3c9a6e713f4a83bd75c63fea6e0b41--16cb8dd8fe2f49579913f339032e0ca4 e5ad213d76c947009a828ee30edfa443 16cb8dd8fe2f49579913f339032e0ca4--e5ad213d76c947009a828ee30edfa443 5df66392743b4b31bc83cf4e26f3e3cd e5ad213d76c947009a828ee30edfa443--5df66392743b4b31bc83cf4e26f3e3cd 32ea710289824718b87c92a1d5fa28c4 5df66392743b4b31bc83cf4e26f3e3cd--32ea710289824718b87c92a1d5fa28c4 6b662d14153c42dfb35f9ca945e396fd 32ea710289824718b87c92a1d5fa28c4--6b662d14153c42dfb35f9ca945e396fd d1c5b58e08dc49efb7b346f56ede96fc 6b662d14153c42dfb35f9ca945e396fd--d1c5b58e08dc49efb7b346f56ede96fc c34d5d7683264db3a0f45c99ac572d0c RX(b0) d1c5b58e08dc49efb7b346f56ede96fc--c34d5d7683264db3a0f45c99ac572d0c 1b8b9f8564174602ad0deebfffffbf8f c34d5d7683264db3a0f45c99ac572d0c--1b8b9f8564174602ad0deebfffffbf8f 2f9fda1034b34e14819d50671ecbc497 7cfb233f963c4b8f8dd6a5e637d93221 H 29b782df7c41461e8ae841ea94389b7e--7cfb233f963c4b8f8dd6a5e637d93221 6ae54e3f68d342f18e946bac613e1c0c 2 d10e62e11eef4aea8d2035ad74a256fe X 7cfb233f963c4b8f8dd6a5e637d93221--d10e62e11eef4aea8d2035ad74a256fe d10e62e11eef4aea8d2035ad74a256fe--49b9c4197e064847b0fb66b42b9e5131 e3e13918a82e40bf871d9a02b31a8767 RZ(g0) d10e62e11eef4aea8d2035ad74a256fe--e3e13918a82e40bf871d9a02b31a8767 25a8aeb5dbb8480ca9a5261694f2583b X e3e13918a82e40bf871d9a02b31a8767--25a8aeb5dbb8480ca9a5261694f2583b 25a8aeb5dbb8480ca9a5261694f2583b--1dd1f0c803454fd79b790231aef25b08 84f42b5d363b43b884265ebdd14c2ff3 25a8aeb5dbb8480ca9a5261694f2583b--84f42b5d363b43b884265ebdd14c2ff3 bb52bd44c2724834baef7e04746fa104 84f42b5d363b43b884265ebdd14c2ff3--bb52bd44c2724834baef7e04746fa104 87b3cd81ed0c4921a1b04023c77a1def bb52bd44c2724834baef7e04746fa104--87b3cd81ed0c4921a1b04023c77a1def 848925a062de448388597390b664b0f4 87b3cd81ed0c4921a1b04023c77a1def--848925a062de448388597390b664b0f4 dea1d94c998c475bb65af285d97bfc9d 848925a062de448388597390b664b0f4--dea1d94c998c475bb65af285d97bfc9d d837e28a42154ed7b4d8f11ae56cd759 dea1d94c998c475bb65af285d97bfc9d--d837e28a42154ed7b4d8f11ae56cd759 c9b3c97d17bd428c96131b417540d430 d837e28a42154ed7b4d8f11ae56cd759--c9b3c97d17bd428c96131b417540d430 1c8775c2cd6947daaa8822190612a8d5 c9b3c97d17bd428c96131b417540d430--1c8775c2cd6947daaa8822190612a8d5 94e3d9be7ba545c48e2919dab4180168 1c8775c2cd6947daaa8822190612a8d5--94e3d9be7ba545c48e2919dab4180168 209150390a19433cb2a6e4b5d363e766 94e3d9be7ba545c48e2919dab4180168--209150390a19433cb2a6e4b5d363e766 109e7c32d6424e48b15f4ac17a5c9024 209150390a19433cb2a6e4b5d363e766--109e7c32d6424e48b15f4ac17a5c9024 222f72492d94483dbd51266a2dc5590f 109e7c32d6424e48b15f4ac17a5c9024--222f72492d94483dbd51266a2dc5590f be7b29b6703a44b38543226face24360 222f72492d94483dbd51266a2dc5590f--be7b29b6703a44b38543226face24360 ef6e1ae7505f420e90f279c72388daf6 be7b29b6703a44b38543226face24360--ef6e1ae7505f420e90f279c72388daf6 f708d0c1a8cb47938304b6446d45adfd ef6e1ae7505f420e90f279c72388daf6--f708d0c1a8cb47938304b6446d45adfd cb30ed0d350f4ff58ad6a7e356a1b0bb RX(b0) f708d0c1a8cb47938304b6446d45adfd--cb30ed0d350f4ff58ad6a7e356a1b0bb cb30ed0d350f4ff58ad6a7e356a1b0bb--2f9fda1034b34e14819d50671ecbc497 0819ee88fc154dafbe3254be8140cbe6 ff95bd6082e14b5789daadbe18c0ee3e H 6ae54e3f68d342f18e946bac613e1c0c--ff95bd6082e14b5789daadbe18c0ee3e 5aa875b232bd4181bf3cb4f41eb05282 3 fc07a5d3eb9a418db2ca91ee89ced6e3 ff95bd6082e14b5789daadbe18c0ee3e--fc07a5d3eb9a418db2ca91ee89ced6e3 df085a2594104a5da96dc0e4782af089 fc07a5d3eb9a418db2ca91ee89ced6e3--df085a2594104a5da96dc0e4782af089 73378e99f33b4f1491847437ef97baed df085a2594104a5da96dc0e4782af089--73378e99f33b4f1491847437ef97baed fd2e7a7f06234855be55b77bd9d96817 X 73378e99f33b4f1491847437ef97baed--fd2e7a7f06234855be55b77bd9d96817 fd2e7a7f06234855be55b77bd9d96817--86227fcd81cc4db7b97055495a3480b6 ae483fe14efd47e386caa8fdc99b8d0e RZ(g0) fd2e7a7f06234855be55b77bd9d96817--ae483fe14efd47e386caa8fdc99b8d0e b3b037b0aa1044c6950452f769e238f0 X ae483fe14efd47e386caa8fdc99b8d0e--b3b037b0aa1044c6950452f769e238f0 b3b037b0aa1044c6950452f769e238f0--38e9778d34014599a2b7a9fbd2d8926a 64476c77abc548c88e1d7e4a44dbcff4 b3b037b0aa1044c6950452f769e238f0--64476c77abc548c88e1d7e4a44dbcff4 9b0a78a35aa1405ba530484c6a300b9a 64476c77abc548c88e1d7e4a44dbcff4--9b0a78a35aa1405ba530484c6a300b9a f4be395efbc24bf389ecde56d340f68c 9b0a78a35aa1405ba530484c6a300b9a--f4be395efbc24bf389ecde56d340f68c dac0651af95e42eeb9cbe6817794ff31 X f4be395efbc24bf389ecde56d340f68c--dac0651af95e42eeb9cbe6817794ff31 dac0651af95e42eeb9cbe6817794ff31--c9b3c97d17bd428c96131b417540d430 5615028799b0439bb78f1d4b20959ff2 RZ(g0) dac0651af95e42eeb9cbe6817794ff31--5615028799b0439bb78f1d4b20959ff2 eddb98aac90049c6b2350435b0228eec X 5615028799b0439bb78f1d4b20959ff2--eddb98aac90049c6b2350435b0228eec eddb98aac90049c6b2350435b0228eec--94e3d9be7ba545c48e2919dab4180168 6c4b68deefab43f785d374898fb80899 eddb98aac90049c6b2350435b0228eec--6c4b68deefab43f785d374898fb80899 166a0d6eaa494f8f889e8d430c6595e7 6c4b68deefab43f785d374898fb80899--166a0d6eaa494f8f889e8d430c6595e7 e3e9eb3265044930aba746e688b4c2c4 166a0d6eaa494f8f889e8d430c6595e7--e3e9eb3265044930aba746e688b4c2c4 6acf1667ad2f4d48869ba091cf060bf9 e3e9eb3265044930aba746e688b4c2c4--6acf1667ad2f4d48869ba091cf060bf9 c6df65fb2b134db0a59317504d40da70 6acf1667ad2f4d48869ba091cf060bf9--c6df65fb2b134db0a59317504d40da70 fde005a7e80843a78218acb6ae050a02 c6df65fb2b134db0a59317504d40da70--fde005a7e80843a78218acb6ae050a02 2f13341f64c446fca47653cd46ec02e6 RX(b0) fde005a7e80843a78218acb6ae050a02--2f13341f64c446fca47653cd46ec02e6 2f13341f64c446fca47653cd46ec02e6--0819ee88fc154dafbe3254be8140cbe6 c37b4c1ddfb141e888acb9034f428092 f14ef74f20aa48edb08de8cefb63e5a4 H 5aa875b232bd4181bf3cb4f41eb05282--f14ef74f20aa48edb08de8cefb63e5a4 186e209646864f4c82df9876922eb3ed f14ef74f20aa48edb08de8cefb63e5a4--186e209646864f4c82df9876922eb3ed e68e4e70c9ee4563925266f0435667d3 186e209646864f4c82df9876922eb3ed--e68e4e70c9ee4563925266f0435667d3 c7901ef598c444cf9bd3a2f43b816c2d e68e4e70c9ee4563925266f0435667d3--c7901ef598c444cf9bd3a2f43b816c2d 00c952d07a884d53b15c157591a54ad1 c7901ef598c444cf9bd3a2f43b816c2d--00c952d07a884d53b15c157591a54ad1 d87b41712f014dbda1ba0ea4b87c2ada 00c952d07a884d53b15c157591a54ad1--d87b41712f014dbda1ba0ea4b87c2ada e526cc8167fa4da4968e27e85a265b50 d87b41712f014dbda1ba0ea4b87c2ada--e526cc8167fa4da4968e27e85a265b50 08ccccd0ca8d4030953c5794b4364898 X e526cc8167fa4da4968e27e85a265b50--08ccccd0ca8d4030953c5794b4364898 08ccccd0ca8d4030953c5794b4364898--eb3f189773fb4a15ab9ad7f447f6ef48 c0a5aa6984d544e399ffc9e8e598d5d5 RZ(g0) 08ccccd0ca8d4030953c5794b4364898--c0a5aa6984d544e399ffc9e8e598d5d5 233a8e0a99b545cdbf4cbb8798c24a0d X c0a5aa6984d544e399ffc9e8e598d5d5--233a8e0a99b545cdbf4cbb8798c24a0d 233a8e0a99b545cdbf4cbb8798c24a0d--a232a757eb0b4f919c55d65fda3875fa 3b66a24f9b854ef6a45cc199d2ac5615 233a8e0a99b545cdbf4cbb8798c24a0d--3b66a24f9b854ef6a45cc199d2ac5615 9ab6823f7eba4a599d9b63692516d2a5 3b66a24f9b854ef6a45cc199d2ac5615--9ab6823f7eba4a599d9b63692516d2a5 159469473a0940a7b606c775bce6a5c6 9ab6823f7eba4a599d9b63692516d2a5--159469473a0940a7b606c775bce6a5c6 6d8a8af8bd014910a055e8ad42de3c43 X 159469473a0940a7b606c775bce6a5c6--6d8a8af8bd014910a055e8ad42de3c43 6d8a8af8bd014910a055e8ad42de3c43--209150390a19433cb2a6e4b5d363e766 42c920ab39b14be5ae05db78216f42cf RZ(g0) 6d8a8af8bd014910a055e8ad42de3c43--42c920ab39b14be5ae05db78216f42cf e9e1ab07d723419684365061fa7dedd2 X 42c920ab39b14be5ae05db78216f42cf--e9e1ab07d723419684365061fa7dedd2 e9e1ab07d723419684365061fa7dedd2--222f72492d94483dbd51266a2dc5590f 191edacb832246dfa7ebb77079b62dfc X e9e1ab07d723419684365061fa7dedd2--191edacb832246dfa7ebb77079b62dfc 191edacb832246dfa7ebb77079b62dfc--6acf1667ad2f4d48869ba091cf060bf9 a787b37689594e3493c9a7a71ab6d998 RZ(g0) 191edacb832246dfa7ebb77079b62dfc--a787b37689594e3493c9a7a71ab6d998 f25b91c7b69840529a4b510141c61fbb X a787b37689594e3493c9a7a71ab6d998--f25b91c7b69840529a4b510141c61fbb f25b91c7b69840529a4b510141c61fbb--fde005a7e80843a78218acb6ae050a02 08f7aed5decc42a3ac0afccf38576daa RX(b0) f25b91c7b69840529a4b510141c61fbb--08f7aed5decc42a3ac0afccf38576daa 08f7aed5decc42a3ac0afccf38576daa--c37b4c1ddfb141e888acb9034f428092"},{"location":"tutorials/qml/qaoa/#train-the-qaoa-circuit-to-solve-maxcut","title":"Train the QAOA circuit to solve MaxCut","text":"<p>Given the QAOA circuit above, one can construct the associated Qadence <code>QuantumModel</code> and train it using standard gradient based optimization.</p> <p>The loss function to be minimized reads:</p> \\[\\mathcal{L} =-\\langle \\psi | H_c| \\psi \\rangle= -\\frac12 \\sum_{\\langle i,j\\rangle}  \\left(1 - \\langle \\psi | Z_i Z_j | \\psi \\rangle \\right)\\] <p>where \\(|\\psi\\rangle(\\beta, \\gamma)\\) is the wavefunction obtained by running the QAQA quantum circuit and the sum runs over the edges of the graph \\(\\langle i,j\\rangle\\).</p> <pre><code>import torch\nfrom qadence import QuantumModel\n\ntorch.manual_seed(seed)\n\n\ndef loss_function(model: QuantumModel):\n    # The loss corresponds to the expectation\n    # value of the cost Hamiltonian\n    return -1.0 * model.expectation().squeeze()\n\n\n# initialize the parameters to random values\nmodel = QuantumModel(circuit, observable=cost_ham)\nmodel.reset_vparams(torch.rand(model.num_vparams))\ninitial_loss = loss_function(model)\nprint(f\"Initial loss: {initial_loss}\")\n\n# train the model\nn_epochs = 100\nlr = 0.1\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_function(model)\n    loss.backward()\n    optimizer.step()\n    if (i + 1) % (n_epochs // 10) == 0:\n        print(f\"MaxCut cost at iteration {i+1}: {-loss.item()}\")\n</code></pre> <pre><code>Initial loss: -2.1782381363858794\nMaxCut cost at iteration 10: 3.7470706807026417\nMaxCut cost at iteration 20: 3.8378810288930216\nMaxCut cost at iteration 30: 3.9424197899236133\nMaxCut cost at iteration 40: 3.9981256255766002\nMaxCut cost at iteration 50: 3.996470528508214\nMaxCut cost at iteration 60: 3.9991374608876606\nMaxCut cost at iteration 70: 3.9994678542919555\nMaxCut cost at iteration 80: 3.999872558672829\nMaxCut cost at iteration 90: 3.9999475834121063\nMaxCut cost at iteration 100: 3.9999793311641003\n</code></pre> <p>Qadence offers some convenience functions to implement this training loop with advanced logging and metrics track features. You can refer to this tutorial for more details.</p>"},{"location":"tutorials/qml/qaoa/#results","title":"Results","text":"<p>Given the trained quantum model, one needs to sample the resulting quantum state to recover the bitstring with the highest probability which corresponds to the maximum cut of the graph.</p> <pre><code>samples = model.sample(n_shots=100)[0]\nmost_frequent = max(samples, key=samples.get)\n\nprint(f\"Most frequently sampled bitstring corresponding to the maximum cut: {most_frequent}\")\n\n# let's now draw the cut obtained with the QAOA procedure\ncolors = []\nlabels = {}\nfor node, b in zip(graph.nodes(), most_frequent):\n    colors.append(\"green\") if int(b) == 0 else colors.append(\"red\")\n    labels[node] = \"A\" if int(b) == 0 else \"B\"\n\nnx.draw_networkx(graph, node_color=colors, with_labels=True, labels=labels)\n</code></pre>   Most frequently sampled bitstring corresponding to the maximum cut: 1001  2025-02-07T10:22:31.935439 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/qaoa/#references","title":"References","text":"<ol> <li> <p>Farhi et al. - A Quantum Approximate Optimization Algorithm\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/qml/qcl/","title":"Quantum circuit learning","text":"<p>This tutorial shows how to apply <code>qadence</code> for solving a basic quantum machine learning application: fitting a simple function with the quantum circuit learning<sup>1</sup> (QCL) algorithm.</p> <p>QCL is a supervised quantum machine learning algorithm that uses a parametrized quantum neural network to learn the behavior of an arbitrary mathematical function using a set of function values as training data. This tutorial shows how to fit the \\(\\sin(x)\\) function in the \\([-1, 1]\\) domain.</p> <p>In the following, train and test data are defined.</p> <pre><code>import torch\nfrom torch.utils.data import random_split\n\n# make sure all tensors are kept on the same device\n# only available from PyTorch 2.0\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.set_default_device(device)\n\ndef qcl_training_data(\n    domain: tuple = (0, 2*torch.pi), n_points: int = 200\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n\n    start, end = domain\n\n    x_rand, _ = torch.sort(torch.DoubleTensor(n_points).uniform_(start, end))\n    y_rand = torch.sin(x_rand)\n\n    return x_rand, y_rand\n\nx, y = qcl_training_data()\n\n# random train/test split of the dataset\ntrain_subset, test_subset = random_split(x, [0.75, 0.25])\ntrain_ind = sorted(train_subset.indices)\ntest_ind = sorted(test_subset.indices)\n\nx_train, y_train = x[train_ind], y[train_ind]\nx_test, y_test = x[test_ind], y[test_ind]\n</code></pre>"},{"location":"tutorials/qml/qcl/#train-the-qcl-model","title":"Train the QCL model","text":"<p>Qadence provides the <code>QNN</code> convenience constructor to build a quantum neural network. The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>Total qubit magnetization is used as observable:</p> \\[ \\hat{O} = \\sum_i^N \\hat{\\sigma}_i^z \\] <p>In the following the observable, quantum circuit and corresponding QNN model are constructed.</p> <pre><code>import qadence as qd\n\nn_qubits = 4\n\n# create a simple feature map to encode the input data\nfeature_param = qd.FeatureParameter(\"phi\")\nfeature_map = qd.kron(qd.RX(i, feature_param) for i in range(n_qubits))\nfeature_map = qd.tag(feature_map, \"feature_map\")\n\n# create a digital-analog variational ansatz using Qadence convenience constructors\nansatz = qd.hea(n_qubits, depth=n_qubits)\nansatz = qd.tag(ansatz, \"ansatz\")\n\n# total qubit magnetization observable\nobservable = qd.hamiltonian_factory(n_qubits, detuning=qd.Z)\n\ncircuit = qd.QuantumCircuit(n_qubits, feature_map, ansatz)\nmodel = qd.QNN(circuit, [observable])\nexpval = model(values=torch.rand(10))\n</code></pre> <pre><code>tensor([[ 0.0092],\n        [-0.0207],\n        [-0.4616],\n        [-0.1845],\n        [-0.3761],\n        [-0.1463],\n        [-0.4563],\n        [-0.0016],\n        [-0.3436],\n        [-0.4252]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> <p>The QCL algorithm uses the output of the quantum neural network as a tunable universal function approximator. Standard PyTorch code is used for training the QNN using a mean-square error loss, Adam optimizer. Training is performend on the GPU if available:</p> <pre><code>n_epochs = 100\nlr = 0.25\n\ninput_values = {\"phi\": x_train}\nmse_loss = torch.nn.MSELoss()  # standard PyTorch loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)  # standard PyTorch Adam optimizer\n\nprint(f\"Initial loss: {mse_loss(model(values=x_train), y_train)}\")\ny_pred_initial = model(values=x_test)\n\nfor i in range(n_epochs):\n\n    optimizer.zero_grad()\n\n    # given a `n_batch` number of input points and a `n_observables`\n    # number of input observables to measure, the QNN returns\n    # an output of the following shape: [n_batch x n_observables]\n    # given that there is only one observable, a squeeze is applied to get\n    # a 1-dimensional tensor\n    loss = mse_loss(model(values=x_train).squeeze(), y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (i+1) % 20 == 0:\n        print(f\"Epoch {i+1} - Loss: {loss.item()}\")\n\nassert loss.item() &lt; 1e-3\n</code></pre> <pre><code>Initial loss: 0.6272721767455237\nEpoch 20 - Loss: 0.008173087377230498\nEpoch 40 - Loss: 0.0011247726222838813\nEpoch 60 - Loss: 0.0001415308609619855\nEpoch 80 - Loss: 2.3606578815826947e-05\nEpoch 100 - Loss: 2.503287372853267e-06\n</code></pre> <p>Qadence offers some convenience functions to implement this training loop with advanced logging and metrics track features. You can refer to this tutorial for more details.</p> <p>The quantum model is now trained on the training data points. To determine the quality of the results, one can check to see how well it fits the function on the test set.</p> <pre><code>import matplotlib.pyplot as plt\n\ny_pred = model({\"phi\": x_test})\n\n# convert all the results to numpy arrays for plotting\nx_train_np = x_train.cpu().detach().numpy().flatten()\ny_train_np = y_train.cpu().detach().numpy().flatten()\nx_test_np = x_test.cpu().detach().numpy().flatten()\ny_test_np = y_test.cpu().detach().numpy().flatten()\ny_pred_initial_np = y_pred_initial.cpu().detach().numpy().flatten()\ny_pred_np = y_pred.cpu().detach().numpy().flatten()\n\nfig, _ = plt.subplots()\nplt.scatter(x_test_np, y_test_np, label=\"Test points\", marker=\"o\", color=\"orange\")\nplt.plot(x_test_np, y_pred_initial_np, label=\"Initial prediction\", color=\"green\", alpha=0.5)\nplt.plot(x_test_np, y_pred_np, label=\"Final prediction\")\nplt.legend()\n</code></pre> 2025-02-07T10:22:35.978470 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/qcl/#references","title":"References","text":"<ol> <li> <p>Mitarai et al., Quantum Circuit Learning \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/ml_tools/callbacks/","title":"Callbacks for Trainer","text":"<p>Qadence <code>ml_tools</code> provides a powerful callback system for customizing various stages of the training process. With callbacks, you can monitor, log, save, and alter your training workflow efficiently. A <code>CallbackManager</code> is used with <code>Trainer</code> to execute the training process with defined callbacks. Following default callbacks are already provided in the <code>Trainer</code>.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#default-callbacks","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks already implemented in the <code>CallbackManager</code> used with <code>Trainer</code>:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>This guide covers how to define and use callbacks in <code>TrainConfig</code>, integrate them with the <code>Trainer</code> class, and create custom callbacks using hooks.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#1-built-in-callbacks","title":"1. Built-in Callbacks","text":"<p>Qadence ml_tools offers several built-in callbacks for common tasks like saving checkpoints, logging metrics, and tracking models. Below is an overview of each.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#11-printmetrics","title":"1.1. <code>PrintMetrics</code>","text":"<p>Prints metrics at specified intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\nprint_metrics_callback = PrintMetrics(on=\"val_batch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[print_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#12-writemetrics","title":"1.2. <code>WriteMetrics</code>","text":"<p>Writes metrics to a specified logging destination.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import WriteMetrics\n\nwrite_metrics_callback = WriteMetrics(on=\"train_epoch_end\", called_every=50)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[write_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#13-plotmetrics","title":"1.3. <code>PlotMetrics</code>","text":"<p>Plots metrics based on user-defined plotting functions.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PlotMetrics\n\nplot_metrics_callback = PlotMetrics(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[plot_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#14-loghyperparameters","title":"1.4. <code>LogHyperparameters</code>","text":"<p>Logs hyperparameters to keep track of training settings.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogHyperparameters\n\nlog_hyper_callback = LogHyperparameters(on=\"train_start\", called_every=1)\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_hyper_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#15-savecheckpoint","title":"1.5. <code>SaveCheckpoint</code>","text":"<p>Saves model checkpoints at specified intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint\n\nsave_checkpoint_callback = SaveCheckpoint(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#16-savebestcheckpoint","title":"1.6. <code>SaveBestCheckpoint</code>","text":"<p>Saves the best model checkpoint based on a validation criterion.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveBestCheckpoint\n\nsave_best_checkpoint_callback = SaveBestCheckpoint(on=\"val_epoch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_best_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#17-loadcheckpoint","title":"1.7. <code>LoadCheckpoint</code>","text":"<p>Loads a saved model checkpoint at the start of training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LoadCheckpoint\n\nload_checkpoint_callback = LoadCheckpoint(on=\"train_start\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[load_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#18-logmodeltracker","title":"1.8. <code>LogModelTracker</code>","text":"<p>Logs the model structure and parameters.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogModelTracker\n\nlog_model_callback = LogModelTracker(on=\"train_end\")\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_model_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#19-lrschedulerstepdecay","title":"1.9. <code>LRSchedulerStepDecay</code>","text":"<p>Reduces the learning rate by a factor at regular intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerStepDecay\n\nlr_step_decay = LRSchedulerStepDecay(on=\"train_epoch_end\", called_every=100, gamma=0.5)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_step_decay]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#110-lrschedulercyclic","title":"1.10. <code>LRSchedulerCyclic</code>","text":"<p>Applies a cyclic learning rate schedule during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCyclic\n\nlr_cyclic = LRSchedulerCyclic(on=\"train_batch_end\", called_every=1, base_lr=0.001, max_lr=0.01, step_size=2000)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_cyclic]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#111-lrschedulercosineannealing","title":"1.11. <code>LRSchedulerCosineAnnealing</code>","text":"<p>Applies cosine annealing to the learning rate during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCosineAnnealing\n\nlr_cosine = LRSchedulerCosineAnnealing(on=\"train_batch_end\", called_every=1, t_max=5000, min_lr=1e-6)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_cosine]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#112-earlystopping","title":"1.12. <code>EarlyStopping</code>","text":"<p>Stops training when a monitored metric has not improved for a specified number of epochs.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(on=\"val_epoch_end\", called_every=1, monitor=\"val_loss\", patience=5, mode=\"min\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[early_stopping]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#113-gradientmonitoring","title":"1.13. <code>GradientMonitoring</code>","text":"<p>Logs gradient statistics (e.g., mean, standard deviation, max) during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import GradientMonitoring\n\ngradient_monitoring = GradientMonitoring(on=\"train_batch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[gradient_monitoring]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#2-custom-callbacks","title":"2. Custom Callbacks","text":"<p>The base <code>Callback</code> class in Qadence allows defining custom behavior that can be triggered at specified events (e.g., start of training, end of epoch). You can set parameters such as when the callback runs (<code>on</code>), frequency of execution (<code>called_every</code>), and optionally define a <code>callback_condition</code>.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#defining-callbacks","title":"Defining Callbacks","text":"<p>There are two main ways to define a callback: 1. Directly providing a function in the <code>Callback</code> instance. 2. Subclassing the <code>Callback</code> class and implementing custom logic.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-1-providing-a-callback-function-directly","title":"Example 1: Providing a Callback Function Directly","text":"<pre><code>from qadence.ml_tools.callbacks import Callback\n\n# Define a custom callback function\ndef custom_callback_function(trainer, config, writer):\n    print(\"Executing custom callback.\")\n\n# Create the callback instance\ncustom_callback = Callback(\n    on=\"train_end\",\n    callback=custom_callback_function\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-2-subclassing-the-callback","title":"Example 2: Subclassing the Callback","text":"<pre><code>from qadence.ml_tools.callbacks import Callback\n\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in run_callback method.\")\n\n# Create the subclassed callback instance\ncustom_callback = CustomCallback(on=\"train_batch_end\", called_every=10)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#3-adding-callbacks-to-trainconfig","title":"3. Adding Callbacks to <code>TrainConfig</code>","text":"<p>To use callbacks in <code>TrainConfig</code>, add them to the <code>callbacks</code> list when configuring the training process.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint, PrintMetrics\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[\n        SaveCheckpoint(on=\"val_epoch_end\", called_every=50),\n        PrintMetrics(on=\"train_epoch_end\", called_every=100),\n    ]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#4-using-callbacks-with-trainer","title":"4. Using Callbacks with <code>Trainer</code>","text":"<p>The <code>Trainer</code> class in <code>qadence.ml_tools</code> provides built-in support for executing callbacks at various stages in the training process, managed through a callback manager. By default, several callbacks are added to specific hooks to automate common tasks, such as check-pointing, metric logging, and model tracking.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#default-callbacks_1","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks and their assigned hooks:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>These defaults handle common needs, but you can also add custom callbacks to any hook.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-adding-a-custom-callback","title":"Example: Adding a Custom Callback","text":"<p>To create a custom <code>Trainer</code> that includes a <code>PrintMetrics</code> callback executed specifically at the end of each epoch, follow the steps below.</p> <pre><code>from qadence.ml_tools.trainer import Trainer\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.print_metrics_callback = PrintMetrics(on=\"train_epoch_end\", called_every = 10)\n\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        self.print_metrics_callback.run_callback(self)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/data_and_config/","title":"Data and Configurations","text":""},{"location":"tutorials/qml/ml_tools/data_and_config/#1-dataloaders","title":"1. Dataloaders","text":"<p>When using Qadence, you can supply classical data to a quantum machine learning algorithm by using a standard PyTorch <code>DataLoader</code> instance. Qadence also provides the <code>DictDataLoader</code> convenience class which allows to build dictionaries of <code>DataLoader</code>s instances and easily iterate over them.</p> <pre><code>import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom qadence.ml_tools import DictDataLoader, to_dataloader\n\n\ndef dataloader(data_size: int = 25, batch_size: int = 5, infinite: bool = False) -&gt; DataLoader:\n    x = torch.linspace(0, 1, data_size).reshape(-1, 1)\n    y = torch.sin(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=infinite)\n\n\ndef dictdataloader(data_size: int = 25, batch_size: int = 5) -&gt; DictDataLoader:\n    dls = {}\n    for k in [\"y1\", \"y2\"]:\n        x = torch.rand(data_size, 1)\n        y = torch.sin(x)\n        dls[k] = to_dataloader(x, y, batch_size=batch_size, infinite=True)\n    return DictDataLoader(dls)\n\n\n# iterate over standard DataLoader\nfor (x,y) in dataloader(data_size=6, batch_size=2):\n    print(f\"Standard {x = }\")\n\n# construct an infinite dataset which will keep sampling indefinitely\nn_epochs = 5\ndl = iter(dataloader(data_size=6, batch_size=2, infinite=True))\nfor _ in range(n_epochs):\n    (x, y) = next(dl)\n    print(f\"Infinite {x = }\")\n\n# iterate over DictDataLoader\nddl = dictdataloader()\ndata = next(iter(ddl))\nprint(f\"{data = }\")\n</code></pre> <pre><code>Standard x = tensor([[0.0000],\n        [0.2000]])\nStandard x = tensor([[0.4000],\n        [0.6000]])\nStandard x = tensor([[0.8000],\n        [1.0000]])\nInfinite x = tensor([[0.2000],\n        [0.4000]])\nInfinite x = tensor([[1.0000],\n        [0.6000]])\nInfinite x = tensor([[0.0000],\n        [0.8000]])\nInfinite x = tensor([[0.2000],\n        [0.4000]])\nInfinite x = tensor([[1.0000],\n        [0.6000]])\ndata = {'y1': [tensor([[0.4409],\n        [0.0250],\n        [0.3490],\n        [0.3025],\n        [0.4626]]), tensor([[0.4267],\n        [0.0250],\n        [0.3420],\n        [0.2979],\n        [0.4463]])], 'y2': [tensor([[0.4027],\n        [0.4736],\n        [0.4973],\n        [0.2373],\n        [0.9213]]), tensor([[0.3919],\n        [0.4561],\n        [0.4770],\n        [0.2350],\n        [0.7964]])]}\n</code></pre> <p>Note:     In case of <code>infinite</code>=True, the dataloader iterator will provide a random sample from the dataset.</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#2-training-configuration","title":"2. Training Configuration","text":"<p>The <code>TrainConfig</code> class provides a comprehensive configuration setup for training quantam machine learning models in Qadence. This configuration includes settings for batch size, logging, check-pointing, validation, and additional custom callbacks that control the training process's granularity and flexibility.</p> <p>The <code>TrainConfig</code> tells <code>Trainer</code>  what batch_size should be used, how many epochs to train, in which intervals to print/log metrics and how often to store intermediate checkpoints. It is also possible to provide custom callback functions by instantiating a <code>Callback</code> with a function <code>callback</code>.</p> <p>For example of how to use the TrainConfig with <code>Trainer</code>, please see Examples in Trainer</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#21-explanation-of-trainconfig-attributes","title":"2.1 Explanation of <code>TrainConfig</code> Attributes","text":"Attribute Type Default Description <code>max_iter</code> <code>int</code> <code>10000</code> Total number of training epochs. <code>batch_size</code> <code>int</code> <code>1</code> Batch size for training. <code>print_every</code> <code>int</code> <code>0</code> Frequency of console output. Set to <code>0</code> to disable. <code>write_every</code> <code>int</code> <code>0</code> Frequency of logging metrics. Set to <code>0</code> to disable. <code>plot_every</code> <code>int</code> <code>0</code> Frequency of plotting metrics. Set to <code>0</code> to disable. <code>checkpoint_every</code> <code>int</code> <code>0</code> Frequency of saving checkpoints. Set to <code>0</code> to disable. <code>val_every</code> <code>int</code> <code>0</code> Frequency of validation checks. Set to <code>0</code> to disable. <code>val_epsilon</code> <code>float</code> <code>1e-5</code> Threshold for validation improvement. <code>validation_criterion</code> <code>Callable</code> <code>None</code> Function for validating metric improvement. <code>trainstop_criterion</code> <code>Callable</code> <code>None</code> Function to stop training early. <code>callbacks</code> <code>list[Callback]</code> <code>[]</code> List of custom callbacks. <code>root_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Root directory for saving logs and checkpoints. <code>log_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Logging directory for saving logs and checkpoints. <code>log_model</code> <code>bool</code> <code>False</code> Enables model logging. <code>verbose</code> <code>bool</code> <code>True</code> Enables detailed logging. <code>tracking_tool</code> <code>ExperimentTrackingTool</code> <code>TENSORBOARD</code> Tool for tracking training metrics. <code>plotting_functions</code> <code>tuple</code> <code>()</code> Functions for plotting metrics. <pre><code>from qadence.ml_tools import OptimizeResult, TrainConfig\nfrom qadence.ml_tools.callbacks import Callback\n\nbatch_size = 5\nn_epochs = 100\n\nprint_parameters = lambda opt_res: print(opt_res.model.parameters())\ncondition_print = lambda opt_res: opt_res.loss &lt; 1.0e-03\nmodify_extra_opt_res = {\"n_epochs\": n_epochs}\ncustom_callback = Callback(on=\"train_end\", callback = print_parameters, callback_condition=condition_print, modify_optimize_result=modify_extra_opt_res, called_every=10,)\n\nconfig = TrainConfig(\n    root_folder=\"some_path/\",\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n    callbacks = [custom_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/data_and_config/#22-key-configuration-options-in-trainconfig","title":"2.2 Key Configuration Options in <code>TrainConfig</code>","text":""},{"location":"tutorials/qml/ml_tools/data_and_config/#iterations-and-batch-size","title":"Iterations and Batch Size","text":"<ul> <li><code>max_iter</code> (int): Specifies the total number of training iterations (epochs). For an <code>InfiniteTensorDataset</code>, each epoch contains one batch; for a <code>TensorDataset</code>, it contains <code>len(dataloader)</code> batches.</li> <li><code>batch_size</code> (int): Defines the number of samples processed in each training iteration.</li> </ul> <p>Example: <pre><code>config = TrainConfig(max_iter=2000, batch_size=32)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#training-parameters","title":"Training Parameters","text":"<ul> <li><code>print_every</code> (int): Controls how often loss and metrics are printed to the console.</li> <li><code>write_every</code> (int): Determines how frequently metrics are written to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>checkpoint_every</code> (int): Sets the frequency for saving model checkpoints.</li> </ul> <p>Note: Set 0 to diable.</p> <p>Example: <pre><code>config = TrainConfig(print_every=100, write_every=50, checkpoint_every=50)\n</code></pre></p> <p>The user can provide either the <code>root_folder</code> or the <code>log_folder</code> for saving checkpoints and logging. When neither are provided, the default <code>root_folder</code> \"./qml_logs\" is used.</p> <ul> <li><code>root_folder</code> (Path): The root directory for saving checkpoints and logs. All training logs will be saved inside a subfolder in this root directory. (The path to these subfolders can be accessed using config._subfolders, and the current logging folder is config.log_folder)</li> <li><code>create_subfolder_per_run</code> (bool): Creates a unique subfolder for each training run within the specified folder.</li> <li><code>tracking_tool</code> (ExperimentTrackingTool): Specifies the tracking tool to log metrics, e.g., TensorBoard or MLflow.</li> <li><code>log_model</code> (bool): Enables logging of a serialized version of the model, which is useful for model versioning. Thi happens at the end of training.</li> </ul> <p>Note     - The user can also provide <code>log_folder</code> argument - which will only be used when <code>create_subfolder_per_run</code> = False.     -  <code>log_folder</code> (Path): The log folder used for saving checkpoints and logs.</p> <p>Example: <pre><code>config = TrainConfig(root_folder=\"path/to/checkpoints\", tracking_tool=ExperimentTrackingTool.MLFLOW, checkpoint_best_only=True)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#validation-parameters","title":"Validation Parameters","text":"<ul> <li><code>checkpoint_best_only</code> (bool): If set to <code>True</code>, saves checkpoints only when there is an improvement in the validation metric.</li> <li><code>val_every</code> (int): Frequency of validation checks. Setting this to <code>0</code> disables validation.</li> <li><code>val_epsilon</code> (float): A small threshold used to compare the current validation loss with previous best losses.</li> <li><code>validation_criterion</code> (Callable): A custom function to assess if the validation metric meets a specified condition.</li> </ul> <p>Example: <pre><code>config = TrainConfig(val_every=200, checkpoint_best_only = True, validation_criterion=lambda current, best: current &lt; best - 0.001)\n</code></pre></p> <p>If it is desired to only the save the \"best\" checkpoint, the following must be ensured:</p> <pre><code>(a) `checkpoint_best_only = True` is used while creating the configuration through `TrainConfig`,\n(b) `val_every` is set to a valid integer value (for example, `val_every = 10`) which controls the no. of iterations after which the validation data should be used to evaluate the model during training, which can also be set through `TrainConfig`,\n(c) a validation criterion is provided through the `validation_criterion`, set through `TrainConfig` to quantify the definition of \"best\", and\n(d) the validation dataloader passed to `Trainer` is of type `DataLoader`. In this case, it is expected that a validation dataloader is also provided along with the train dataloader since the validation data will be used to decide the \"best\" checkpoint.\n</code></pre> <p>The criterion used to decide the \"best\" checkpoint can be customized by <code>validation_criterion</code>, which should be a function that can take val_loss, best_loss, and val_epsilon arguments and return a boolean value (True or False) indicating whether some validation metric is satisfied or not. An example of a simple <code>validation_criterion</code> is: <pre><code>def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:\n    return val_loss &lt; (best_val_loss - val_epsilon)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#custom-callbacks","title":"Custom Callbacks","text":"<p><code>TrainConfig</code> supports custom callbacks that can be triggered at specific stages of training. The <code>callbacks</code> attribute accepts a list of callback instances, which allow for custom behaviors like early stopping or additional logging. See Callbacks for more details.</p> <ul> <li><code>callbacks</code> (list[Callback]): List of custom callbacks to execute during training.</li> </ul> <p>Example: <pre><code>from qadence.ml_tools.callbacks import Callback\n\ndef callback_fn(trainer, config, writer):\n    if trainer.opt_res.loss &lt; 0.001:\n        print(\"Custom Callback: Loss threshold reached!\")\n\ncustom_callback = Callback(on = \"train_epoch_end\", called_every = 10, callback_function = callback_fn )\n\nconfig = TrainConfig(callbacks=[custom_callback])\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#hyperparameters-and-plotting","title":"Hyperparameters and Plotting","text":"<ul> <li><code>hyperparams</code> (dict): A dictionary of hyperparameters (e.g., learning rate, regularization) to be tracked by the tracking tool.</li> <li><code>plot_every</code> (int): Determines how frequently plots are saved to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>plotting_functions</code> (tuple[LoggablePlotFunction, ...]): Functions for in-training plotting of metrics or model state.</li> </ul> <p>Note: Please ensure that plotting_functions are provided when plot_every &gt; 0</p> <p>Example: <pre><code>config = TrainConfig(\n    plot_every=10,\n    hyperparams={\"learning_rate\": 0.001, \"batch_size\": 32},\n    plotting_functions=(plot_loss_function,)\n)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#3-experiment-tracking-with-mlflow","title":"3. Experiment tracking with mlflow","text":"<p>Qadence allows to track runs and log hyperparameters, models and plots with tensorboard and mlflow. In the following, we demonstrate the integration with mlflow.</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#mlflow-configuration","title":"mlflow configuration","text":"<p>We have control over our tracking configuration by setting environment variables. First, let's look at the tracking URI. For the purpose of this demo we will be working with a local database, in a similar fashion as described here, <pre><code>export MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n</code></pre></p> <p>Qadence can also read the following two environment variables to define the mlflow experiment name and run name <pre><code>export MLFLOW_EXPERIMENT=test_experiment\nexport MLFLOW_RUN_NAME=run_0\n</code></pre></p> <p>If no tracking URI is provided, mlflow stores run information and artifacts in the local <code>./mlflow</code> directory and if no names are defined, the experiment and run will be named with random UUIDs.</p>"},{"location":"tutorials/qml/ml_tools/trainer/","title":"Qadence Trainer Guide","text":"<p>The <code>Trainer</code> class in <code>qadence.ml_tools</code> is a versatile tool designed to streamline the training of quantum machine learning models. It offers flexibility for both gradient-based and gradient-free optimization methods, supports custom loss functions, and integrates seamlessly with tracking tools like TensorBoard and MLflow. Additionally, it provides hooks for implementing custom behaviors during the training process.</p> <p>For training QML models, Qadence offers this out-of-the-box <code>Trainer</code> for optimizing differentiable models, e.g. <code>QNN</code>s and <code>QuantumModel</code>, containing either trainable and/or non-trainable parameters (see the parameters tutorial for detailed information about parameter types):</p>"},{"location":"tutorials/qml/ml_tools/trainer/#1-overview","title":"1. Overview","text":"<p>The <code>Trainer</code> class simplifies the training workflow by managing the training loop, handling data loading, and facilitating model evaluation. It is compatible with various optimization strategies and allows for extensive customization to meet specific training requirements.</p> <p>Example of initializing the <code>Trainer</code>:</p> <pre><code>from qadence.ml_tools import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\n# Initialize Trainer with model, optimizer, and configuration\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\n</code></pre> <p>Notes: <code>qadence</code> versions prior to 1.9.0 provided <code>train_with_grad</code> and <code>train_no_grad</code> functions, which are being replaced with <code>Trainer</code>. The user can transition as following. <pre><code>from qadence.ml_tools import train_with_grad\ntrain_with_grad(model=model, optimizer=optimizer, config=config, data = data)\n</code></pre> to <pre><code>from qadence.ml_tools import Trainer\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\ntrainer.fit(train_dataloader = data)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/trainer/#2-gradient-based-and-gradient-free-optimization","title":"2. Gradient-Based and Gradient-Free Optimization","text":"<p>The <code>Trainer</code> supports both gradient-based and gradient-free optimization methods. Default is gradient-based optimization.</p> <ul> <li>Gradient-Based Optimization: Utilizes optimizers from PyTorch's <code>torch.optim</code> module. This is the default behaviour of the <code>Trainer</code>, thus setting this is not necessary. However, it can be explicity mentioned as follows. Example of using gradient-based optimization:</li> </ul> <pre><code>from qadence.ml_tools import Trainer\n\n# set_use_grad(True) to enable gradient based training. This is the default behaviour of Trainer.\nTrainer.set_use_grad(True)\n</code></pre> <ul> <li>Gradient-Free Optimization: Employs optimization algorithms from the Nevergrad library.</li> </ul> <p>Example of using gradient-free optimization with Nevergrad:</p> <pre><code>from qadence.ml_tools import Trainer\n\n# set_use_grad(False) to disable gradient based training.\nTrainer.set_use_grad(False)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#using-context-managers-for-mixed-optimization","title":"Using Context Managers for Mixed Optimization","text":"<p>For cases requiring both optimization methods in a single training session, the <code>Trainer</code> class provides context managers to enable or disable gradients.</p> <pre><code># Temporarily switch to gradient-based optimization\nwith trainer.enable_grad_opt(optimizer):\n    print(\"Gradient Based Optimization\")\n    # trainer.fit(train_loader)\n\n# Switch to gradient-free optimization for specific steps\nwith trainer.disable_grad_opt(ng_optimizer):\n    print(\"Gradient Free Optimization\")\n    # trainer.fit(train_loader)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#3-custom-loss-functions","title":"3. Custom Loss Functions","text":"<p>Users can define custom loss functions tailored to their specific tasks. The <code>Trainer</code> accepts a <code>loss_fn</code> parameter, which should be a callable that takes the model and data as inputs and returns a tuple containing the loss tensor and a dictionary of metrics.</p> <p>Example of using a custom loss function:</p> <pre><code>import torch\nfrom itertools import count\ncnt = count()\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n</code></pre> <p>This custom loss function can be used in the trainer <pre><code>from qadence.ml_tools import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\ntrainer = Trainer(model=model, optimizer=optimizer, config=config, loss_fn=loss_fn)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/trainer/#4-hooks-for-custom-behavior","title":"4. Hooks for Custom Behavior","text":"<p>The <code>Trainer</code> class provides several hooks that enable users to inject custom behavior at different stages of the training process. These hooks are methods that can be overridden in a subclass to execute custom code. The available hooks include:</p> <ul> <li><code>on_train_start</code>: Called at the beginning of the training process.</li> <li><code>on_train_end</code>: Called at the end of the training process.</li> <li><code>on_train_epoch_start</code>: Called at the start of each training epoch.</li> <li><code>on_train_epoch_end</code>: Called at the end of each training epoch.</li> <li><code>on_train_batch_start</code>: Called at the start of each training batch.</li> <li><code>on_train_batch_end</code>: Called at the end of each training batch.</li> </ul> <p>Each \"start\" and \"end\" hook receives data and loss metrics as arguments. The specific values provided for these arguments depend on the training stage associated with the hook. The context of the training stage (e.g., training, validation, or testing) determines which metrics are relevant and how they are populated. For details of inputs on each hook, please review the documentation of <code>BaseTrainer</code>.</p> <pre><code>- Example of what inputs are provided to training hooks.\n\n    ```\n    def on_train_batch_start(self, batch: Tuple[torch.Tensor, ...] | None) -&gt; None:\n        \"\"\"\n        Called at the start of each training batch.\n\n        Args:\n            batch: A batch of data from the DataLoader. Typically a tuple containing\n                input tensors and corresponding target tensors.\n        \"\"\"\n        pass\n    ```\n    ```\n    def on_train_batch_end(self, train_batch_loss_metrics: Tuple[torch.Tensor, Any]) -&gt; None:\n        \"\"\"\n        Called at the end of each training batch.\n\n        Args:\n            train_batch_loss_metrics: Metrics for the training batch loss.\n                Tuple of (loss, metrics)\n        \"\"\"\n        pass\n    ```\n</code></pre> <p>Example of using a hook to log a message at the end of each epoch:</p> <pre><code>from qadence.ml_tools import Trainer\n\nclass CustomTrainer(Trainer):\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        print(f\"End of epoch - Loss and Metrics: {train_epoch_loss_metrics}\")\n</code></pre> <p>Notes: Trainer offers inbuilt callbacks as well. Callbacks are mainly for logging/tracking purposes, but the above mentioned hooks are generic. The workflow for every train batch looks like: 1. perform on_train_batch_start callbacks, 2. call the on_train_batch_start hook, 3. do the batch training, 4. call the on_train_batch_end hook, and 5. perform on_train_batch_end callbacks.</p> <p>The use of <code>on_</code>{phase}<code>_start</code> and <code>on_</code>{phase}<code>_end</code> hooks is not specifically to add extra callbacks, but for any other generic pre/post processing. For example, reshaping input batch in case of RNNs/LSTMs, post processing loss and adding an extra metric. They could also be used to add more callbacks (which is not recommended - as we provide methods to add extra callbacks in the TrainCofig)</p>"},{"location":"tutorials/qml/ml_tools/trainer/#5-experiment-tracking-with-tensorboard-and-mlflow","title":"5. Experiment Tracking with TensorBoard and MLflow","text":"<p>The <code>Trainer</code> integrates with TensorBoard and MLflow for experiment tracking:</p> <ul> <li> <p>TensorBoard: Logs metrics and visualizations during training, allowing users to monitor the training process.</p> </li> <li> <p>MLflow: Tracks experiments, logs parameters, metrics, and artifacts, and provides a user-friendly interface for comparing different runs.</p> </li> </ul> <p>To utilize these tracking tools, the <code>Trainer</code> can be configured with appropriate writers that handle the logging of metrics and other relevant information during training.</p> <p>Example of using TensorBoard tracking:</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.types import ExperimentTrackingTool\n\n# Set up tracking with TensorBoard\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.TENSORBOARD)\n</code></pre> <p>Example of using MLflow tracking:</p> <pre><code>from qadence.types import ExperimentTrackingTool\n\n# Set up tracking with MLflow\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.MLFLOW)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#6-examples","title":"6. Examples","text":""},{"location":"tutorials/qml/ml_tools/trainer/#61-training-with-trainer-and-trainconfig","title":"6.1. Training with <code>Trainer</code> and <code>TrainConfig</code>","text":""},{"location":"tutorials/qml/ml_tools/trainer/#setup","title":"Setup","text":"<p>Let's do the necessary imports and declare a <code>DataLoader</code>. We can already define some hyperparameters here, including the seed for random number generators. mlflow can log hyperparameters with arbitrary types, for example the observable that we want to monitor (<code>Z</code> in this case, which has a <code>qadence.Operation</code> type).</p> <pre><code>import random\nfrom itertools import count\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom matplotlib.figure import Figure\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\n\nfrom qadence import hea, QuantumCircuit, Z\nfrom qadence.constructors import feature_map, hamiltonian_factory\nfrom qadence.ml_tools import Trainer, TrainConfig\nfrom qadence.ml_tools.data import to_dataloader\nfrom qadence.ml_tools.utils import rand_featureparameters\nfrom qadence.ml_tools.models import QNN, QuantumModel\nfrom qadence.types import ExperimentTrackingTool\n\nhyperparams = {\n    \"seed\": 42,\n    \"batch_size\": 10,\n    \"n_qubits\": 2,\n    \"ansatz_depth\": 1,\n    \"observable\": Z,\n}\n\nnp.random.seed(hyperparams[\"seed\"])\ntorch.manual_seed(hyperparams[\"seed\"])\nrandom.seed(hyperparams[\"seed\"])\n\n\ndef dataloader(batch_size: int = 25) -&gt; DataLoader:\n    x = torch.linspace(0, 1, batch_size).reshape(-1, 1)\n    y = torch.cos(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=True)\n</code></pre> <p>We continue with the regular QNN definition, together with the loss function and optimizer.</p> <pre><code>obs = hamiltonian_factory(register=hyperparams[\"n_qubits\"], detuning=hyperparams[\"observable\"])\n\ndata = dataloader(hyperparams[\"batch_size\"])\nfm = feature_map(hyperparams[\"n_qubits\"], param=\"x\")\n\nmodel = QNN(\n    QuantumCircuit(\n        hyperparams[\"n_qubits\"], fm, hea(hyperparams[\"n_qubits\"], hyperparams[\"ansatz_depth\"])\n    ),\n    observable=obs,\n    inputs=[\"x\"],\n)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ninputs = rand_featureparameters(model, 1)\n\ndef loss_fn(model: QuantumModel, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    out = model.expectation(inputs)\n    loss = criterion(out, torch.rand(1))\n    return loss, {}\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#trainconfig-specifications","title":"<code>TrainConfig</code> specifications","text":"<p>Qadence offers different tracking options via <code>TrainConfig</code>. Here we use the <code>ExperimentTrackingTool</code> type to specify that we want to track the experiment with mlflow. Tracking with tensorboard is also possible. We can then indicate what and how often we want to track or log.</p> <p>For Training <code>write_every</code> controls the number of epochs after which the loss values is logged. Thanks to the <code>plotting_functions</code> and <code>plot_every</code>arguments, we are also able to plot model-related quantities throughout training. Notice that arbitrary plotting functions can be passed, as long as the signature is the same as <code>plot_fn</code> below. Finally, the trained model can be logged by setting <code>log_model=True</code>. Here is an example of plotting function and training configuration</p> <pre><code>def plot_fn(model: Module, iteration: int) -&gt; tuple[str, Figure]:\n    descr = f\"ufa_prediction_epoch_{iteration}.png\"\n    fig, ax = plt.subplots()\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    out = model.expectation(x)\n    ax.plot(x.detach().numpy(), out.detach().numpy())\n    return descr, fig\n\n\nconfig = TrainConfig(\n    root_folder=\"mlflow_demonstration\",\n    max_iter=10,\n    checkpoint_every=1,\n    plot_every=2,\n    write_every=1,\n    log_model=True,\n    tracking_tool=ExperimentTrackingTool.MLFLOW,\n    hyperparams=hyperparams,\n    plotting_functions=(plot_fn,),\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#training-and-inspecting","title":"Training and inspecting","text":"<p>Model training happens as usual <pre><code>trainer = Trainer(model, optimizer, config, loss_fn)\ntrainer.fit(train_dataloader=data)\n</code></pre></p> <p>After training , we can inspect our experiment via the mlflow UI <pre><code>mlflow ui --port 8080 --backend-store-uri sqlite:///mlruns.db\n</code></pre> In this case, since we're running on a local server, we can access the mlflow UI by navigating to http://localhost:8080/.</p>"},{"location":"tutorials/qml/ml_tools/trainer/#62-fitting-a-function-with-a-qnn-using-ml_tools","title":"6.2. Fitting a function with a QNN using <code>ml_tools</code>","text":"<p>In Quantum Machine Learning, the general consensus is to use <code>complex128</code> precision for states and operators and <code>float64</code> precision for parameters. This is also the convention which is used in <code>qadence</code>. However, for specific usecases, lower precision can greatly speed up training and reduce memory consumption. When using the <code>pyqtorch</code> backend, <code>qadence</code> offers the option to move a <code>QuantumModel</code> instance to a specific precision using the torch <code>to</code> syntax.</p> <p>Let's look at a complete example of how to use <code>Trainer</code> now. Here we perform a validation check during training and use a validation criterion that checks whether the validation loss in the current iteration has decreased compared to the lowest validation loss from all previous iterations. For demonstration, the train and the validation data are kept the same here. However, it is beneficial and encouraged to keep them distinct in practice to understand model's generalization capabilities.</p> <pre><code>from pathlib import Path\nimport torch\nfrom functools import reduce\nfrom operator import add\nfrom itertools import count\nimport matplotlib.pyplot as plt\n\nfrom qadence import Parameter, QuantumCircuit, Z\nfrom qadence import hamiltonian_factory, hea, feature_map, chain\nfrom qadence import QNN\nfrom qadence.ml_tools import  TrainConfig, Trainer, to_dataloader\n\nTrainer.set_use_grad(True)\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDTYPE = torch.complex64\nn_qubits = 4\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 100\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data[0], data[1]\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n\ndef validation_criterion(\n    current_validation_loss: float, current_best_validation_loss: float, val_epsilon: float\n) -&gt; bool:\n    return current_validation_loss &lt;= current_best_validation_loss - val_epsilon\n\nn_epochs = 300\n\nconfig = TrainConfig(\n    max_iter=n_epochs,\n    batch_size=batch_size,\n    checkpoint_best_only=True,\n    val_every=10,  # The model will be run on the validation data after every `val_every` epochs.\n    validation_criterion=validation_criterion\n)\n\nfn = lambda x, degree: .05 * reduce(add, (torch.cos(i*x) + torch.sin(i*x) for i in range(degree)), 0.)\nx = torch.linspace(0, 10, batch_size, dtype=torch.float32).reshape(-1, 1)\ny = fn(x, 5)\n\ntrain_dataloader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_dataloader =  to_dataloader(x, y, batch_size=batch_size, infinite=True)\n\ntrainer = Trainer(model, optimizer, config, loss_fn=loss_fn,\n                    train_dataloader = train_dataloader, val_dataloader = val_dataloader,\n                    device=DEVICE, dtype=DTYPE)\ntrainer.fit()\n\nplt.clf()\nplt.plot(x.numpy(), y.numpy(), label='truth')\nplt.plot(x.numpy(), model(x).detach().numpy(), \"--\", label=\"final\", linewidth=3)\nplt.legend()\n</code></pre> 2025-02-07T10:22:57.360783 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/ml_tools/trainer/#63-fitting-a-function-low-level-api","title":"6.3. Fitting a function - Low-level API","text":"<p>For users who want to use the low-level API of <code>qadence</code>, here an example written without <code>Trainer</code>.</p> <pre><code>from pathlib import Path\nimport torch\nfrom itertools import count\nfrom qadence.constructors import hamiltonian_factory, hea, feature_map\nfrom qadence import chain, Parameter, QuantumCircuit, Z\nfrom qadence import QNN\nfrom qadence.ml_tools import TrainConfig\n\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 1\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nn_epochs=50\ncnt = count()\n\ntmp_path = Path(\"/tmp\")\n\nconfig = TrainConfig(\n    root_folder=tmp_path,\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n)\n\nx = torch.linspace(0, 1, batch_size).reshape(-1, 1)\ny = torch.sin(x)\n\nfor i in range(n_epochs):\n    out = model(x)\n    loss = criterion(out, y)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#64-custom-train-loop","title":"6.4. Custom <code>train</code> loop","text":"<p>If you need custom training functionality that goes beyond what is available in <code>qadence.ml_tools.Trainer</code> you can write your own training loop based on the building blocks that are available in Qadence.</p> <p>A simplified version of Qadence's train loop is defined below. Feel free to copy it and modify at will.</p> <p>For logging we can use the <code>get_writer</code> from the <code>Writer Registry</code>. This will set up the default writer based on the experiment tracking tool. All writers from the <code>Writer Registry</code> offer <code>open</code>, <code>close</code>, <code>print_metrics</code>, <code>write_metrics</code>, <code>plot_metrics</code>, etc methods.</p> <pre><code>from typing import Callable, Union\n\nfrom torch.nn import Module\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom qadence.ml_tools.config import TrainConfig\nfrom qadence.ml_tools.data import DictDataLoader, data_to_device\nfrom qadence.ml_tools.optimize_step import optimize_step\nfrom qadence.ml_tools.callbacks import get_writer\nfrom qadence.ml_tools.callbacks.saveload import load_checkpoint, write_checkpoint\n\n\ndef train(\n    model: Module,\n    data: DataLoader,\n    optimizer: Optimizer,\n    config: TrainConfig,\n    loss_fn: Callable,\n    device: str = \"cpu\",\n    optimize_step: Callable = optimize_step,\n    write_tensorboard: Callable = write_tensorboard,\n) -&gt; tuple[Module, Optimizer]:\n\n    # Move model to device before optimizer is loaded\n    model = model.to(device)\n\n    # load available checkpoint\n    init_iter = 0\n    if config.log_folder:\n        model, optimizer, init_iter = load_checkpoint(config.log_folder, model, optimizer)\n\n    # Initialize writer based on the tracking tool specified in the configuration\n    writer = get_writer(config.tracking_tool)  # Uses ExperimentTrackingTool to select writer\n    writer.open(config, iteration=init_iter)\n\n    dl_iter = iter(dataloader)\n\n    # outer epoch loop\n    for iteration in range(init_iter, init_iter + config.max_iter):\n        data = data_to_device(next(dl_iter), device)\n        loss, metrics = optimize_step(model, optimizer, loss_fn, data)\n\n        if iteration % config.print_every == 0 and config.verbose:\n            writer.print_metrics(OptimizeResult(iteration, model, optimizer, loss, metrics))\n\n        if iteration % config.write_every == 0:\n            writer.write(iteration, metrics)\n\n        if config.log_folder:\n            if iteration % config.checkpoint_every == 0:\n                write_checkpoint(config.log_folder, model, optimizer, iteration)\n\n    # Final writing and checkpointing\n    if config.log_folder:\n        write_checkpoint(config.log_folder, model, optimizer, iteration)\n    writer.write(iteration,metrics)\n    writer.close()\n\n    return model, optimizer\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#65-gradient-free-optimization-using-trainer","title":"6.5. Gradient-free optimization using <code>Trainer</code>","text":"<p>We can achieve gradient free optimization with <code>Trainer.set_use_grad(False)</code> or <code>trainer.disable_grad_opt(ng_optimizer)</code>. An example solving a QUBO using gradient free optimization based on <code>Nevergrad</code> optimizers and <code>Trainer</code> is shown in the analog QUBO Tutorial.</p>"},{"location":"tutorials/realistic_sims/","title":"Realistic simulations","text":"<p>This section describes how to perform realistic simulations in Qadence.</p>"},{"location":"tutorials/realistic_sims/measurements/","title":"Measurement protocols","text":"<p>Sample-based measurement protocols are fundamental tools for the prediction and estimation of a quantum state as the result of NISQ programs executions. Their resource efficient implementation is a current and active research field. Qadence offers two main measurement protocols: quantum state tomography and classical shadows.</p>"},{"location":"tutorials/realistic_sims/measurements/#quantum-state-tomography","title":"Quantum state tomography","text":"<p>The fundamental task of quantum state tomography is to learn an approximate classical description of an output quantum state described by a density matrix \\(\\rho\\), from repeated measurements of copies on a chosen basis. To do so, \\(\\rho\\) is expanded in a basis of observables (the tomography step) and for a given observable \\(\\hat{\\mathcal{O}}\\), the expectation value is calculated with \\(\\langle \\hat{\\mathcal{O}} \\rangle=\\textrm{Tr}(\\hat{\\mathcal{O}}\\rho)\\). A number of measurement repetitions in a suitable basis is then required to estimate \\(\\langle \\hat{\\mathcal{O}} \\rangle\\).</p> <p>The main drawback is the scaling in measurements for the retrieval of the classical expression for a \\(n\\)-qubit quantum state as \\(2^n \\times 2^n\\), together with a large amount of classical post-processing.</p> <p>For an observable expressed as a Pauli string \\(\\hat{\\mathcal{P}}\\), the expectation value for a state \\(|\\psi \\rangle\\) can be derived as:</p> \\[ \\langle \\hat{\\mathcal{P}} \\rangle=\\langle \\psi | \\hat{\\mathcal{P}} |\\psi \\rangle=\\langle \\psi | \\hat{\\mathcal{R}}^\\dagger \\hat{\\mathcal{D}} \\hat{\\mathcal{R}} |\\psi \\rangle \\] <p>The operator \\(\\hat{\\mathcal{R}}\\) diagonalizes \\(\\hat{\\mathcal{P}}\\) and rotates the state into an eigenstate in the computational basis. Therefore, \\(\\hat{\\mathcal{R}}|\\psi \\rangle=\\sum\\limits_{z}a_z|z\\rangle\\) and the expectation value can finally be expressed as:</p> \\[ \\langle \\hat{\\mathcal{P}} \\rangle=\\sum_{z,z'}\\langle z |\\bar{a}_z\\hat{\\mathcal{D}}a_{z'}|z'\\rangle = \\sum_{z}|a_z|^2(-1)^{\\phi_z(\\hat{\\mathcal{P}})} \\] <p>In Qadence, running a tomographical experiment is made simple by defining a <code>Measurements</code> object that captures all options for execution:</p> <pre><code>from torch import tensor\nfrom qadence import hamiltonian_factory, BackendName, DiffMode\nfrom qadence import Parameter, chain, kron, RX, RY, Z, QuantumCircuit, QuantumModel\nfrom qadence.measurements import Measurements\n\n# Define parameters for a circuit.\ntheta1 = Parameter(\"theta1\", trainable=False)\ntheta2 = Parameter(\"theta2\", trainable=False)\ntheta3 = Parameter(\"theta3\", trainable=False)\ntheta4 = Parameter(\"theta4\", trainable=False)\n\nblocks = chain(\n    kron(RX(0, theta1), RY(1, theta2)),\n    kron(RX(0, theta3), RY(1, theta4)),\n)\n\nvalues = {\n    \"theta1\": tensor([0.5]),\n    \"theta2\": tensor([1.5]),\n    \"theta3\": tensor([2.0]),\n    \"theta4\": tensor([2.5]),\n}\n\n# Create a circuit and an observable.\ncircuit = QuantumCircuit(2, blocks)\nobservable = hamiltonian_factory(2, detuning=Z)\n\n# Create a model.\nmodel = QuantumModel(\n    circuit=circuit,\n    observable=observable,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.GPSR,\n)\n\n# Define a measurement protocol by passing the shot budget as an option.\ntomo_options = {\"n_shots\": 100000}\ntomo_measurement = Measurements(protocol=Measurements.TOMOGRAPHY, options=tomo_options)\n\n# Get the exact expectation value.\nexact_values = model.expectation(\n    values=values,\n)\n\n# Run the tomography experiment.\nestimated_values_tomo = model.expectation(\n    values=values,\n    measurement=tomo_measurement,\n)\n</code></pre> <pre><code>Exact expectation value = tensor([[-1.4548]])\nEstimated expectation value tomo = tensor([[-1.4566]])\n</code></pre>"},{"location":"tutorials/realistic_sims/measurements/#classical-shadows","title":"Classical shadows","text":"<p>Recently, a much less resource demanding protocol based on classical shadows has been proposed<sup>1</sup>. It combines ideas from shadow tomography<sup>2</sup> and randomized measurement protocols capable of learning a classical shadow of an unknown quantum state \\(\\rho\\). It relies on deliberately discarding the full classical characterization of the quantum state, and instead focuses on accurately predicting a restricted set of properties that provide efficient protocols for the study of the system.</p> <p>A random measurement consists of applying random unitary rotations before a fixed measurement on each copy of a state. Appropriately averaging over these measurements produces an efficient estimator for the expectation value of an observable. This protocol therefore creates a robust classical representation of the quantum state or classical shadow. The captured measurement information is then reuseable for multiple purposes, i.e. any observable expected value and available for noise mitigation postprocessing.</p> <p>A classical shadow is therefore an unbiased estimator of a quantum state \\(\\rho\\). Such an estimator is obtained with the following procedure<sup>1</sup>: first, apply a random unitary gate \\(U\\) to rotate the state: \\(\\rho \\rightarrow U \\rho U^\\dagger\\) and then perform a basis measurement to obtain a \\(n\\)-bit measurement \\(|\\hat{b}\\rangle \\in \\{0, 1\\}^n\\). Both unitary gates \\(U\\) and the measurement outcomes \\(|\\hat{b}\\rangle\\) are stored on a classical computer for postprocessing v \\(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U\\), a classical snapshot of the state \\(\\rho\\). The whole procedure can be seen as a quantum channel \\(\\mathcal{M}\\) that maps the initial unknown quantum state \\(\\rho\\) to the average result of the measurement protocol:</p> \\[ \\mathbb{E}[U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U] = \\mathcal{M}(\\rho) \\Rightarrow \\rho = \\mathbb{E}[\\mathcal{M}^{-1}(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U)] \\] <p>It is worth noting that the single classical snapshot \\(\\hat{\\rho}=\\mathcal{M}^{-1}(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U)\\) equals \\(\\rho\\) in expectation: \\(\\mathbb{E}[\\hat{\\rho}]=\\rho\\) despite \\(\\mathcal{M}^{-1}\\) not being a completely positive map. Repeating this procedure \\(N\\) times results in an array of \\(N\\) independent, classical snapshots of \\(\\rho\\) called the classical shadow:</p> \\[ S(\\rho, N) = \\{ \\hat{\\rho}_1=\\mathcal{M}^{-1}(U_1^\\dagger |\\hat{b}_1\\rangle\\langle \\hat{b}_1|U_1),\\cdots,\\hat{\\rho}_N=\\mathcal{M}^{-1}(U_N^\\dagger |\\hat{b}_N\\rangle\\langle \\hat{b}_N|U_N)\\} \\] <p>Along the same lines as the example before, estimating the expectation value using classical shadows in Qadence only requires to pass the right set of parameters to the <code>Measurements</code> object:</p> <pre><code># Classical shadows are defined up to some accuracy and confidence.\nshadow_options = {\"accuracy\": 0.1, \"confidence\": 0.1}  # Shadow size N=54400.\nshadow_measurement = Measurements(protocol=Measurements.SHADOW, options=shadow_options)\n\n# Run the experiment with classical shadows.\nestimated_values_shadow = model.expectation(\n    values=values,\n    measurement=shadow_measurement,\n)\n</code></pre> <pre><code>Estimated expectation value shadow = tensor([[-1.4572]])\n</code></pre>"},{"location":"tutorials/realistic_sims/measurements/#references","title":"References","text":"<ol> <li> <p>Hsin-Yuan Huang, Richard Kueng and John Preskill, Predicting Many Properties of a Quantum System from Very Few Measurements (2020) \u21a9\u21a9</p> </li> <li> <p>S. Aaronson. Shadow tomography of quantum states. In Proceedings of the 50th Annual A ACM SIGACT Symposium on Theory of Computing, STOC 2018, pages 325\u2013338, New York, NY, USA, 2018. ACM\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/realistic_sims/mitigation/","title":"Error mitigation","text":"<p>Beyond running noisy simulations, Qadence offers a number of noise mitigation techniques to achieve better accuracy of simulation outputs. Currently, mitigation addresses readout errors and depolarizing and dephasing noise for analog blocks.</p>"},{"location":"tutorials/realistic_sims/mitigation/#readout-error-mitigation","title":"Readout error mitigation","text":"<p>The complete implementation of the mitigation technique is to measure \\(T\\) and classically apply \\(T^{\u22121}\\) to measured probability distributions. However there are several limitations of this approach:</p> <ul> <li>The complete implementation requires \\(2^n\\) characterization experiments (probability measurements), which is not scalable. The classical processing of the calibration data is also inefficient.</li> <li>The matrix \\(T\\) may become singular for large \\(n\\), preventing direct inversion.</li> <li>The inverse \\(T^{\u22121}\\) might not be a stochastic matrix, meaning that it can produce negative corrected probabilities.</li> <li>The correction is not rigorously justified, so we cannot be sure that we are only removing SPAM errors and not otherwise corrupting an estimated probability distribution.</li> </ul> <p>Qadence relies on the assumption of uncorrelated readout errors:</p> \\[ T=T_1\\otimes T_2\\otimes \\dots \\otimes T_n \\] <p>for which the inversion is straightforward:</p> \\[ T^{-1}=T_1^{-1}\\otimes T_2^{-1}\\otimes \\dots \\otimes T_n^{-1} \\] <p>However, even for a reduced \\(n\\) the third limitation holds. This can be avoided by reformulating into a minimization problem<sup>1</sup>:</p> \\[ \\lVert Tp_{\\textrm{corr}}-p_{\\textrm{raw}}\\rVert_{2}^{2} \\] <p>subjected to physicality constraints \\(0 \\leq p_{corr}(x) \\leq 1\\) and \\(\\lVert p_{corr} \\rVert = 1\\). At this point, two methods are implemented to solve this problem. The first one relies on solving using standard optimization tools, the second on Maximum-Likelihood Estimation<sup>2</sup>. In Qadence, this can be user defined using the mitigation protocol:</p> <pre><code>from qadence import QuantumModel, QuantumCircuit, kron, H, Z\nfrom qadence import hamiltonian_factory\nfrom qadence.noise import NoiseHandler\nfrom qadence.mitigations import Mitigations\nfrom qadence.types import ReadOutOptimization, NoiseProtocol\n\n# Simple circuit and observable construction.\nblock = kron(H(0), Z(1))\ncircuit = QuantumCircuit(2, block)\nobservable = hamiltonian_factory(circuit.n_qubits, detuning=Z)\n\n# Construct a quantum model.\nmodel = QuantumModel(circuit=circuit, observable=observable)\n\n# Define a noise model to use:\nnoise = NoiseHandler(NoiseProtocol.READOUT.INDEPENDENT)\n# Define the mitigation method solving the minimization problem:\noptions={\"optimization_type\": ReadOutOptimization.CONSTRAINED}  # ReadOutOptimization.MLE for the alternative method.\nmitigation = Mitigations(protocol=Mitigations.READOUT, options=options)\n\n# Run noiseless, noisy and mitigated simulations.\nn_shots = 100\nnoiseless_samples = model.sample(n_shots=n_shots)\nnoisy_samples = model.sample(noise=noise, n_shots=n_shots)\nmitigated_samples = model.sample(\n    noise=noise, mitigation=mitigation, n_shots=n_shots\n)\n\nprint(f\"noiseless {noiseless_samples}\")\nprint(f\"noisy {noisy_samples}\")\nprint(f\"mitigated {mitigated_samples}\")\n</code></pre> <pre><code>noiseless [OrderedCounter({'10': 53, '00': 47})]\nnoisy [OrderedCounter({'10': 60, '00': 39, '01': 1})]\nmitigated [Counter({'11': 61, '10': 31, '01': 8})]\n</code></pre>"},{"location":"tutorials/realistic_sims/mitigation/#wip-zero-noise-extrapolation-for-analog-blocks","title":"[WIP] Zero-noise extrapolation for analog blocks","text":"<p>Zero-noise extrapolation (ZNE) is an error mitigation technique in which an expectation value is computed at different noise levels and, as a second step, the ideal expectation value is inferred by extrapolating the measured results to the zero-noise limit. In digital computing, this is typically implemented by \"folding\" the circuit and its dagger to artificially increase the noise through sequences of identities<sup>3</sup>. In the analog ZNE variation, analog blocks are time stretched to again artificially increase noise<sup>3</sup>.</p>"},{"location":"tutorials/realistic_sims/mitigation/#references","title":"References","text":"<ol> <li> <p>Michael R. Geller and Mingyu Sun, Efficient correction of multiqubit measurement errors, (2020) \u21a9</p> </li> <li> <p>Smolin et al., Maximum Likelihood, Minimum Effort, (2011) \u21a9</p> </li> <li> <p>Mitiq: What's the theory behind ZNE? \u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/realistic_sims/noise/","title":"Simulated errors","text":"<p>Running programs on NISQ devices often leads to partially useful results due to the presence of noise. In order to perform realistic simulations, a number of noise models (for digital operations, analog operations and simulated readout errors) are supported in Qadence through their implementation in backends and corresponding error mitigation techniques whenever possible.</p>"},{"location":"tutorials/realistic_sims/noise/#noisehandler","title":"NoiseHandler","text":"<p>Noise models can be defined via the <code>NoiseHandler</code>. It is a container of several noise instances which require to specify a <code>protocols</code> and a dictionary of <code>options</code> (or lists). The <code>protocol</code> field is to be instantiated from <code>NoiseProtocol</code>.</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\nanalog_noise = NoiseHandler(protocol=NoiseProtocol.ANALOG.DEPOLARIZING, options={\"noise_probs\": 0.1})\ndigital_noise = NoiseHandler(protocol=NoiseProtocol.DIGITAL.DEPOLARIZING, options={\"error_probability\": 0.1})\nreadout_noise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options={\"error_probability\": 0.1, \"seed\": 0})\n</code></pre> <pre><code>\n</code></pre> <p>One can also define a <code>NoiseHandler</code> passing a list of protocols and a list of options (careful with the order):</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\nprotocols = [NoiseProtocol.DIGITAL.DEPOLARIZING, NoiseProtocol.READOUT]\noptions = [{\"error_probability\": 0.1}, {\"error_probability\": 0.1, \"seed\": 0}]\n\nnoise_combination = NoiseHandler(protocols, options)\nprint(noise_combination)\n</code></pre> <pre><code>Noise(Depolarizing, {'error_probability': 0.1})\nNoise(&lt;enum 'ReadoutNoise'&gt;, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>One can also append to a <code>NoiseHandler</code> other <code>NoiseHandler</code> instances:</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\ndepo_noise = NoiseHandler(protocol=NoiseProtocol.DIGITAL.DEPOLARIZING, options={\"error_probability\": 0.1})\nreadout_noise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options={\"error_probability\": 0.1, \"seed\": 0})\n\nnoise_combination = NoiseHandler(protocol=NoiseProtocol.DIGITAL.BITFLIP, options={\"error_probability\": 0.1})\nnoise_combination.append([depo_noise, readout_noise])\nprint(noise_combination)\n</code></pre> <pre><code>Noise(BitFlip, {'error_probability': 0.1})\nNoise(Depolarizing, {'error_probability': 0.1})\nNoise(Independent Readout, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>Finally, one can add directly a few pre-defined types using several <code>NoiseHandler</code> methods:</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\nnoise_combination = NoiseHandler(protocol=NoiseProtocol.DIGITAL.BITFLIP, options={\"error_probability\": 0.1})\nnoise_combination.digital_depolarizing({\"error_probability\": 0.1}).readout_independent({\"error_probability\": 0.1, \"seed\": 0})\nprint(noise_combination)\n</code></pre> <pre><code>Noise(BitFlip, {'error_probability': 0.1})\nNoise(Depolarizing, {'error_probability': 0.1})\nNoise(Independent Readout, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>NoiseHandler scope</p> <p>Note it is not possible to define a <code>NoiseHandler</code> instances with both digital and analog noises, both readout and analog noises, several analog noises, several readout noises, or a readout noise that is not the last defined protocol within <code>NoiseHandler</code>.</p>"},{"location":"tutorials/realistic_sims/noise/#readout-errors","title":"Readout errors","text":"<p>State Preparation and Measurement (SPAM) in the hardware is a major source of noise in the execution of quantum programs. They are typically described using confusion matrices of the form:</p> \\[ T(x|x')=\\delta_{xx'} \\] <p>Two types of readout protocols are available: - <code>NoiseProtocol.READOUT.INDEPENDENT</code> where each bit can be corrupted independently of each other. - <code>NoiseProtocol.READOUT.CORRELATED</code> where we can define of confusion matrix of corruption between each possible bitstrings.</p> <p>Qadence offers to simulate readout errors with the <code>NoiseHandler</code> to corrupt the output samples of a simulation, through execution via a <code>QuantumModel</code>:</p> <pre><code>from qadence import QuantumModel, QuantumCircuit, kron, H, Z\nfrom qadence import hamiltonian_factory\n\n# Simple circuit and observable construction.\nblock = kron(H(0), Z(1))\ncircuit = QuantumCircuit(2, block)\nobservable = hamiltonian_factory(circuit.n_qubits, detuning=Z)\n\n# Construct a quantum model.\nmodel = QuantumModel(circuit=circuit, observable=observable)\n\n# Define a noise model to use.\nnoise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT)\n\n# Run noiseless and noisy simulations.\nnoiseless_samples = model.sample(n_shots=100)\nnoisy_samples = model.sample(noise=noise, n_shots=100)\n</code></pre> <pre><code>noiseless = [OrderedCounter({'10': 53, '00': 47})]\nnoisy = [OrderedCounter({'10': 52, '00': 40, '01': 5, '11': 3})]\n</code></pre> <p>It is possible to pass options to the noise model. In the previous example, a noise matrix is implicitly computed from a uniform distribution.</p> <p>For <code>NoiseProtocol.READOUT.INDEPENDENT</code>, the <code>option</code> dictionary argument accepts the following options:</p> <ul> <li><code>seed</code>: defaulted to <code>None</code>, for reproducibility purposes</li> <li><code>error_probability</code>: If float, the same probability is applied to every bit. By default, this is 0.1.     If a 1D tensor with the number of elements equal to the number of qubits, a different probability can be set for each qubit. If a tensor of shape (n_qubits, 2, 2) is passed, that is a confusion matrix obtained from experiments, we extract the error_probability.     and do not compute internally the confusion matrix as in the other cases.</li> <li><code>noise_distribution</code>: defaulted to <code>WhiteNoise.UNIFORM</code>, for non-uniform noise distributions</li> </ul> <p>For <code>NoiseProtocol.READOUT.CORRELATED</code>, the <code>option</code> dictionary argument accepts the following options: - <code>confusion_matrix</code>: The square matrix representing \\(T(x|x')\\) for each possible bitstring of length <code>n</code> qubits. Should be of size (2n, 2n). - <code>seed</code>: defaulted to <code>None</code>, for reproducibility purposes</p> <p>Noisy simulations go hand-in-hand with measurement protocols discussed in the previous section, to assess the impact of noise on expectation values. In this case, both measurement and noise protocols have to be defined appropriately. Please note that a noise protocol without a measurement protocol will be ignored for expectation values computations.</p> <pre><code>from qadence.measurements import Measurements\n\n# Define a noise model with options.\noptions = {\"error_probability\": 0.01}\nnoise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options=options)\n\n# Define a tomographical measurement protocol with options.\noptions = {\"n_shots\": 10000}\nmeasurement = Measurements(protocol=Measurements.TOMOGRAPHY, options=options)\n\n# Run noiseless and noisy simulations.\nnoiseless_exp = model.expectation(measurement=measurement)\nnoisy_exp = model.expectation(measurement=measurement, noise=noise)\n</code></pre> <pre><code>noiseless = tensor([[0.9998]], grad_fn=&lt;TransposeBackward0&gt;)\nnoisy = tensor([[0.9972]], grad_fn=&lt;TransposeBackward0&gt;)\n</code></pre>"},{"location":"tutorials/realistic_sims/noise/#analog-noisy-simulation","title":"Analog noisy simulation","text":"<p>At the moment, analog noisy simulations are only compatable with the Pulser backend. <pre><code>from qadence import DiffMode, NoiseHandler, QuantumModel\nfrom qadence.blocks import chain, kron\nfrom qadence.circuit import QuantumCircuit\nfrom qadence.operations import AnalogRX, AnalogRZ, Z\nfrom qadence.types import PI, BackendName, NoiseProtocol\n\n\nanalog_block = chain(AnalogRX(PI / 2.0), AnalogRZ(PI))\nobservable = Z(0) + Z(1)\ncircuit = QuantumCircuit(2, analog_block)\n\noptions = {\"noise_probs\": 0.1}\nnoise = NoiseHandler(protocol=NoiseProtocol.ANALOG.DEPOLARIZING, options=options)\nmodel_noisy = QuantumModel(\n    circuit=circuit,\n    observable=observable,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR,\n    noise=noise,\n)\nnoisy_expectation = model_noisy.expectation()\n</code></pre> <pre><code>noisy = tensor([[0.3597]])\n</code></pre> </p>"},{"location":"tutorials/realistic_sims/noise/#digital-noisy-simulation","title":"Digital noisy simulation","text":"<p>When dealing with programs involving only digital operations, several options are made available from PyQTorch via the <code>NoiseProtocol.DIGITAL</code>. One can define noisy digital operations as follows:</p> <pre><code>from qadence import NoiseProtocol, RX, run\nimport torch\n\nnoise = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, {\"error_probability\": 0.2})\nop = RX(0, torch.pi, noise = noise)\n\nprint(run(op))\n</code></pre> <pre><code>DensityMatrix([[[0.2000+0.0000e+00j, 0.0000+3.6739e-17j],\n                [0.0000-3.6739e-17j, 0.8000+0.0000e+00j]]])\n</code></pre> <p>It is also possible to set a noise configuration to gates within a composite block or circuit as follows:</p> <pre><code>from qadence import set_noise, chain\n\nn_qubits = 2\n\nblock = chain(RX(i, f\"theta_{i}\") for i in range(n_qubits))\n\nnoise = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, {\"error_probability\": 0.1})\n\n# The function changes the block in place:\nset_noise(block, noise)\nprint(run(block))\n</code></pre> <pre><code>DensityMatrix([[[ 0.6710+0.0000j,  0.0000+0.1450j,  0.0000+0.2399j,\n                 -0.0518+0.0000j],\n                [ 0.0000-0.1450j,  0.1140+0.0000j,  0.0518+0.0000j,\n                  0.0000+0.0407j],\n                [ 0.0000-0.2399j,  0.0518+0.0000j,  0.1838+0.0000j,\n                  0.0000+0.0397j],\n                [-0.0518+0.0000j,  0.0000-0.0407j,  0.0000-0.0397j,\n                  0.0312+0.0000j]]])\n</code></pre> <p>There is an extra optional argument to specify the type of block we want to apply noise to. E.g., let's say we want to apply noise only to <code>X</code> gates, a <code>target_class</code> argument can be passed with the corresponding block:</p> <pre><code>from qadence import X\nblock = chain(RX(0, \"theta\"), X(0))\nset_noise(block, noise, target_class = X)\n\nfor block in block.blocks:\n    print(block.noise)\n</code></pre> <pre><code>None\nNoise(BitFlip, {'error_probability': 0.1})\n</code></pre>"}]}