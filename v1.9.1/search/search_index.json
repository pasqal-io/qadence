{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/blocks/","title":"Block system","text":"<p><code>qadence</code> offers a block-based system to construct quantum circuits in a flexible manner.</p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock","title":"<code>AbstractBlock(tag=None, __array_priority__=1000)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for both primitive and composite blocks.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>A human-readable name attached to the block type. Notice, this is the same for all the class instances so it cannot be used for identifying different blocks</p> <p> TYPE: <code>str</code> </p> <code>qubit_support</code> <p>The qubit support of the block expressed as a tuple of integers</p> <p> TYPE: <code>tuple[int, ...]</code> </p> <code>tag</code> <p>A tag identifying a particular instance of the block which can be used for identification and pretty printing</p> <p> TYPE: <code>str | None</code> </p> <code>eigenvalues</code> <p>The eigenvalues of the matrix representing the block. This is used mainly for primitive blocks and it's needed for generalized parameter shift rule computations. Currently unused.</p> <p> TYPE: <code>list[float] | None</code> </p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.is_identity","title":"<code>is_identity: bool</code>  <code>property</code>","text":"<p>Identity predicate for blocks.</p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.n_qubits","title":"<code>n_qubits()</code>","text":"<p>The number of qubits in the whole system.</p> <p>A block acting on qubit N would has at least n_qubits &gt;= N + 1.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef n_qubits(self) -&gt; int:\n    \"\"\"The number of qubits in the whole system.\n\n    A block acting on qubit N would has at least n_qubits &gt;= N + 1.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.n_supports","title":"<code>n_supports()</code>","text":"<p>The number of qubits the block is acting on.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef n_supports(self) -&gt; int:\n    \"\"\"The number of qubits the block is acting on.\"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.qubit_support","title":"<code>qubit_support()</code>","text":"<p>The indices of the qubit(s) the block is acting on.</p> <p>Qadence uses the ordering [0..,N-1] for qubits.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef qubit_support(self) -&gt; Tuple[int, ...]:\n    \"\"\"The indices of the qubit(s) the block is acting on.\n\n    Qadence uses the ordering [0..,N-1] for qubits.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#primitive-blocks","title":"Primitive blocks","text":""},{"location":"api/blocks/#qadence.blocks.primitive.ControlBlock","title":"<code>ControlBlock(control, target_block, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The abstract ControlBlock.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: tuple[int, ...],\n    target_block: PrimitiveBlock,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.control = control\n    self.blocks = (target_block,)\n    self.target = target_block.qubit_support\n\n    # using tuple expansion because some control operations could\n    # have multiple targets, e.g. CSWAP\n    super().__init__((*control, *self.target), noise=noise)  # target_block.qubit_support[0]))\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricBlock","title":"<code>ParametricBlock(qubit_support, noise=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>Parameterized primitive blocks.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricBlock.num_parameters","title":"<code>num_parameters()</code>  <code>abstractmethod</code>","text":"<p>The number of parameters required by the block.</p> <p>This is a class property since the number of parameters is defined automatically before instantiating the operation. Also, this could correspond to a larger number of actual user-facing parameters since any parameter expression is allowed</p> <p>Examples: - RX operation has 1 parameter - U operation has 3 parameters - HamEvo has 2 parameters (generator and time evolution)</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>@abstractmethod\ndef num_parameters(cls) -&gt; int:\n    \"\"\"The number of parameters required by the block.\n\n    This is a class property since the number of parameters is defined\n    automatically before instantiating the operation. Also, this could\n    correspond to a larger number of actual user-facing parameters\n    since any parameter expression is allowed\n\n    Examples:\n    - RX operation has 1 parameter\n    - U operation has 3 parameters\n    - HamEvo has 2 parameters (generator and time evolution)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricControlBlock","title":"<code>ParametricControlBlock(control, target_block, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The abstract parametrized ControlBlock.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: tuple[int, ...],\n    target_block: ParametricBlock,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.blocks = (target_block,)\n    self.control = control\n    self.parameters = target_block.parameters\n    super().__init__((*control, *target_block.qubit_support), noise=noise)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.PrimitiveBlock","title":"<code>PrimitiveBlock(qubit_support, noise=None)</code>","text":"<p>               Bases: <code>AbstractBlock</code></p> <p>Primitive blocks represent elementary unitary operations.</p> <p>Examples are single/multi-qubit gates or Hamiltonian evolution. See <code>qadence.operations</code> for a full list of primitive blocks.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.PrimitiveBlock.digital_decomposition","title":"<code>digital_decomposition()</code>","text":"<p>Decomposition into purely digital gates.</p> <p>This method returns a decomposition of the Block in a combination of purely digital single-qubit and two-qubit 'gates', by manual/custom knowledge of how this can be done efficiently. :return:</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def digital_decomposition(self) -&gt; AbstractBlock:\n    \"\"\"Decomposition into purely digital gates.\n\n    This method returns a decomposition of the Block in a\n    combination of purely digital single-qubit and two-qubit\n    'gates', by manual/custom knowledge of how this can be done efficiently.\n    :return:\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ProjectorBlock","title":"<code>ProjectorBlock(ket, bra, qubit_support, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The abstract ProjectorBlock.</p> <p>Arguments:</p> <pre><code>ket (str): The ket given as a bitstring.\nbra (str): The bra given as a bitstring.\nqubit_support (int | tuple[int]): The qubit_support of the block.\n</code></pre> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    ket: str,\n    bra: str,\n    qubit_support: int | tuple[int, ...],\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    \"\"\"\n    Arguments:\n\n        ket (str): The ket given as a bitstring.\n        bra (str): The bra given as a bitstring.\n        qubit_support (int | tuple[int]): The qubit_support of the block.\n    \"\"\"\n    if isinstance(qubit_support, int):\n        qubit_support = (qubit_support,)\n    if len(bra) != len(ket):\n        raise ValueError(\n            \"Bra and ket must be bitstrings of same length in the 'Projector' definition.\"\n        )\n    elif len(bra) != len(qubit_support):\n        raise ValueError(\"Bra or ket must be of same length as the 'qubit_support'\")\n    for wf in [bra, ket]:\n        if not all(int(item) == 0 or int(item) == 1 for item in wf):\n            raise ValueError(\n                \"All qubits must be either in the '0' or '1' state\"\n                \" in the 'ProjectorBlock' definition.\"\n            )\n\n    self.ket = ket\n    self.bra = bra\n    super().__init__(qubit_support, noise=noise)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ScaleBlock","title":"<code>ScaleBlock(block, parameter)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>Scale blocks are created when multiplying a block by a number or parameter.</p> <p>Example: <pre><code>from qadence import X\n\nprint(X(0) * 2)\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 X(0)\n</code></pre> </p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(self, block: AbstractBlock, parameter: Any):\n    self.block = block\n    # TODO: more meaningful name like `scale`?\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    super().__init__(block.qubit_support)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.TimeEvolutionBlock","title":"<code>TimeEvolutionBlock(qubit_support, noise=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>Simple time evolution block with time-independent Hamiltonian.</p> <p>This class is just a convenience class which is used to label blocks which contains simple time evolution with time-independent Hamiltonian operators</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#analog-blocks","title":"Analog blocks","text":"<p>To learn how to use analog blocks and how to mix digital &amp; analog blocks, check out the digital-analog section of the documentation.</p> <p>Examples on how to use digital-analog blocks can be found in the *examples folder of the qadence repo:</p> <ul> <li>Fit a simple sinus: <code>examples/digital-analog/fit-sin.py</code></li> <li>Solve a QUBO: <code>examples/digital-analog/qubo.py</code></li> </ul>"},{"location":"api/blocks/#qadence.blocks.analog.AnalogChain","title":"<code>AnalogChain(blocks)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogComposite</code></p> <p>A chain of analog blocks.</p> <p>Needed because analog blocks require stricter validation than the general <code>ChainBlock</code>.</p> <p><code>AnalogChain</code>s can only be constructed from <code>AnalogKron</code> blocks or globally supported, primitive, analog blocks (like <code>InteractionBlock</code>s and <code>ConstantAnalogRotation</code>s).</p> <p>Automatically constructed by the <code>chain</code> function if only analog blocks are given.</p> <p>Example: <pre><code>from qadence import X, chain, AnalogInteraction\n\nb = chain(AnalogInteraction(200), AnalogInteraction(200))\nprint(type(b))  # this is an `AnalogChain`\n\nb = chain(X(0), AnalogInteraction(200))\nprint(type(b))  # this is a general `ChainBlock`\n</code></pre> <pre><code>&lt;class 'qadence.blocks.analog.AnalogChain'&gt;\n&lt;class 'qadence.blocks.composite.ChainBlock'&gt;\n</code></pre> </p> Source code in <code>qadence/blocks/analog.py</code> <pre><code>def __init__(self, blocks: Tuple[AnalogBlock, ...]):\n    \"\"\"A chain of analog blocks.\n\n    Needed because analog blocks require\n    stricter validation than the general `ChainBlock`.\n\n    `AnalogChain`s can only be constructed from `AnalogKron` blocks or\n    _**globally supported**_, primitive, analog blocks (like `InteractionBlock`s and\n    `ConstantAnalogRotation`s).\n\n    Automatically constructed by the [`chain`][qadence.blocks.utils.chain]\n    function if only analog blocks are given.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, chain, AnalogInteraction\n\n    b = chain(AnalogInteraction(200), AnalogInteraction(200))\n    print(type(b))  # this is an `AnalogChain`\n\n    b = chain(X(0), AnalogInteraction(200))\n    print(type(b))  # this is a general `ChainBlock`\n    ```\n    \"\"\"\n    for b in blocks:\n        if not (isinstance(b, AnalogKron) or b.qubit_support.is_global):\n            raise ValueError(\"Only KronBlocks or global blocks can be chain'ed.\")\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.analog.AnalogKron","title":"<code>AnalogKron(blocks, interaction=Interaction.NN)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogComposite</code></p> <p>Stack analog blocks vertically (i.e. in time).</p> <p>Needed because analog require stricter validation than the general <code>KronBlock</code>.</p> <p><code>AnalogKron</code>s can only be constructed from non-global, analog blocks with the same duration.</p> Source code in <code>qadence/blocks/analog.py</code> <pre><code>def __init__(self, blocks: Tuple[AnalogBlock, ...], interaction: Interaction = Interaction.NN):\n    \"\"\"Stack analog blocks vertically (i.e. in time).\n\n    Needed because analog require\n    stricter validation than the general `KronBlock`.\n\n    `AnalogKron`s can only be constructed from _**non-global**_, analog blocks\n    with the _**same duration**_.\n    \"\"\"\n    if len(blocks) == 0:\n        raise NotImplementedError(\"Empty KronBlocks not supported\")\n\n    self.blocks = blocks\n    self.interaction = interaction\n\n    qubit_support = QubitSupport()\n    duration = blocks[0].duration\n    for b in blocks:\n        if not isinstance(b, AnalogBlock):\n            raise ValueError(\"Can only kron `AnalgoBlock`s with other `AnalgoBlock`s.\")\n\n        if b.qubit_support == QubitSupport(\"global\"):\n            raise ValueError(\"Blocks with global support cannot be kron'ed.\")\n\n        if not qubit_support.is_disjoint(b.qubit_support):\n            raise ValueError(\"Make sure blocks act on distinct qubits!\")\n\n        if not np.isclose(evaluate(duration), evaluate(b.duration)):\n            raise ValueError(\"Kron'ed blocks have to have same duration.\")\n\n        qubit_support += b.qubit_support\n\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.analog.ConstantAnalogRotation","title":"<code>ConstantAnalogRotation(tag=None, __array_priority__=1000, _eigenvalues_generator=None, parameters=ParamMap(alpha=0.0, duration=1000.0, omega=0.0, delta=0.0, phase=0.0), qubit_support=QubitSupport('global'), add_pattern=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogBlock</code></p> <p>Implements a constant analog rotation with interaction dictated by the chosen Hamiltonian.</p> <pre><code>H/h = \u2211\u1d62(\u03a9/2 cos(\u03c6)*X\u1d62 - sin(\u03c6)*Y\u1d62 - \u03b4n\u1d62) + H\u1d62\u2099\u209c.\n</code></pre> <p>To construct this block you can use of the following convenience wrappers: - The general rotation operation <code>AnalogRot</code> - Shorthands for rotatins around an axis:   <code>AnalogRX</code>,   <code>AnalogRY</code>,   <code>AnalogRZ</code></p> <p>WARNING: do not use <code>ConstantAnalogRotation</code> with <code>alpha</code> as differentiable parameter - use the convenience wrappers mentioned above.</p>"},{"location":"api/blocks/#qadence.blocks.analog.InteractionBlock","title":"<code>InteractionBlock(tag=None, __array_priority__=1000, _eigenvalues_generator=None, parameters=ParamMap(duration=1000.0), qubit_support=QubitSupport('global'), add_pattern=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogBlock</code></p> <p>Free-evolution for the Hamiltonian interaction term of a register of qubits.</p> <p>In real interacting quantum devices, it means letting the system evolve freely according to the time-dependent Schrodinger equation. With emulators, this block is translated to an appropriate interaction Hamiltonian, for example, an Ising interaction</p> <pre><code>H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c\n</code></pre> <p>or an XY-interaction</p> <pre><code>H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2083/r\u2c7c\u2c7c\u00b3 (X\u1d62X\u2c7c + Z\u1d62Z\u2c7c)\n</code></pre> <p>with <code>n\u1d62 = (1-Z\u1d62)/2</code>.</p> <p>To construct, use the <code>AnalogInteraction</code> function.</p>"},{"location":"api/blocks/#composite-blocks","title":"Composite blocks","text":""},{"location":"api/blocks/#qadence.blocks.utils.chain","title":"<code>chain(*args)</code>","text":"<p>Chain blocks sequentially.</p> <p>On digital backends this can be interpreted loosely as a matrix mutliplication of blocks. In the analog case it chains blocks in time.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to chain. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator, List[AbstractBlock]]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>ChainBlock</p> <p>Example: <pre><code>from qadence import X, Y, chain\n\nb = chain(X(0), Y(0))\n\n# or use a generator\nb = chain(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>ChainBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def chain(*args: Union[AbstractBlock, Generator, List[AbstractBlock]]) -&gt; ChainBlock:\n    \"\"\"Chain blocks sequentially.\n\n    On digital backends this can be interpreted\n    loosely as a matrix mutliplication of blocks. In the analog case it chains\n    blocks in time.\n\n    Arguments:\n        *args: Blocks to chain. Can also be a generator.\n\n    Returns:\n        ChainBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, chain\n\n    b = chain(X(0), Y(0))\n\n    # or use a generator\n    b = chain(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    # ugly hack to use `AnalogChain` if we are dealing only with analog blocks\n    if len(args) and all(\n        isinstance(a, AnalogBlock) or isinstance(a, AnalogComposite) for a in args\n    ):\n        return analog_chain(*args)  # type: ignore[return-value,arg-type]\n    return _construct(ChainBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.utils.kron","title":"<code>kron(*args)</code>","text":"<p>Stack blocks vertically.</p> <p>On digital backends this can be intepreted loosely as a kronecker product of blocks. In the analog case it executes blocks parallel in time.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to kron. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>KronBlock</p> <p>Example: <pre><code>from qadence import X, Y, kron\n\nb = kron(X(0), Y(1))\n\n# or use a generator\nb = kron(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def kron(*args: Union[AbstractBlock, Generator]) -&gt; KronBlock:\n    \"\"\"Stack blocks vertically.\n\n    On digital backends this can be intepreted\n    loosely as a kronecker product of blocks. In the analog case it executes\n    blocks parallel in time.\n\n    Arguments:\n        *args: Blocks to kron. Can also be a generator.\n\n    Returns:\n        KronBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, kron\n\n    b = kron(X(0), Y(1))\n\n    # or use a generator\n    b = kron(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    # ugly hack to use `AnalogKron` if we are dealing only with analog blocks\n    if len(args) and all(\n        isinstance(a, AnalogBlock) or isinstance(a, AnalogComposite) for a in args\n    ):\n        return analog_kron(*args)  # type: ignore[return-value,arg-type]\n    return _construct(KronBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.utils.add","title":"<code>add(*args)</code>","text":"<p>Sums blocks.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to add. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>AddBlock</code> <p>AddBlock</p> <p>Example: <pre><code>from qadence import X, Y, add\n\nb = add(X(0), Y(0))\n\n# or use a generator\nb = add(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def add(*args: Union[AbstractBlock, Generator]) -&gt; AddBlock:\n    \"\"\"Sums blocks.\n\n    Arguments:\n        *args: Blocks to add. Can also be a generator.\n\n    Returns:\n        AddBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, add\n\n    b = add(X(0), Y(0))\n\n    # or use a generator\n    b = add(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    return _construct(AddBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.AddBlock","title":"<code>AddBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Adds blocks.</p> <p>Constructed via <code>add</code>.</p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.ChainBlock","title":"<code>ChainBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Chains blocks sequentially.</p> <p>Constructed via <code>chain</code></p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.CompositeBlock","title":"<code>CompositeBlock(tag=None, __array_priority__=1000)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractBlock</code></p> <p>Block which composes multiple blocks into one larger block (which can again be composed).</p> <p>Composite blocks are constructed via <code>chain</code>, <code>kron</code>, and <code>add</code>.</p>"},{"location":"api/blocks/#qadence.blocks.composite.KronBlock","title":"<code>KronBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Stacks blocks horizontally.</p> <p>Constructed via <code>kron</code>.</p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    if len(blocks) == 0:\n        raise NotImplementedError(\"Empty KronBlocks not supported\")\n\n    qubit_support = QubitSupport()\n    for b in blocks:\n        assert (\n            QubitSupportType.GLOBAL,\n        ) != b.qubit_support, \"Blocks with global support cannot be kron'ed.\"\n        assert qubit_support.is_disjoint(\n            b.qubit_support\n        ), \"Make sure blocks act on distinct qubits!\"\n        qubit_support += b.qubit_support\n\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#converting-blocks-to-matrices","title":"Converting blocks to matrices","text":""},{"location":"api/blocks/#qadence.blocks.block_to_tensor.block_to_tensor","title":"<code>block_to_tensor(block, values={}, qubit_support=None, use_full_support=True, tensor_type=TensorType.DENSE, endianness=Endianness.BIG, device=None)</code>","text":"<p>Convert a block into a torch tensor.</p> PARAMETER DESCRIPTION <code>block</code> <p>The block to convert.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>values</code> <p>A optional dict with values for parameters.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>qubit_support</code> <p>The qubit_support of the block.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>use_full_support</code> <p>True infers the total number of qubits.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>tensor_type</code> <p>the target tensor type.</p> <p> TYPE: <code>TensorType</code> DEFAULT: <code>DENSE</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence import hea, hamiltonian_factory, Z, block_to_tensor\n\nblock = hea(2,2)\nprint(block_to_tensor(block))\n\n# In case you have a diagonal observable, you can use\nobs = hamiltonian_factory(2, detuning = Z)\nprint(block_to_tensor(obs, tensor_type=\"SparseDiagonal\"))\n</code></pre> <pre><code>tensor([[[-0.1263+0.3718j, -0.2890-0.5904j, -0.5422-0.1020j, -0.2209+0.2461j],\n         [-0.0461-0.4404j, -0.0527+0.4613j, -0.5867-0.0333j, -0.4894-0.0591j],\n         [-0.3529-0.4149j, -0.3802-0.3835j,  0.2421+0.4150j, -0.1862-0.3824j],\n         [-0.3779-0.4585j, -0.2464+0.0058j, -0.0836-0.3354j,  0.5551+0.3983j]]],\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\ntensor(indices=tensor([[0, 3],\n                       [0, 3]]),\n       values=tensor([ 2.+0.j, -2.+0.j]),\n       size=(4, 4), nnz=2, layout=torch.sparse_coo)\n</code></pre> </p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def block_to_tensor(\n    block: AbstractBlock,\n    values: dict[str, TNumber | torch.Tensor] = {},\n    qubit_support: tuple | None = None,\n    use_full_support: bool = True,\n    tensor_type: TensorType = TensorType.DENSE,\n    endianness: Endianness = Endianness.BIG,\n    device: torch.device = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a block into a torch tensor.\n\n    Arguments:\n        block (AbstractBlock): The block to convert.\n        values (dict): A optional dict with values for parameters.\n        qubit_support (tuple): The qubit_support of the block.\n        use_full_support (bool): True infers the total number of qubits.\n        tensor_type (TensorType): the target tensor type.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import hea, hamiltonian_factory, Z, block_to_tensor\n\n    block = hea(2,2)\n    print(block_to_tensor(block))\n\n    # In case you have a diagonal observable, you can use\n    obs = hamiltonian_factory(2, detuning = Z)\n    print(block_to_tensor(obs, tensor_type=\"SparseDiagonal\"))\n    ```\n    \"\"\"\n\n    # FIXME: default use_full_support to False. In general, it would\n    # be more efficient to do that, and make sure that computations such\n    # as observables only do the matmul of the size of the qubit support.\n\n    if tensor_type == TensorType.DENSE:\n        from qadence.blocks import embedding\n\n        (ps, embed) = embedding(block)\n        return _block_to_tensor_embedded(\n            block,\n            embed(ps, values),\n            qubit_support,\n            use_full_support,\n            endianness=endianness,\n            device=device,\n        )\n\n    elif tensor_type == TensorType.SPARSEDIAGONAL:\n        t = block_to_diagonal(block, endianness=endianness)\n        indices, values, size = torch.nonzero(t), t[t != 0], len(t)\n        indices = torch.stack((indices.flatten(), indices.flatten()))\n        return torch.sparse_coo_tensor(indices, values, (size, size))\n</code></pre>"},{"location":"api/constructors/","title":"Constructors for common quantum circuits","text":""},{"location":"api/constructors/#qadence.constructors.feature_maps.exp_fourier_feature_map","title":"<code>exp_fourier_feature_map(n_qubits, support=None, param='x', feature_range=None)</code>","text":"<p>Exponential fourier feature map.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the feature</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>qubit support</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>param</code> <p>name of feature <code>Parameter</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'x'</code> </p> <code>feature_range</code> <p>min and max value of the feature, as floats in a Tuple</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/constructors/feature_maps.py</code> <pre><code>def exp_fourier_feature_map(\n    n_qubits: int,\n    support: tuple[int, ...] = None,\n    param: str = \"x\",\n    feature_range: tuple[float, float] = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Exponential fourier feature map.\n\n    Args:\n        n_qubits: number of qubits in the feature\n        support: qubit support\n        param: name of feature `Parameter`\n        feature_range: min and max value of the feature, as floats in a Tuple\n    \"\"\"\n\n    if feature_range is None:\n        feature_range = (0.0, 2.0**n_qubits)\n\n    support = tuple(range(n_qubits)) if support is None else support\n    hlayer = kron(H(qubit) for qubit in support)\n    rlayer = feature_map(\n        n_qubits,\n        support=support,\n        param=param,\n        op=RZ,\n        fm_type=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.EXP,\n        feature_range=feature_range,\n        target_range=(0.0, 2 * PI),\n    )\n    rlayer.tag = None\n    return tag(chain(hlayer, rlayer), f\"ExpFourierFM({param})\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.feature_maps.feature_map","title":"<code>feature_map(n_qubits, support=None, param='phi', op=RX, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT, feature_range=None, target_range=None, multiplier=None, param_prefix=None)</code>","text":"<p>Construct a feature map of a given type.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Number of qubits the feature map covers. Results in <code>support=range(n_qubits)</code>.</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>Puts one feature-encoding rotation gate on every qubit in <code>support</code>. n_qubits in this case specifies the total overall qubits of the circuit, which may be wider than the support itself, but not narrower.</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>param</code> <p>Parameter of the feature map; you can pass a string or Parameter; it will be set as non-trainable (FeatureParameter) regardless.</p> <p> TYPE: <code>Parameter | str</code> DEFAULT: <code>'phi'</code> </p> <code>op</code> <p>Rotation operation of the feature map; choose from RX, RY, RZ or PHASE.</p> <p> TYPE: <code>RotationTypes</code> DEFAULT: <code>RX</code> </p> <code>fm_type</code> <p>Basis set for data encoding; choose from <code>BasisSet.FOURIER</code> for Fourier encoding, or <code>BasisSet.CHEBYSHEV</code> for Chebyshev polynomials of the first kind.</p> <p> TYPE: <code>BasisSet | Callable | str</code> DEFAULT: <code>FOURIER</code> </p> <code>reupload_scaling</code> <p>how the feature map scales the data that is re-uploaded for each qubit. choose from <code>ReuploadScaling</code> enumeration or provide your own function with a single int as input and int or float as output.</p> <p> TYPE: <code>ReuploadScaling | Callable | str</code> DEFAULT: <code>CONSTANT</code> </p> <code>feature_range</code> <p>range of data that the input data provided comes from. Used to map input data to the correct domain of the feature-encoding function.</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> <code>target_range</code> <p>range of data the data encoder assumes as the natural range. For example, in Chebyshev polynomials it is (-1, 1), while for Fourier it may be chosen as (0, 2*PI). Used to map data to the correct domain of the feature-encoding function.</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> <code>multiplier</code> <p>overall multiplier; this is useful for reuploading the feature map serially with different scalings; can be a number or parameter/expression.</p> <p> TYPE: <code>Parameter | TParameter | None</code> DEFAULT: <code>None</code> </p> <code>param_prefix</code> <p>string prefix to create trainable parameters multiplying the feature parameter inside the feature-encoding function. Note that currently this does not take into account the domain of the feature-encoding function.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <p>Example: <pre><code>from qadence import feature_map, BasisSet, ReuploadScaling\n\nfm = feature_map(3, fm_type=BasisSet.FOURIER)\nprint(f\"{fm = }\")\n\nfm = feature_map(3, fm_type=BasisSet.CHEBYSHEV)\nprint(f\"{fm = }\")\n\nfm = feature_map(3, fm_type=BasisSet.FOURIER, reupload_scaling = ReuploadScaling.TOWER)\nprint(f\"{fm = }\")\n</code></pre> <pre><code>fm = KronBlock(0,1,2) [tag: Constant Fourier FM]\n\u251c\u2500\u2500 RX(0) [params: ['phi']]\n\u251c\u2500\u2500 RX(1) [params: ['phi']]\n\u2514\u2500\u2500 RX(2) [params: ['phi']]\nfm = KronBlock(0,1,2) [tag: Constant Chebyshev FM]\n\u251c\u2500\u2500 RX(0) [params: ['acos(phi)']]\n\u251c\u2500\u2500 RX(1) [params: ['acos(phi)']]\n\u2514\u2500\u2500 RX(2) [params: ['acos(phi)']]\nfm = KronBlock(0,1,2) [tag: Tower Fourier FM]\n\u251c\u2500\u2500 RX(0) [params: ['1_0*phi']]\n\u251c\u2500\u2500 RX(1) [params: ['2_0*phi']]\n\u2514\u2500\u2500 RX(2) [params: ['3_0*phi']]\n</code></pre> </p> Source code in <code>qadence/constructors/feature_maps.py</code> <pre><code>def feature_map(\n    n_qubits: int,\n    support: tuple[int, ...] | None = None,\n    param: Parameter | str = \"phi\",\n    op: RotationTypes = RX,\n    fm_type: BasisSet | Callable | str = BasisSet.FOURIER,\n    reupload_scaling: ReuploadScaling | Callable | str = ReuploadScaling.CONSTANT,\n    feature_range: tuple[float, float] | None = None,\n    target_range: tuple[float, float] | None = None,\n    multiplier: Parameter | TParameter | None = None,\n    param_prefix: str | None = None,\n) -&gt; KronBlock:\n    \"\"\"Construct a feature map of a given type.\n\n    Arguments:\n        n_qubits: Number of qubits the feature map covers. Results in `support=range(n_qubits)`.\n        support: Puts one feature-encoding rotation gate on every qubit in `support`. n_qubits in\n            this case specifies the total overall qubits of the circuit, which may be wider than the\n            support itself, but not narrower.\n        param: Parameter of the feature map; you can pass a string or Parameter;\n            it will be set as non-trainable (FeatureParameter) regardless.\n        op: Rotation operation of the feature map; choose from RX, RY, RZ or PHASE.\n        fm_type: Basis set for data encoding; choose from `BasisSet.FOURIER` for Fourier\n            encoding, or `BasisSet.CHEBYSHEV` for Chebyshev polynomials of the first kind.\n        reupload_scaling: how the feature map scales the data that is re-uploaded for each qubit.\n            choose from `ReuploadScaling` enumeration or provide your own function with a single\n            int as input and int or float as output.\n        feature_range: range of data that the input data provided comes from. Used to map input data\n            to the correct domain of the feature-encoding function.\n        target_range: range of data the data encoder assumes as the natural range. For example,\n            in Chebyshev polynomials it is (-1, 1), while for Fourier it may be chosen as (0, 2*PI).\n            Used to map data to the correct domain of the feature-encoding function.\n        multiplier: overall multiplier; this is useful for reuploading the feature map serially with\n            different scalings; can be a number or parameter/expression.\n        param_prefix: string prefix to create trainable parameters multiplying the feature parameter\n            inside the feature-encoding function. Note that currently this does not take into\n            account the domain of the feature-encoding function.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import feature_map, BasisSet, ReuploadScaling\n\n    fm = feature_map(3, fm_type=BasisSet.FOURIER)\n    print(f\"{fm = }\")\n\n    fm = feature_map(3, fm_type=BasisSet.CHEBYSHEV)\n    print(f\"{fm = }\")\n\n    fm = feature_map(3, fm_type=BasisSet.FOURIER, reupload_scaling = ReuploadScaling.TOWER)\n    print(f\"{fm = }\")\n    ```\n    \"\"\"\n\n    # Process input\n    if support is None:\n        support = tuple(range(n_qubits))\n    elif len(support) != n_qubits:\n        raise ValueError(\"Wrong qubit support supplied\")\n\n    if op not in ROTATIONS:\n        raise ValueError(\n            f\"Operation {op} not supported. \"\n            f\"Please provide one from {[rot.__name__ for rot in ROTATIONS]}.\"\n        )\n\n    scaled_fparam = fm_parameter_scaling(\n        fm_type, param, feature_range=feature_range, target_range=target_range\n    )\n\n    transform_func = fm_parameter_func(fm_type)\n\n    basis_tag = fm_type.value if isinstance(fm_type, BasisSet) else str(fm_type)\n    rs_func, rs_tag = fm_reupload_scaling_fn(reupload_scaling)\n\n    # Set overall multiplier\n    multiplier = 1 if multiplier is None else Parameter(multiplier)\n\n    # Build feature map\n    op_list = []\n    fparam = scaled_fparam\n    for i, qubit in enumerate(support):\n        if param_prefix is not None:\n            train_param = VariationalParameter(param_prefix + f\"_{i}\")\n            fparam = train_param * scaled_fparam\n        op_list.append(op(qubit, multiplier * rs_func(i) * transform_func(fparam)))\n    fm = kron(*op_list)\n\n    fm.tag = rs_tag + \" \" + basis_tag + \" FM\"\n\n    return fm\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.alt","title":"<code>alt(n_qubits, m_block_qubits, depth=1, param_prefix='theta', support=None, strategy=Strategy.DIGITAL, **strategy_args)</code>","text":"<p>Factory function for the alternating layer ansatz (alt).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>number of qubits in the local entangling block</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the alt</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indexes where the alt is applied</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>strategy</code> <p>Strategy.Digital or Strategy.DigitalAnalog</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>**strategy_args</code> <p>see below</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> PARAMETER DESCRIPTION <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer. Valid for Digital .</p> <p> TYPE: <code>list</code> </p> <code>entangler</code> <ul> <li>Digital: 2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational parameters on the rotation angles.</li> </ul> <p> TYPE: <code>AbstractBlock</code> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def alt(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] = None,\n    strategy: Strategy = Strategy.DIGITAL,\n    **strategy_args: Any,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Factory function for the alternating layer ansatz (alt).\n\n    Args:\n        n_qubits: number of qubits in the block\n        m_block_qubits: number of qubits in the local entangling block\n        depth: number of layers of the alt\n        param_prefix: the base name of the variational parameters\n        support: qubit indexes where the alt is applied\n        strategy: Strategy.Digital or Strategy.DigitalAnalog\n        **strategy_args: see below\n\n    Keyword Arguments:\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer. Valid for\n            Digital .\n        entangler (AbstractBlock):\n            - Digital: 2-qubit entangling operation. Supports CNOT, CZ,\n            CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational\n            parameters on the rotation angles.\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    alt_func_dict = {\n        Strategy.DIGITAL: alt_digital,\n        Strategy.SDAQC: alt_sDAQC,\n        Strategy.BDAQC: alt_bDAQC,\n        Strategy.ANALOG: alt_analog,\n    }\n\n    try:\n        alt_func = alt_func_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    hea_block: AbstractBlock = alt_func(\n        n_qubits=n_qubits,\n        m_block_qubits=m_block_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        **strategy_args,\n    )  # type: ignore\n\n    return hea_block\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.alt_digital","title":"<code>alt_digital(n_qubits, m_block_qubits, depth=1, support=None, param_prefix='theta', operations=[RX, RY], entangler=CNOT)</code>","text":"<p>Construct the digital alternating layer ansatz (ALT).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the ansatz.</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>number of qubits in the local entangling block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the ALT.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY]</code> </p> <code>support</code> <p>qubit indexes where the ALT is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def alt_digital(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int = 1,\n    support: tuple[int, ...] = None,\n    param_prefix: str = \"theta\",\n    operations: list[type[AbstractBlock]] = [RX, RY],\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the digital alternating layer ansatz (ALT).\n\n    Args:\n        n_qubits (int): number of qubits in the ansatz.\n        m_block_qubits (int): number of qubits in the local entangling block.\n        depth (int): number of layers of the ALT.\n        param_prefix (str): the base name of the variational parameters\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        support (tuple): qubit indexes where the ALT is applied.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n    \"\"\"\n\n    try:\n        if entangler not in [CNOT, CZ, CRX, CRY, CRZ, CPHASE]:\n            raise ValueError(\n                \"Please provide a valid two-qubit entangler operation for digital ALT.\"\n            )\n    except TypeError:\n        raise ValueError(\"Please provide a valid two-qubit entangler operation for digital ALT.\")\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        support=support,\n        param_prefix=param_prefix,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_block_digital(\n        n_qubits,\n        m_block_qubits,\n        param_prefix=param_prefix + \"_ent\",\n        depth=depth,\n        support=support,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n\n    return tag(chain(*layers), \"ALT\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.hea","title":"<code>hea(n_qubits, depth=1, param_prefix='theta', support=None, strategy=Strategy.DIGITAL, **strategy_args)</code>","text":"<p>Factory function for the Hardware Efficient Ansatz (HEA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indexes where the HEA is applied</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>strategy</code> <p>Strategy.Digital or Strategy.DigitalAnalog</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>**strategy_args</code> <p>see below</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> PARAMETER DESCRIPTION <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer. Valid for Digital and DigitalAnalog HEA.</p> <p> TYPE: <code>list</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c. Valid for only for Digital HEA.</p> <p> TYPE: <code>bool</code> </p> <code>entangler</code> <ul> <li>Digital: 2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational parameters on the rotation angles.</li> <li>DigitaAnalog | Analog: Hamiltonian generator for the analog entangling layer. Defaults to global ZZ Hamiltonian. Time parameter is considered variational.</li> </ul> <p> TYPE: <code>AbstractBlock</code> </p> <p>Examples: <pre><code>from qadence import RZ, RX\nfrom qadence import hea\n\n# create the circuit\nn_qubits, depth = 2, 4\nansatz = hea(\n    n_qubits=n_qubits,\n    depth=depth,\n    strategy=\"sDAQC\",\n    operations=[RZ,RX,RZ]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def hea(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] = None,\n    strategy: Strategy = Strategy.DIGITAL,\n    **strategy_args: Any,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Factory function for the Hardware Efficient Ansatz (HEA).\n\n    Args:\n        n_qubits: number of qubits in the block\n        depth: number of layers of the HEA\n        param_prefix: the base name of the variational parameters\n        support: qubit indexes where the HEA is applied\n        strategy: Strategy.Digital or Strategy.DigitalAnalog\n        **strategy_args: see below\n\n    Keyword Arguments:\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer. Valid for\n            Digital and DigitalAnalog HEA.\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c. Valid for only\n            for Digital HEA.\n        entangler (AbstractBlock):\n            - Digital: 2-qubit entangling operation. Supports CNOT, CZ,\n            CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational\n            parameters on the rotation angles.\n            - DigitaAnalog | Analog: Hamiltonian generator for the\n            analog entangling layer. Defaults to global ZZ Hamiltonian.\n            Time parameter is considered variational.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import RZ, RX\n    from qadence import hea\n\n    # create the circuit\n    n_qubits, depth = 2, 4\n    ansatz = hea(\n        n_qubits=n_qubits,\n        depth=depth,\n        strategy=\"sDAQC\",\n        operations=[RZ,RX,RZ]\n    )\n    ```\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    hea_func_dict = {\n        Strategy.DIGITAL: hea_digital,\n        Strategy.SDAQC: hea_sDAQC,\n        Strategy.BDAQC: hea_bDAQC,\n        Strategy.ANALOG: hea_analog,\n    }\n\n    try:\n        hea_func = hea_func_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    hea_block: AbstractBlock = hea_func(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        **strategy_args,\n    )  # type: ignore\n\n    return hea_block\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.hea_digital","title":"<code>hea_digital(n_qubits, depth=1, param_prefix='theta', periodic=False, operations=[RX, RY, RX], support=None, entangler=CNOT)</code>","text":"<p>Construct the Digital Hardware Efficient Ansatz (HEA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY, RX]</code> </p> <code>support</code> <p>qubit indexes where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def hea_digital(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    periodic: bool = False,\n    operations: list[type[AbstractBlock]] = [RX, RY, RX],\n    support: tuple[int, ...] = None,\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the Digital Hardware Efficient Ansatz (HEA).\n\n    Args:\n        n_qubits (int): number of qubits in the block.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c.\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        support (tuple): qubit indexes where the HEA is applied.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n    \"\"\"\n    try:\n        if entangler not in [CNOT, CZ, CRX, CRY, CRZ, CPHASE]:\n            raise ValueError(\n                \"Please provide a valid two-qubit entangler operation for digital HEA.\"\n            )\n    except TypeError:\n        raise ValueError(\"Please provide a valid two-qubit entangler operation for digital HEA.\")\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        periodic=periodic,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n    return tag(chain(*layers), \"HEA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.hea_sDAQC","title":"<code>hea_sDAQC(n_qubits, depth=1, param_prefix='theta', operations=[RX, RY, RX], support=None, entangler=None)</code>","text":"<p>Construct the Hardware Efficient Ansatz (HEA) with analog entangling layers.</p> <p>It uses step-wise digital-analog computation.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY, RX]</code> </p> <code>support</code> <p>qubit indexes where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>entangler</code> <p>Hamiltonian generator for the analog entangling layer. Defaults to global ZZ Hamiltonian. Time parameter is considered variational.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def hea_sDAQC(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    operations: list[type[AbstractBlock]] = [RX, RY, RX],\n    support: tuple[int, ...] = None,\n    entangler: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the Hardware Efficient Ansatz (HEA) with analog entangling layers.\n\n    It uses step-wise digital-analog computation.\n\n    Args:\n        n_qubits (int): number of qubits in the block.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        support (tuple): qubit indexes where the HEA is applied.\n        entangler (AbstractBlock): Hamiltonian generator for the\n            analog entangling layer. Defaults to global ZZ Hamiltonian.\n            Time parameter is considered variational.\n    \"\"\"\n\n    # TODO: Add qubit support\n    if entangler is None:\n        entangler = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n    try:\n        if not block_is_qubit_hamiltonian(entangler):\n            raise ValueError(\n                \"Please provide a valid Pauli Hamiltonian generator for digital-analog HEA.\"\n            )\n    except NotImplementedError:\n        raise ValueError(\n            \"Please provide a valid Pauli Hamiltonian generator for digital-analog HEA.\"\n        )\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_analog(\n        depth=depth,\n        param_prefix=param_prefix,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n    return tag(chain(*layers), \"HEA-sDA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig","title":"<code>ObservableConfig(detuning, scale=1.0, shift=0.0, transformation_type=ObservableTransform.NONE, trainable_transform=None)</code>  <code>dataclass</code>","text":""},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.detuning","title":"<code>detuning: TDetuning</code>  <code>instance-attribute</code>","text":"<p>Single qubit detuning of the observable Hamiltonian.</p> <p>Accepts single-qubit operator N, X, Y, or Z.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.scale","title":"<code>scale: TParameter = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The scale by which to multiply the output of the observable.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.shift","title":"<code>shift: TParameter = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The shift to add to the output of the observable.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.trainable_transform","title":"<code>trainable_transform: bool | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to have a trainable transformation on the output of the observable.</p> <p>If None, the scale and shift are numbers. If True, the scale and shift are VariationalParameter. If False, the scale and shift are FeatureParameter.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.transformation_type","title":"<code>transformation_type: ObservableTransform = ObservableTransform.NONE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The type of transformation.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.hamiltonian_factory","title":"<code>hamiltonian_factory(register, interaction=None, detuning=None, interaction_strength=None, detuning_strength=None, random_strength=False, use_all_node_pairs=False)</code>","text":"<p>General Hamiltonian creation function.</p> <p>Can be used to create Hamiltonians with 2-qubit interactions and single-qubit detunings, both with arbitrary strength or parameterized.</p> PARAMETER DESCRIPTION <code>register</code> <p>register of qubits with a specific graph topology, or number of qubits. When passing a number of qubits a register with all-to-all connectivity is created.</p> <p> TYPE: <code>Register | int</code> </p> <code>interaction</code> <p>Interaction.ZZ, Interaction.NN, Interaction.XY, or Interacton.XYZ.</p> <p> TYPE: <code>Interaction | Callable | None</code> DEFAULT: <code>None</code> </p> <code>detuning</code> <p>single-qubit operator N, X, Y, or Z.</p> <p> TYPE: <code>TDetuning | None</code> DEFAULT: <code>None</code> </p> <code>interaction_strength</code> <p>list of values to be used as the interaction strength for each pair of qubits. Should be ordered following the order of <code>Register(n_qubits).edges</code>. Alternatively, some string \"x\" can be passed, which will create a parameterized interactions for each pair of qubits, each labelled as <code>\"x_ij\"</code>.</p> <p> TYPE: <code>TArray | str | None</code> DEFAULT: <code>None</code> </p> <code>detuning_strength</code> <p>list of values to be used as the detuning strength for each qubit. Alternatively, some string \"x\" can be passed, which will create a parameterized detuning for each qubit, each labelled as <code>\"x_i\"</code>.</p> <p> TYPE: <code>TArray | str | None</code> DEFAULT: <code>None</code> </p> <code>random_strength</code> <p>set random interaction and detuning strengths between -1 and 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_all_node_pairs</code> <p>computes an interaction term for every pair of nodes in the graph, independent of the edge topology in the register. Useful for defining Hamiltonians where the interaction strength decays with the distance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>from qadence import hamiltonian_factory, Interaction, Register, Z\n\nn_qubits = 3\n\n# Constant total magnetization observable:\nobservable = hamiltonian_factory(n_qubits, detuning = Z)\n\n# Parameterized total magnetization observable:\nobservable = hamiltonian_factory(n_qubits, detuning = Z, detuning_strength = \"z\")\n\n# Random all-to-all XY Hamiltonian generator:\ngenerator = hamiltonian_factory(\n    n_qubits,\n    interaction = Interaction.XY,\n    random_strength = True,\n    )\n\n# Parameterized NN Hamiltonian generator with a square grid interaction topology:\nregister = Register.square(qubits_side = n_qubits)\ngenerator = hamiltonian_factory(\n    register,\n    interaction = Interaction.NN,\n    interaction_strength = \"theta\"\n    )\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def hamiltonian_factory(\n    register: Register | int,\n    interaction: Interaction | Callable | None = None,\n    detuning: TDetuning | None = None,\n    interaction_strength: TArray | str | None = None,\n    detuning_strength: TArray | str | None = None,\n    random_strength: bool = False,\n    use_all_node_pairs: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    General Hamiltonian creation function.\n\n    Can be used to create Hamiltonians with 2-qubit\n    interactions and single-qubit detunings, both with arbitrary strength or parameterized.\n\n    Arguments:\n        register: register of qubits with a specific graph topology, or number of qubits.\n            When passing a number of qubits a register with all-to-all connectivity\n            is created.\n        interaction: Interaction.ZZ, Interaction.NN, Interaction.XY, or Interacton.XYZ.\n        detuning: single-qubit operator N, X, Y, or Z.\n        interaction_strength: list of values to be used as the interaction strength for each\n            pair of qubits. Should be ordered following the order of `Register(n_qubits).edges`.\n            Alternatively, some string \"x\" can be passed, which will create a parameterized\n            interactions for each pair of qubits, each labelled as `\"x_ij\"`.\n        detuning_strength: list of values to be used as the detuning strength for each qubit.\n            Alternatively, some string \"x\" can be passed, which will create a parameterized\n            detuning for each qubit, each labelled as `\"x_i\"`.\n        random_strength: set random interaction and detuning strengths between -1 and 1.\n        use_all_node_pairs: computes an interaction term for every pair of nodes in the graph,\n            independent of the edge topology in the register. Useful for defining Hamiltonians\n            where the interaction strength decays with the distance.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import hamiltonian_factory, Interaction, Register, Z\n\n        n_qubits = 3\n\n        # Constant total magnetization observable:\n        observable = hamiltonian_factory(n_qubits, detuning = Z)\n\n        # Parameterized total magnetization observable:\n        observable = hamiltonian_factory(n_qubits, detuning = Z, detuning_strength = \"z\")\n\n        # Random all-to-all XY Hamiltonian generator:\n        generator = hamiltonian_factory(\n            n_qubits,\n            interaction = Interaction.XY,\n            random_strength = True,\n            )\n\n        # Parameterized NN Hamiltonian generator with a square grid interaction topology:\n        register = Register.square(qubits_side = n_qubits)\n        generator = hamiltonian_factory(\n            register,\n            interaction = Interaction.NN,\n            interaction_strength = \"theta\"\n            )\n        ```\n    \"\"\"\n\n    if interaction is None and detuning is None:\n        raise ValueError(\"Please provide an interaction and/or detuning for the Hamiltonian.\")\n\n    # If number of qubits is given, creates all-to-all register\n    register = Register(register) if isinstance(register, int) else register\n\n    # Get interaction function\n    if interaction is not None:\n        if callable(interaction):\n            int_fn = interaction\n            try:\n                if not block_is_qubit_hamiltonian(interaction(0, 1)):\n                    raise ValueError(\"Custom interactions must be composed of Pauli operators.\")\n            except TypeError:\n                raise TypeError(\n                    \"Please use a custom interaction function signed with two integer parameters.\"\n                )\n        else:\n            int_fn = INTERACTION_DICT.get(interaction, None)  # type: ignore [arg-type]\n            if int_fn is None:\n                raise KeyError(f\"Interaction {interaction} not supported.\")\n\n    # Check single-qubit detuning\n    if (detuning is not None) and (detuning not in DETUNINGS):\n        raise TypeError(f\"Detuning of type {type(detuning)} not supported.\")\n\n    # Pre-process detuning and interaction strengths and update register\n    detuning_strength_array = _preprocess_strengths(\n        register, detuning_strength, \"nodes\", random_strength\n    )\n\n    edge_str = \"all_node_pairs\" if use_all_node_pairs else \"edges\"\n    interaction_strength_array = _preprocess_strengths(\n        register, interaction_strength, edge_str, random_strength\n    )\n\n    # Create single-qubit detunings:\n    single_qubit_terms: List[AbstractBlock] = []\n    if detuning is not None:\n        for strength, node in zip(detuning_strength_array, register.nodes):\n            single_qubit_terms.append(strength * detuning(node))\n\n    # Create two-qubit interactions:\n    two_qubit_terms: List[AbstractBlock] = []\n    edge_data = register.all_node_pairs if use_all_node_pairs else register.edges\n    if interaction is not None and int_fn is not None:\n        for strength, edge in zip(interaction_strength_array, edge_data):\n            two_qubit_terms.append(strength * int_fn(*edge))\n\n    return add(*single_qubit_terms, *two_qubit_terms)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_nn","title":"<code>interaction_nn(i, j)</code>","text":"<p>Ising NN interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_nn(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Ising NN interaction.\"\"\"\n    return N(i) @ N(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_xy","title":"<code>interaction_xy(i, j)</code>","text":"<p>XY interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_xy(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"XY interaction.\"\"\"\n    return X(i) @ X(j) + Y(i) @ Y(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_xyz","title":"<code>interaction_xyz(i, j)</code>","text":"<p>Heisenberg XYZ interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_xyz(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Heisenberg XYZ interaction.\"\"\"\n    return X(i) @ X(j) + Y(i) @ Y(j) + Z(i) @ Z(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_zz","title":"<code>interaction_zz(i, j)</code>","text":"<p>Ising ZZ interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_zz(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Ising ZZ interaction.\"\"\"\n    return Z(i) @ Z(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft.qft","title":"<code>qft(n_qubits, support=None, inverse=False, reverse_in=False, swaps_out=False, strategy=Strategy.DIGITAL, gen_build=None)</code>","text":"<p>The Quantum Fourier Transform.</p> <p>Depending on the application, user should be careful with qubit ordering in the input and output. This can be controlled with reverse_in and swaps_out arguments.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the QFT</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>qubit support to use</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>inverse</code> <p>True performs the inverse QFT</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>reverse_in</code> <p>Reverses the input qubits to account for endianness</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>swaps_out</code> <p>Performs swaps on the output qubits to match the \"textbook\" QFT.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>strategy</code> <p>Strategy.Digital or Strategy.sDAQC</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>gen_build</code> <p>building block Ising Hamiltonian for the DAQC transform. Defaults to constant all-to-all Ising.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>from qadence import qft\n\nn_qubits = 3\n\nqft_circuit = qft(n_qubits, strategy = \"sDAQC\")\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def qft(\n    n_qubits: int,\n    support: tuple[int, ...] = None,\n    inverse: bool = False,\n    reverse_in: bool = False,\n    swaps_out: bool = False,\n    strategy: Strategy = Strategy.DIGITAL,\n    gen_build: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    The Quantum Fourier Transform.\n\n    Depending on the application, user should be careful with qubit ordering\n    in the input and output. This can be controlled with reverse_in and swaps_out\n    arguments.\n\n    Args:\n        n_qubits: number of qubits in the QFT\n        support: qubit support to use\n        inverse: True performs the inverse QFT\n        reverse_in: Reverses the input qubits to account for endianness\n        swaps_out: Performs swaps on the output qubits to match the \"textbook\" QFT.\n        strategy: Strategy.Digital or Strategy.sDAQC\n        gen_build: building block Ising Hamiltonian for the DAQC transform.\n            Defaults to constant all-to-all Ising.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import qft\n\n        n_qubits = 3\n\n        qft_circuit = qft(n_qubits, strategy = \"sDAQC\")\n        ```\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    assert len(support) &lt;= n_qubits, \"Wrong qubit support supplied\"\n\n    if reverse_in:\n        support = support[::-1]\n\n    qft_layer_dict = {\n        Strategy.DIGITAL: _qft_layer_digital,\n        Strategy.SDAQC: _qft_layer_sDAQC,\n        Strategy.BDAQC: _qft_layer_bDAQC,\n        Strategy.ANALOG: _qft_layer_analog,\n    }\n\n    try:\n        layer_func = qft_layer_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    qft_layers = reversed(range(n_qubits)) if inverse else range(n_qubits)\n\n    qft_circ = chain(\n        layer_func(\n            n_qubits=n_qubits, support=support, layer=layer, inverse=inverse, gen_build=gen_build\n        )  # type: ignore\n        for layer in qft_layers\n    )\n\n    if swaps_out:\n        swap_ops = [SWAP(support[i], support[n_qubits - i - 1]) for i in range(n_qubits // 2)]\n        qft_circ = chain(*swap_ops, qft_circ) if inverse else chain(qft_circ, *swap_ops)\n\n    return tag(qft_circ, tag=\"iQFT\") if inverse else tag(qft_circ, tag=\"QFT\")\n</code></pre>"},{"location":"api/constructors/#hardware-efficient-ansatz-for-rydberg-atom-arrays","title":"Hardware efficient ansatz for Rydberg atom arrays","text":""},{"location":"api/constructors/#qadence.constructors.rydberg_hea.rydberg_hea","title":"<code>rydberg_hea(register, n_layers=1, addressable_detuning=True, addressable_drive=False, tunable_phase=False, additional_prefix=None)</code>","text":"<p>Hardware efficient ansatz for neutral atom (Rydberg) platforms.</p> <p>This constructor implements a variational ansatz which is very close to what is implementable on 2nd generation PASQAL quantum devices. In particular, it implements evolution over a specific Hamiltonian which can be realized on the device. This Hamiltonian contains:</p> <ul> <li> <p>an interaction term given by the standard NN interaction and determined starting     from the positions in the input register: H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c</p> </li> <li> <p>a detuning term which corresponding to a n_i = (1+sigma_i^z)/2 applied to     all the qubits. If the <code>addressable_detuning</code> flag is set to True, the routine     effectively a local n_i = (1+sigma_i^z)/2 term in the     evolved Hamiltonian with a different coefficient for each atom. These     coefficients determine a local addressing pattern for the detuning on a subset     of the qubits. In this routine, the coefficients are variational parameters     and they will therefore be optimized at each optimizer step</p> </li> <li> <p>a drive term which corresponding to a sigma^x evolution operation applied to     all the qubits. If the <code>addressable_drive</code> flag is set to True, the routine     effectively a local sigma_i^x term in the evolved Hamiltonian with a different     coefficient for each atom. These coefficients determine a local addressing pattern     for the drive on a subset of the qubits. In this routine, the coefficients are     variational parameters and they will therefore be optimized at each optimizer step</p> </li> <li> <p>if the <code>tunable_phase</code> flag is set to True, the drive term is modified in the following     way: drive = cos(phi) * sigma^x - sin(phi) * sigma^y     The addressable pattern above is maintained and the phase is considered just as an     additional variational parameter which is optimized with the rest</p> </li> </ul> <p>Notice that, on real devices, the coefficients assigned to each qubit in both the detuning and drive patterns should be non-negative and they should always sum to 1. This is not the case for the implementation in this routine since the coefficients (weights) do not have any constraint. Therefore, this HEA is not completely realizable on neutral atom devices.</p> PARAMETER DESCRIPTION <code>register</code> <p>the input atomic register with Cartesian coordinates.</p> <p> TYPE: <code>Register</code> </p> <code>n_layers</code> <p>number layers in the HEA, each layer includes a drive, detuning and pure interaction pulses whose is a variational parameter</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>addressable_detuning</code> <p>whether to turn on the trainable semi-local addressing pattern on the detuning (n_i terms in the Hamiltonian)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>addressable_drive</code> <p>whether to turn on the trainable semi-local addressing pattern on the drive (sigma_i^x terms in the Hamiltonian)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tunable_phase</code> <p>whether to have a tunable phase to get both sigma^x and sigma^y rotations in the drive term. If False, only a sigma^x term will be included in the drive part of the Hamiltonian generator</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>additional_prefix</code> <p>an additional prefix to attach to the parameter names</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>The Rydberg HEA block</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def rydberg_hea(\n    register: qd.Register,\n    n_layers: int = 1,\n    addressable_detuning: bool = True,\n    addressable_drive: bool = False,\n    tunable_phase: bool = False,\n    additional_prefix: str = None,\n) -&gt; qd.blocks.ChainBlock:\n    \"\"\"Hardware efficient ansatz for neutral atom (Rydberg) platforms.\n\n    This constructor implements a variational ansatz which is very close to\n    what is implementable on 2nd generation PASQAL quantum devices. In particular,\n    it implements evolution over a specific Hamiltonian which can be realized on\n    the device. This Hamiltonian contains:\n\n    * an interaction term given by the standard NN interaction and determined starting\n        from the positions in the input register: H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c\n\n    * a detuning term which corresponding to a n_i = (1+sigma_i^z)/2 applied to\n        all the qubits. If the `addressable_detuning` flag is set to True, the routine\n        effectively a local n_i = (1+sigma_i^z)/2 term in the\n        evolved Hamiltonian with a different coefficient for each atom. These\n        coefficients determine a local addressing pattern for the detuning on a subset\n        of the qubits. In this routine, the coefficients are variational parameters\n        and they will therefore be optimized at each optimizer step\n\n    * a drive term which corresponding to a sigma^x evolution operation applied to\n        all the qubits. If the `addressable_drive` flag is set to True, the routine\n        effectively a local sigma_i^x term in the evolved Hamiltonian with a different\n        coefficient for each atom. These coefficients determine a local addressing pattern\n        for the drive on a subset of the qubits. In this routine, the coefficients are\n        variational parameters and they will therefore be optimized at each optimizer step\n\n    * if the `tunable_phase` flag is set to True, the drive term is modified in the following\n        way: drive = cos(phi) * sigma^x - sin(phi) * sigma^y\n        The addressable pattern above is maintained and the phase is considered just as an\n        additional variational parameter which is optimized with the rest\n\n    Notice that, on real devices, the coefficients assigned to each qubit in both the detuning\n    and drive patterns should be non-negative and they should always sum to 1. This is not the\n    case for the implementation in this routine since the coefficients (weights) do not have any\n    constraint. Therefore, this HEA is not completely realizable on neutral atom devices.\n\n    Args:\n        register: the input atomic register with Cartesian coordinates.\n        n_layers: number layers in the HEA, each layer includes a drive, detuning and\n            pure interaction pulses whose is a variational parameter\n        addressable_detuning: whether to turn on the trainable semi-local addressing pattern\n            on the detuning (n_i terms in the Hamiltonian)\n        addressable_drive: whether to turn on the trainable semi-local addressing pattern\n            on the drive (sigma_i^x terms in the Hamiltonian)\n        tunable_phase: whether to have a tunable phase to get both sigma^x and sigma^y rotations\n            in the drive term. If False, only a sigma^x term will be included in the drive part\n            of the Hamiltonian generator\n        additional_prefix: an additional prefix to attach to the parameter names\n\n    Returns:\n        The Rydberg HEA block\n    \"\"\"\n    n_qubits = register.n_qubits\n    prefix = \"\" if additional_prefix is None else \"_\" + additional_prefix\n\n    detunings = None\n    # add a detuning pattern locally addressing the atoms\n    if addressable_detuning:\n        detunings = [qd.VariationalParameter(f\"detmap_{j}\") for j in range(n_qubits)]\n\n    drives = None\n    # add a drive pattern locally addressing the atoms\n    if addressable_drive:\n        drives = [qd.VariationalParameter(f\"drivemap_{j}\") for j in range(n_qubits)]\n\n    phase = None\n    if tunable_phase:\n        phase = qd.VariationalParameter(\"phase\")\n\n    return chain(\n        rydberg_hea_layer(\n            register,\n            VariationalParameter(f\"At{prefix}_{layer}\"),\n            VariationalParameter(f\"Omega{prefix}_{layer}\"),\n            VariationalParameter(f\"wait{prefix}_{layer}\"),\n            detunings=detunings,\n            drives=drives,\n            phase=phase,\n        )\n        for layer in range(n_layers)\n    )\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.rydberg_hea.rydberg_hea_layer","title":"<code>rydberg_hea_layer(register, tevo_drive, tevo_det, tevo_wait, phase=None, detunings=None, drives=None, drive_scaling=1.0)</code>","text":"<p>A single layer of the Rydberg hardware efficient ansatz.</p> PARAMETER DESCRIPTION <code>register</code> <p>the input register with atomic coordinates needed to build the interaction.</p> <p> TYPE: <code>Register</code> </p> <code>tevo_drive</code> <p>a variational parameter for the duration of the drive term of the Hamiltonian generator, including optional semi-local addressing</p> <p> TYPE: <code>Parameter | float</code> </p> <code>tevo_det</code> <p>a variational parameter for the duration of the detuning term of the Hamiltonian generator, including optional semi-local addressing</p> <p> TYPE: <code>Parameter | float</code> </p> <code>tevo_wait</code> <p>a variational parameter for the duration of the waiting time with interaction only</p> <p> TYPE: <code>Parameter | float</code> </p> <code>phase</code> <p>a variational parameter representing the global phase. If None, the global phase is set to 0 which results in a drive term in sigma^x only. Otherwise both sigma^x and sigma^y terms will be present</p> <p> TYPE: <code>Parameter | float | None</code> DEFAULT: <code>None</code> </p> <code>detunings</code> <p>a list of parameters with the weights of the locally addressed detuning terms. These are variational parameters which are tuned by the optimizer</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> <code>drives</code> <p>a list of parameters with the weights of the locally addressed drive terms. These are variational parameters which are tuned by the optimizer</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> <code>drive_scaling</code> <p>a scaling term to be added to the drive Hamiltonian generator</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>A block with a single layer of Rydberg HEA</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def rydberg_hea_layer(\n    register: qd.Register,\n    tevo_drive: Parameter | float,\n    tevo_det: Parameter | float,\n    tevo_wait: Parameter | float,\n    phase: Parameter | float | None = None,\n    detunings: list[Parameter] | list[float] | None = None,\n    drives: list[Parameter] | list[float] | None = None,\n    drive_scaling: float = 1.0,\n) -&gt; ChainBlock:\n    \"\"\"A single layer of the Rydberg hardware efficient ansatz.\n\n    Args:\n        register: the input register with atomic coordinates needed to build the interaction.\n        tevo_drive: a variational parameter for the duration of the drive term of\n            the Hamiltonian generator, including optional semi-local addressing\n        tevo_det: a variational parameter for the duration of the detuning term of the\n            Hamiltonian generator, including optional semi-local addressing\n        tevo_wait: a variational parameter for the duration of the waiting\n            time with interaction only\n        phase: a variational parameter representing the global phase. If None, the\n            global phase is set to 0 which results in a drive term in sigma^x only. Otherwise\n            both sigma^x and sigma^y terms will be present\n        detunings: a list of parameters with the weights of the locally addressed\n            detuning terms. These are variational parameters which are tuned by the optimizer\n        drives: a list of parameters with the weights of the locally addressed\n            drive terms. These are variational parameters which are tuned by the optimizer\n        drive_scaling: a scaling term to be added to the drive Hamiltonian generator\n\n    Returns:\n        A block with a single layer of Rydberg HEA\n    \"\"\"\n    n_qubits = register.n_qubits\n\n    drive_x = _amplitude_map(n_qubits, qd.X, weights=drives)\n    drive_y = _amplitude_map(n_qubits, qd.Y, weights=drives)\n    detuning = _amplitude_map(n_qubits, qd.N, weights=detunings)\n    interaction = hamiltonian_factory(register, qd.Interaction.NN)\n\n    # drive and interaction are not commuting thus they need to be\n    # added directly into the final Hamiltonian generator\n    if phase is not None:\n        generator = (\n            drive_scaling * sympy.cos(phase) * drive_x\n            - drive_scaling * sympy.sin(phase) * drive_y\n            + interaction\n        )\n    else:\n        generator = drive_scaling * drive_x + interaction\n\n    return chain(\n        qd.HamEvo(generator, tevo_drive),\n        # detuning and interaction are commuting, so they\n        # can be ordered arbitrarily and treated separately\n        qd.HamEvo(interaction, tevo_wait),\n        qd.HamEvo(detuning, tevo_det),\n    )\n</code></pre>"},{"location":"api/constructors/#the-daqc-transform","title":"The DAQC Transform","text":""},{"location":"api/constructors/#qadence.constructors.daqc.daqc.daqc_transform","title":"<code>daqc_transform(n_qubits, gen_target, t_f, gen_build=None, zero_tol=1e-08, strategy=Strategy.SDAQC, ignore_global_phases=False)</code>","text":"<p>Implements the DAQC transform for representing an arbitrary 2-body Hamiltonian.</p> <p>The result is another fixed 2-body Hamiltonian.</p> <p>Reference for universality of 2-body Hamiltonians:</p> <p>-- https://arxiv.org/abs/quant-ph/0106064</p> <p>Based on the transformation for Ising (ZZ) interactions, as described in the paper</p> <p>-- https://arxiv.org/abs/1812.03637</p> <p>The transform translates a target weighted generator of the type:</p> <pre><code>`gen_target = add(g_jk * kron(op(j), op(k)) for j &lt; k)`\n</code></pre> <p>To a circuit using analog evolutions with a fixed building block generator:</p> <pre><code>`gen_build = add(f_jk * kron(op(j), op(k)) for j &lt; k)`\n</code></pre> <p>where <code>op = Z</code> or <code>op = N</code>.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>total number of qubits to use.</p> <p> TYPE: <code>int</code> </p> <code>gen_target</code> <p>target generator built with the structure above. The type of the generator will be automatically evaluated when parsing.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>t_f</code> <p>total time for the gen_target evolution.</p> <p> TYPE: <code>float</code> </p> <code>gen_build</code> <p>fixed generator to act as a building block. Defaults to constant NN: add(1.0 * kron(N(j), N(k)) for j &lt; k). The type of the generator will be automatically evaluated when parsing.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <code>zero_tol</code> <p>default \"zero\" for a missing interaction. Included for numerical reasons, see notes below.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>strategy</code> <p>sDAQC or bDAQC, following definitions in the reference paper.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>SDAQC</code> </p> <code>ignore_global_phases</code> <p>if <code>True</code> the transform does not correct the global phases coming from the mapping between ZZ and NN interactions.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Notes:</p> <pre><code>The paper follows an index convention of running from 1 to N. A few functions\nhere also use that convention to be consistent with the paper. However, for qadence\nrelated things the indices are converted to [0, N-1].\n\nThe case for `n_qubits = 4` is an edge case where the sign matrix is not invertible.\nThere is a workaround for this described in the paper, but it is currently not implemented.\n\nThe current implementation may result in evolution times that are both positive or\nnegative. In practice, both can be represented by simply changing the signs of the\ninteractions. However, for a real implementation where the interactions should remain\nfixed, the paper discusses a workaround that is not currently implemented.\n\nThe transformation works by representing each interaction in the target hamiltonian by\na set of evolutions using the build hamiltonian. As a consequence, some care must be\ntaken when choosing the build hamiltonian. Some cases:\n\n- The target hamiltonian can have any interaction, as long as it is sufficiently\nrepresented in the build hamiltonian. E.g., if the interaction `g_01 * kron(Z(0), Z(1))`\nis in the target hamiltonian, the corresponding interaction `f_01 * kron(Z(0), Z(1))`\nneeds to be in the build hamiltonian. This is checked when the generators are parsed.\n\n- The build hamiltonian can have any interaction, irrespectively of it being needed\nfor the target hamiltonian. This is especially useful for designing local operations\nthrough the repeated evolution of a \"global\" hamiltonian.\n\n- The parameter `zero_tol` controls what it means for an interaction to be \"missing\".\nAny interaction strength smaller than `zero_tol` in the build hamiltonian will not be\nconsidered, and thus that interaction is missing.\n\n- The various ratios `g_jk / f_jk` will influence the time parameter for the various\nevolution slices, meaning that if there is a big discrepancy in the interaction strength\nfor a given qubit pair (j, k), the output circuit may require the usage of hamiltonian\nevolutions with very large times.\n\n- A warning will be issued for evolution times larger than `1/sqrt(zero_tol)`. Evolution\ntimes smaller than `zero_tol` will not be represented.\n</code></pre> <p>Examples:</p> <pre><code>from qadence import Z, N, daqc_transform\n\nn_qubits = 3\n\ngen_build = 0.5 * (N(0)@N(1)) + 0.7 * (N(1)@N(2)) + 0.2 * (N(0)@N(2))\n\ngen_target = 0.1 * (Z(1)@Z(2))\n\nt_f = 2.0\n\ntransformed_circuit = daqc_transform(\n    n_qubits = n_qubits,\n    gen_target = gen_target,\n    t_f = t_f,\n    gen_build = gen_build,\n)\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/daqc/daqc.py</code> <pre><code>def daqc_transform(\n    n_qubits: int,\n    gen_target: AbstractBlock,\n    t_f: float,\n    gen_build: AbstractBlock | None = None,\n    zero_tol: float = 1e-08,\n    strategy: Strategy = Strategy.SDAQC,\n    ignore_global_phases: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Implements the DAQC transform for representing an arbitrary 2-body Hamiltonian.\n\n    The result is another fixed 2-body Hamiltonian.\n\n    Reference for universality of 2-body Hamiltonians:\n\n    -- https://arxiv.org/abs/quant-ph/0106064\n\n    Based on the transformation for Ising (ZZ) interactions, as described in the paper\n\n    -- https://arxiv.org/abs/1812.03637\n\n    The transform translates a target weighted generator of the type:\n\n        `gen_target = add(g_jk * kron(op(j), op(k)) for j &lt; k)`\n\n    To a circuit using analog evolutions with a fixed building block generator:\n\n        `gen_build = add(f_jk * kron(op(j), op(k)) for j &lt; k)`\n\n    where `op = Z` or `op = N`.\n\n    Args:\n        n_qubits: total number of qubits to use.\n        gen_target: target generator built with the structure above. The type\n            of the generator will be automatically evaluated when parsing.\n        t_f (float): total time for the gen_target evolution.\n        gen_build: fixed generator to act as a building block. Defaults to\n            constant NN: add(1.0 * kron(N(j), N(k)) for j &lt; k). The type\n            of the generator will be automatically evaluated when parsing.\n        zero_tol: default \"zero\" for a missing interaction. Included for\n            numerical reasons, see notes below.\n        strategy: sDAQC or bDAQC, following definitions in the reference paper.\n        ignore_global_phases: if `True` the transform does not correct the global\n            phases coming from the mapping between ZZ and NN interactions.\n\n    Notes:\n\n        The paper follows an index convention of running from 1 to N. A few functions\n        here also use that convention to be consistent with the paper. However, for qadence\n        related things the indices are converted to [0, N-1].\n\n        The case for `n_qubits = 4` is an edge case where the sign matrix is not invertible.\n        There is a workaround for this described in the paper, but it is currently not implemented.\n\n        The current implementation may result in evolution times that are both positive or\n        negative. In practice, both can be represented by simply changing the signs of the\n        interactions. However, for a real implementation where the interactions should remain\n        fixed, the paper discusses a workaround that is not currently implemented.\n\n        The transformation works by representing each interaction in the target hamiltonian by\n        a set of evolutions using the build hamiltonian. As a consequence, some care must be\n        taken when choosing the build hamiltonian. Some cases:\n\n        - The target hamiltonian can have any interaction, as long as it is sufficiently\n        represented in the build hamiltonian. E.g., if the interaction `g_01 * kron(Z(0), Z(1))`\n        is in the target hamiltonian, the corresponding interaction `f_01 * kron(Z(0), Z(1))`\n        needs to be in the build hamiltonian. This is checked when the generators are parsed.\n\n        - The build hamiltonian can have any interaction, irrespectively of it being needed\n        for the target hamiltonian. This is especially useful for designing local operations\n        through the repeated evolution of a \"global\" hamiltonian.\n\n        - The parameter `zero_tol` controls what it means for an interaction to be \"missing\".\n        Any interaction strength smaller than `zero_tol` in the build hamiltonian will not be\n        considered, and thus that interaction is missing.\n\n        - The various ratios `g_jk / f_jk` will influence the time parameter for the various\n        evolution slices, meaning that if there is a big discrepancy in the interaction strength\n        for a given qubit pair (j, k), the output circuit may require the usage of hamiltonian\n        evolutions with very large times.\n\n        - A warning will be issued for evolution times larger than `1/sqrt(zero_tol)`. Evolution\n        times smaller than `zero_tol` will not be represented.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import Z, N, daqc_transform\n\n        n_qubits = 3\n\n        gen_build = 0.5 * (N(0)@N(1)) + 0.7 * (N(1)@N(2)) + 0.2 * (N(0)@N(2))\n\n        gen_target = 0.1 * (Z(1)@Z(2))\n\n        t_f = 2.0\n\n        transformed_circuit = daqc_transform(\n            n_qubits = n_qubits,\n            gen_target = gen_target,\n            t_f = t_f,\n            gen_build = gen_build,\n        )\n        ```\n    \"\"\"\n\n    ##################\n    # Input controls #\n    ##################\n\n    if strategy != Strategy.SDAQC:\n        raise NotImplementedError(\"Currently only the sDAQC transform is implemented.\")\n\n    if n_qubits == 4:\n        raise NotImplementedError(\"DAQC transform 4-qubit edge case not implemented.\")\n\n    if gen_build is None:\n        gen_build = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n\n    try:\n        if (not block_is_qubit_hamiltonian(gen_target)) or (\n            not block_is_qubit_hamiltonian(gen_build)\n        ):\n            raise ValueError(\n                \"Generator block is not a qubit Hamiltonian. Only ZZ or NN interactions allowed.\"\n            )\n    except NotImplementedError:\n        # Happens when block_is_qubit_hamiltonian is called on something that is not a block.\n        raise TypeError(\n            \"Generator block is not a qubit Hamiltonian. Only ZZ or NN interactions allowed.\"\n        )\n\n    #####################\n    # Generator parsing #\n    #####################\n\n    g_jk_target, mat_jk_target, target_type = _parse_generator(n_qubits, gen_target, 0.0)\n    g_jk_build, mat_jk_build, build_type = _parse_generator(n_qubits, gen_build, zero_tol)\n\n    # Get the global phase hamiltonian and single-qubit detuning hamiltonian\n    if build_type == GenDAQC.NN:\n        h_phase_build, h_sq_build = _nn_phase_and_detunings(n_qubits, mat_jk_build)\n\n    if target_type == GenDAQC.NN:\n        h_phase_target, h_sq_target = _nn_phase_and_detunings(n_qubits, mat_jk_target)\n\n    # Time re-scalings\n    if build_type == GenDAQC.ZZ and target_type == GenDAQC.NN:\n        t_star = t_f / 4.0\n    elif build_type == GenDAQC.NN and target_type == GenDAQC.ZZ:\n        t_star = 4.0 * t_f\n    else:\n        t_star = t_f\n\n    # Check if target Hamiltonian can be mapped with the build Hamiltonian\n    assert _check_compatibility(g_jk_target, g_jk_build, zero_tol)\n\n    ##################\n    # DAQC Transform #\n    ##################\n\n    # Section III A of https://arxiv.org/abs/1812.03637:\n\n    # Matrix M for the linear system, exemplified in Table I:\n    matrix_M = _build_matrix_M(n_qubits)\n\n    # Linear system mapping interaction ratios -&gt; evolution times.\n    t_slices = torch.linalg.solve(matrix_M, g_jk_target / g_jk_build) * t_star\n\n    # ZZ-DAQC with ZZ or NN build Hamiltonian\n    daqc_slices = []\n    for m in range(2, n_qubits + 1):\n        for n in range(1, m):\n            alpha = _ix_map(n_qubits, n, m)\n            t = t_slices[alpha - 1]\n            if abs(t) &gt; zero_tol:\n                if abs(t) &gt; (1 / (zero_tol**0.5)):\n                    logger.warning(\n                        \"\"\"\nTransformed circuit with very long evolution time.\nMake sure your target interactions are sufficiently\nrepresented in the build Hamiltonian.\"\"\"\n                    )\n                x_gates = kron(X(n - 1), X(m - 1))\n                analog_evo = HamEvo(gen_build, t)\n                # TODO: Fix repeated X-gates\n                if build_type == GenDAQC.NN:\n                    # Local detuning at each DAQC layer for NN build Hamiltonian\n                    sq_detuning_build = HamEvo(h_sq_build, t)\n                    daqc_slices.append(chain(x_gates, sq_detuning_build, analog_evo, x_gates))\n                elif build_type == GenDAQC.ZZ:\n                    daqc_slices.append(chain(x_gates, analog_evo, x_gates))\n\n    daqc_circuit = chain(*daqc_slices)\n\n    ########################\n    # Phases and Detunings #\n    ########################\n\n    if target_type == GenDAQC.NN:\n        # Local detuning given a NN target Hamiltonian\n        sq_detuning_target = HamEvo(h_sq_target, t_f).dagger()\n        daqc_circuit = chain(sq_detuning_target, daqc_circuit)\n\n    if not ignore_global_phases:\n        if build_type == GenDAQC.NN:\n            # Constant global phase given a NN build Hamiltonian\n            global_phase_build = HamEvo(h_phase_build, t_slices.sum())\n            daqc_circuit = chain(global_phase_build, daqc_circuit)\n\n        if target_type == GenDAQC.NN:\n            # Constant global phase and given a NN target Hamiltonian\n            global_phase_target = HamEvo(h_phase_target, t_f).dagger()\n            daqc_circuit = chain(global_phase_target, daqc_circuit)\n\n    return daqc_circuit\n</code></pre>"},{"location":"api/constructors/#some-utility-functions","title":"Some utility functions","text":""},{"location":"api/constructors/#qadence.constructors.utils.build_idx_fms","title":"<code>build_idx_fms(basis, fm_pauli, multivariate_strategy, n_features, n_qubits, reupload_scaling)</code>","text":"<p>Builds the index feature maps based on the given parameters.</p> PARAMETER DESCRIPTION <code>basis</code> <p>Type of basis chosen for the feature map.</p> <p> TYPE: <code>BasisSet</code> </p> <code>fm_pauli</code> <p>The chosen Pauli rotation type.</p> <p> TYPE: <code>PrimitiveBlock type</code> </p> <code>multivariate_strategy</code> <p>The strategy used for encoding the multivariate feature map.</p> <p> TYPE: <code>MultivariateStrategy</code> </p> <code>n_features</code> <p>The number of features.</p> <p> TYPE: <code>int</code> </p> <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>reupload_scaling</code> <p>The chosen scaling for the reupload.</p> <p> TYPE: <code>ReuploadScaling</code> </p> RETURNS DESCRIPTION <code>list[KronBlock]</code> <p>List[KronBlock]: The list of index feature maps.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def build_idx_fms(\n    basis: BasisSet,\n    fm_pauli: Type[RY],\n    multivariate_strategy: MultivariateStrategy,\n    n_features: int,\n    n_qubits: int,\n    reupload_scaling: ReuploadScaling,\n) -&gt; list[KronBlock]:\n    \"\"\"Builds the index feature maps based on the given parameters.\n\n    Args:\n        basis (BasisSet): Type of basis chosen for the feature map.\n        fm_pauli (PrimitiveBlock type): The chosen Pauli rotation type.\n        multivariate_strategy (MultivariateStrategy): The strategy used for encoding\n            the multivariate feature map.\n        n_features (int): The number of features.\n        n_qubits (int): The number of qubits.\n        reupload_scaling (ReuploadScaling): The chosen scaling for the reupload.\n\n    Returns:\n        List[KronBlock]: The list of index feature maps.\n    \"\"\"\n    idx_fms = []\n    for i in range(n_features):\n        target_qubits = get_fm_qubits(multivariate_strategy, i, n_qubits, n_features)\n        param = FeatureParameter(f\"x{i}\")\n        block = kron(\n            *[\n                fm_pauli(qubit, generator_prefactor(reupload_scaling, j) * basis_func(basis, param))\n                for j, qubit in enumerate(target_qubits)\n            ]\n        )\n        idx_fm = block\n        idx_fms.append(idx_fm)\n    return idx_fms\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.utils.generator_prefactor","title":"<code>generator_prefactor(reupload_scaling, qubit_index)</code>","text":"<p>Converts a spectrum string, e.g. tower or exponential.</p> <p>The result is the correct generator prefactor.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def generator_prefactor(reupload_scaling: ReuploadScaling, qubit_index: int) -&gt; float | int:\n    \"\"\"Converts a spectrum string, e.g. tower or exponential.\n\n    The result is the correct generator prefactor.\n    \"\"\"\n    conversion_dict: dict[str, float | int] = {\n        ReuploadScaling.CONSTANT: 1,\n        ReuploadScaling.TOWER: qubit_index + 1,\n        ReuploadScaling.EXP: 2 * PI / (2 ** (qubit_index + 1)),\n    }\n    return conversion_dict[reupload_scaling]\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.utils.get_fm_qubits","title":"<code>get_fm_qubits(multivariate_strategy, i, n_qubits, n_features)</code>","text":"<p>Returns the list of target qubits for the given feature map strategy and feature index.</p> PARAMETER DESCRIPTION <code>multivariate_strategy</code> <p>The strategy used for encoding the multivariate feature map.</p> <p> TYPE: <code>MultivariateStrategy</code> </p> <code>i</code> <p>The feature index.</p> <p> TYPE: <code>int</code> </p> <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>The number of features.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Iterable</code> <p>List[int]: The list of target qubits.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the feature map strategy is not implemented.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def get_fm_qubits(\n    multivariate_strategy: MultivariateStrategy, i: int, n_qubits: int, n_features: int\n) -&gt; Iterable:\n    \"\"\"Returns the list of target qubits for the given feature map strategy and feature index.\n\n    Args:\n        multivariate_strategy (MultivariateStrategy): The strategy used for encoding\n            the multivariate feature map.\n        i (int): The feature index.\n        n_qubits (int): The number of qubits.\n        n_features (int): The number of features.\n\n    Returns:\n        List[int]: The list of target qubits.\n\n    Raises:\n        ValueError: If the feature map strategy is not implemented.\n    \"\"\"\n    if multivariate_strategy == MultivariateStrategy.PARALLEL:\n        n_qubits_per_feature = int(n_qubits / n_features)\n        target_qubits = range(i * n_qubits_per_feature, (i + 1) * n_qubits_per_feature)\n    elif multivariate_strategy == MultivariateStrategy.SERIES:\n        target_qubits = range(0, n_qubits)\n    else:\n        raise ValueError(f\"Multivariate strategy {multivariate_strategy} not implemented.\")\n    return target_qubits\n</code></pre>"},{"location":"api/draw/","title":"Drawing","text":""},{"location":"api/draw/#drawing","title":"Drawing","text":""},{"location":"api/draw/#qadence.draw.display","title":"<code>display(x, qcd=None, layout='LR', theme='light', fill=True, **kwargs)</code>","text":"<p>Display a block, circuit, or quantum model.</p> <p>The <code>kwargs</code> are forwarded to the underlying <code>nx.Graph</code>, so you can e.g. specify the size of the resulting plot via <code>size=\"2,2\"</code> (see examples)</p> PARAMETER DESCRIPTION <code>x</code> <p><code>AbstractBlock</code>, <code>QuantumCircuit</code>, or <code>QuantumModel</code>.</p> <p> TYPE: <code>Any</code> </p> <code>qcd</code> <p>Circuit diagram to plot the block into.</p> <p> TYPE: <code>QuantumCircuitDiagram | Cluster | None</code> DEFAULT: <code>None</code> </p> <code>layout</code> <p>Can be either \"LR\" (left-right), or \"TB\" (top-bottom).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'LR'</code> </p> <code>theme</code> <p>Available themes are: [\"light\", \"dark\", \"black\", \"white\"].</p> <p> TYPE: <code>str</code> DEFAULT: <code>'light'</code> </p> <code>fill</code> <p>Whether to fill the passed <code>x</code> with identities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>Passed on to <code>nx.Graph</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Examples: <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\ndisplay(b, size=\"1,1\", theme=\"dark\")\n</code></pre> </p> Source code in <code>qadence/draw/__init__.py</code> <pre><code>def display(\n    x: Any,\n    qcd: QuantumCircuitDiagram | Cluster | None = None,\n    layout: str = \"LR\",\n    theme: str = \"light\",\n    fill: bool = True,\n    **kwargs: Any,\n) -&gt; Graph:\n    \"\"\"Display a block, circuit, or quantum model.\n\n    The `kwargs` are forwarded to\n    the underlying `nx.Graph`, so you can e.g. specify the size of the resulting plot via\n    `size=\"2,2\"` (see examples)\n\n    Arguments:\n        x: `AbstractBlock`, `QuantumCircuit`, or `QuantumModel`.\n        qcd: Circuit diagram to plot the block into.\n        layout: Can be either \"LR\" (left-right), or \"TB\" (top-bottom).\n        theme: Available themes are: [\"light\", \"dark\", \"black\", \"white\"].\n        fill: Whether to fill the passed `x` with identities.\n        kwargs: Passed on to `nx.Graph`\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" html=\"1\"\n    from qadence import X, Y, kron\n    from qadence.draw import display\n\n    b = kron(X(0), Y(1))\n    def display(*args, **kwargs): return args # markdown-exec: hide\n    display(b, size=\"1,1\", theme=\"dark\")\n    ```\n    \"\"\"\n    return make_diagram(x, **kwargs).show()\n</code></pre>"},{"location":"api/draw/#qadence.draw.savefig","title":"<code>savefig(x, filename, *args, **kwargs)</code>","text":"<p>Save a block, circuit, or quantum model to file. Accepts the same args/kwargs as <code>display</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>AbstractBlock</code>, <code>QuantumCircuit</code>, or <code>QuantumModel</code>.</p> <p> TYPE: <code>Any</code> </p> <code>filename</code> <p>Should end in svg/png.</p> <p> TYPE: <code>str</code> </p> <code>args</code> <p>Same as in <code>display</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>Same as in <code>display</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Examples: <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\nsavefig(b, \"test.svg\", size=\"1,1\", theme=\"dark\")\n</code></pre> </p> Source code in <code>qadence/draw/__init__.py</code> <pre><code>def savefig(x: Any, filename: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Save a block, circuit, or quantum model to file. Accepts the same args/kwargs as `display`.\n\n    Arguments:\n        x: `AbstractBlock`, `QuantumCircuit`, or `QuantumModel`.\n        filename: Should end in svg/png.\n        args: Same as in `display`.\n        kwargs: Same as in `display`.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" html=\"1\"\n    from qadence import X, Y, kron\n    from qadence.draw import display\n\n    b = kron(X(0), Y(1))\n    def savefig(*args, **kwargs): return args # markdown-exec: hide\n    savefig(b, \"test.svg\", size=\"1,1\", theme=\"dark\")\n    ```\n    \"\"\"\n    make_diagram(x, *args, **kwargs).savefig(filename)\n</code></pre>"},{"location":"api/execution/","title":"Execution","text":""},{"location":"api/execution/#qadence.execution.expectation","title":"<code>expectation(x, observable, values=None, state=None, backend=BackendName.PYQTORCH, diff_mode=None, noise=None, endianness=Endianness.BIG, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.expectation</code> method.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>observable</code> <p>Observable(s) w.r.t. which the expectation is computed.</p> <p> TYPE: <code>Union[list[AbstractBlock], AbstractBlock]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>Which differentiation mode to use.</p> <p> TYPE: <code>Union[DiffMode, str, None]</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A wavefunction</p> <pre><code>from qadence import RX, Z, Register, QuantumCircuit, expectation\n\nreg = Register(1)\nblock = RX(0, 0.5)\nobservable = Z(0)\ncirc = QuantumCircuit(reg, block)\n\n# You can compute the expectation for a\n# QuantumCircuit with a given observable.\nexpectation(circ, observable)\n\n# You can also use only a block.\n# In this case the register is constructed automatically to\n# Register.line(block.n_qubits)\nexpectation(block, observable)\n\n# Or a register and block\nexpectation(reg, block, observable)\n</code></pre> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef expectation(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    observable: Union[list[AbstractBlock], AbstractBlock],\n    values: Union[dict, None] = None,\n    state: Tensor = None,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: Union[DiffMode, str, None] = None,\n    noise: Union[NoiseHandler, None] = None,\n    endianness: Endianness = Endianness.BIG,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; Tensor:\n    \"\"\"Convenience wrapper for the `QuantumModel.expectation` method.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        observable: Observable(s) w.r.t. which the expectation is computed.\n        values: User-facing parameter dict.\n        state: Initial state.\n        backend: Name of the backend to run on.\n        diff_mode: Which differentiation mode to use.\n        endianness: The target device endianness.\n        configuration: The backend configuration.\n\n    Returns:\n        A wavefunction\n\n    ```python exec=\"on\" source=\"material-block\"\n    from qadence import RX, Z, Register, QuantumCircuit, expectation\n\n    reg = Register(1)\n    block = RX(0, 0.5)\n    observable = Z(0)\n    circ = QuantumCircuit(reg, block)\n\n    # You can compute the expectation for a\n    # QuantumCircuit with a given observable.\n    expectation(circ, observable)\n\n    # You can also use only a block.\n    # In this case the register is constructed automatically to\n    # Register.line(block.n_qubits)\n    expectation(block, observable)\n\n    # Or a register and block\n    expectation(reg, block, observable)\n    ```\n    \"\"\"\n\n    raise ValueError(f\"Cannot execute {type(x)}\")\n</code></pre>"},{"location":"api/execution/#qadence.execution.run","title":"<code>run(x, *args, values=None, state=None, backend=BackendName.PYQTORCH, endianness=Endianness.BIG, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.run</code> method.</p> <p>This is a <code>functools.singledispatch</code>ed function so it can be called with a number of different arguments. See the examples of the <code>expectation</code> function. This function works exactly the same.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A wavefunction</p> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef run(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    *args: Any,\n    values: Union[dict, None] = None,\n    state: Tensor = None,\n    backend: BackendName = BackendName.PYQTORCH,\n    endianness: Endianness = Endianness.BIG,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; Tensor:\n    \"\"\"Convenience wrapper for the `QuantumModel.run` method.\n\n     This is a\n    `functools.singledispatch`ed function so it can be called with a number of different arguments.\n    See the examples of the [`expectation`][qadence.execution.expectation] function. This function\n    works exactly the same.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        values: User-facing parameter dict.\n        state: Initial state.\n        backend: Name of the backend to run on.\n        endianness: The target device endianness.\n        configuration: The backend configuration.\n\n    Returns:\n        A wavefunction\n    \"\"\"\n    raise ValueError(f\"Cannot run {type(x)}\")\n</code></pre>"},{"location":"api/execution/#qadence.execution.sample","title":"<code>sample(x, *args, values=None, state=None, n_shots=100, backend=BackendName.PYQTORCH, endianness=Endianness.BIG, noise=None, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.sample</code> method.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Union[Tensor, None]</code> DEFAULT: <code>None</code> </p> <code>n_shots</code> <p>Number of shots per element in the batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>noise</code> <p>The noise model to use if any.</p> <p> TYPE: <code>Union[NoiseHandler, None]</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Counter]</code> <p>A list of Counter instances with the sample results</p> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef sample(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    *args: Any,\n    values: Union[dict, None] = None,\n    state: Union[Tensor, None] = None,\n    n_shots: int = 100,\n    backend: BackendName = BackendName.PYQTORCH,\n    endianness: Endianness = Endianness.BIG,\n    noise: Union[NoiseHandler, None] = None,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; list[Counter]:\n    \"\"\"Convenience wrapper for the `QuantumModel.sample` method.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        values: User-facing parameter dict.\n        state: Initial state.\n        n_shots: Number of shots per element in the batch.\n        backend: Name of the backend to run on.\n        endianness: The target device endianness.\n        noise: The noise model to use if any.\n        configuration: The backend configuration.\n\n    Returns:\n        A list of Counter instances with the sample results\n    \"\"\"\n    raise ValueError(f\"Cannot sample from {type(x)}\")\n</code></pre>"},{"location":"api/ml_tools/","title":"QML tools","text":""},{"location":"api/ml_tools/#ml-tools","title":"ML Tools","text":"<p>This module implements a <code>Trainer</code> class for torch <code>Modules</code> and <code>QuantumModel</code>. It also implements the <code>QNN</code> class and callbacks that can be used with the trainer module.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer","title":"<code>Trainer(model, optimizer, config, loss_fn='mse', train_dataloader=None, val_dataloader=None, test_dataloader=None, optimize_step=optimize_step, device=None, dtype=None, max_batches=None)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class to manage and execute training, validation, and testing loops for a model (eg.</p> <p>QNN).</p> <p>This class handles the overall training process, including: - Managing epochs and steps - Handling data loading and batching - Computing and updating gradients - Logging and monitoring training metrics</p> ATTRIBUTE DESCRIPTION <code>current_epoch</code> <p>The current epoch number.</p> <p> TYPE: <code>int</code> </p> <code>global_step</code> <p>The global step across all epochs.</p> <p> TYPE: <code>int</code> </p> <code>log_device</code> <p>Device for logging, default is \"cpu\".</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device used for computation.</p> <p> TYPE: <code>device</code> </p> <code>dtype</code> <p>Data type used for computation.</p> <p> TYPE: <code>dtype | None</code> </p> <code>data_dtype</code> <p>Data type for data. Depends on the model's data type.</p> <p> TYPE: <code>dtype | None</code> </p> Inherited Attributes <p>use_grad (bool): Indicates if gradients are used for optimization. Default is True.</p> <p>model (nn.Module): The neural network model. optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training. config (TrainConfig): The configuration settings for training. train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data. val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data. test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.</p> <p>optimize_step (Callable): Function for performing an optimization step. loss_fn (Callable): loss function to use.</p> <p>num_training_batches (int): Number of training batches. num_validation_batches (int): Number of validation batches. num_test_batches (int): Number of test batches.</p> <p>state (str): Current state in the training process</p> <p>Default training routine <pre><code>for epoch in max_iter + 1:\n    # Training\n    for batch in train_batches:\n        train model\n    # Validation\n    if val_every % epoch == 0:\n        for batch in val_batches:\n            train model\n</code></pre></p> Notes <ul> <li>In case of InfiniteTensorDataset, number of batches = 1.</li> <li>In case of TensorDataset, number of batches are default.</li> <li>Training is run for max_iter + 1 epochs. Epoch 0 logs untrained model.</li> <li>Please look at the CallbackManager initialize_callbacks method to review the default     logging behavior.</li> </ul> <p>Examples:</p> <pre><code>import torch\nfrom torch.optim import SGD\nfrom qadence import (\n    feature_map,\n    hamiltonian_factory,\n    hea,\n    QNN,\n    QuantumCircuit,\n    TrainConfig,\n    Z,\n)\nfrom qadence.ml_tools.trainer import Trainer\nfrom qadence.ml_tools.optimize_step import optimize_step\nfrom qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.data import to_dataloader\n\n# Initialize the model\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=2)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\n\n# Set up the optimizer\noptimizer = SGD(model.parameters(), lr=0.001)\n\n# Use TrainConfig for configuring the training process\nconfig = TrainConfig(\n    max_iter=100,\n    print_every=10,\n    write_every=10,\n    checkpoint_every=10,\n    val_every=10\n)\n\n# Create the Trainer instance with TrainConfig\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\",\n    optimize_step=optimize_step\n)\n\nbatch_size = 25\nx = torch.linspace(0, 1, 32).reshape(-1, 1)\ny = torch.sin(x)\ntrain_loader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_loader = to_dataloader(x, y, batch_size=batch_size, infinite=False)\n\n# Train the model\nmodel, optimizer = trainer.fit(train_loader, val_loader)\n</code></pre> <p>This also supports both gradient based and gradient free optimization. The default support is for gradient based optimization.</p> <p>Notes:</p> <ul> <li>set_use_grad() (class level):This method is used to set the global <code>use_grad</code> flag,     controlling whether the trainer uses gradient-based optimization. <pre><code># gradient based\nTrainer.set_use_grad(True)\n\n# gradient free\nTrainer.set_use_grad(False)\n</code></pre></li> <li>Context Managers (instance level):  <code>enable_grad_opt()</code> and <code>disable_grad_opt()</code> are     context managers that temporarily switch the optimization mode for specific code blocks.     This is useful when you want to mix gradient-based and gradient-free optimization     in the same training process. <pre><code># gradient based\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit()\n\n# gradient free\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit()\n</code></pre></li> </ul> <p>Examples</p> <p>Gradient based optimization example Usage: <pre><code>from torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nTrainer.set_use_grad(True)\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>trainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Gradient free optimization example Usage: <pre><code>import nevergrad as ng\nfrom qadence.ml_tools.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n                budget=config.max_iter, parametrization= num_parameters(model)\n                )\n\nTrainer.set_use_grad(False)\ntrainer = Trainer(\n    model=model,\n    optimizer=ng_optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>import nevergrad as ng\nfrom qadence.ml_tools.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n        budget=config.max_iter, parametrization= num_parameters(model)\n        )\n\ntrainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Initializes the Trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>Training configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>Loss function used for training. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>optimize_step</code> <p>Function to execute an optimization step.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>optimize_step</code> </p> <code>device</code> <p>Device to use for computation.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for computation.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    optimize_step: Callable = optimize_step,\n    device: torch_device | None = None,\n    dtype: torch_dtype | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the Trainer class.\n\n    Args:\n        model (nn.Module): The PyTorch model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training.\n        config (TrainConfig): Training configuration object.\n        loss_fn (str | Callable ): Loss function used for training.\n            If not specified, default mse loss will be used.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for test data.\n        optimize_step (Callable): Function to execute an optimization step.\n        device (torch_device): Device to use for computation.\n        dtype (torch_dtype): Data type for computation.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    super().__init__(\n        model=model,\n        optimizer=optimizer,\n        config=config,\n        loss_fn=loss_fn,\n        optimize_step=optimize_step,\n        train_dataloader=train_dataloader,\n        val_dataloader=val_dataloader,\n        test_dataloader=test_dataloader,\n        max_batches=max_batches,\n    )\n    self.current_epoch: int = 0\n    self.global_step: int = 0\n    self.log_device: str = \"cpu\" if device is None else device\n    self.device: torch_device | None = device\n    self.dtype: torch_dtype | None = dtype\n    self.data_dtype: torch_dtype | None = None\n    if self.dtype:\n        self.data_dtype = float64 if (self.dtype == complex128) else float32\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.build_optimize_result","title":"<code>build_optimize_result(result)</code>","text":"<p>Builds and stores the optimization result by calculating the average loss and metrics.</p> <p>Result (or loss_metrics) can have multiple formats: - <code>None</code> Indicates no loss or metrics data is provided. - <code>tuple[torch.Tensor, dict[str, Any]]</code> A single tuple containing the loss tensor     and metrics dictionary - at the end of batch. - <code>list[tuple[torch.Tensor, dict[str, Any]]]</code> A list of tuples for     multiple batches. - <code>list[list[tuple[torch.Tensor, dict[str, Any]]]]</code> A list of lists of tuples, where each inner list represents metrics across multiple batches within an epoch.</p> PARAMETER DESCRIPTION <code>result</code> <p>(None |     tuple[torch.Tensor, dict[Any, Any]] |     list[tuple[torch.Tensor, dict[Any, Any]]] |     list[list[tuple[torch.Tensor, dict[Any, Any]]]])         The loss and metrics data, which can have multiple formats</p> <p> TYPE: <code>None | tuple[Tensor, dict[Any, Any]] | list[tuple[Tensor, dict[Any, Any]]] | list[list[tuple[Tensor, dict[Any, Any]]]]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>This method does not return anything. It sets <code>self.opt_result</code> with</p> <p> TYPE: <code>None</code> </p> <code>None</code> <p>the computed average loss and metrics.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def build_optimize_result(\n    self,\n    result: None\n    | tuple[torch.Tensor, dict[Any, Any]]\n    | list[tuple[torch.Tensor, dict[Any, Any]]]\n    | list[list[tuple[torch.Tensor, dict[Any, Any]]]],\n) -&gt; None:\n    \"\"\"\n    Builds and stores the optimization result by calculating the average loss and metrics.\n\n    Result (or loss_metrics) can have multiple formats:\n    - `None` Indicates no loss or metrics data is provided.\n    - `tuple[torch.Tensor, dict[str, Any]]` A single tuple containing the loss tensor\n        and metrics dictionary - at the end of batch.\n    - `list[tuple[torch.Tensor, dict[str, Any]]]` A list of tuples for\n        multiple batches.\n    - `list[list[tuple[torch.Tensor, dict[str, Any]]]]` A list of lists of tuples,\n    where each inner list represents metrics across multiple batches within an epoch.\n\n    Args:\n        result: (None |\n                tuple[torch.Tensor, dict[Any, Any]] |\n                list[tuple[torch.Tensor, dict[Any, Any]]] |\n                list[list[tuple[torch.Tensor, dict[Any, Any]]]])\n                    The loss and metrics data, which can have multiple formats\n\n    Returns:\n        None: This method does not return anything. It sets `self.opt_result` with\n        the computed average loss and metrics.\n    \"\"\"\n    loss_metrics = result\n    if loss_metrics is None:\n        loss = None\n        metrics: dict[Any, Any] = {}\n    elif isinstance(loss_metrics, tuple):\n        # Single tuple case\n        loss, metrics = loss_metrics\n    else:\n        last_epoch: list[tuple[torch.Tensor, dict[Any, Any]]] = []\n        if isinstance(loss_metrics, list):\n            # Check if it's a list of tuples\n            if all(isinstance(item, tuple) for item in loss_metrics):\n                last_epoch = cast(list[tuple[torch.Tensor, dict[Any, Any]]], loss_metrics)\n            # Check if it's a list of lists of tuples\n            elif all(isinstance(item, list) for item in loss_metrics):\n                last_epoch = cast(\n                    list[tuple[torch.Tensor, dict[Any, Any]]],\n                    loss_metrics[-1] if loss_metrics else [],\n                )\n            else:\n                raise ValueError(\n                    \"Invalid format for result: Expected None, tuple, list of tuples,\"\n                    \" or list of lists of tuples.\"\n                )\n\n        if not last_epoch:\n            loss, metrics = None, {}\n        else:\n            # Compute the average loss over the batches\n            loss_tensor = torch.stack([loss_batch for loss_batch, _ in last_epoch])\n            avg_loss = loss_tensor.mean()\n\n            # Collect and average metrics for all batches\n            metric_keys = last_epoch[0][1].keys()\n            metrics_stacked: dict = {key: [] for key in metric_keys}\n\n            for _, metrics_batch in last_epoch:\n                for key in metric_keys:\n                    value = metrics_batch[key]\n                    metrics_stacked[key].append(value)\n\n            avg_metrics = {key: torch.stack(metrics_stacked[key]).mean() for key in metric_keys}\n\n            loss, metrics = avg_loss, avg_metrics\n\n    # Store the optimization result\n    self.opt_result = OptimizeResult(\n        self.current_epoch, self.model_old, self.optimizer_old, loss, metrics\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.fit","title":"<code>fit(train_dataloader=None, val_dataloader=None)</code>","text":"<p>Fits the model using the specified training configuration.</p> <p>The dataloaders can be provided to train on new datasets, or the default dataloaders provided in the trainer will be used.</p> PARAMETER DESCRIPTION <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Module, Optimizer]</code> <p>tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def fit(\n    self,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; tuple[nn.Module, optim.Optimizer]:\n    \"\"\"\n    Fits the model using the specified training configuration.\n\n    The dataloaders can be provided to train on new datasets, or the default dataloaders\n    provided in the trainer will be used.\n\n    Args:\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n\n    Returns:\n        tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.\n    \"\"\"\n    if train_dataloader is not None:\n        self.train_dataloader = train_dataloader\n    if val_dataloader is not None:\n        self.val_dataloader = val_dataloader\n\n    self._fit_setup()\n    self._train()\n    self._fit_end()\n    self.training_stage = TrainingStage(\"idle\")\n    return self.model, self.optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_test_batch","title":"<code>run_test_batch(batch)</code>","text":"<p>Runs a single test batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"test_batch\")\ndef run_test_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single test batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_train_batch","title":"<code>run_train_batch(batch)</code>","text":"<p>Runs a single training batch, performing optimization.</p> <p>We use the step function to optimize the model based on use_grad.     use_grad = True entails gradient based optimization, for which we use     optimize_step function.     use_grad = False entails gradient free optimization, for which we use     update_ng_parameters function.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch. tuple of (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_batch\")\ndef run_train_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single training batch, performing optimization.\n\n    We use the step function to optimize the model based on use_grad.\n        use_grad = True entails gradient based optimization, for which we use\n        optimize_step function.\n        use_grad = False entails gradient free optimization, for which we use\n        update_ng_parameters function.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n            tuple of (loss, metrics)\n    \"\"\"\n\n    if self.use_grad:\n        # Perform gradient-based optimization\n        loss_metrics = self.optimize_step(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            xs=batch,\n            device=self.device,\n            dtype=self.data_dtype,\n        )\n    else:\n        # Perform optimization using Nevergrad\n        loss, metrics, ng_params = update_ng_parameters(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            data=batch,\n            ng_params=self.ng_params,  # type: ignore[arg-type]\n        )\n        self.ng_params = ng_params\n        loss_metrics = loss, metrics\n\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_training","title":"<code>run_training(dataloader)</code>","text":"<p>Runs the training for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_epoch\")\ndef run_training(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the training for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for training data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.train()\n    train_epoch_loss_metrics = []\n    # Quick Fix for iteration 0\n    self._reset_model_and_opt()\n\n    for batch in self._batch_iter(dataloader, self.num_training_batches):\n        self.on_train_batch_start(batch)\n        train_batch_loss_metrics = self.run_train_batch(batch)\n        train_epoch_loss_metrics.append(train_batch_loss_metrics)\n        self.on_train_batch_end(train_batch_loss_metrics)\n\n    return train_epoch_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_val_batch","title":"<code>run_val_batch(batch)</code>","text":"<p>Runs a single validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_batch\")\ndef run_val_batch(self, batch: tuple[torch.Tensor, ...]) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single validation batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_validation","title":"<code>run_validation(dataloader)</code>","text":"<p>Runs the validation loop for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_epoch\")\ndef run_validation(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the validation loop for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for validation data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.eval()\n    val_epoch_loss_metrics = []\n\n    for batch in self._batch_iter(dataloader, self.num_validation_batches):\n        self.on_val_batch_start(batch)\n        val_batch_loss_metrics = self.run_val_batch(batch)\n        val_epoch_loss_metrics.append(val_batch_loss_metrics)\n        self.on_val_batch_end(val_batch_loss_metrics)\n\n    return val_epoch_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.test","title":"<code>test(test_dataloader=None)</code>","text":"<p>Runs the testing loop if a test DataLoader is provided.</p> <p>if the test_dataloader is not provided, default test_dataloader defined in the Trainer class is used.</p> PARAMETER DESCRIPTION <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                    -&gt; tuples Test Batches            -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def test(self, test_dataloader: DataLoader = None) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the testing loop if a test DataLoader is provided.\n\n    if the test_dataloader is not provided, default test_dataloader defined\n    in the Trainer class is used.\n\n    Args:\n        test_dataloader (DataLoader): DataLoader for test data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                    -&gt; tuples\n            Test Batches            -&gt; (loss, metrics)\n    \"\"\"\n    if test_dataloader is not None:\n        self.test_dataloader = test_dataloader\n\n    self.model.eval()\n    test_loss_metrics = []\n\n    for batch in self._batch_iter(test_dataloader, self.num_training_batches):\n        self.on_test_batch_start(batch)\n        loss_metrics = self.run_test_batch(batch)\n        test_loss_metrics.append(loss_metrics)\n        self.on_test_batch_end(loss_metrics)\n\n    return test_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig","title":"<code>AnsatzConfig(depth=1, ansatz_type=AnsatzType.HEA, ansatz_strategy=Strategy.DIGITAL, strategy_args=dict(), param_prefix='theta', tag=None)</code>  <code>dataclass</code>","text":""},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.ansatz_strategy","title":"<code>ansatz_strategy: Strategy = Strategy.DIGITAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Ansatz strategy.</p> <p><code>Strategy.DIGITAL</code> for fully digital ansatz. Required if <code>ansatz_type</code> is <code>AnsatzType.IIA</code>. <code>Strategy.SDAQC</code> for analog entangling block. <code>Strategy.RYDBERG</code> for fully rydberg hea ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.ansatz_type","title":"<code>ansatz_type: AnsatzType = AnsatzType.HEA</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>What type of ansatz.</p> <p><code>AnsatzType.HEA</code> for Hardware Efficient Ansatz. <code>AnsatzType.IIA</code> for Identity intialized Ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.depth","title":"<code>depth: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of layers of the ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.param_prefix","title":"<code>param_prefix: str = 'theta'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The base bame of the variational parameter.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.strategy_args","title":"<code>strategy_args: dict = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary containing keyword arguments to the function creating the ansatz.</p> <p>Details about each below.</p> <p>For <code>Strategy.DIGITAL</code> strategy, accepts the following:     periodic (bool): if the qubits should be linked periodically.         periodic=False is not supported in emu-c.     operations (list): list of operations to cycle through in the         digital single-qubit rotations of each layer.         Defaults to  [RX, RY, RX] for hea and [RX, RY] for iia.     entangler (AbstractBlock): 2-qubit entangling operation.         Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlld rotations         will have variational parameters on the rotation angles.         Defaults to CNOT</p> <p>For <code>Strategy.SDAQC</code> strategy, accepts the following:     operations (list): list of operations to cycle through in the         digital single-qubit rotations of each layer.         Defaults to  [RX, RY, RX] for hea and [RX, RY] for iia.     entangler (AbstractBlock): Hamiltonian generator for the         analog entangling layer. Time parameter is considered variational.         Defaults to NN interaction.</p> <p>For <code>Strategy.RYDBERG</code> strategy, accepts the following:     addressable_detuning: whether to turn on the trainable semi-local addressing pattern         on the detuning (n_i terms in the Hamiltonian).         Defaults to True.     addressable_drive: whether to turn on the trainable semi-local addressing pattern         on the drive (sigma_i^x terms in the Hamiltonian).         Defaults to False.     tunable_phase: whether to have a tunable phase to get both sigma^x and sigma^y rotations         in the drive term. If False, only a sigma^x term will be included in the drive part         of the Hamiltonian generator.         Defaults to False.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.tag","title":"<code>tag: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to indicate the name tag of the ansatz.</p> <p>Defaults to None, in which case no tag will be applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig","title":"<code>FeatureMapConfig(num_features=0, basis_set=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT, feature_range=None, target_range=None, multivariate_strategy=MultivariateStrategy.PARALLEL, feature_map_strategy=Strategy.DIGITAL, param_prefix=None, num_repeats=0, operation=None, inputs=None, tag=None)</code>  <code>dataclass</code>","text":""},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.basis_set","title":"<code>basis_set: BasisSet | dict[str, BasisSet] = BasisSet.FOURIER</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basis set for feature encoding.</p> <p>Takes qadence.BasisSet. Give a single BasisSet to use the same for all features. Give a dict of (str, BasisSet) where the key is the name of the variable and the value is the BasisSet to use for encoding that feature. BasisSet.FOURIER for Fourier encoding. BasisSet.CHEBYSHEV for Chebyshev encoding.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.feature_map_strategy","title":"<code>feature_map_strategy: Strategy = Strategy.DIGITAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Strategy for feature map.</p> <p>Accepts DIGITAL, ANALOG or RYDBERG. Defaults to DIGITAL. If the strategy is incompatible with the <code>operation</code> chosen, then <code>operation</code> gets preference and the given strategy is ignored.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.feature_range","title":"<code>feature_range: tuple[float, float] | dict[str, tuple[float, float]] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Range of data that the input data is assumed to come from.</p> <p>Give a single tuple to use the same range for all features. Give a dict of (str, tuple) where the key is the name of the variable and the value is the feature range to use for that feature.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.inputs","title":"<code>inputs: list[Basic | str] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List that indicates the order of variables of the tensors that are passed.</p> <p>Optional if a single feature is being encoded, required otherwise. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.multivariate_strategy","title":"<code>multivariate_strategy: MultivariateStrategy = MultivariateStrategy.PARALLEL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The encoding strategy in case of multi-variate function.</p> <p>Takes qadence.MultivariateStrategy. If PARALLEL, the features are encoded in one block of rotation gates with the register being split in sub-registers for each feature. If SERIES, the features are encoded sequentially using the full register for each feature, with an ansatz block between them. PARALLEL is allowed only for DIGITAL <code>feature_map_strategy</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.num_features","title":"<code>num_features: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of feature parameters to be encoded.</p> <p>Defaults to 0. Thus, no feature parameters are encoded.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.num_repeats","title":"<code>num_repeats: int | dict[str, int] = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of feature map layers repeated in the data reuploading step.</p> <p>If all features are to be repeated the same number of times, then can give a single <code>int</code>. For different number of repetitions for each feature, provide a dict of (str, int) where the key is the name of the variable and the value is the number of repetitions for that feature. This amounts to the number of additional reuploads. So if <code>num_repeats</code> is N, the data gets uploaded N+1 times. Defaults to no repetition.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.operation","title":"<code>operation: Callable[[Parameter | Basic], AnalogBlock] | Type[RX] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of operation.</p> <p>Choose among the analog or digital rotations or a custom callable function returning an AnalogBlock instance. If the type of operation is incompatible with the <code>strategy</code> chosen, then <code>operation</code> gets preference and the given strategy is ignored.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.param_prefix","title":"<code>param_prefix: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String prefix to create trainable parameters in Feature Map.</p> <p>A string prefix to create trainable parameters multiplying the feature parameter inside the feature-encoding function. Note that currently this does not take into account the domain of the feature-encoding function. Defaults to <code>None</code> and thus, the feature map is not trainable. Note that this is separate from the name of the parameter. The user can provide a single prefix for all features, and it will be appended by appropriate feature name automatically.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.reupload_scaling","title":"<code>reupload_scaling: ReuploadScaling | dict[str, ReuploadScaling] = ReuploadScaling.CONSTANT</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Scaling for encoding the same feature on different qubits.</p> <p>Scaling used to encode the same feature on different qubits in the same layer of the feature maps. Takes qadence.ReuploadScaling. Give a single ReuploadScaling to use the same for all features. Give a dict of (str, ReuploadScaling) where the key is the name of the variable and the value is the ReuploadScaling to use for encoding that feature. ReuploadScaling.CONSTANT for constant scaling. ReuploadScaling.TOWER for linearly increasing scaling. ReuploadScaling.EXP for exponentially increasing scaling.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.tag","title":"<code>tag: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to indicate the name tag of the feature map.</p> <p>Defaults to None, in which case no tag will be applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.target_range","title":"<code>target_range: tuple[float, float] | dict[str, tuple[float, float]] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Range of data the data encoder assumes as natural range.</p> <p>Give a single tuple to use the same range for all features. Give a dict of (str, tuple) where the key is the name of the variable and the value is the target range to use for that feature.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig","title":"<code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=lambda: list()(), log_model=False, root_folder=Path('./qml_logs'), create_subfolder_per_run=False, log_folder=Path('./'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=ExperimentTrackingTool.TENSORBOARD, hyperparams=dict(), plotting_functions=tuple(), _subfolders=list())</code>  <code>dataclass</code>","text":"<p>Default configuration for the training process.</p> <p>This class provides default settings for various aspects of the training loop, such as logging, checkpointing, and validation. The default values for these fields can be customized when an instance of <code>TrainConfig</code> is created.</p> <p>Example: <pre><code>from qadence.ml_tools import TrainConfig\nc = TrainConfig(root_folder=\"/tmp/train\")\n</code></pre> <pre><code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=[], log_model=False, root_folder='/tmp/train', create_subfolder_per_run=False, log_folder=PosixPath('.'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=&lt;ExperimentTrackingTool.TENSORBOARD: 'tensorboard'&gt;, hyperparams={}, plotting_functions=(), _subfolders=[])\n</code></pre> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.batch_size","title":"<code>batch_size: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The batch size to use when processing a list or tuple of torch.Tensors.</p> <p>This specifies how many samples are processed in each training iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.callbacks","title":"<code>callbacks: list = field(default_factory=lambda: list())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of callbacks to execute during training.</p> <p>Callbacks can be used for custom behaviors, such as early stopping, custom logging, or other actions triggered at specific events.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.checkpoint_best_only","title":"<code>checkpoint_best_only: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>True</code>, checkpoints are only saved if there is an improvement in the.</p> <p>validation metric. This conserves storage by only keeping the best models.</p> <p>validation_criterion is required when this is set to True.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.checkpoint_every","title":"<code>checkpoint_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for saving model and optimizer checkpoints during training.</p> <p>Set to 0 to disable checkpointing. This helps in resuming training or recovering models. Note that setting checkpoint_best_only = True will disable this and only best checkpoints will be saved.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.create_subfolder_per_run","title":"<code>create_subfolder_per_run: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to create a subfolder for each run, named <code>&lt;id&gt;_&lt;timestamp&gt;_&lt;PID&gt;</code>.</p> <p>This ensures logs and checkpoints from different runs do not overwrite each other, which is helpful for rapid prototyping. If <code>False</code>, training will resume from the latest checkpoint if one exists in the specified log folder.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.hyperparams","title":"<code>hyperparams: dict = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of hyperparameters to be tracked.</p> <p>This can include learning rates, regularization parameters, or any other training-related configurations.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.log_folder","title":"<code>log_folder: Path = Path('./')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The log folder for saving checkpoints and tensorboard logs.</p> <p>This stores the path where all logs and checkpoints are being saved for this training session. <code>log_folder</code> takes precedence over <code>root_folder</code> and <code>create_subfolder_per_run</code> arguments. If the user specifies a log_folder, all checkpoints will be saved in this folder and <code>root_folder</code> argument will not be used.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.log_model","title":"<code>log_model: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to log a serialized version of the model.</p> <p>When set to <code>True</code>, the model's state will be logged, useful for model versioning and reproducibility.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.max_iter","title":"<code>max_iter: int = 10000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of training iterations (epochs) to perform.</p> <p>This defines the total number of times the model will be updated.</p> <p>In case of InfiniteTensorDataset, each epoch will have 1 batch. In case of TensorDataset, each epoch will have len(dataloader) batches.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.plot_every","title":"<code>plot_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for generating and saving figures during training.</p> <p>Set to 0 to disable plotting.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.plotting_functions","title":"<code>plotting_functions: tuple[LoggablePlotFunction, ...] = field(default_factory=tuple)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Functions used for in-training plotting.</p> <p>These are called to generate plots that are logged or saved at specified intervals.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.print_every","title":"<code>print_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for printing loss and metrics to the console during training.</p> <p>Set to 0 to disable this output, meaning that metrics and loss will not be printed during training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.root_folder","title":"<code>root_folder: Path = Path('./qml_logs')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The root folder for saving checkpoints and tensorboard logs.</p> <p>The default path is \"./qml_logs\"</p> <p>This can be set to a specific directory where training artifacts are to be stored. Checkpoints will be saved inside a subfolder in this directory. Subfolders will be created based on <code>create_subfolder_per_run</code> argument.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.tracking_tool","title":"<code>tracking_tool: ExperimentTrackingTool = ExperimentTrackingTool.TENSORBOARD</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The tool used for tracking training progress and logging metrics.</p> <p>Options include tools like TensorBoard, which help visualize and monitor model training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.trainstop_criterion","title":"<code>trainstop_criterion: Callable | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to determine if the training process should stop based on a.</p> <p>specific stopping metric. If <code>None</code>, training continues until <code>max_iter</code> is reached.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.val_epsilon","title":"<code>val_epsilon: float = 1e-05</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A small safety margin used to compare the current validation loss with the.</p> <p>best previous validation loss. This is used to determine improvements in metrics.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.val_every","title":"<code>val_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for performing validation.</p> <p>If set to 0, validation is not performed. Note that metrics from validation are always written, regardless of the <code>write_every</code> setting. Note that initial validation happens at the start of training (when val_every &gt; 0)     For initial validation  - initial metrics are written.                             - checkpoint is saved (when checkpoint_best_only = False)</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.validation_criterion","title":"<code>validation_criterion: Callable | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to evaluate whether a given validation metric meets a desired condition.</p> <p>The validation_criterion has the following format: def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:     # process</p> <p>If <code>None</code>, no custom validation criterion is applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.verbose","title":"<code>verbose: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to print metrics and status messages during training.</p> <p>If <code>True</code>, detailed metrics and status updates will be displayed in the console.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.write_every","title":"<code>write_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for writing loss and metrics using the tracking tool during training.</p> <p>Set to 0 to disable this logging, which prevents metrics from being logged to the tracking tool. Note that the metrics will always be written at the end of training regardless of this setting.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.get_parameters","title":"<code>get_parameters(model)</code>","text":"<p>Retrieve all trainable model parameters in a single vector.</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a 1-dimensional tensor with the parameters</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def get_parameters(model: Module) -&gt; Tensor:\n    \"\"\"Retrieve all trainable model parameters in a single vector.\n\n    Args:\n        model (Module): the input PyTorch model\n\n    Returns:\n        Tensor: a 1-dimensional tensor with the parameters\n    \"\"\"\n    ps = [p.reshape(-1) for p in model.parameters() if p.requires_grad]\n    return torch.concat(ps)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.num_parameters","title":"<code>num_parameters(model)</code>","text":"<p>Return the total number of parameters of the given model.</p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def num_parameters(model: Module) -&gt; int:\n    \"\"\"Return the total number of parameters of the given model.\"\"\"\n    return len(get_parameters(model))\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.set_parameters","title":"<code>set_parameters(model, theta)</code>","text":"<p>Set all trainable parameters of a model from a single vector.</p> <p>Notice that this function assumes prior knowledge of right number of parameters in the model</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> <code>theta</code> <p>the parameters to assign</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def set_parameters(model: Module, theta: Tensor) -&gt; None:\n    \"\"\"Set all trainable parameters of a model from a single vector.\n\n    Notice that this function assumes prior knowledge of right number\n    of parameters in the model\n\n    Args:\n        model (Module): the input PyTorch model\n        theta (Tensor): the parameters to assign\n    \"\"\"\n\n    with torch.no_grad():\n        idx = 0\n        for ps in model.parameters():\n            if ps.requires_grad:\n                n = torch.numel(ps)\n                if ps.ndim == 0:\n                    ps[()] = theta[idx : idx + n]\n                else:\n                    ps[:] = theta[idx : idx + n].reshape(ps.size())\n                idx += n\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.optimize_step.optimize_step","title":"<code>optimize_step(model, optimizer, loss_fn, xs, device=None, dtype=None)</code>","text":"<p>Default Torch optimize step with closure.</p> <p>This is the default optimization step.</p> PARAMETER DESCRIPTION <code>model</code> <p>The input model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The chosen Torch optimizer.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable</code> </p> <code>xs</code> <p>The input data. If None, it means the given model does not require any input data.</p> <p> TYPE: <code>dict | list | Tensor | None</code> </p> <code>device</code> <p>A target device to run computations on.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for <code>xs</code> conversion.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor | float, dict | None]</code> <p>tuple[Tensor | float, dict | None]: A tuple containing the computed loss value and a dictionary with collected metrics.</p> Source code in <code>qadence/ml_tools/optimize_step.py</code> <pre><code>def optimize_step(\n    model: Module,\n    optimizer: Optimizer,\n    loss_fn: Callable,\n    xs: dict | list | torch.Tensor | None,\n    device: torch.device = None,\n    dtype: torch.dtype = None,\n) -&gt; tuple[torch.Tensor | float, dict | None]:\n    \"\"\"Default Torch optimize step with closure.\n\n    This is the default optimization step.\n\n    Args:\n        model (Module): The input model to be optimized.\n        optimizer (Optimizer): The chosen Torch optimizer.\n        loss_fn (Callable): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        xs (dict | list | Tensor | None): The input data. If None, it means\n            the given model does not require any input data.\n        device (torch.device): A target device to run computations on.\n        dtype (torch.dtype): Data type for `xs` conversion.\n\n    Returns:\n        tuple[Tensor | float, dict | None]: A tuple containing the computed loss value\n            and a dictionary with collected metrics.\n    \"\"\"\n\n    loss, metrics = None, {}\n    xs_to_device = data_to_device(xs, device=device, dtype=dtype)\n\n    def closure() -&gt; Any:\n        # NOTE: We need the nonlocal as we can't return a metric dict and\n        # because e.g. LBFGS calls this closure multiple times but for some\n        # reason the returned loss is always the first one...\n        nonlocal metrics, loss\n        optimizer.zero_grad()\n        loss, metrics = loss_fn(model, xs_to_device)\n        loss.backward(retain_graph=True)\n        return loss.item()\n\n    optimizer.step(closure)\n    # return the loss/metrics that are being mutated inside the closure...\n    return loss, metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.optimize_step.update_ng_parameters","title":"<code>update_ng_parameters(model, optimizer, loss_fn, data, ng_params)</code>","text":"<p>Update the model parameters using Nevergrad.</p> <p>This function integrates Nevergrad for derivative-free optimization.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>A Nevergrad optimizer instance.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable[[Module, Tensor | None], tuple[float, dict]]</code> </p> <code>data</code> <p>Input data for the model. If None, it means the model does not require input data.</p> <p> TYPE: <code>Tensor | None</code> </p> <code>ng_params</code> <p>The current set of parameters managed by Nevergrad.</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>tuple[float, dict, Array]</code> <p>tuple[float, dict, ng.p.Array]: A tuple containing the computed loss value, a dictionary of metrics, and the updated Nevergrad parameters.</p> Source code in <code>qadence/ml_tools/optimize_step.py</code> <pre><code>def update_ng_parameters(\n    model: Module,\n    optimizer: ng.optimizers.Optimizer,\n    loss_fn: Callable[[Module, torch.Tensor | None], tuple[float, dict]],\n    data: torch.Tensor | None,\n    ng_params: ng.p.Array,\n) -&gt; tuple[float, dict, ng.p.Array]:\n    \"\"\"Update the model parameters using Nevergrad.\n\n    This function integrates Nevergrad for derivative-free optimization.\n\n    Args:\n        model (Module): The PyTorch model to be optimized.\n        optimizer (ng.optimizers.Optimizer): A Nevergrad optimizer instance.\n        loss_fn (Callable[[Module, Tensor | None], tuple[float, dict]]): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        data (Tensor | None): Input data for the model. If None, it means the model does\n            not require input data.\n        ng_params (ng.p.Array): The current set of parameters managed by Nevergrad.\n\n    Returns:\n        tuple[float, dict, ng.p.Array]: A tuple containing the computed loss value,\n            a dictionary of metrics, and the updated Nevergrad parameters.\n    \"\"\"\n    loss, metrics = loss_fn(model, data)  # type: ignore[misc]\n    optimizer.tell(ng_params, float(loss))\n    ng_params = optimizer.ask()  # type: ignore[assignment]\n    params = promote_to_tensor(ng_params.value, requires_grad=False)\n    set_parameters(model, params)\n    return loss, metrics, ng_params\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.DictDataLoader","title":"<code>DictDataLoader(dataloaders)</code>  <code>dataclass</code>","text":"<p>This class only holds a dictionary of <code>DataLoader</code>s and samples from them.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.InfiniteTensorDataset","title":"<code>InfiniteTensorDataset(*tensors)</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Randomly sample points from the first dimension of the given tensors.</p> <p>Behaves like a normal torch <code>Dataset</code> just that we can sample from it as many times as we want.</p> <p>Examples: <pre><code>import torch\nfrom qadence.ml_tools.data import InfiniteTensorDataset\n\nx_data, y_data = torch.rand(5,2), torch.ones(5,1)\n# The dataset accepts any number of tensors with the same batch dimension\nds = InfiniteTensorDataset(x_data, y_data)\n\n# call `next` to get one sample from each tensor:\nxs = next(iter(ds))\n</code></pre> <pre><code>(tensor([0.0156, 0.7122]), tensor([1.]))\n</code></pre></p> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>def __init__(self, *tensors: Tensor):\n    \"\"\"Randomly sample points from the first dimension of the given tensors.\n\n    Behaves like a normal torch `Dataset` just that we can sample from it as\n    many times as we want.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from qadence.ml_tools.data import InfiniteTensorDataset\n\n    x_data, y_data = torch.rand(5,2), torch.ones(5,1)\n    # The dataset accepts any number of tensors with the same batch dimension\n    ds = InfiniteTensorDataset(x_data, y_data)\n\n    # call `next` to get one sample from each tensor:\n    xs = next(iter(ds))\n    print(str(xs)) # markdown-exec: hide\n    ```\n    \"\"\"\n    self.tensors = tensors\n    self.indices = list(range(self.tensors[0].size(0)))\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult","title":"<code>OptimizeResult(iteration, model, optimizer, loss=None, metrics=lambda: dict()(), extra=lambda: dict()())</code>  <code>dataclass</code>","text":"<p>OptimizeResult stores many optimization intermediate values.</p> <p>We store at a current iteration, the model, optimizer, loss values, metrics. An extra dict can be used for saving other information to be used for callbacks.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.extra","title":"<code>extra: dict = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Extra dict for saving anything else to be used in callbacks.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.iteration","title":"<code>iteration: int</code>  <code>instance-attribute</code>","text":"<p>Current iteration number.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.loss","title":"<code>loss: Tensor | float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Loss value.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.metrics","title":"<code>metrics: dict = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that can be saved during training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.model","title":"<code>model: Module</code>  <code>instance-attribute</code>","text":"<p>Model at iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.optimizer","title":"<code>optimizer: Optimizer | NGOptimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer at iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.data_to_device","title":"<code>data_to_device(xs, *args, **kwargs)</code>","text":"<p>Utility method to move arbitrary data to 'device'.</p> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>@singledispatch\ndef data_to_device(xs: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Utility method to move arbitrary data to 'device'.\"\"\"\n    raise ValueError(f\"Unable to move {type(xs)} with input args: {args} and kwargs: {kwargs}.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.to_dataloader","title":"<code>to_dataloader(*tensors, batch_size=1, infinite=False)</code>","text":"<p>Convert torch tensors an (infinite) Dataloader.</p> PARAMETER DESCRIPTION <code>*tensors</code> <p>Torch tensors to use in the dataloader.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>()</code> </p> <code>batch_size</code> <p>batch size of sampled tensors</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>infinite</code> <p>if <code>True</code>, the dataloader will keep sampling indefinitely even after the whole dataset was sampled once</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>import torch\nfrom qadence.ml_tools import to_dataloader\n\n(x, y, z) = [torch.rand(10) for _ in range(3)]\nloader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\nprint(next(loader))\nprint(next(loader))\nprint(next(loader))\n</code></pre> <pre><code>[tensor([0.6089, 0.5589, 0.6142, 0.2165, 0.6462]), tensor([0.3149, 0.6751, 0.1210, 0.8103, 0.9142]), tensor([0.2031, 0.9552, 0.8790, 0.5408, 0.5527])]\n[tensor([0.5184, 0.1583, 0.2599, 0.3707, 0.7417]), tensor([0.3621, 0.0112, 0.1667, 0.2752, 0.2930]), tensor([0.1747, 0.9271, 0.7204, 0.5249, 0.1339])]\n[tensor([0.6089, 0.5589, 0.6142, 0.2165, 0.6462]), tensor([0.3149, 0.6751, 0.1210, 0.8103, 0.9142]), tensor([0.2031, 0.9552, 0.8790, 0.5408, 0.5527])]\n</code></pre> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>def to_dataloader(*tensors: Tensor, batch_size: int = 1, infinite: bool = False) -&gt; DataLoader:\n    \"\"\"Convert torch tensors an (infinite) Dataloader.\n\n    Arguments:\n        *tensors: Torch tensors to use in the dataloader.\n        batch_size: batch size of sampled tensors\n        infinite: if `True`, the dataloader will keep sampling indefinitely even after the whole\n            dataset was sampled once\n\n    Examples:\n\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from qadence.ml_tools import to_dataloader\n\n    (x, y, z) = [torch.rand(10) for _ in range(3)]\n    loader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\n    print(next(loader))\n    print(next(loader))\n    print(next(loader))\n    ```\n    \"\"\"\n    ds = InfiniteTensorDataset(*tensors) if infinite else TensorDataset(*tensors)\n    return DataLoader(ds, batch_size=batch_size)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN","title":"<code>QNN(circuit, observable, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, inputs=None, input_diff_mode=InputDiffMode.AD)</code>","text":"<p>               Bases: <code>QuantumModel</code></p> <p>Quantum neural network model for n-dimensional inputs.</p> <p>Examples: <pre><code>import torch\nfrom qadence import QuantumCircuit, QNN, Z\nfrom qadence import hea, feature_map, hamiltonian_factory, kron\n\n# create the circuit\nn_qubits, depth = 2, 4\nfm = kron(\n    feature_map(1, support=(0,), param=\"x\"),\n    feature_map(1, support=(1,), param=\"y\")\n)\nansatz = hea(n_qubits=n_qubits, depth=depth)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nobs_base = hamiltonian_factory(n_qubits, detuning=Z)\n\n# the QNN will yield two outputs\nobs = [2.0 * obs_base, 4.0 * obs_base]\n\n# initialize and use the model\nqnn = QNN(circuit, obs, inputs=[\"x\", \"y\"])\ny = qnn(torch.rand(3, 2))\n</code></pre> <pre><code>tensor([[-1.5711, -3.1421],\n        [-1.9156, -3.8311],\n        [-1.3072, -2.6145]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> <p>Initialize the QNN.</p> <p>The number of inputs is determined by the feature parameters in the input quantum circuit while the number of outputs is determined by how many observables are provided as input</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The quantum circuit to use for the QNN.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>The observable.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>optional configuration for the backend</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>inputs</code> <p>List that indicates the order of variables of the tensors that are passed to the model. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p> <p> TYPE: <code>list[Basic | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode | str</code> DEFAULT: <code>AD</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    inputs: list[sympy.Basic | str] | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n):\n    \"\"\"Initialize the QNN.\n\n    The number of inputs is determined by the feature parameters in the input\n    quantum circuit while the number of outputs is determined by how many\n    observables are provided as input\n\n    Args:\n        circuit: The quantum circuit to use for the QNN.\n        observable: The observable.\n        backend: The chosen quantum backend.\n        diff_mode: The differentiation engine to use. Choices 'gpsr' or 'ad'.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        configuration: optional configuration for the backend\n        inputs: List that indicates the order of variables of the tensors that are passed\n            to the model. Given input tensors `xs = torch.rand(batch_size, input_size:=2)` a QNN\n            with `inputs=[\"t\", \"x\"]` will assign `t, x = xs[:,0], xs[:,1]`.\n        input_diff_mode: The differentiation mode for the input tensor.\n    \"\"\"\n    super().__init__(\n        circuit,\n        observable=observable,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        configuration=configuration,\n        noise=noise,\n    )\n    if self._observable is None:\n        raise ValueError(\"You need to provide at least one observable in the QNN constructor\")\n    if (inputs is not None) and (len(self.inputs) == len(inputs)):\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in inputs]  # type: ignore[union-attr]\n    elif (inputs is None) and len(self.inputs) &lt;= 1:\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in self.inputs]  # type: ignore[union-attr]\n    else:\n        raise ValueError(\n            \"\"\"\n            Your QNN has more than one input. Please provide a list of inputs in the order of\n            your tensor domain. For example, if you want to pass\n            `xs = torch.rand(batch_size, input_size:=3)` to you QNN, where\n            ```\n            t = x[:,0]\n            x = x[:,1]\n            y = x[:,2]\n            ```\n            you have to specify\n            ```\n            QNN(circuit, observable, inputs=[\"t\", \"x\", \"y\"])\n            ```\n            You can also pass a list of sympy symbols.\n        \"\"\"\n        )\n    self.format_to_dict = format_to_dict_fn(self.inputs)  # type: ignore[arg-type]\n    self.input_diff_mode = InputDiffMode(input_diff_mode)\n    if self.input_diff_mode == InputDiffMode.FD:\n        from qadence.backends.utils import finitediff\n\n        self.__derivative = finitediff\n    elif self.input_diff_mode == InputDiffMode.AD:\n        self.__derivative = _torch_derivative  # type: ignore[assignment]\n    else:\n        raise ValueError(f\"Unkown forward diff mode: {self.input_diff_mode}\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN.forward","title":"<code>forward(values=None, state=None, measurement=None, noise=None, endianness=Endianness.BIG)</code>","text":"<p>Forward pass of the model.</p> <p>This returns the (differentiable) expectation value of the given observable operator defined in the constructor. Differently from the base QuantumModel class, the QNN accepts also a tensor as input for the forward pass. The tensor is expected to have shape: <code>n_batches x in_features</code> where <code>n_batches</code> is the number of data points and <code>in_features</code> is the dimensionality of the problem</p> <p>The output of the forward pass is the expectation value of the input observable(s). If a single observable is given, the output shape is <code>n_batches</code> while if multiple observables are given the output shape is instead <code>n_batches x n_observables</code></p> PARAMETER DESCRIPTION <code>values</code> <p>the values of the feature parameters</p> <p> TYPE: <code>dict[str, Tensor] | Tensor</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a tensor with the expectation value of the observables passed in the constructor of the model</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def forward(\n    self,\n    values: dict[str, Tensor] | Tensor = None,\n    state: Tensor | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the model.\n\n    This returns the (differentiable) expectation value of the given observable\n    operator defined in the constructor. Differently from the base QuantumModel\n    class, the QNN accepts also a tensor as input for the forward pass. The\n    tensor is expected to have shape: `n_batches x in_features` where `n_batches`\n    is the number of data points and `in_features` is the dimensionality of the problem\n\n    The output of the forward pass is the expectation value of the input\n    observable(s). If a single observable is given, the output shape is\n    `n_batches` while if multiple observables are given the output shape\n    is instead `n_batches x n_observables`\n\n    Args:\n        values: the values of the feature parameters\n        state: Initial state.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n\n    Returns:\n        Tensor: a tensor with the expectation value of the observables passed\n            in the constructor of the model\n    \"\"\"\n    return self.expectation(\n        values, state=state, measurement=measurement, noise=noise, endianness=endianness\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN.from_configs","title":"<code>from_configs(register, obs_config, fm_config=FeatureMapConfig(), ansatz_config=AnsatzConfig(), backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, input_diff_mode=InputDiffMode.AD)</code>  <code>classmethod</code>","text":"<p>Create a QNN from a set of configurations.</p> PARAMETER DESCRIPTION <code>register</code> <p>The number of qubits or a register object.</p> <p> TYPE: <code>int | Register</code> </p> <code>obs_config</code> <p>The configuration(s) for the observable(s).</p> <p> TYPE: <code>list[ObservableConfig] | ObservableConfig</code> </p> <code>fm_config</code> <p>The configuration for the feature map. Defaults to no feature encoding block.</p> <p> TYPE: <code>FeatureMapConfig</code> DEFAULT: <code>FeatureMapConfig()</code> </p> <code>ansatz_config</code> <p>The configuration for the ansatz. Defaults to a single layer of hardware efficient ansatz.</p> <p> TYPE: <code>AnsatzConfig</code> DEFAULT: <code>AnsatzConfig()</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices are 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>Noise</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Optional backend configuration.</p> <p> TYPE: <code>BackendConfiguration | dict</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode</code> DEFAULT: <code>AD</code> </p> RETURNS DESCRIPTION <code>QNN</code> <p>A QNN object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the observable configuration is not provided.</p> <p>Example: <pre><code>import torch\nfrom qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\nfrom qadence.ml_tools import QNN\nfrom qadence.constructors import ObservableConfig\nfrom qadence.operations import Z\nfrom qadence.types import (\n    AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n)\n\nregister = 4\nobs_config = ObservableConfig(\n    detuning=Z,\n    scale=5.0,\n    shift=0.0,\n    transformation_type=ObservableTransform.SCALE,\n    trainable_transform=None,\n)\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs=[\"x\", \"y\"],\n    basis_set=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.CONSTANT,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nqnn = QNN.from_configs(\n    register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n)\n\nx = torch.rand(2, 2)\ny = qnn(x)\n</code></pre> <pre><code>tensor([[1.5914],\n        [2.4987]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>@classmethod\ndef from_configs(\n    cls,\n    register: int | Register,\n    obs_config: Any,\n    fm_config: Any = FeatureMapConfig(),\n    ansatz_config: Any = AnsatzConfig(),\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n) -&gt; QNN:\n    \"\"\"Create a QNN from a set of configurations.\n\n    Args:\n        register (int | Register): The number of qubits or a register object.\n        obs_config (list[ObservableConfig] | ObservableConfig): The configuration(s)\n            for the observable(s).\n        fm_config (FeatureMapConfig): The configuration for the feature map.\n            Defaults to no feature encoding block.\n        ansatz_config (AnsatzConfig): The configuration for the ansatz.\n            Defaults to a single layer of hardware efficient ansatz.\n        backend (BackendName): The chosen quantum backend.\n        diff_mode (DiffMode): The differentiation engine to use. Choices are\n            'gpsr' or 'ad'.\n        measurement (Measurements): Optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator.\n        noise (Noise): A noise model to use.\n        configuration (BackendConfiguration | dict): Optional backend configuration.\n        input_diff_mode (InputDiffMode): The differentiation mode for the input tensor.\n\n    Returns:\n        A QNN object.\n\n    Raises:\n        ValueError: If the observable configuration is not provided.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\n    from qadence.ml_tools import QNN\n    from qadence.constructors import ObservableConfig\n    from qadence.operations import Z\n    from qadence.types import (\n        AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n    )\n\n    register = 4\n    obs_config = ObservableConfig(\n        detuning=Z,\n        scale=5.0,\n        shift=0.0,\n        transformation_type=ObservableTransform.SCALE,\n        trainable_transform=None,\n    )\n    fm_config = FeatureMapConfig(\n        num_features=2,\n        inputs=[\"x\", \"y\"],\n        basis_set=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.CONSTANT,\n        feature_range={\n            \"x\": (-1.0, 1.0),\n            \"y\": (0.0, 1.0),\n        },\n    )\n    ansatz_config = AnsatzConfig(\n        depth=2,\n        ansatz_type=AnsatzType.HEA,\n        ansatz_strategy=Strategy.DIGITAL,\n    )\n\n    qnn = QNN.from_configs(\n        register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n    )\n\n    x = torch.rand(2, 2)\n    y = qnn(x)\n    print(str(y)) # markdown-exec: hide\n    ```\n    \"\"\"\n    from .constructors import build_qnn_from_configs\n\n    return build_qnn_from_configs(\n        register=register,\n        observable_config=obs_config,\n        fm_config=fm_config,\n        ansatz_config=ansatz_config,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        noise=noise,\n        configuration=configuration,\n        input_diff_mode=input_diff_mode,\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.derivative","title":"<code>derivative(ufa, x, derivative_indices)</code>","text":"<p>Compute derivatives w.r.t.</p> <p>inputs of a UFA with a single output. The <code>derivative_indices</code> specify which derivative(s) are computed.  E.g. <code>derivative_indices=(1,2)</code> would compute the a second order derivative w.r.t to the indices <code>1</code> and <code>2</code> of the input tensor.</p> PARAMETER DESCRIPTION <code>ufa</code> <p>The model for which we want to compute the derivative.</p> <p> TYPE: <code>Module</code> </p> <code>x</code> <p>(batch_size, input_size) input tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>derivative_indices</code> <p>Define which derivatives to compute.</p> <p> TYPE: <code>tuple</code> </p> <p>Examples: If we create a UFA with three inputs and denote the first, second, and third input with <code>x</code>, <code>y</code>, and <code>z</code> we can compute the following derivatives w.r.t to those inputs: <pre><code>import torch\nfrom qadence.ml_tools.models import derivative, QNN\nfrom qadence.ml_tools.config import FeatureMapConfig, AnsatzConfig\nfrom qadence.constructors.hamiltonians import ObservableConfig\nfrom qadence.operations import Z\n\nfm_config = FeatureMapConfig(num_features=3, inputs=[\"x\", \"y\", \"z\"])\nansatz_config = AnsatzConfig()\nobs_config = ObservableConfig(detuning=Z)\n\nf = QNN.from_configs(\n    register=3, obs_config=obs_config, fm_config=fm_config, ansatz_config=ansatz_config,\n)\ninputs = torch.rand(5,3,requires_grad=True)\n\n# df_dx\nderivative(f, inputs, (0,))\n\n# d2f_dydz\nderivative(f, inputs, (1,2))\n\n# d3fdy2dx\nderivative(f, inputs, (1,1,0))\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def derivative(ufa: torch.nn.Module, x: Tensor, derivative_indices: tuple[int, ...]) -&gt; Tensor:\n    \"\"\"Compute derivatives w.r.t.\n\n    inputs of a UFA with a single output. The\n    `derivative_indices` specify which derivative(s) are computed.  E.g.\n    `derivative_indices=(1,2)` would compute the a second order derivative w.r.t\n    to the indices `1` and `2` of the input tensor.\n\n    Arguments:\n        ufa: The model for which we want to compute the derivative.\n        x (Tensor): (batch_size, input_size) input tensor.\n        derivative_indices (tuple): Define which derivatives to compute.\n\n    Examples:\n    If we create a UFA with three inputs and denote the first, second, and third\n    input with `x`, `y`, and `z` we can compute the following derivatives w.r.t\n    to those inputs:\n    ```py exec=\"on\" source=\"material-block\"\n    import torch\n    from qadence.ml_tools.models import derivative, QNN\n    from qadence.ml_tools.config import FeatureMapConfig, AnsatzConfig\n    from qadence.constructors.hamiltonians import ObservableConfig\n    from qadence.operations import Z\n\n    fm_config = FeatureMapConfig(num_features=3, inputs=[\"x\", \"y\", \"z\"])\n    ansatz_config = AnsatzConfig()\n    obs_config = ObservableConfig(detuning=Z)\n\n    f = QNN.from_configs(\n        register=3, obs_config=obs_config, fm_config=fm_config, ansatz_config=ansatz_config,\n    )\n    inputs = torch.rand(5,3,requires_grad=True)\n\n    # df_dx\n    derivative(f, inputs, (0,))\n\n    # d2f_dydz\n    derivative(f, inputs, (1,2))\n\n    # d3fdy2dx\n    derivative(f, inputs, (1,1,0))\n    ```\n    \"\"\"\n    assert ufa.out_features == 1, \"Can only call `derivative` on models with 1D output.\"\n    return ufa._derivative(x, derivative_indices)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.format_to_dict_fn","title":"<code>format_to_dict_fn(inputs=[])</code>","text":"<p>Format an input tensor into the format required by the forward pass.</p> <p>The tensor is assumed to have dimensions: n_batches x in_features where in_features corresponds to the number of input features of the QNN</p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def format_to_dict_fn(\n    inputs: list[sympy.Symbol | str] = [],\n) -&gt; Callable[[Tensor | ParamDictType], ParamDictType]:\n    \"\"\"Format an input tensor into the format required by the forward pass.\n\n    The tensor is assumed to have dimensions: n_batches x in_features where in_features\n    corresponds to the number of input features of the QNN\n    \"\"\"\n    in_features = len(inputs)\n\n    def tensor_to_dict(values: Tensor | ParamDictType) -&gt; ParamDictType:\n        if isinstance(values, Tensor):\n            values = values.reshape(-1, 1) if len(values.size()) == 1 else values\n            if not values.shape[1] == in_features:\n                raise ValueError(\n                    f\"Model expects in_features={in_features} but got {values.shape[1]}.\"\n                )\n            values = {fparam.name: values[:, inputs.index(fparam)] for fparam in inputs}  # type: ignore[union-attr]\n        return values\n\n    return tensor_to_dict\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback","title":"<code>Callback(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>Base class for defining various training callbacks.</p> ATTRIBUTE DESCRIPTION <code>on</code> <p>The event on which to trigger the callback. Must be a valid on value from: [\"train_start\", \"train_end\",     \"train_epoch_start\", \"train_epoch_end\", \"train_batch_start\",     \"train_batch_end\",\"val_epoch_start\", \"val_epoch_end\",     \"val_batch_start\", \"val_batch_end\", \"test_batch_start\",     \"test_batch_end\"]</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>callback</code> <p>The function to call if the condition is met.</p> <p> TYPE: <code>CallbackFunction | None</code> </p> <code>callback_condition</code> <p>Condition to check before calling.</p> <p> TYPE: <code>CallbackConditionFunction | None</code> </p> <code>modify_optimize_result</code> <p>Function to modify <code>OptimizeResult</code>.</p> <p> TYPE: <code>CallbackFunction | dict[str, Any] | None</code> </p> <p>A callback can be defined in two ways:</p> <ol> <li>By providing a callback function directly in the base class:    This is useful for simple callbacks that don't require subclassing.</li> </ol> <p>Example:    <pre><code>from qadence.ml_tools.callbacks import Callback\n\ndef custom_callback_function(trainer, config, writer):\n    print(\"Custom callback executed.\")\n\ncustom_callback = Callback(\n    on=\"train_end\",\n    called_every=5,\n    callback=custom_callback_function\n)\n</code></pre> <pre><code>\n</code></pre> </p> <ol> <li>By inheriting and implementing the <code>run_callback</code> method:    This is suitable for more complex callbacks that require customization.</li> </ol> <p>Example:    <pre><code>from qadence.ml_tools.callbacks import Callback\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in the inherited run_callback method.\")\n\ncustom_callback = CustomCallback(on=\"train_end\", called_every=10)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.on","title":"<code>on: TrainingStage | str</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the TrainingStage.</p> RETURNS DESCRIPTION <code>TrainingStage</code> <p>TrainingStage for the callback</p> <p> TYPE: <code>TrainingStage | str</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.__call__","title":"<code>__call__(when, trainer, config, writer)</code>","text":"<p>Executes the callback if conditions are met.</p> PARAMETER DESCRIPTION <code>when</code> <p>The event when the callback is triggered.</p> <p> TYPE: <code>str</code> </p> <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback function if executed.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __call__(\n    self, when: TrainingStage, trainer: Any, config: TrainConfig, writer: BaseWriter\n) -&gt; Any:\n    \"\"\"Executes the callback if conditions are met.\n\n    Args:\n        when (str): The event when the callback is triggered.\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback function if executed.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if self.on == when:\n        if opt_result:\n            opt_result = self.modify_optimize_result(opt_result)\n        if self._should_call(when, opt_result):\n            return self.run_callback(trainer, config, writer)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Executes the defined callback.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback execution.</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If not implemented in subclasses.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Executes the defined callback.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback execution.\n\n    Raises:\n        NotImplementedError: If not implemented in subclasses.\n    \"\"\"\n    if self.callback is not None:\n        return self.callback(trainer, config, writer)\n    raise NotImplementedError(\"Subclasses should override the run_callback method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LoadCheckpoint","title":"<code>LoadCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to load a model checkpoint.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LoadCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Loads a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The result of loading the checkpoint.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Loads a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: The result of loading the checkpoint.\n    \"\"\"\n    folder = config.log_folder\n    model = trainer.model\n    optimizer = trainer.optimizer\n    device = trainer.log_device\n    return load_checkpoint(folder, model, optimizer, device=device)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogHyperparameters","title":"<code>LogHyperparameters(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log hyperparameters using the writer.</p> <p>The <code>LogHyperparameters</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LogHyperparameters</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogHyperparameters\n\n# Create an instance of the LogHyperparameters callback\nlog_hyper_callback = LogHyperparameters(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[log_hyper_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogHyperparameters.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs hyperparameters using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs hyperparameters using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    hyperparams = config.hyperparams\n    writer.log_hyperparams(hyperparams)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogModelTracker","title":"<code>LogModelTracker(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log the model using the writer.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogModelTracker.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs the model using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs the model using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    model = trainer.model\n    writer.log_model(\n        model, trainer.train_dataloader, trainer.val_dataloader, trainer.test_dataloader\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PlotMetrics","title":"<code>PlotMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to plot metrics using the writer.</p> <p>The <code>PlotMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PlotMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PlotMetrics\n\n# Create an instance of the PlotMetrics callback\nplot_metrics_callback = PlotMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[plot_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PlotMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Plots metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Plots metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    plotting_functions = config.plotting_functions\n    writer.plot(trainer.model, opt_result.iteration, plotting_functions)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PrintMetrics","title":"<code>PrintMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to print metrics using the writer.</p> <p>The <code>PrintMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PrintMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\n# Create an instance of the PrintMetrics callback\nprint_metrics_callback = PrintMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[print_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PrintMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Prints metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Prints metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.print_metrics(opt_result)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveBestCheckpoint","title":"<code>SaveBestCheckpoint(on, called_every)</code>","text":"<p>               Bases: <code>SaveCheckpoint</code></p> <p>Callback to save the best model checkpoint based on a validation criterion.</p> <p>Initializes the SaveBestCheckpoint callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int):\n    \"\"\"Initializes the SaveBestCheckpoint callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.best_loss = float(\"inf\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveBestCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves the checkpoint if the current loss is better than the best loss.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves the checkpoint if the current loss is better than the best loss.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if config.validation_criterion and config.validation_criterion(\n        opt_result.loss, self.best_loss, config.val_epsilon\n    ):\n        self.best_loss = opt_result.loss\n\n        folder = config.log_folder\n        model = trainer.model\n        optimizer = trainer.optimizer\n        opt_result = trainer.opt_result\n        write_checkpoint(folder, model, optimizer, \"best\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveCheckpoint","title":"<code>SaveCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to save a model checkpoint.</p> <p>The <code>SaveCheckpoint</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>SaveCheckpoint</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint\n\n# Create an instance of the SaveCheckpoint callback\nsave_checkpoint_callback = SaveCheckpoint(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    folder = config.log_folder\n    model = trainer.model\n    optimizer = trainer.optimizer\n    opt_result = trainer.opt_result\n    write_checkpoint(folder, model, optimizer, opt_result.iteration)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.WriteMetrics","title":"<code>WriteMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to write metrics using the writer.</p> <p>The <code>WriteMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>WriteMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import WriteMetrics\n\n# Create an instance of the WriteMetrics callback\nwrite_metrics_callback = WriteMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[write_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.WriteMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Writes metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Writes metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.write(opt_result)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer","title":"<code>BaseTrainer(model, optimizer, config, loss_fn='mse', optimize_step=optimize_step, train_dataloader=None, val_dataloader=None, test_dataloader=None, max_batches=None)</code>","text":"<p>Base class for training machine learning models using a given optimizer.</p> <p>The base class implements contextmanager for gradient based/free optimization, properties, property setters, input validations, callback decorator generator, and empty hooks for different training steps.</p> This class provides <ul> <li>Context managers for enabling/disabling gradient-based optimization</li> <li>Properties for managing models, optimizers, and dataloaders</li> <li>Input validations and a callback decorator generator</li> <li>Config and callback managers using the provided <code>TrainConfig</code></li> </ul> ATTRIBUTE DESCRIPTION <code>use_grad</code> <p>Indicates if gradients are used for optimization. Default is True.</p> <p> TYPE: <code>bool</code> </p> <code>model</code> <p>The neural network model.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The configuration settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>optimize_step</code> <p>Function for performing an optimization step.</p> <p> TYPE: <code>Callable</code> </p> <code>loss_fn</code> <p>loss function to use. Default loss function used is 'mse'</p> <p> TYPE: <code>Callable | str ]</code> </p> <code>num_training_batches</code> <p>Number of training batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_validation_batches</code> <p>Number of validation batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_test_batches</code> <p>Number of test batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>state</code> <p>Current state in the training process</p> <p> TYPE: <code>str</code> </p> <p>Initializes the BaseTrainer.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The TrainConfig settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>The loss function to use. str input to be specified to use a default loss function. currently supported loss functions: 'mse', 'cross_entropy'. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data. If the model does not need data to evaluate loss, no dataset should be provided.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    optimize_step: Callable = optimize_step,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the BaseTrainer.\n\n    Args:\n        model (nn.Module): The model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer\n            for training.\n        config (TrainConfig): The TrainConfig settings for training.\n        loss_fn (str | Callable): The loss function to use.\n            str input to be specified to use a default loss function.\n            currently supported loss functions: 'mse', 'cross_entropy'.\n            If not specified, default mse loss will be used.\n        train_dataloader (Dataloader | DictDataLoader | None): DataLoader for training data.\n            If the model does not need data to evaluate loss, no dataset\n            should be provided.\n        val_dataloader (Dataloader | DictDataLoader | None): DataLoader for validation data.\n        test_dataloader (Dataloader | DictDataLoader | None): DataLoader for testing data.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    self._model: nn.Module\n    self._optimizer: optim.Optimizer | NGOptimizer | None\n    self._config: TrainConfig\n    self._train_dataloader: DataLoader | DictDataLoader | None = None\n    self._val_dataloader: DataLoader | DictDataLoader | None = None\n    self._test_dataloader: DataLoader | DictDataLoader | None = None\n\n    self.config = config\n    self.model = model\n    self.optimizer = optimizer\n    self.max_batches = max_batches\n\n    self.num_training_batches: int\n    self.num_validation_batches: int\n    self.num_test_batches: int\n\n    self.train_dataloader = train_dataloader\n    self.val_dataloader = val_dataloader\n    self.test_dataloader = test_dataloader\n\n    self.loss_fn: Callable = get_loss_fn(loss_fn)\n    self.optimize_step: Callable = optimize_step\n    self.ng_params: ng.p.Array\n    self.training_stage: TrainingStage = TrainingStage(\"idle\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.config","title":"<code>config: TrainConfig</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training configuration.</p> RETURNS DESCRIPTION <code>TrainConfig</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.model","title":"<code>model: nn.Module</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the model if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Module</code> <p>nn.Module: The model.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.optimizer","title":"<code>optimizer: optim.Optimizer | NGOptimizer | None</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimizer if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Optimizer | Optimizer | None</code> <p>optim.Optimizer | NGOptimizer | None: The optimizer.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.test_dataloader","title":"<code>test_dataloader: DataLoader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the test DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for testing data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.train_dataloader","title":"<code>train_dataloader: DataLoader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.use_grad","title":"<code>use_grad: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimization framework for the trainer.</p> <p>use_grad = True : Gradient based optimization use_grad = False : Gradient free optimization</p> RETURNS DESCRIPTION <code>bool</code> <p>Bool value for using gradient.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.val_dataloader","title":"<code>val_dataloader: DataLoader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the validation DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.callback","title":"<code>callback(phase)</code>  <code>staticmethod</code>","text":"<p>Decorator for executing callbacks before and after a phase.</p> <p>Phase are different hooks during the training. list of valid phases is defined in Callbacks. We also update the current state of the training process in the callback decorator.</p> PARAMETER DESCRIPTION <code>phase</code> <p>The phase for which the callback is executed (e.g., \"train\", \"train_epoch\", \"train_batch\").</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>The decorated function.</p> <p> TYPE: <code>Callable</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@staticmethod\ndef callback(phase: str) -&gt; Callable:\n    \"\"\"\n    Decorator for executing callbacks before and after a phase.\n\n    Phase are different hooks during the training. list of valid\n    phases is defined in Callbacks.\n    We also update the current state of the training process in\n    the callback decorator.\n\n    Args:\n        phase (str): The phase for which the callback is executed (e.g., \"train\",\n            \"train_epoch\", \"train_batch\").\n\n    Returns:\n        Callable: The decorated function.\n    \"\"\"\n\n    def decorator(method: Callable) -&gt; Callable:\n        def wrapper(self: Any, *args: Any, **kwargs: Any) -&gt; Any:\n            start_event = f\"{phase}_start\"\n            end_event = f\"{phase}_end\"\n\n            self.training_stage = TrainingStage(start_event)\n            self.callback_manager.run_callbacks(trainer=self)\n            result = method(self, *args, **kwargs)\n\n            self.training_stage = TrainingStage(end_event)\n            # build_optimize_result method is defined in the trainer.\n            self.build_optimize_result(result)\n            self.callback_manager.run_callbacks(trainer=self)\n\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.disable_grad_opt","title":"<code>disable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily disable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The Nevergrad optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef disable_grad_opt(self, optimizer: NGOptimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily disable gradient-based optimization.\n\n    Args:\n        optimizer (NGOptimizer): The Nevergrad optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = False\n        self.callback_manager.use_grad = False\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.enable_grad_opt","title":"<code>enable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily enable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The PyTorch optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef enable_grad_opt(self, optimizer: optim.Optimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily enable gradient-based optimization.\n\n    Args:\n        optimizer (optim.Optimizer): The PyTorch optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = True\n        self.callback_manager.use_grad = True\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_test_batch_end","title":"<code>on_test_batch_end(test_batch_loss_metrics)</code>","text":"<p>Called at the end of each testing batch.</p> PARAMETER DESCRIPTION <code>test_batch_loss_metrics</code> <p>Metrics for the testing batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_end(self, test_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each testing batch.\n\n    Args:\n        test_batch_loss_metrics: Metrics for the testing batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_test_batch_start","title":"<code>on_test_batch_start(batch)</code>","text":"<p>Called at the start of each testing batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each testing batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_batch_end","title":"<code>on_train_batch_end(train_batch_loss_metrics)</code>","text":"<p>Called at the end of each training batch.</p> PARAMETER DESCRIPTION <code>train_batch_loss_metrics</code> <p>Metrics for the training batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_end(self, train_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each training batch.\n\n    Args:\n        train_batch_loss_metrics: Metrics for the training batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_batch_start","title":"<code>on_train_batch_start(batch)</code>","text":"<p>Called at the start of each training batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each training batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_end","title":"<code>on_train_end(train_losses, val_losses=None)</code>","text":"<p>Called at the end of training.</p> PARAMETER DESCRIPTION <code>train_losses</code> <p>Metrics for the training losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]]</code> </p> <code>val_losses</code> <p>Metrics for the validation losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_end(\n    self,\n    train_losses: list[list[tuple[torch.Tensor, Any]]],\n    val_losses: list[list[tuple[torch.Tensor, Any]]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Called at the end of training.\n\n    Args:\n        train_losses (list[list[tuple[torch.Tensor, Any]]]):\n            Metrics for the training losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Training Batches      -&gt; (loss, metrics)\n        val_losses (list[list[tuple[torch.Tensor, Any]]] | None):\n            Metrics for the validation losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_epoch_end","title":"<code>on_train_epoch_end(train_epoch_loss_metrics)</code>","text":"<p>Called at the end of each training epoch.</p> PARAMETER DESCRIPTION <code>train_epoch_loss_metrics</code> <p>Metrics for the training epoch losses. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_end(self, train_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each training epoch.\n\n    Args:\n        train_epoch_loss_metrics: Metrics for the training epoch losses.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>Called at the start of each training epoch.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each training epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_start","title":"<code>on_train_start()</code>","text":"<p>Called at the start of training.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_start(self) -&gt; None:\n    \"\"\"Called at the start of training.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_batch_end","title":"<code>on_val_batch_end(val_batch_loss_metrics)</code>","text":"<p>Called at the end of each validation batch.</p> PARAMETER DESCRIPTION <code>val_batch_loss_metrics</code> <p>Metrics for the validation batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_end(self, val_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation batch.\n\n    Args:\n        val_batch_loss_metrics: Metrics for the validation batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_batch_start","title":"<code>on_val_batch_start(batch)</code>","text":"<p>Called at the start of each validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each validation batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_epoch_end","title":"<code>on_val_epoch_end(val_epoch_loss_metrics)</code>","text":"<p>Called at the end of each validation epoch.</p> PARAMETER DESCRIPTION <code>val_epoch_loss_metrics</code> <p>Metrics for the validation epoch loss. list                    -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_end(self, val_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation epoch.\n\n    Args:\n        val_epoch_loss_metrics: Metrics for the validation epoch loss.\n            list                    -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_epoch_start","title":"<code>on_val_epoch_start()</code>","text":"<p>Called at the start of each validation epoch.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each validation epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.set_use_grad","title":"<code>set_use_grad(value)</code>  <code>classmethod</code>","text":"<p>Sets the global use_grad flag.</p> PARAMETER DESCRIPTION <code>value</code> <p>Whether to use gradient-based optimization.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@classmethod\ndef set_use_grad(cls, value: bool) -&gt; None:\n    \"\"\"\n    Sets the global use_grad flag.\n\n    Args:\n        value (bool): Whether to use gradient-based optimization.\n    \"\"\"\n    if not isinstance(value, bool):\n        raise TypeError(\"use_grad must be a boolean value.\")\n    cls._use_grad = value\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for experiment tracking writers.</p> METHOD DESCRIPTION <code>open</code> <p>Opens the writer and sets up the logging environment.</p> <code>close</code> <p>Closes the writer and finalizes any ongoing logging processes.</p> <code>print_metrics</code> <p>Prints metrics and loss in a formatted manner.</p> <code>write</code> <p>Writes the optimization results to the tracking tool.</p> <code>log_hyperparams</code> <p>Logs the hyperparameters to the tracking tool.</p> <code>plot</code> <p>Logs model plots using provided plotting functions.</p> <code>log_model</code> <p>Logs the model and any relevant information.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Closes the writer and finalizes logging.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Closes the writer and finalizes logging.\"\"\"\n    raise NotImplementedError(\"Writers must implement a close method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Logs hyperparameters.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_hyperparams method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>  <code>abstractmethod</code>","text":"<p>Logs the model and associated data.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and associated data.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_model method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.open","title":"<code>open(config, iteration=None)</code>  <code>abstractmethod</code>","text":"<p>Opens the writer and prepares it for logging.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef open(self, config: TrainConfig, iteration: int | None = None) -&gt; Any:\n    \"\"\"\n    Opens the writer and prepares it for logging.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement an open method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>  <code>abstractmethod</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used to\n            generate plots.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a plot method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.print_metrics","title":"<code>print_metrics(result)</code>","text":"<p>Prints the metrics and loss in a readable format.</p> PARAMETER DESCRIPTION <code>result</code> <p>The optimization results to display.</p> <p> TYPE: <code>OptimizeResult</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def print_metrics(self, result: OptimizeResult) -&gt; None:\n    \"\"\"Prints the metrics and loss in a readable format.\n\n    Args:\n        result (OptimizeResult): The optimization results to display.\n    \"\"\"\n\n    # Find the key in result.metrics that contains \"loss\" (case-insensitive)\n    loss_key = next((k for k in result.metrics if \"loss\" in k.lower()), None)\n    if loss_key:\n        loss_value = result.metrics[loss_key]\n        msg = f\"Iteration {result.iteration: &gt;7} | {loss_key.title()}: {loss_value:.7f} -\"\n    else:\n        msg = f\"Iteration {result.iteration: &gt;7} | Loss: None -\"\n    msg += \" \".join([f\"{k}: {v:.7f}\" for k, v in result.metrics.items() if k != loss_key])\n    print(msg)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.write","title":"<code>write(result)</code>  <code>abstractmethod</code>","text":"<p>Logs the results of the current iteration.</p> PARAMETER DESCRIPTION <code>result</code> <p>The optimization results to log.</p> <p> TYPE: <code>OptimizeResult</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef write(self, result: OptimizeResult) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration.\n\n    Args:\n        result (OptimizeResult): The optimization results to log.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a write method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter","title":"<code>MLFlowWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to MLflow.</p> ATTRIBUTE DESCRIPTION <code>run</code> <p>The active MLflow run.</p> <p> TYPE: <code>Run</code> </p> <code>mlflow</code> <p>The MLflow module.</p> <p> TYPE: <code>ModuleType</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    try:\n        from mlflow.entities import Run\n    except ImportError:\n        raise ImportError(\n            \"mlflow is not installed. Please install qadence with the mlflow feature: \"\n            \"`pip install qadence[mlflow]`.\"\n        )\n\n    self.run: Run\n    self.mlflow: ModuleType\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.close","title":"<code>close()</code>","text":"<p>Closes the MLflow run.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the MLflow run.\"\"\"\n    if self.run:\n        self.mlflow.end_run()\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.get_signature_from_dataloader","title":"<code>get_signature_from_dataloader(model, dataloader)</code>","text":"<p>Infers the signature of the model based on the input data from the dataloader.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to use for inference.</p> <p> TYPE: <code>Module</code> </p> <code>dataloader</code> <p>DataLoader for model inputs.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Optional[Any]: The inferred signature, if available.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def get_signature_from_dataloader(\n    self, model: Module, dataloader: DataLoader | DictDataLoader | None\n) -&gt; Any:\n    \"\"\"\n    Infers the signature of the model based on the input data from the dataloader.\n\n    Args:\n        model (Module): The model to use for inference.\n        dataloader (DataLoader | DictDataLoader |  None): DataLoader for model inputs.\n\n    Returns:\n        Optional[Any]: The inferred signature, if available.\n    \"\"\"\n    from mlflow.models import infer_signature\n\n    if dataloader is None:\n        return None\n\n    xs: InputData\n    xs, *_ = next(iter(dataloader))\n    preds = model(xs)\n\n    if isinstance(xs, Tensor):\n        xs = xs.detach().cpu().numpy()\n        preds = preds.detach().cpu().numpy()\n        return infer_signature(xs, preds)\n\n    return None\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to MLflow.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to MLflow.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_params(hyperparams)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model and its signature to MLflow using the provided data loaders.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and its signature to MLflow using the provided data loaders.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    if not self.mlflow:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n\n    signatures = self.get_signature_from_dataloader(model, train_dataloader)\n    self.mlflow.pytorch.log_model(model, artifact_path=\"model\", signature=signatures)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the MLflow writer and initializes an MLflow run.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>mlflow</code> <p>The MLflow module instance.</p> <p> TYPE: <code>ModuleType | None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; ModuleType | None:\n    \"\"\"\n    Opens the MLflow writer and initializes an MLflow run.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        mlflow: The MLflow module instance.\n    \"\"\"\n    import mlflow\n\n    self.mlflow = mlflow\n    tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"\")\n    experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", str(uuid4()))\n    run_name = os.getenv(\"MLFLOW_RUN_NAME\", str(uuid4()))\n\n    if self.mlflow:\n        self.mlflow.set_tracking_uri(tracking_uri)\n\n        # Create or get the experiment\n        exp_filter_string = f\"name = '{experiment_name}'\"\n        experiments = self.mlflow.search_experiments(filter_string=exp_filter_string)\n        if not experiments:\n            self.mlflow.create_experiment(name=experiment_name)\n\n        self.mlflow.set_experiment(experiment_name)\n        self.run = self.mlflow.start_run(run_name=run_name, nested=False)\n\n    return self.mlflow\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.mlflow:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.mlflow.log_figure(fig, descr)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.write","title":"<code>write(result)</code>","text":"<p>Logs the results of the current iteration to MLflow.</p> PARAMETER DESCRIPTION <code>result</code> <p>The optimization results to log.</p> <p> TYPE: <code>OptimizeResult</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def write(self, result: OptimizeResult) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to MLflow.\n\n    Args:\n        result (OptimizeResult): The optimization results to log.\n    \"\"\"\n    # Not writing loss as loss is available in the metrics\n    # if result.loss is not None:\n    #     self.mlflow.log_metric(\"loss\", float(result.loss), step=result.iteration)\n    if self.mlflow:\n        self.mlflow.log_metrics(result.metrics, step=result.iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter","title":"<code>TensorBoardWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to TensorBoard.</p> ATTRIBUTE DESCRIPTION <code>writer</code> <p>The TensorBoard SummaryWriter instance.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.writer = None\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.close","title":"<code>close()</code>","text":"<p>Closes the TensorBoard writer.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the TensorBoard writer.\"\"\"\n    if self.writer:\n        self.writer.close()\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to TensorBoard.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to TensorBoard.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.writer:\n        self.writer.add_hparams(hyperparams, {})\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model.</p> <p>Currently not supported by TensorBoard.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model.\n\n    Currently not supported by TensorBoard.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    logger.warning(\"Model logging is not supported by tensorboard. No model will be logged.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the TensorBoard writer.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SummaryWriter</code> <p>The initialized TensorBoard writer.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; SummaryWriter:\n    \"\"\"\n    Opens the TensorBoard writer.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        SummaryWriter: The initialized TensorBoard writer.\n    \"\"\"\n    log_dir = str(config.log_folder)\n    purge_step = iteration if isinstance(iteration, int) else None\n    self.writer = SummaryWriter(log_dir=log_dir, purge_step=purge_step)\n    return self.writer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.writer:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.writer.add_figure(descr, fig, global_step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.write","title":"<code>write(result)</code>","text":"<p>Logs the results of the current iteration to TensorBoard.</p> PARAMETER DESCRIPTION <code>result</code> <p>The optimization results to log.</p> <p> TYPE: <code>OptimizeResult</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def write(self, result: OptimizeResult) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to TensorBoard.\n\n    Args:\n        result (OptimizeResult): The optimization results to log.\n    \"\"\"\n    # Not writing loss as loss is available in the metrics\n    # if result.loss is not None:\n    #     self.writer.add_scalar(\"loss\", float(result.loss), result.iteration)\n    if self.writer:\n        for key, value in result.metrics.items():\n            self.writer.add_scalar(key, value, result.iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.get_writer","title":"<code>get_writer(tracking_tool)</code>","text":"<p>Factory method to get the appropriate writer based on the tracking tool.</p> PARAMETER DESCRIPTION <code>tracking_tool</code> <p>The experiment tracking tool to use.</p> <p> TYPE: <code>ExperimentTrackingTool</code> </p> RETURNS DESCRIPTION <code>BaseWriter</code> <p>An instance of the appropriate writer.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def get_writer(tracking_tool: ExperimentTrackingTool) -&gt; BaseWriter:\n    \"\"\"Factory method to get the appropriate writer based on the tracking tool.\n\n    Args:\n        tracking_tool (ExperimentTrackingTool): The experiment tracking tool to use.\n\n    Returns:\n        BaseWriter: An instance of the appropriate writer.\n    \"\"\"\n    writer_class = WRITER_REGISTRY.get(tracking_tool)\n    if writer_class:\n        return writer_class()\n    else:\n        raise ValueError(f\"Unsupported tracking tool: {tracking_tool}\")\n</code></pre>"},{"location":"api/models/","title":"Quantum models","text":"<p>               Bases: <code>QuantumModel</code></p> <p>Quantum neural network model for n-dimensional inputs.</p> <p>Examples: <pre><code>import torch\nfrom qadence import QuantumCircuit, QNN, Z\nfrom qadence import hea, feature_map, hamiltonian_factory, kron\n\n# create the circuit\nn_qubits, depth = 2, 4\nfm = kron(\n    feature_map(1, support=(0,), param=\"x\"),\n    feature_map(1, support=(1,), param=\"y\")\n)\nansatz = hea(n_qubits=n_qubits, depth=depth)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nobs_base = hamiltonian_factory(n_qubits, detuning=Z)\n\n# the QNN will yield two outputs\nobs = [2.0 * obs_base, 4.0 * obs_base]\n\n# initialize and use the model\nqnn = QNN(circuit, obs, inputs=[\"x\", \"y\"])\ny = qnn(torch.rand(3, 2))\n</code></pre> <pre><code>tensor([[0.3483, 0.6966],\n        [0.6764, 1.3527],\n        [0.1195, 0.2390]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> <p>Initialize the QNN.</p> <p>The number of inputs is determined by the feature parameters in the input quantum circuit while the number of outputs is determined by how many observables are provided as input</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The quantum circuit to use for the QNN.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>The observable.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>optional configuration for the backend</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>inputs</code> <p>List that indicates the order of variables of the tensors that are passed to the model. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p> <p> TYPE: <code>list[Basic | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode | str</code> DEFAULT: <code>AD</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    inputs: list[sympy.Basic | str] | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n):\n    \"\"\"Initialize the QNN.\n\n    The number of inputs is determined by the feature parameters in the input\n    quantum circuit while the number of outputs is determined by how many\n    observables are provided as input\n\n    Args:\n        circuit: The quantum circuit to use for the QNN.\n        observable: The observable.\n        backend: The chosen quantum backend.\n        diff_mode: The differentiation engine to use. Choices 'gpsr' or 'ad'.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        configuration: optional configuration for the backend\n        inputs: List that indicates the order of variables of the tensors that are passed\n            to the model. Given input tensors `xs = torch.rand(batch_size, input_size:=2)` a QNN\n            with `inputs=[\"t\", \"x\"]` will assign `t, x = xs[:,0], xs[:,1]`.\n        input_diff_mode: The differentiation mode for the input tensor.\n    \"\"\"\n    super().__init__(\n        circuit,\n        observable=observable,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        configuration=configuration,\n        noise=noise,\n    )\n    if self._observable is None:\n        raise ValueError(\"You need to provide at least one observable in the QNN constructor\")\n    if (inputs is not None) and (len(self.inputs) == len(inputs)):\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in inputs]  # type: ignore[union-attr]\n    elif (inputs is None) and len(self.inputs) &lt;= 1:\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in self.inputs]  # type: ignore[union-attr]\n    else:\n        raise ValueError(\n            \"\"\"\n            Your QNN has more than one input. Please provide a list of inputs in the order of\n            your tensor domain. For example, if you want to pass\n            `xs = torch.rand(batch_size, input_size:=3)` to you QNN, where\n            ```\n            t = x[:,0]\n            x = x[:,1]\n            y = x[:,2]\n            ```\n            you have to specify\n            ```\n            QNN(circuit, observable, inputs=[\"t\", \"x\", \"y\"])\n            ```\n            You can also pass a list of sympy symbols.\n        \"\"\"\n        )\n    self.format_to_dict = format_to_dict_fn(self.inputs)  # type: ignore[arg-type]\n    self.input_diff_mode = InputDiffMode(input_diff_mode)\n    if self.input_diff_mode == InputDiffMode.FD:\n        from qadence.backends.utils import finitediff\n\n        self.__derivative = finitediff\n    elif self.input_diff_mode == InputDiffMode.AD:\n        self.__derivative = _torch_derivative  # type: ignore[assignment]\n    else:\n        raise ValueError(f\"Unkown forward diff mode: {self.input_diff_mode}\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel","title":"<code>QuantumModel(circuit, observable=None, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, mitigation=None, configuration=None)</code>","text":"<p>               Bases: <code>Module</code></p> <p>The central class of qadence that executes <code>QuantumCircuit</code>s and make them differentiable.</p> <p>This class should be used as base class for any new quantum model supported in the qadence framework for information on the implementation of custom models see here.</p> <p>Example: <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit, RX, RY, Z, PI, chain, kron\nfrom qadence import FeatureParameter, VariationalParameter\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, theta), RY(1, theta)),\n    kron(RX(0, phi), RY(1, phi)),\n)\n\ncircuit = QuantumCircuit(2, block)\n\nobservable = Z(0) + Z(1)\n\nmodel = QuantumModel(circuit, observable)\nvalues = {\"phi\": torch.tensor([PI, PI/2]), \"theta\": torch.tensor([PI, PI/2])}\n\nwf = model.run(values)\nxs = model.sample(values, n_shots=100)\nex = model.expectation(values)\nprint(wf)\nprint(xs)\nprint(ex)\n</code></pre> <pre><code>tensor([[ 1.0000e+00+0.0000e+00j, -1.2246e-16+0.0000e+00j,\n          0.0000e+00+1.2246e-16j,  0.0000e+00-1.4998e-32j],\n        [ 4.9304e-32+0.0000e+00j,  2.2204e-16+0.0000e+00j,\n          0.0000e+00-2.2204e-16j,  0.0000e+00-1.0000e+00j]])\n[OrderedCounter({'00': 100}), OrderedCounter({'11': 100})]\ntensor([[ 2.],\n        [-2.]], requires_grad=True)\n</code></pre>  ```</p> <p>Initialize a generic QuantumModel instance.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The circuit that is executed.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>Optional observable(s) that are used only in the <code>expectation</code> method. You can also provide observables on the fly to the expectation call directly.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>A backend for circuit execution.</p> <p> TYPE: <code>BackendName | str</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>A differentiability mode. Parameter shift based modes work on all backends. AD based modes only on PyTorch based backends.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Configuration for the backend.</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the <code>diff_mode</code> argument is set to None</p> Source code in <code>qadence/model.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock | None = None,\n    backend: BackendName | str = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n):\n    \"\"\"Initialize a generic QuantumModel instance.\n\n    Arguments:\n        circuit: The circuit that is executed.\n        observable: Optional observable(s) that are used only in the `expectation` method. You\n            can also provide observables on the fly to the expectation call directly.\n        backend: A backend for circuit execution.\n        diff_mode: A differentiability mode. Parameter shift based modes work on all backends.\n            AD based modes only on PyTorch based backends.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        configuration: Configuration for the backend.\n        noise: A noise model to use.\n\n    Raises:\n        ValueError: if the `diff_mode` argument is set to None\n    \"\"\"\n    super().__init__()\n\n    if not isinstance(circuit, QuantumCircuit):\n        TypeError(\n            f\"The circuit should be of type '&lt;class QuantumCircuit&gt;'. Got {type(circuit)}.\"\n        )\n\n    if diff_mode is None:\n        raise ValueError(\"`diff_mode` cannot be `None` in a `QuantumModel`.\")\n\n    self.backend = backend_factory(\n        backend=backend, diff_mode=diff_mode, configuration=configuration\n    )\n\n    if isinstance(observable, list) or observable is None:\n        observable = observable\n    else:\n        observable = [observable]\n\n    def _is_feature_param(p: Parameter) -&gt; bool:\n        return not p.trainable and not p.is_number\n\n    if observable is None:\n        self.inputs = list(filter(_is_feature_param, circuit.unique_parameters))\n    else:\n        uparams = unique_parameters(chain(circuit.block, *observable))\n        self.inputs = list(filter(_is_feature_param, uparams))\n\n    conv = self.backend.convert(circuit, observable)\n    self.embedding_fn = conv.embedding_fn\n    self._circuit = conv.circuit\n    self._observable = conv.observable\n    self._backend_name = backend\n    self._diff_mode = diff_mode\n    self._measurement = measurement\n    self._noise = noise\n    self._mitigation = mitigation\n    self._params = nn.ParameterDict(\n        {\n            str(key): nn.Parameter(val, requires_grad=val.requires_grad)\n            for key, val in conv.params.items()\n        }\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Get device.</p> RETURNS DESCRIPTION <code>device</code> <p>torch.device</p>"},{"location":"api/models/#qadence.model.QuantumModel.in_features","title":"<code>in_features: int</code>  <code>property</code>","text":"<p>Number of inputs.</p>"},{"location":"api/models/#qadence.model.QuantumModel.num_vparams","title":"<code>num_vparams: int</code>  <code>property</code>","text":"<p>The number of variational parameters.</p>"},{"location":"api/models/#qadence.model.QuantumModel.out_features","title":"<code>out_features: int | None</code>  <code>property</code>","text":"<p>Number of outputs.</p>"},{"location":"api/models/#qadence.model.QuantumModel.vals_vparams","title":"<code>vals_vparams: Tensor</code>  <code>property</code>","text":"<p>Dictionary with parameters which are actually updated during optimization.</p>"},{"location":"api/models/#qadence.model.QuantumModel.vparams","title":"<code>vparams: OrderedDict</code>  <code>property</code>","text":"<p>Variational parameters.</p>"},{"location":"api/models/#qadence.model.QuantumModel.assign_parameters","title":"<code>assign_parameters(values)</code>","text":"<p>Return the final, assigned circuit that is used in e.g. <code>backend.run</code>.</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Final, assigned circuit that is used in e.g. <code>backend.run</code></p> Source code in <code>qadence/model.py</code> <pre><code>def assign_parameters(self, values: dict[str, Tensor]) -&gt; Any:\n    \"\"\"Return the final, assigned circuit that is used in e.g. `backend.run`.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n\n    Returns:\n        Final, assigned circuit that is used in e.g. `backend.run`\n    \"\"\"\n    params = self.embedding_fn(self._params, values)\n    return self.backend.assign_parameters(self._circuit, params)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.circuit","title":"<code>circuit(circuit)</code>","text":"<p>Get backend-converted circuit.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>QuantumCircuit instance.</p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>Backend circuit.</p> Source code in <code>qadence/model.py</code> <pre><code>def circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Get backend-converted circuit.\n\n    Args:\n        circuit: QuantumCircuit instance.\n\n    Returns:\n        Backend circuit.\n    \"\"\"\n    return self.backend.circuit(circuit)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.expectation","title":"<code>expectation(values={}, observable=None, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute expectation using the given backend.</p> <p>Given an input state \\(|\\psi_0 \\rangle\\), a set of variational parameters \\(\\vec{\\theta}\\) and the unitary representation of the model \\(U(\\vec{\\theta})\\) we return \\(\\langle \\psi_0 | U(\\vec{\\theta}) | \\psi_0 \\rangle\\).</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>observable</code> <p>Observable part of the expectation.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable | None</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Optional input state.</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>A mitigation protocol to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>when no observable is set.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor of shape n_batches x n_obs</p> Source code in <code>qadence/model.py</code> <pre><code>def expectation(\n    self,\n    values: dict[str, Tensor] = {},\n    observable: list[ConvertedObservable] | ConvertedObservable | None = None,\n    state: Optional[Tensor] = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    r\"\"\"Compute expectation using the given backend.\n\n\n\n    Given an input state $|\\psi_0 \\rangle$,\n    a set of variational parameters $\\vec{\\theta}$\n    and the unitary representation of the model $U(\\vec{\\theta})$\n    we return $\\langle \\psi_0 | U(\\vec{\\theta}) | \\psi_0 \\rangle$.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        observable: Observable part of the expectation.\n        state: Optional input state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: A mitigation protocol to use.\n        endianness: Storage convention for binary information.\n\n    Raises:\n        ValueError: when no observable is set.\n\n    Returns:\n        A torch.Tensor of shape n_batches x n_obs\n    \"\"\"\n    if observable is None:\n        if self._observable is None:\n            raise ValueError(\n                \"Provide an AbstractBlock as the observable to compute expectation.\"\n                \"Either pass a 'native_observable' directly to 'QuantumModel.expectation'\"\n                \"or pass a (non-native) '&lt;class AbstractBlock&gt;' to the 'QuantumModel.__init__'.\"\n            )\n        observable = self._observable\n\n    params = self.embedding_fn(self._params, values)\n    if measurement is None:\n        measurement = self._measurement\n    if noise is None:\n        noise = self._noise\n    else:\n        self._noise = noise\n    if mitigation is None:\n        mitigation = self._mitigation\n    return self.backend.expectation(\n        circuit=self._circuit,\n        observable=observable,\n        param_values=params,\n        state=state,\n        measurement=measurement,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Calls run method with arguments.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor representing output.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/model.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Calls run method with arguments.\n\n    Returns:\n        Tensor: A torch.Tensor representing output.\n    \"\"\"\n    return self.run(*args, **kwargs)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.load","title":"<code>load(file_path, as_torch=False, map_location='cpu')</code>  <code>classmethod</code>","text":"<p>Load QuantumModel.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>File path to load model from.</p> <p> TYPE: <code>str | Path</code> </p> <code>as_torch</code> <p>Load parameters as torch tensor. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>map_location</code> <p>Location for loading. Defaults to \"cpu\".</p> <p> TYPE: <code>str | device</code> DEFAULT: <code>'cpu'</code> </p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel from file_path.</p> Source code in <code>qadence/model.py</code> <pre><code>@classmethod\ndef load(\n    cls, file_path: str | Path, as_torch: bool = False, map_location: str | torch.device = \"cpu\"\n) -&gt; QuantumModel:\n    \"\"\"Load QuantumModel.\n\n    Arguments:\n        file_path: File path to load model from.\n        as_torch: Load parameters as torch tensor. Defaults to False.\n        map_location (str | torch.device, optional): Location for loading. Defaults to \"cpu\".\n\n    Returns:\n        QuantumModel from file_path.\n    \"\"\"\n    qm_pt = {}\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if os.path.isdir(file_path):\n        from qadence.ml_tools.callbacks.saveload import get_latest_checkpoint_name\n\n        file_path = file_path / get_latest_checkpoint_name(file_path, \"model\")\n\n    try:\n        qm_pt = torch.load(file_path, map_location=map_location)\n    except Exception as e:\n        logger.error(f\"Unable to load QuantumModel due to {e}\")\n    return cls._from_dict(qm_pt, as_torch)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.load_params_from_dict","title":"<code>load_params_from_dict(d, strict=True)</code>","text":"<p>Copy parameters from dictionary into this QuantumModel.</p> <p>Unlike :meth:<code>~qadence.QuantumModel.from_dict</code>, this method does not create a new QuantumModel instance, but rather loads the parameters into the same QuantumModel. The behaviour of this method is similar to :meth:<code>~torch.nn.Module.load_state_dict</code>.</p> <p>The dictionary is assumed to have the format as saved via :meth:<code>~qadence.QuantumModel.to_dict</code></p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary</p> <p> TYPE: <code>dict</code> </p> <code>strict</code> <p>Whether to strictly enforce that the parameter keys in the dictionary and in the model match exactly. Default: <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>qadence/model.py</code> <pre><code>def load_params_from_dict(self, d: dict, strict: bool = True) -&gt; None:\n    \"\"\"Copy parameters from dictionary into this QuantumModel.\n\n    Unlike :meth:`~qadence.QuantumModel.from_dict`, this method does not create a new\n    QuantumModel instance, but rather loads the parameters into the same QuantumModel.\n    The behaviour of this method is similar to :meth:`~torch.nn.Module.load_state_dict`.\n\n    The dictionary is assumed to have the format as saved via\n    :meth:`~qadence.QuantumModel.to_dict`\n\n    Args:\n        d (dict): The dictionary\n        strict (bool, optional):\n            Whether to strictly enforce that the parameter keys in the dictionary and\n            in the model match exactly. Default: ``True``.\n    \"\"\"\n    param_dict = d[\"param_dict\"]\n    missing_keys = set(self._params.keys()) - set(param_dict.keys())\n    unexpected_keys = set(param_dict.keys()) - set(self._params.keys())\n\n    if strict:\n        error_msgs = []\n        if len(unexpected_keys) &gt; 0:\n            error_msgs.append(f\"Unexpected key(s) in dictionary: {unexpected_keys}\")\n        if len(missing_keys) &gt; 0:\n            error_msgs.append(f\"Missing key(s) in dictionary: {missing_keys}\")\n        if len(error_msgs) &gt; 0:\n            errors_string = \"\\n\\t\".join(error_msgs)\n            raise RuntimeError(\n                f\"Error(s) loading the parameter dictionary due to: \\n\\t{errors_string}\\n\"\n                \"This error was thrown because the `strict` argument is set `True`.\"\n                \"If you don't need the parameter keys of the dictionary to exactly match \"\n                \"the model parameters, set `strict=False`.\"\n            )\n\n    for n, param in param_dict.items():\n        try:\n            with torch.no_grad():\n                self._params[n].copy_(\n                    torch.nn.Parameter(param, requires_grad=param.requires_grad)\n                )\n        except Exception as e:\n            logger.warning(f\"Unable to load parameter {n} from dictionary due to {e}.\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.observable","title":"<code>observable(observable, n_qubits)</code>","text":"<p>Get backend observable.</p> PARAMETER DESCRIPTION <code>observable</code> <p>Observable block.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Backend observable.</p> Source code in <code>qadence/model.py</code> <pre><code>def observable(self, observable: AbstractBlock, n_qubits: int) -&gt; Any:\n    \"\"\"Get backend observable.\n\n    Args:\n        observable: Observable block.\n        n_qubits: Number of qubits\n\n    Returns:\n        Backend observable.\n    \"\"\"\n    return self.backend.observable(observable, n_qubits)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.overlap","title":"<code>overlap()</code>","text":"<p>Overlap of model.</p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>The overlap method is not implemented for this model.</p> Source code in <code>qadence/model.py</code> <pre><code>def overlap(self) -&gt; Tensor:\n    \"\"\"Overlap of model.\n\n    Raises:\n        NotImplementedError: The overlap method is not implemented for this model.\n    \"\"\"\n    raise NotImplementedError(\"The overlap method is not implemented for this model.\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.reset_vparams","title":"<code>reset_vparams(values)</code>","text":"<p>Reset all the variational parameters with a given list of values.</p> Source code in <code>qadence/model.py</code> <pre><code>def reset_vparams(self, values: Sequence) -&gt; None:\n    \"\"\"Reset all the variational parameters with a given list of values.\"\"\"\n    current_vparams = OrderedDict({k: v for k, v in self._params.items() if v.requires_grad})\n\n    assert (\n        len(values) == self.num_vparams\n    ), \"Pass an iterable with the values of all variational parameters\"\n    for i, k in enumerate(current_vparams.keys()):\n        current_vparams[k].data = torch.tensor([values[i]])\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.run","title":"<code>run(values=None, state=None, endianness=Endianness.BIG)</code>","text":"<p>Run model.</p> <p>Given an input state \\(| \\psi_0 \\rangle\\), a set of variational parameters \\(\\vec{\\theta}\\) and the unitary representation of the model \\(U(\\vec{\\theta})\\) we return \\(U(\\vec{\\theta}) | \\psi_0 \\rangle\\).</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Optional input state to apply model on.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor representing output.</p> Source code in <code>qadence/model.py</code> <pre><code>def run(\n    self,\n    values: dict[str, Tensor] = None,\n    state: Tensor | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    r\"\"\"Run model.\n\n    Given an input state $| \\psi_0 \\rangle$,\n    a set of variational parameters $\\vec{\\theta}$\n    and the unitary representation of the model $U(\\vec{\\theta})$\n    we return $U(\\vec{\\theta}) | \\psi_0 \\rangle$.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        state: Optional input state to apply model on.\n        endianness: Storage convention for binary information.\n\n    Returns:\n        A torch.Tensor representing output.\n    \"\"\"\n    if values is None:\n        values = {}\n\n    params = self.embedding_fn(self._params, values)\n\n    return self.backend.run(self._circuit, params, state=state, endianness=endianness)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.sample","title":"<code>sample(values={}, n_shots=1000, state=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Obtain samples from model.</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>n_shots</code> <p>Observable part of the expectation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>state</code> <p>Optional input state to apply model on.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>A mitigation protocol to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>list[Counter]</code> <p>A list of Counter instances with the sample results.</p> Source code in <code>qadence/model.py</code> <pre><code>def sample(\n    self,\n    values: dict[str, torch.Tensor] = {},\n    n_shots: int = 1000,\n    state: torch.Tensor | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; list[Counter]:\n    \"\"\"Obtain samples from model.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        n_shots: Observable part of the expectation.\n        state: Optional input state to apply model on.\n        noise: A noise model to use.\n        mitigation: A mitigation protocol to use.\n        endianness: Storage convention for binary information.\n\n    Returns:\n        A list of Counter instances with the sample results.\n    \"\"\"\n    params = self.embedding_fn(self._params, values)\n    if noise is None:\n        noise = self._noise\n    if mitigation is None:\n        mitigation = self._mitigation\n    return self.backend.sample(\n        self._circuit,\n        params,\n        n_shots=n_shots,\n        state=state,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.save","title":"<code>save(folder, file_name='quantum_model.pt', save_params=True)</code>","text":"<p>Save model.</p> PARAMETER DESCRIPTION <code>folder</code> <p>Folder where model is saved.</p> <p> TYPE: <code>str | Path</code> </p> <code>file_name</code> <p>File name for saving model. Defaults to \"quantum_model.pt\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'quantum_model.pt'</code> </p> <code>save_params</code> <p>Save parameters if True. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If folder is not a directory.</p> Source code in <code>qadence/model.py</code> <pre><code>def save(\n    self, folder: str | Path, file_name: str = \"quantum_model.pt\", save_params: bool = True\n) -&gt; None:\n    \"\"\"Save model.\n\n    Arguments:\n        folder: Folder where model is saved.\n        file_name: File name for saving model. Defaults to \"quantum_model.pt\".\n        save_params: Save parameters if True. Defaults to True.\n\n    Raises:\n        FileNotFoundError: If folder is not a directory.\n    \"\"\"\n    if not os.path.isdir(folder):\n        raise FileNotFoundError\n    try:\n        torch.save(self._to_dict(save_params), folder / Path(file_name))\n    except Exception as e:\n        logger.error(f\"Unable to write QuantumModel to disk due to {e}\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.to","title":"<code>to(*args, **kwargs)</code>","text":"<p>Conversion method for device or types.</p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel with conversions.</p> Source code in <code>qadence/model.py</code> <pre><code>def to(self, *args: Any, **kwargs: Any) -&gt; QuantumModel:\n    \"\"\"Conversion method for device or types.\n\n    Returns:\n        QuantumModel with conversions.\n    \"\"\"\n    from pyqtorch import QuantumCircuit as PyQCircuit\n\n    try:\n        if isinstance(self._circuit.native, PyQCircuit):\n            self._circuit.native = self._circuit.native.to(*args, **kwargs)\n            if self._observable is not None:\n                if isinstance(self._observable, ConvertedObservable):\n                    self._observable.native = self._observable.native.to(*args, **kwargs)\n                elif isinstance(self._observable, list):\n                    for obs in self._observable:\n                        obs.native = obs.native.to(*args, **kwargs)\n            self._params = self._params.to(\n                device=self._circuit.native.device,\n                dtype=(\n                    torch.float64\n                    if self._circuit.native.dtype == torch.cdouble\n                    else torch.float32\n                ),\n            )\n            logger.debug(f\"Moved {self} to {args}, {kwargs}.\")\n        else:\n            logger.debug(\"QuantumModel.to only supports pyqtorch.QuantumCircuits.\")\n    except Exception as e:\n        logger.warning(f\"Unable to move {self} to {args}, {kwargs} due to {e}.\")\n    return self\n</code></pre>"},{"location":"api/models/#qadence.ml_tools.models.QNN.forward","title":"<code>forward(values=None, state=None, measurement=None, noise=None, endianness=Endianness.BIG)</code>","text":"<p>Forward pass of the model.</p> <p>This returns the (differentiable) expectation value of the given observable operator defined in the constructor. Differently from the base QuantumModel class, the QNN accepts also a tensor as input for the forward pass. The tensor is expected to have shape: <code>n_batches x in_features</code> where <code>n_batches</code> is the number of data points and <code>in_features</code> is the dimensionality of the problem</p> <p>The output of the forward pass is the expectation value of the input observable(s). If a single observable is given, the output shape is <code>n_batches</code> while if multiple observables are given the output shape is instead <code>n_batches x n_observables</code></p> PARAMETER DESCRIPTION <code>values</code> <p>the values of the feature parameters</p> <p> TYPE: <code>dict[str, Tensor] | Tensor</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a tensor with the expectation value of the observables passed in the constructor of the model</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def forward(\n    self,\n    values: dict[str, Tensor] | Tensor = None,\n    state: Tensor | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the model.\n\n    This returns the (differentiable) expectation value of the given observable\n    operator defined in the constructor. Differently from the base QuantumModel\n    class, the QNN accepts also a tensor as input for the forward pass. The\n    tensor is expected to have shape: `n_batches x in_features` where `n_batches`\n    is the number of data points and `in_features` is the dimensionality of the problem\n\n    The output of the forward pass is the expectation value of the input\n    observable(s). If a single observable is given, the output shape is\n    `n_batches` while if multiple observables are given the output shape\n    is instead `n_batches x n_observables`\n\n    Args:\n        values: the values of the feature parameters\n        state: Initial state.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n\n    Returns:\n        Tensor: a tensor with the expectation value of the observables passed\n            in the constructor of the model\n    \"\"\"\n    return self.expectation(\n        values, state=state, measurement=measurement, noise=noise, endianness=endianness\n    )\n</code></pre>"},{"location":"api/models/#qadence.ml_tools.models.QNN.from_configs","title":"<code>from_configs(register, obs_config, fm_config=FeatureMapConfig(), ansatz_config=AnsatzConfig(), backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, input_diff_mode=InputDiffMode.AD)</code>  <code>classmethod</code>","text":"<p>Create a QNN from a set of configurations.</p> PARAMETER DESCRIPTION <code>register</code> <p>The number of qubits or a register object.</p> <p> TYPE: <code>int | Register</code> </p> <code>obs_config</code> <p>The configuration(s) for the observable(s).</p> <p> TYPE: <code>list[ObservableConfig] | ObservableConfig</code> </p> <code>fm_config</code> <p>The configuration for the feature map. Defaults to no feature encoding block.</p> <p> TYPE: <code>FeatureMapConfig</code> DEFAULT: <code>FeatureMapConfig()</code> </p> <code>ansatz_config</code> <p>The configuration for the ansatz. Defaults to a single layer of hardware efficient ansatz.</p> <p> TYPE: <code>AnsatzConfig</code> DEFAULT: <code>AnsatzConfig()</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices are 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>Noise</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Optional backend configuration.</p> <p> TYPE: <code>BackendConfiguration | dict</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode</code> DEFAULT: <code>AD</code> </p> RETURNS DESCRIPTION <code>QNN</code> <p>A QNN object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the observable configuration is not provided.</p> <p>Example: <pre><code>import torch\nfrom qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\nfrom qadence.ml_tools import QNN\nfrom qadence.constructors import ObservableConfig\nfrom qadence.operations import Z\nfrom qadence.types import (\n    AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n)\n\nregister = 4\nobs_config = ObservableConfig(\n    detuning=Z,\n    scale=5.0,\n    shift=0.0,\n    transformation_type=ObservableTransform.SCALE,\n    trainable_transform=None,\n)\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs=[\"x\", \"y\"],\n    basis_set=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.CONSTANT,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nqnn = QNN.from_configs(\n    register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n)\n\nx = torch.rand(2, 2)\ny = qnn(x)\n</code></pre> <pre><code>tensor([[3.7460],\n        [3.6527]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>@classmethod\ndef from_configs(\n    cls,\n    register: int | Register,\n    obs_config: Any,\n    fm_config: Any = FeatureMapConfig(),\n    ansatz_config: Any = AnsatzConfig(),\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n) -&gt; QNN:\n    \"\"\"Create a QNN from a set of configurations.\n\n    Args:\n        register (int | Register): The number of qubits or a register object.\n        obs_config (list[ObservableConfig] | ObservableConfig): The configuration(s)\n            for the observable(s).\n        fm_config (FeatureMapConfig): The configuration for the feature map.\n            Defaults to no feature encoding block.\n        ansatz_config (AnsatzConfig): The configuration for the ansatz.\n            Defaults to a single layer of hardware efficient ansatz.\n        backend (BackendName): The chosen quantum backend.\n        diff_mode (DiffMode): The differentiation engine to use. Choices are\n            'gpsr' or 'ad'.\n        measurement (Measurements): Optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator.\n        noise (Noise): A noise model to use.\n        configuration (BackendConfiguration | dict): Optional backend configuration.\n        input_diff_mode (InputDiffMode): The differentiation mode for the input tensor.\n\n    Returns:\n        A QNN object.\n\n    Raises:\n        ValueError: If the observable configuration is not provided.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\n    from qadence.ml_tools import QNN\n    from qadence.constructors import ObservableConfig\n    from qadence.operations import Z\n    from qadence.types import (\n        AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n    )\n\n    register = 4\n    obs_config = ObservableConfig(\n        detuning=Z,\n        scale=5.0,\n        shift=0.0,\n        transformation_type=ObservableTransform.SCALE,\n        trainable_transform=None,\n    )\n    fm_config = FeatureMapConfig(\n        num_features=2,\n        inputs=[\"x\", \"y\"],\n        basis_set=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.CONSTANT,\n        feature_range={\n            \"x\": (-1.0, 1.0),\n            \"y\": (0.0, 1.0),\n        },\n    )\n    ansatz_config = AnsatzConfig(\n        depth=2,\n        ansatz_type=AnsatzType.HEA,\n        ansatz_strategy=Strategy.DIGITAL,\n    )\n\n    qnn = QNN.from_configs(\n        register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n    )\n\n    x = torch.rand(2, 2)\n    y = qnn(x)\n    print(str(y)) # markdown-exec: hide\n    ```\n    \"\"\"\n    from .constructors import build_qnn_from_configs\n\n    return build_qnn_from_configs(\n        register=register,\n        observable_config=obs_config,\n        fm_config=fm_config,\n        ansatz_config=ansatz_config,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        noise=noise,\n        configuration=configuration,\n        input_diff_mode=input_diff_mode,\n    )\n</code></pre>"},{"location":"api/noise/","title":"Noise","text":""},{"location":"api/noise/#noise-for-simulations","title":"Noise for simulations","text":""},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler","title":"<code>NoiseHandler(protocol, options=dict())</code>","text":"<p>A container for multiple sources of noise.</p> <p>Note <code>NoiseProtocol.ANALOG</code> and <code>NoiseProtocol.DIGITAL</code> sources cannot be both present. Also <code>NoiseProtocol.READOUT</code> can only be present once as the last noise sources, and only exclusively with <code>NoiseProtocol.DIGITAL</code> sources.</p> PARAMETER DESCRIPTION <code>protocol</code> <p>The protocol(s) applied. To be defined from <code>NoiseProtocol</code>.</p> <p> TYPE: <code>NoiseEnum | list[NoiseEnum]</code> </p> <code>options</code> <p>A list of options defining the protocol. For <code>NoiseProtocol.ANALOG</code>, options should contain a field <code>noise_probs</code>. For <code>NoiseProtocol.DIGITAL</code>, options should contain a field <code>error_probability</code>.</p> <p> TYPE: <code>dict | list[dict]</code> DEFAULT: <code>dict()</code> </p> <p>Examples:</p> <pre><code>    from qadence import NoiseProtocol, NoiseHandler\n\n    analog_options = {\"noise_probs\": 0.1}\n    digital_options = {\"error_probability\": 0.1}\n    readout_options = {\"error_probability\": 0.1, \"seed\": 0}\n\n    # single noise sources\n    analog_noise = NoiseHandler(NoiseProtocol.ANALOG.DEPOLARIZING, analog_options)\n    digital_depo_noise = NoiseHandler(NoiseProtocol.DIGITAL.DEPOLARIZING, digital_options)\n    readout_noise = NoiseHandler(NoiseProtocol.READOUT, readout_options)\n\n    # init from multiple sources\n    protocols: list = [NoiseProtocol.DIGITAL.DEPOLARIZING, NoiseProtocol.READOUT]\n    options: list = [digital_options, readout_noise]\n    noise_combination = NoiseHandler(protocols, options)\n\n    # Appending noise sources\n    noise_combination = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, digital_options)\n    noise_combination.append([digital_depo_noise, readout_noise])\n</code></pre> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def __init__(\n    self,\n    protocol: NoiseEnum | list[NoiseEnum],\n    options: dict | list[dict] = dict(),\n) -&gt; None:\n    self.protocol = protocol if isinstance(protocol, list) else [protocol]\n    self.options = options if isinstance(options, list) else [options] * len(self.protocol)\n    self.verify_all_protocols()\n</code></pre>"},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler.append","title":"<code>append(other)</code>","text":"<p>Append noises.</p> PARAMETER DESCRIPTION <code>other</code> <p>The noises to add.</p> <p> TYPE: <code>NoiseHandler | list[NoiseHandler]</code> </p> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def append(self, other: NoiseHandler | list[NoiseHandler]) -&gt; None:\n    \"\"\"Append noises.\n\n    Args:\n        other (NoiseHandler | list[NoiseHandler]): The noises to add.\n    \"\"\"\n    # To avoid overwriting the noise_sources list if an error is raised, make a copy\n    other_list = other if isinstance(other, list) else [other]\n    protocols = self.protocol[:]\n    options = self.options[:]\n\n    for noise in other_list:\n        protocols += noise.protocol\n        options += noise.options\n\n    # init may raise an error\n    temp_handler = NoiseHandler(protocols, options)\n    # if verify passes, replace protocols and options\n    self.protocol = temp_handler.protocol\n    self.options = temp_handler.options\n</code></pre>"},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler.verify_all_protocols","title":"<code>verify_all_protocols()</code>","text":"<p>Make sure all protocols are correct in terms and their combination too.</p> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def verify_all_protocols(self) -&gt; None:\n    \"\"\"Make sure all protocols are correct in terms and their combination too.\"\"\"\n\n    if len(self.protocol) == 0:\n        raise ValueError(\"NoiseHandler should be specified with one valid configuration.\")\n\n    if len(self.protocol) != len(self.options):\n        raise ValueError(\"Specify lists of same length when defining noises.\")\n\n    for protocol, option in zip(self.protocol, self.options):\n        self._verify_single_protocol(protocol, option)\n\n    types = [type(p) for p in self.protocol]\n    unique_types = set(types)\n    if NoiseProtocol.DIGITAL in unique_types and NoiseProtocol.ANALOG in unique_types:\n        raise ValueError(\"Cannot define a config with both Digital and Analog noises.\")\n\n    if NoiseProtocol.ANALOG in unique_types:\n        if NoiseProtocol.READOUT in unique_types:\n            raise ValueError(\"Cannot define a config with both READOUT and Analog noises.\")\n        if types.count(NoiseProtocol.ANALOG) &gt; 1:\n            raise ValueError(\"Multiple Analog Noises are not supported yet.\")\n\n    if NoiseProtocol.READOUT in unique_types:\n        if (\n            not isinstance(self.protocol[-1], NoiseProtocol.READOUT)\n            or types.count(NoiseProtocol.READOUT) &gt; 1\n        ):\n            raise ValueError(\"Only define a NoiseHandler with one READOUT as the last Noise.\")\n</code></pre>"},{"location":"api/operations/","title":"Operations","text":"<p>Operations are common <code>PrimitiveBlocks</code>, these are often called gates elsewhere.</p>"},{"location":"api/operations/#constant-blocks","title":"Constant blocks","text":"<p>CY gate not implemented</p>"},{"location":"api/operations/#qadence.operations.X","title":"<code>X(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The X gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.Y","title":"<code>Y(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Y gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.Z","title":"<code>Z(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Z gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.I","title":"<code>I(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The identity gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.H","title":"<code>H(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hadamard or H gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = (1 / np.sqrt(2)) * (X(target) + Z(target) - np.sqrt(2) * I(target))\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.S","title":"<code>S(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The S / Phase gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.SDagger","title":"<code>SDagger(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hermitian adjoint/conjugate transpose of the S / Phase gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.SWAP","title":"<code>SWAP(control, target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The SWAP gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    a11 = 0.5 * (Z(control) - I(control))\n    a22 = -0.5 * (Z(target) + I(target))\n    a12 = 0.5 * (chain(X(control), Z(control)) + X(control))\n    a21 = 0.5 * (chain(Z(target), X(target)) + X(target))\n    self.generator = (\n        kron(-1.0 * a22, a11) + kron(-1.0 * a11, a22) + kron(a12, a21) + kron(a21, a12)\n    )\n    super().__init__((control, target), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.T","title":"<code>T(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The T gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.TDagger","title":"<code>TDagger(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hermitian adjoint/conjugate transpose of the T gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CNOT","title":"<code>CNOT(control, target, noise=None)</code>","text":"<p>               Bases: <code>ControlBlock</code></p> <p>The CNot, or CX, gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(self, control: int, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    self.generator = kron(N(control), X(target) - I(target))\n    super().__init__((control,), X(target), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CZ","title":"<code>CZ(control, target, noise=None)</code>","text":"<p>               Bases: <code>MCZ</code></p> <p>The CZ gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(self, control: int, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((control,), target, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CPHASE","title":"<code>CPHASE(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCPHASE</code></p> <p>The CPHASE gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#parametrized-blocks","title":"Parametrized blocks","text":""},{"location":"api/operations/#qadence.operations.RX","title":"<code>RX(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Rx gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    # TODO: should we give them more meaningful names? like 'angle'?\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = X(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.RY","title":"<code>RY(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Ry gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = Y(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.RZ","title":"<code>RZ(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Rz gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRX","title":"<code>CRX(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRX</code></p> <p>The CRX gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRY","title":"<code>CRY(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRY</code></p> <p>The CRY gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self, control: int, target: int, parameter: TParameter, noise: NoiseHandler | None = None\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRZ","title":"<code>CRZ(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRZ</code></p> <p>The CRZ gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.PHASE","title":"<code>PHASE(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Parametric Phase / S gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = ParamMap(parameter=parameter)\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#hamiltonian-evolution","title":"Hamiltonian Evolution","text":"<p>AnalogSWAP should be turned into a proper analog block</p>"},{"location":"api/operations/#qadence.operations.HamEvo","title":"<code>HamEvo(generator, parameter, qubit_support=None, duration=None, noise_operators=list())</code>","text":"<p>               Bases: <code>TimeEvolutionBlock</code></p> <p>The Hamiltonian evolution operator U(t).</p> <p>For time-independent Hamiltonians the solution is exact:</p> <pre><code>U(t) = exp(-iGt)\n</code></pre> <p>where G represents an Hermitian generator, or Hamiltonian and t represents the time parameter. For time-dependent Hamiltonians, the solution is obtained by numerical integration of the Schrodinger equation.</p> PARAMETER DESCRIPTION <code>generator</code> <p>Hamiltonian generator, either symbolic as an AbstractBlock, or as a torch.Tensor or numpy.ndarray.</p> <p> TYPE: <code>Union[TGenerator, AbstractBlock]</code> </p> <code>parameter</code> <p>The time parameter for evolution operator. For the time-independent case, it represents the actual value for which the evolution will be evaluated. For the time-dependent case, it should be an instance of TimeParameter to signal the solver the variable that will be integrated over.</p> <p> TYPE: <code>TParameter</code> </p> <code>qubit_support</code> <p>The qubits on which the evolution will be performed on. Only required for generators that are not a composition of blocks.</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>(optional) duration of the evolution in case of time-dependent generator. By default, a FeatureParameter with tag \"duration\" will be initialized, and the value will then be required in the values dict.</p> <p> TYPE: <code>TParameter | None</code> DEFAULT: <code>None</code> </p> <code>noise_operators</code> <p>(optional) the list of jump operators to use when using a shrodinger solver, allowing to perform noisy simulations.</p> <p> TYPE: <code>list[AbstractBlock]</code> DEFAULT: <code>list()</code> </p> <p>Examples:</p> <pre><code>from qadence import X, HamEvo, PI, add, run\nfrom qadence import FeatureParameter, TimeParameter\nimport torch\n\nn_qubits = 3\n\n# Hamiltonian as a block composition\nhamiltonian = add(X(i) for i in range(n_qubits))\nhevo = HamEvo(hamiltonian, parameter=torch.rand(2))\nstate = run(hevo)\n\n# Hamiltonian as a random matrix\nhamiltonian = torch.rand(2, 2, dtype=torch.complex128)\nhevo = HamEvo(hamiltonian, parameter=torch.rand(2), qubit_support=(0,))\nstate = run(hevo)\n\n# Time-dependent Hamiltonian\nt = TimeParameter(\"t\")\nhamiltonian = t * add(X(i) for i in range(n_qubits))\nhevo = HamEvo(hamiltonian, parameter=t)\nstate = run(hevo, values = {\"duration\": torch.tensor(1.0)})\n\n# Adding noise operators\nnoise_ops = [X(0)]\nhevo = HamEvo(hamiltonian, parameter=t, noise_operators=noise_ops)\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/operations/ham_evo.py</code> <pre><code>def __init__(\n    self,\n    generator: Union[TGenerator, AbstractBlock],\n    parameter: TParameter,\n    qubit_support: tuple[int, ...] = None,\n    duration: TParameter | None = None,\n    noise_operators: list[AbstractBlock] = list(),\n):\n    params = {}\n    if qubit_support is None and not isinstance(generator, AbstractBlock):\n        raise ValueError(\"You have to supply a qubit support for non-block generators.\")\n    super().__init__(qubit_support if qubit_support else generator.qubit_support)\n    if isinstance(generator, AbstractBlock):\n        qubit_support = generator.qubit_support\n        if generator.is_parametric:\n            params = {str(e): e for e in expressions(generator)}\n        if generator.is_time_dependent:\n            if isinstance(duration, str):\n                duration = Parameter(duration, trainable=False)\n            elif duration is None:\n                duration = Parameter(\"duration\", trainable=False)\n        if not generator.is_time_dependent and duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n    elif isinstance(generator, torch.Tensor):\n        if duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n        msg = \"Please provide a square generator.\"\n        if len(generator.shape) == 2:\n            assert generator.shape[0] == generator.shape[1], msg\n        elif len(generator.shape) == 3:\n            assert generator.shape[1] == generator.shape[2], msg\n            assert generator.shape[0] == 1, \"Qadence doesnt support batched generators.\"\n        else:\n            raise TypeError(\n                \"Only 2D or 3D generators are supported.\\\n                            In case of a 3D generator, the batch dim\\\n                            is expected to be at dim 0.\"\n            )\n        params = {str(generator.__hash__()): generator}\n    elif isinstance(generator, (sympy.Basic, sympy.Array)):\n        if duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n        params = {str(generator): generator}\n    else:\n        raise TypeError(\n            f\"Generator of type {type(generator)} not supported.\\\n                        If you're using a numpy.ndarray, please cast it to a torch tensor.\"\n        )\n    if duration is not None:\n        params = {\"duration\": Parameter(duration), **params}\n    params = {\"parameter\": Parameter(parameter), **params}\n    self.parameters = ParamMap(**params)\n    self.time_param = parameter\n    self.generator = generator\n    self.duration = duration\n\n    if len(noise_operators) &gt; 0:\n        if not all(\n            [\n                len(set(op.qubit_support + self.qubit_support) - set(self.qubit_support)) == 0\n                for op in noise_operators\n            ]\n        ):\n            raise ValueError(\n                \"Noise operators should be defined\"\n                \" over the same or a subset of the qubit support\"\n            )\n        if True in [op.is_parametric for op in noise_operators]:\n            raise ValueError(\"Parametric operators are not supported\")\n    self.noise_operators = noise_operators\n</code></pre>"},{"location":"api/operations/#qadence.operations.HamEvo.digital_decomposition","title":"<code>digital_decomposition(approximation=LTSOrder.ST4)</code>","text":"<p>Decompose the Hamiltonian evolution into digital gates.</p> PARAMETER DESCRIPTION <code>approximation</code> <p>Choose the type of decomposition. Defaults to \"st4\". Available types are: * 'basic' = apply first-order Trotter formula and decompose each term of     the exponential into digital gates. It is exact only if applied to an     operator whose terms are mutually commuting. * 'st2' = Trotter-Suzuki 2nd order formula for approximating non-commuting     Hamiltonians. * 'st4' = Trotter-Suzuki 4th order formula for approximating non-commuting     Hamiltonians.</p> <p> TYPE: <code>str</code> DEFAULT: <code>ST4</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>a block with the digital decomposition</p> <p> TYPE: <code>AbstractBlock</code> </p> Source code in <code>qadence/operations/ham_evo.py</code> <pre><code>def digital_decomposition(self, approximation: LTSOrder = LTSOrder.ST4) -&gt; AbstractBlock:\n    \"\"\"Decompose the Hamiltonian evolution into digital gates.\n\n    Args:\n        approximation (str, optional): Choose the type of decomposition. Defaults to \"st4\".\n            Available types are:\n            * 'basic' = apply first-order Trotter formula and decompose each term of\n                the exponential into digital gates. It is exact only if applied to an\n                operator whose terms are mutually commuting.\n            * 'st2' = Trotter-Suzuki 2nd order formula for approximating non-commuting\n                Hamiltonians.\n            * 'st4' = Trotter-Suzuki 4th order formula for approximating non-commuting\n                Hamiltonians.\n\n    Returns:\n        AbstractBlock: a block with the digital decomposition\n    \"\"\"\n\n    # psi(t) = exp(-i * H * t * psi0)\n    # psi(t) = exp(-i * lambda * t * psi0)\n    # H = sum(Paulin) + sum(Pauli1*Pauli2)\n    logger.info(\"Quantum simulation of the time-independent Schr\u00f6dinger equation.\")\n\n    blocks = []\n\n    # how to change the type/dict to enum effectively\n\n    # when there is a term including non-commuting matrices use st2 or st4\n\n    # 1) should check that the given generator respects the constraints\n    # single-qubit gates\n\n    assert isinstance(\n        self.generator, AbstractBlock\n    ), \"Only a generator represented as a block can be decomposed\"\n\n    if block_is_qubit_hamiltonian(self.generator):\n        try:\n            block_is_commuting_hamiltonian(self.generator)\n            approximation = LTSOrder.BASIC  # use the simpler approach if the H is commuting\n        except TypeError:\n            logger.warning(\n                \"\"\"Non-commuting terms in the Pauli operator.\n                The Suzuki-Trotter approximation is applied.\"\"\"\n            )\n\n        blocks.extend(\n            lie_trotter_suzuki(\n                block=self.generator,\n                parameter=self.parameters.parameter,\n                order=LTSOrder[approximation],\n            )\n        )\n\n        # 2) return an AbstractBlock instance with the set of gates\n        # resulting from the decomposition\n\n        return chain(*blocks)\n    else:\n        raise NotImplementedError(\n            \"The current digital decomposition can be applied only to Pauli Hamiltonians.\"\n        )\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogSWAP","title":"<code>AnalogSWAP(control, target, parameter=3 * PI / 4)</code>","text":"<p>               Bases: <code>HamEvo</code></p> <p>Single time-independent Hamiltonian evolution over a Rydberg Ising.</p> <p>hamiltonian yielding a SWAP (up to global phase).</p> <p>Derived from Bapat et al. where it is applied to XX-type Hamiltonian</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def __init__(self, control: int, target: int, parameter: TParameter = 3 * PI / 4):\n    rydberg_ising_hamiltonian_generator = (\n        4.0 * kron((I(control) - Z(control)) / 2.0, (I(target) - Z(target)) / 2.0)\n        + (2.0 / 3.0) * np.sqrt(2.0) * X(control)\n        + (2.0 / 3.0) * np.sqrt(2.0) * X(target)\n        + (1.0 + np.sqrt(5.0) / 3) * Z(control)\n        + (1.0 + np.sqrt(5.0) / 3) * Z(target)\n    )\n    super().__init__(rydberg_ising_hamiltonian_generator, parameter, (control, target))\n</code></pre>"},{"location":"api/operations/#analog-blocks","title":"Analog blocks","text":""},{"location":"api/operations/#qadence.operations.AnalogRX","title":"<code>AnalogRX(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog X rotation.</p> <p>Shorthand for <code>AnalogRot</code>:</p> <pre><code>\u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\nAnalogRot(duration=t, omega=\u03a9)\n</code></pre> PARAMETER DESCRIPTION <code>angle</code> <p>Rotation angle [rad]</p> <p> TYPE: <code>float | str | Parameter</code> </p> <code>qubit_support</code> <p>Defines the (local/global) qubit support</p> <p> TYPE: <code>str | QubitSupport | Tuple</code> DEFAULT: <code>'global'</code> </p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRX(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog X rotation.\n\n    Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n\n    ```python\n    \u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\n    AnalogRot(duration=t, omega=\u03a9)\n    ```\n\n    Arguments:\n        angle: Rotation angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n    return _analog_rot(angle, qubit_support, phase=0, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRY","title":"<code>AnalogRY(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog Y rotation.</p> <p>Shorthand for <code>AnalogRot</code>:</p> <p><pre><code>\u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\nAnalogRot(duration=t, omega=\u03a9, phase=-\u03c0/2)\n</code></pre> Arguments:     angle: Rotation angle [rad]     qubit_support: Defines the (local/global) qubit support</p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRY(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog Y rotation.\n\n    Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n\n    ```python\n    \u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\n    AnalogRot(duration=t, omega=\u03a9, phase=-\u03c0/2)\n    ```\n    Arguments:\n        angle: Rotation angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n    return _analog_rot(angle, qubit_support, phase=-PI / 2, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRZ","title":"<code>AnalogRZ(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog Z rotation. Shorthand for <code>AnalogRot</code>: <pre><code>\u03c6=2.4; \u03b4=\u03c0; t = \u03c6/\u03b4 * 100)\nAnalogRot(duration=t, delta=\u03b4, phase=\u03c0/2)\n</code></pre></p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRZ(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog Z rotation. Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n    ```\n    \u03c6=2.4; \u03b4=\u03c0; t = \u03c6/\u03b4 * 100)\n    AnalogRot(duration=t, delta=\u03b4, phase=\u03c0/2)\n    ```\n    \"\"\"\n    q = _cast(QubitSupport, qubit_support)\n    alpha = _cast(Parameter, angle)\n    delta = PI\n    omega = 0\n    duration = alpha / delta * 1000\n    h_norm = sympy.sqrt(omega**2 + delta**2)\n    ps = ParamMap(\n        alpha=alpha, duration=duration, omega=omega, delta=delta, phase=0.0, h_norm=h_norm\n    )\n    return ConstantAnalogRotation(qubit_support=q, parameters=ps, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRot","title":"<code>AnalogRot(duration, omega=0, delta=0, phase=0, qubit_support='global', add_pattern=True)</code>","text":"<p>General analog rotation operation.</p> PARAMETER DESCRIPTION <code>duration</code> <p>Duration of the rotation [ns].</p> <p> TYPE: <code>float | str | Parameter</code> </p> <code>omega</code> <p>Rotation frequency [rad/\u03bcs]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>delta</code> <p>Rotation frequency [rad/\u03bcs]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>phase</code> <p>Phase angle [rad]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>qubit_support</code> <p>Defines the (local/global) qubit support</p> <p> TYPE: <code>str | QubitSupport | Tuple</code> DEFAULT: <code>'global'</code> </p> <code>add_pattern</code> <p>False disables the semi-local addressing pattern for the execution of this specific block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRot(\n    duration: float | str | Parameter,\n    omega: float | str | Parameter = 0,\n    delta: float | str | Parameter = 0,\n    phase: float | str | Parameter = 0,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"General analog rotation operation.\n\n    Arguments:\n        duration: Duration of the rotation [ns].\n        omega: Rotation frequency [rad/\u03bcs]\n        delta: Rotation frequency [rad/\u03bcs]\n        phase: Phase angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n        add_pattern: False disables the semi-local addressing pattern\n            for the execution of this specific block.\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n\n    if omega == 0 and delta == 0:\n        raise ValueError(\"Parameters omega and delta cannot both be 0.\")\n\n    q = _cast(QubitSupport, qubit_support)\n    duration = Parameter(duration)\n    omega = Parameter(omega)\n    delta = Parameter(delta)\n    phase = Parameter(phase)\n    h_norm = sympy.sqrt(omega**2 + delta**2)\n    alpha = duration * h_norm / 1000\n    ps = ParamMap(\n        alpha=alpha, duration=duration, omega=omega, delta=delta, phase=phase, h_norm=h_norm\n    )\n    return ConstantAnalogRotation(parameters=ps, qubit_support=q, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogInteraction","title":"<code>AnalogInteraction(duration, qubit_support='global', add_pattern=True)</code>","text":"<p>Evolution of the interaction term for a register of qubits.</p> <p>Constructs a <code>InteractionBlock</code>.</p> PARAMETER DESCRIPTION <code>duration</code> <p>Time to evolve the interaction for in nanoseconds.</p> <p> TYPE: <code>TNumber | Basic</code> </p> <code>qubit_support</code> <p>Qubits the <code>InteractionBlock</code> is applied to. Can be either <code>\"global\"</code> to evolve the interaction block to all qubits or a tuple of integers.</p> <p> TYPE: <code>str | QubitSupport | tuple</code> DEFAULT: <code>'global'</code> </p> <code>add_pattern</code> <p>False disables the semi-local addressing pattern for the execution of this specific block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>InteractionBlock</code> <p>a <code>InteractionBlock</code></p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogInteraction(\n    duration: TNumber | sympy.Basic,\n    qubit_support: str | QubitSupport | tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; InteractionBlock:\n    \"\"\"Evolution of the interaction term for a register of qubits.\n\n    Constructs a [`InteractionBlock`][qadence.blocks.analog.InteractionBlock].\n\n    Arguments:\n        duration: Time to evolve the interaction for in nanoseconds.\n        qubit_support: Qubits the `InteractionBlock` is applied to. Can be either\n            `\"global\"` to evolve the interaction block to all qubits or a tuple of integers.\n        add_pattern: False disables the semi-local addressing pattern\n            for the execution of this specific block.\n\n    Returns:\n        a `InteractionBlock`\n    \"\"\"\n    q = _cast(QubitSupport, qubit_support)\n    ps = ParamMap(duration=duration)\n    return InteractionBlock(parameters=ps, qubit_support=q, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/parameters/","title":"Parameters","text":""},{"location":"api/parameters/#parameters","title":"Parameters","text":""},{"location":"api/parameters/#qadence.parameters.ParamMap","title":"<code>ParamMap(**kwargs)</code>","text":"<p>Connects UUIDs of parameters to their expressions and names.</p> <p>This class is not user-facing and only needed for more complex block definitions. It provides convenient access to expressions/UUIDs/names needed in different backends.</p> PARAMETER DESCRIPTION <code>kwargs</code> <p>Parameters.</p> <p> TYPE: <code>str | TNumber | Tensor | Basic | Parameter</code> DEFAULT: <code>{}</code> </p> <p>Example: <pre><code>import sympy\nfrom qadence.parameters import ParamMap\n\n(x,y) = sympy.symbols(\"x y\")\nps = ParamMap(omega=2.0, duration=x+y)\n\nprint(f\"{ps.names() = }\")\nprint(f\"{ps.expressions() = }\")\nprint(f\"{ps.uuids() = }\")\n</code></pre> <pre><code>ps.names() = dict_keys(['omega', 'duration'])\nps.expressions() = dict_values([2.00000000000000, x + y])\nps.uuids() = dict_keys(['9ee25a1e-10d3-4aba-b2c9-6c2d9a25ca5f', '11b21646-07a2-4e01-a681-dff019ab1fba'])\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def __init__(self, **kwargs: str | TNumber | Tensor | Basic | Parameter):\n    self._name_dict: dict[str, tuple[str, Basic]] = {}\n    self._uuid_dict: dict[str, str] = {}\n    for name, v in kwargs.items():\n        param = v if isinstance(v, sympy.Basic) else Parameter(v)\n        uuid = str(uuid4())\n        self._name_dict[name] = (uuid, param)\n        self._uuid_dict[uuid] = param\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.Parameter","title":"<code>Parameter</code>","text":"<p>               Bases: <code>Symbol</code></p> <p>A wrapper on top of <code>sympy.Symbol</code>.</p> <p>Includes two additional keywords: <code>trainable</code> and <code>value</code>. This class is to define both feature parameter and variational parameters.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.trainable","title":"<code>trainable: bool</code>  <code>instance-attribute</code>","text":"<p>Trainable parameters are variational parameters.</p> <p>Non-trainable parameters are feature parameters.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.value","title":"<code>value: TNumber</code>  <code>instance-attribute</code>","text":"<p>(Initial) value of the parameter.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.__new__","title":"<code>__new__(name, **assumptions)</code>","text":"<p>Arguments:</p> <pre><code>name: When given a string only, the class\n    constructs a trainable Parameter with a a randomly initialized value.\n**assumptions: are passed on to the parent class `sympy.Symbol`. Two new assumption\n    kwargs are supported by this constructor: `trainable: bool`, and `value: TNumber`.\n</code></pre> <p>Example: <pre><code>from qadence.parameters import Parameter, VariationalParameter\n\ntheta = Parameter(\"theta\")\nprint(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\nassert not theta.is_number\n\n# you can specify both trainable/value in the constructor\ntheta = Parameter(\"theta\", trainable=True, value=2.0)\nprint(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n\n# VariationalParameter/FeatureParameter are constructing\n# trainable/untrainable Parameters\ntheta = VariationalParameter(\"theta\", value=2.0)\nassert theta == Parameter(\"theta\", trainable=True, value=2.0)\n\n# When provided with a numeric type, Parameter constructs a sympy numeric type\":\nconstant_zero = Parameter(0)\nassert constant_zero.is_number\n\n# When passed a Parameter or a sympy expression, it just returns it.\nexpr = Parameter(\"x\") * Parameter(\"y\")\nprint(f\"{expr=} : {expr.free_symbols}\")\n</code></pre> <pre><code>theta: trainable=True value=0.4070144804034517\ntheta: trainable=True value=2.0\nexpr=x*y : {y, x}\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def __new__(\n    cls, name: str | TNumber | Tensor | Basic | Parameter, **assumptions: Any\n) -&gt; Parameter | Basic | Expr | Array:\n    \"\"\"\n    Arguments:\n\n        name: When given a string only, the class\n            constructs a trainable Parameter with a a randomly initialized value.\n        **assumptions: are passed on to the parent class `sympy.Symbol`. Two new assumption\n            kwargs are supported by this constructor: `trainable: bool`, and `value: TNumber`.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.parameters import Parameter, VariationalParameter\n\n    theta = Parameter(\"theta\")\n    print(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n    assert not theta.is_number\n\n    # you can specify both trainable/value in the constructor\n    theta = Parameter(\"theta\", trainable=True, value=2.0)\n    print(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n\n    # VariationalParameter/FeatureParameter are constructing\n    # trainable/untrainable Parameters\n    theta = VariationalParameter(\"theta\", value=2.0)\n    assert theta == Parameter(\"theta\", trainable=True, value=2.0)\n\n    # When provided with a numeric type, Parameter constructs a sympy numeric type\":\n    constant_zero = Parameter(0)\n    assert constant_zero.is_number\n\n    # When passed a Parameter or a sympy expression, it just returns it.\n    expr = Parameter(\"x\") * Parameter(\"y\")\n    print(f\"{expr=} : {expr.free_symbols}\")\n    ```\n    \"\"\"\n    p: Parameter\n    if isinstance(name, get_args(TNumber)):\n        return sympify(name)\n    elif isinstance(name, Tensor):\n        if name.numel() == 1:\n            return sympify(name)\n        else:\n            return Array(name.detach().numpy())\n    elif isinstance(name, Parameter):\n        p = super().__new__(cls, name.name, **assumptions)\n        p.name = name.name\n        p.trainable = name.trainable\n        p.value = name.value\n        p.is_time = name.is_time\n        return p\n    elif isinstance(name, (Basic, Expr)):\n        if name.is_number:\n            return sympify(evaluate(name))\n        return name\n    elif isinstance(name, str):\n        p = super().__new__(cls, name, **assumptions)\n        p.trainable = assumptions.get(\"trainable\", True)\n        p.value = assumptions.get(\"value\", None)\n        p.is_time = assumptions.get(\"is_time\", False)\n        if p.value is None:\n            p.value = rand(1).item()\n        return p\n    else:\n        raise TypeError(f\"Parameter does not support type {type(name)}\")\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.FeatureParameter","title":"<code>FeatureParameter(name, **kwargs)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=False)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def FeatureParameter(name: str, **kwargs: Any) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=False)`.\"\"\"\n    return Parameter(name, trainable=False, **kwargs)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.TimeParameter","title":"<code>TimeParameter(name)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=False, is_time=True)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def TimeParameter(name: str) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=False, is_time=True)`.\"\"\"\n    return Parameter(name, trainable=False, is_time=True)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.VariationalParameter","title":"<code>VariationalParameter(name, **kwargs)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=True)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def VariationalParameter(name: str, **kwargs: Any) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=True)`.\"\"\"\n    return Parameter(name, trainable=True, **kwargs)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.evaluate","title":"<code>evaluate(expr, values=None, as_torch=False)</code>","text":"<p>Arguments:</p> <pre><code>expr: An expression consisting of Parameters.\nvalues: values dict which contains values for the Parameters,\n    if empty, Parameter.value will be used.\nas_torch: Whether to retrieve a torch-differentiable expression result.\n</code></pre> <p>Example: <pre><code>from qadence.parameters import Parameter, evaluate\n\nexpr = Parameter(\"x\") * Parameter(\"y\")\n\n# Unless specified, Parameter initialized random values\n# Lets evaluate this expression and see what the result is\nres = evaluate(expr)\nprint(res)\n\n# We can also evaluate the expr using a custom dict\nd = {\"x\": 1, \"y\":2}\nres = evaluate(expr, d)\nprint(res)\n\n# Lastly, if we want a differentiable result, lets put the as_torch flag\nres = evaluate(expr, d, as_torch=True)\nprint(res)\n</code></pre> <pre><code>0.17848995456684127\n2.0\ntensor([2])\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def evaluate(expr: Expr, values: dict | None = None, as_torch: bool = False) -&gt; TNumber | Tensor:\n    \"\"\"\n    Arguments:\n\n        expr: An expression consisting of Parameters.\n        values: values dict which contains values for the Parameters,\n            if empty, Parameter.value will be used.\n        as_torch: Whether to retrieve a torch-differentiable expression result.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.parameters import Parameter, evaluate\n\n    expr = Parameter(\"x\") * Parameter(\"y\")\n\n    # Unless specified, Parameter initialized random values\n    # Lets evaluate this expression and see what the result is\n    res = evaluate(expr)\n    print(res)\n\n    # We can also evaluate the expr using a custom dict\n    d = {\"x\": 1, \"y\":2}\n    res = evaluate(expr, d)\n    print(res)\n\n    # Lastly, if we want a differentiable result, lets put the as_torch flag\n    res = evaluate(expr, d, as_torch=True)\n    print(res)\n    ```\n    \"\"\"\n    res: Basic\n    res_value: TNumber | Tensor\n    query: dict[Parameter, TNumber | Tensor] = dict()\n    values = values or dict()\n    if isinstance(expr, Array):\n        return Tensor(expr.tolist())\n    else:\n        if not expr.is_number:\n            for s in expr.free_symbols:\n                if s.name in values.keys():\n                    query[s] = values[s.name]\n                elif hasattr(s, \"value\"):\n                    query[s] = s.value\n                else:\n                    raise ValueError(f\"No value provided for symbol {s.name}\")\n        if as_torch:\n            res_value = make_differentiable(expr)(**{s.name: tensor(v) for s, v in query.items()})\n        else:\n            res = expr.subs(query)\n            res_value = sympy_to_numeric(res)\n        return res_value\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.extract_original_param_entry","title":"<code>extract_original_param_entry(param)</code>","text":"<p>Given an Expression, what was the original \"param\" given by the user? It is either.</p> <p>going to be a numeric value, or a sympy Expression (in case a string was given, it was converted via Parameter(\"string\").</p> Source code in <code>qadence/parameters.py</code> <pre><code>def extract_original_param_entry(\n    param: Expr,\n) -&gt; TNumber | Tensor | Expr:\n    \"\"\"\n    Given an Expression, what was the original \"param\" given by the user? It is either.\n\n    going to be a numeric value, or a sympy Expression (in case a string was given,\n    it was converted via Parameter(\"string\").\n    \"\"\"\n    return param if not param.is_number else evaluate(param)\n</code></pre>"},{"location":"api/parameters/#parameter-embedding","title":"Parameter embedding","text":""},{"location":"api/parameters/#qadence.blocks.embedding.embedding","title":"<code>embedding(block, to_gate_params=False, engine=Engine.TORCH)</code>","text":"<p>Construct embedding function which maps user-facing parameters to either expression-level.</p> <p>parameters or gate-level parameters. The constructed embedding function has the signature:</p> <pre><code> embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n</code></pre> <p>which means that it maps the variational parameter dict <code>params</code> and the feature parameter dict <code>inputs</code> to one new parameter dict <code>embedded_dict</code> which holds all parameters that are needed to execute a circuit on a given backend. There are two different modes for this mapping:</p> <ul> <li>Expression-level parameters: For AD-based optimization. For every unique expression we end   up with one entry in the embedded dict:   <code>len(embedded_dict) == len(unique_parameter_expressions)</code>.</li> <li>Gate-level parameters: For PSR-based optimization or real devices. One parameter for each   gate parameter, regardless if they are based on the same expression. <code>len(embedded_dict) ==   len(parametric_gates)</code>. This is needed because PSR requires to shift the angles of every   gate where the same parameter appears.</li> </ul> PARAMETER DESCRIPTION <code>block</code> <p>parametrized block into which we want to embed parameters.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>to_gate_params</code> <p>A boolean flag whether to generate gate-level parameters or expression-level parameters.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[ParamDictType, Callable[[ParamDictType, ParamDictType], ParamDictType]]</code> <p>A tuple with variational parameter dict and the embedding function.</p> Source code in <code>qadence/blocks/embedding.py</code> <pre><code>def embedding(\n    block: AbstractBlock, to_gate_params: bool = False, engine: Engine = Engine.TORCH\n) -&gt; tuple[ParamDictType, Callable[[ParamDictType, ParamDictType], ParamDictType],]:\n    \"\"\"Construct embedding function which maps user-facing parameters to either *expression-level*.\n\n    parameters or *gate-level* parameters. The constructed embedding function has the signature:\n\n         embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n\n    which means that it maps the *variational* parameter dict `params` and the *feature* parameter\n    dict `inputs` to one new parameter dict `embedded_dict` which holds all parameters that are\n    needed to execute a circuit on a given backend. There are two different *modes* for this\n    mapping:\n\n    - *Expression-level* parameters: For AD-based optimization. For every unique expression we end\n      up with one entry in the embedded dict:\n      `len(embedded_dict) == len(unique_parameter_expressions)`.\n    - *Gate-level* parameters: For PSR-based optimization or real devices. One parameter for each\n      gate parameter, regardless if they are based on the same expression. `len(embedded_dict) ==\n      len(parametric_gates)`. This is needed because PSR requires to shift the angles of **every**\n      gate where the same parameter appears.\n\n    Arguments:\n        block: parametrized block into which we want to embed parameters.\n        to_gate_params: A boolean flag whether to generate gate-level parameters or\n            expression-level parameters.\n\n    Returns:\n        A tuple with variational parameter dict and the embedding function.\n    \"\"\"\n    concretize_parameter = _concretize_parameter(engine)\n    if engine == Engine.TORCH:\n        cast_dtype = tensor\n    else:\n        from jax.numpy import array\n\n        cast_dtype = array\n\n    unique_expressions = unique(expressions(block))\n    unique_symbols = [p for p in unique(parameters(block)) if not isinstance(p, sympy.Array)]\n    unique_const_matrices = [e for e in unique_expressions if isinstance(e, sympy.Array)]\n    unique_expressions = [e for e in unique_expressions if not isinstance(e, sympy.Array)]\n\n    # NOTE\n    # there are 3 kinds of parameters in qadence\n    # - non-trainable which are considered as inputs for classical data\n    # - trainable which are the variational parameters to be optimized\n    # - fixed: which are non-trainable parameters with fixed value (e.g. pi/2)\n    #\n    # both non-trainable and trainable parameters can have the same element applied\n    # to different operations in the quantum circuit, e.g. assigning the same parameter\n    # to multiple gates.\n    non_numeric_symbols = [p for p in unique_symbols if not p.is_number]\n    trainable_symbols = [p for p in non_numeric_symbols if p.trainable]\n    constant_expressions = [expr for expr in unique_expressions if expr.is_number]\n    # we dont need to care about constant symbols if they are contained in an symbolic expression\n    # we only care about gate params which are ONLY a constant\n\n    embeddings: dict[sympy.Expr, DifferentiableExpression] = {\n        expr: make_differentiable(expr=expr, engine=engine)\n        for expr in unique_expressions\n        if not expr.is_number\n    }\n\n    uuid_to_expr = uuid_to_expression(block)\n\n    def embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n        embedded_params: dict[sympy.Expr, ArrayLike] = {}\n        for expr, fn in embeddings.items():\n            angle: ArrayLike\n            values = {}\n            for symbol in expr.free_symbols:\n                if symbol.name in inputs:\n                    value = inputs[symbol.name]\n                elif symbol.name in params:\n                    value = params[symbol.name]\n                else:\n                    if symbol.is_time:\n                        value = tensor(1.0)\n                    else:\n                        msg_trainable = \"Trainable\" if symbol.trainable else \"Non-trainable\"\n                        raise KeyError(\n                            f\"{msg_trainable} parameter '{symbol.name}' not found in the \"\n                            f\"inputs list: {list(inputs.keys())} nor the \"\n                            f\"params list: {list(params.keys())}.\"\n                        )\n                values[symbol.name] = value\n            angle = fn(**values)\n            # do not reshape parameters which are multi-dimensional\n            # tensors, such as for example generator matrices\n            if not len(angle.squeeze().shape) &gt; 1:\n                angle = angle.reshape(-1)\n            embedded_params[expr] = angle\n\n        for e in constant_expressions + unique_const_matrices:\n            embedded_params[e] = params[stringify(e)]\n\n        if to_gate_params:\n            gate_lvl_params: ParamDictType = {}\n            for uuid, e in uuid_to_expr.items():\n                gate_lvl_params[uuid] = embedded_params[e]\n            return gate_lvl_params\n        else:\n            embedded_params.update(inputs)\n            for k, v in params.items():\n                if k not in embedded_params:\n                    embedded_params[k] = v\n            out = {\n                stringify(k)\n                if not isinstance(k, str)\n                else k: as_tensor(v)[None]\n                if as_tensor(v).ndim == 0\n                else v\n                for k, v in embedded_params.items()\n            }\n            return out\n\n    params: ParamDictType\n    params = {\n        p.name: concretize_parameter(value=p.value, trainable=True) for p in trainable_symbols\n    }\n    params.update(\n        {\n            stringify(expr): concretize_parameter(value=evaluate(expr), trainable=False)\n            for expr in constant_expressions\n        }\n    )\n    params.update(\n        {\n            stringify(expr): cast_dtype(nparray(expr.tolist(), dtype=npcdouble))\n            for expr in unique_const_matrices\n        }\n    )\n    return params, embedding_fn\n</code></pre>"},{"location":"api/quantumcircuit/","title":"QuantumCircuit","text":""},{"location":"api/quantumcircuit/#quantumcircuit","title":"QuantumCircuit","text":"<p>The abstract <code>QuantumCircuit</code> is the key object in Qadence, as it is what can be executed.</p>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit","title":"<code>QuantumCircuit(support, *blocks)</code>  <code>dataclass</code>","text":"<p>Am abstract QuantumCircuit instance.</p> <p>It needs to be passed to a quantum backend for execution.</p> <p>Arguments:</p> <pre><code>support: `Register` or number of qubits. If an integer is provided, a register is\n    constructed with `Register.all_to_all(x)`\n*blocks: (Possibly multiple) blocks to construct the circuit from.\n</code></pre> Source code in <code>qadence/circuit.py</code> <pre><code>def __init__(self, support: int | Register, *blocks: AbstractBlock):\n    \"\"\"\n    Arguments:\n\n        support: `Register` or number of qubits. If an integer is provided, a register is\n            constructed with `Register.all_to_all(x)`\n        *blocks: (Possibly multiple) blocks to construct the circuit from.\n    \"\"\"\n    self.block = chain(*blocks) if len(blocks) != 1 else blocks[0]\n    self.register = Register(support) if isinstance(support, int) else support\n\n    global_block = isinstance(self.block, AnalogBlock) and self.block.qubit_support.is_global\n    if not global_block and len(self.block) and self.block.n_qubits &gt; self.register.n_qubits:\n        raise ValueError(\n            f\"Register with {self.register.n_qubits} qubits is too small for the \"\n            f\"given block with {self.block.n_qubits} qubits\"\n        )\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.unique_parameters","title":"<code>unique_parameters: list[Parameter]</code>  <code>property</code>","text":"<p>Return the unique parameters in the circuit.</p> <p>These parameters are the actual user-facing parameters which can be assigned by the user. Multiple gates can contain the same unique parameter</p> RETURNS DESCRIPTION <code>list[Parameter]</code> <p>list[Parameter]: List of unique parameters in the circuit</p>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.dagger","title":"<code>dagger()</code>","text":"<p>Reverse the QuantumCircuit by calling dagger on the block.</p> Source code in <code>qadence/circuit.py</code> <pre><code>def dagger(self) -&gt; QuantumCircuit:\n    \"\"\"Reverse the QuantumCircuit by calling dagger on the block.\"\"\"\n    return QuantumCircuit(self.n_qubits, self.block.dagger())\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.get_blocks_by_tag","title":"<code>get_blocks_by_tag(tag)</code>","text":"<p>Extract one or more blocks using the human-readable tag.</p> <p>This function recursively explores all composite blocks to find all the occurrences of a certain tag in the blocks.</p> PARAMETER DESCRIPTION <code>tag</code> <p>the tag to look for</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>list[AbstractBlock]: The block(s) corresponding to the given tag</p> Source code in <code>qadence/circuit.py</code> <pre><code>def get_blocks_by_tag(self, tag: str) -&gt; list[AbstractBlock]:\n    \"\"\"Extract one or more blocks using the human-readable tag.\n\n    This function recursively explores all composite blocks to find\n    all the occurrences of a certain tag in the blocks.\n\n    Args:\n        tag (str): the tag to look for\n\n    Returns:\n        list[AbstractBlock]: The block(s) corresponding to the given tag\n    \"\"\"\n\n    def _get_block(block: AbstractBlock) -&gt; list[AbstractBlock]:\n        blocks = []\n        if block.tag == tag:\n            blocks += [block]\n        if isinstance(block, CompositeBlock):\n            blocks += flatten(*[_get_block(b) for b in block.blocks])\n        return blocks\n\n    return _get_block(self.block)\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.parameters","title":"<code>parameters()</code>","text":"<p>Extract all parameters for primitive blocks in the circuit.</p> <p>Notice that this function returns all the unique Parameters used in the quantum circuit. These can correspond to constants too.</p> RETURNS DESCRIPTION <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>List[tuple[Parameter]]: A list of tuples containing the Parameter</p> <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>instance of each of the primitive blocks in the circuit or, if the <code>flatten</code></p> <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>flag is set to True, a flattened list of all circuit parameters</p> Source code in <code>qadence/circuit.py</code> <pre><code>def parameters(self) -&gt; list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]:\n    \"\"\"Extract all parameters for primitive blocks in the circuit.\n\n    Notice that this function returns all the unique Parameters used\n    in the quantum circuit. These can correspond to constants too.\n\n    Returns:\n        List[tuple[Parameter]]: A list of tuples containing the Parameter\n        instance of each of the primitive blocks in the circuit or, if the `flatten`\n        flag is set to True, a flattened list of all circuit parameters\n    \"\"\"\n    return parameters(self.block)\n</code></pre>"},{"location":"api/register/","title":"Register","text":""},{"location":"api/register/#quantum-registers","title":"Quantum Registers","text":""},{"location":"api/register/#qadence.register.Register","title":"<code>Register(support, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>","text":"<p>A register of qubits including 2D coordinates.</p> <p>Instantiating the Register class directly is only recommended for building custom registers. For most uses where a predefined lattice is desired it is recommended to use the various class methods available, e.g. <code>Register.triangular_lattice</code>.</p> PARAMETER DESCRIPTION <code>support</code> <p>A NetworkX graph or number of qubits. Nodes can include a <code>\"pos\"</code> attribute such that e.g.: <code>graph.nodes = {0: {\"pos\": (2,3)}, 1: {\"pos\": (0,0)}, ...}</code> which will be used in backends that need qubit coordinates. Passing a number of qubits calls <code>Register.all_to_all(n_qubits)</code>.</p> <p> TYPE: <code>Graph | int</code> </p> <code>spacing</code> <p>Value set as the distance between the two closest qubits. The spacing argument is also available for all the class method constructors.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>1.0</code> </p> <p>Examples: <pre><code>from qadence import Register\n\nreg_all = Register.all_to_all(n_qubits = 4)\nreg_line = Register.line(n_qubits = 4)\nreg_circle = Register.circle(n_qubits = 4)\nreg_squre = Register.square(qubits_side = 2)\nreg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\nreg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\nreg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n</code></pre> </p> Source code in <code>qadence/register.py</code> <pre><code>def __init__(\n    self,\n    support: nx.Graph | int,\n    spacing: float | None = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n):\n    \"\"\"\n    A register of qubits including 2D coordinates.\n\n    Instantiating the Register class directly is only recommended for building custom registers.\n    For most uses where a predefined lattice is desired it is recommended to use the various\n    class methods available, e.g. `Register.triangular_lattice`.\n\n    Arguments:\n        support: A NetworkX graph or number of qubits. Nodes can include a `\"pos\"` attribute\n            such that e.g.: `graph.nodes = {0: {\"pos\": (2,3)}, 1: {\"pos\": (0,0)}, ...}` which\n            will be used in backends that need qubit coordinates. Passing a number of qubits\n            calls `Register.all_to_all(n_qubits)`.\n        spacing: Value set as the distance between the two closest qubits. The spacing\n            argument is also available for all the class method constructors.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\"\n    from qadence import Register\n\n    reg_all = Register.all_to_all(n_qubits = 4)\n    reg_line = Register.line(n_qubits = 4)\n    reg_circle = Register.circle(n_qubits = 4)\n    reg_squre = Register.square(qubits_side = 2)\n    reg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\n    reg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\n    reg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n    ```\n    \"\"\"\n    if device_specs is not None and not isinstance(device_specs, RydbergDevice):\n        raise ValueError(\"Device specs are not valid. Please pass a `RydbergDevice` instance.\")\n\n    self.device_specs = device_specs\n\n    self.graph = support if isinstance(support, nx.Graph) else alltoall_graph(support)\n\n    if spacing is not None and self.min_distance != 0.0:\n        _scale_node_positions(self.graph, self.min_distance, spacing)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.all_node_pairs","title":"<code>all_node_pairs: EdgeView</code>  <code>property</code>","text":"<p>Return a list of all possible qubit pairs in the register.</p>"},{"location":"api/register/#qadence.register.Register.coords","title":"<code>coords: dict</code>  <code>property</code>","text":"<p>Return the dictionary of qubit coordinates.</p>"},{"location":"api/register/#qadence.register.Register.distances","title":"<code>distances: dict</code>  <code>property</code>","text":"<p>Return a dictionary of distances for all qubit pairs in the register.</p>"},{"location":"api/register/#qadence.register.Register.edge_distances","title":"<code>edge_distances: dict</code>  <code>property</code>","text":"<p>Return a dictionary of distances for the qubit pairs that are.</p> <p>connected by an edge in the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.edges","title":"<code>edges: EdgeView</code>  <code>property</code>","text":"<p>Return the EdgeView of the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.min_distance","title":"<code>min_distance: float</code>  <code>property</code>","text":"<p>Return the minimum distance between two qubts in the register.</p>"},{"location":"api/register/#qadence.register.Register.n_qubits","title":"<code>n_qubits: int</code>  <code>property</code>","text":"<p>Total number of qubits in the register.</p>"},{"location":"api/register/#qadence.register.Register.nodes","title":"<code>nodes: NodeView</code>  <code>property</code>","text":"<p>Return the NodeView of the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.support","title":"<code>support: set</code>  <code>property</code>","text":"<p>Return the set of qubits in the register.</p>"},{"location":"api/register/#qadence.register.Register.all_to_all","title":"<code>all_to_all(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a register with an all-to-all connectivity graph.</p> <p>The graph is projected onto a 2D space and the qubit coordinates are set using a spring layout algorithm.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef all_to_all(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a register with an all-to-all connectivity graph.\n\n    The graph is projected\n    onto a 2D space and the qubit coordinates are set using a spring layout algorithm.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    return cls(alltoall_graph(n_qubits), spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.circle","title":"<code>circle(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a circle register.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef circle(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a circle register.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    graph = nx.grid_2d_graph(n_qubits, 1, periodic=True)\n    graph = nx.relabel_nodes(graph, {(i, 0): i for i in range(n_qubits)})\n    coords = nx.circular_layout(graph)\n    values = {i: {\"pos\": pos} for i, pos in coords.items()}\n    nx.set_node_attributes(graph, values)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.draw","title":"<code>draw(show=True)</code>","text":"<p>Draw the underlying NetworkX graph representing the register.</p> Source code in <code>qadence/register.py</code> <pre><code>def draw(self, show: bool = True) -&gt; None:\n    \"\"\"Draw the underlying NetworkX graph representing the register.\"\"\"\n    coords = {i: n[\"pos\"] for i, n in self.graph.nodes.items()}\n    nx.draw(self.graph, with_labels=True, pos=coords)\n    if show:\n        plt.gcf().show()\n</code></pre>"},{"location":"api/register/#qadence.register.Register.from_coordinates","title":"<code>from_coordinates(coords, lattice=LatticeTopology.ARBITRARY, spacing=None, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a register from a list of qubit coordinates.</p> <p>Each node is added to the underlying graph with the respective coordinates, but the edges are left empty.</p> PARAMETER DESCRIPTION <code>coords</code> <p>List of qubit coordinate tuples.</p> <p> TYPE: <code>list[tuple]</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef from_coordinates(\n    cls,\n    coords: list[tuple],\n    lattice: LatticeTopology | str = LatticeTopology.ARBITRARY,\n    spacing: float | None = None,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a register from a list of qubit coordinates.\n\n    Each node is added to the underlying\n    graph with the respective coordinates, but the edges are left empty.\n\n    Arguments:\n        coords: List of qubit coordinate tuples.\n    \"\"\"\n    graph = nx.Graph()\n    for i, pos in enumerate(coords):\n        graph.add_node(i, pos=pos)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.honeycomb_lattice","title":"<code>honeycomb_lattice(n_cells_row, n_cells_col, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a honeycomb lattice register.</p> <p>Each cell is an hexagon made up of six qubits.</p> PARAMETER DESCRIPTION <code>n_cells_row</code> <p>Number of cells in each row.</p> <p> TYPE: <code>int</code> </p> <code>n_cells_col</code> <p>Number of cells in each column.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef honeycomb_lattice(\n    cls,\n    n_cells_row: int,\n    n_cells_col: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a honeycomb lattice register.\n\n    Each cell is an hexagon made up of six qubits.\n\n    Arguments:\n        n_cells_row: Number of cells in each row.\n        n_cells_col: Number of cells in each column.\n    \"\"\"\n    graph = nx.hexagonal_lattice_graph(n_cells_row, n_cells_col)\n    graph = nx.relabel_nodes(graph, {(i, j): k for k, (i, j) in enumerate(graph.nodes)})\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.line","title":"<code>line(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a line register.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef line(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a line register.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    return cls(line_graph(n_qubits), spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.rescale_coords","title":"<code>rescale_coords(scaling)</code>","text":"<p>Rescale the coordinates of all qubits in the register.</p> PARAMETER DESCRIPTION <code>scaling</code> <p>Scaling value.</p> <p> TYPE: <code>float</code> </p> Source code in <code>qadence/register.py</code> <pre><code>def rescale_coords(self, scaling: float) -&gt; Register:\n    \"\"\"\n    Rescale the coordinates of all qubits in the register.\n\n    Arguments:\n        scaling: Scaling value.\n    \"\"\"\n    g = deepcopy(self.graph)\n    _scale_node_positions(g, min_distance=1.0, spacing=scaling)\n    return Register(g, spacing=None, device_specs=self.device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.square","title":"<code>square(qubits_side, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a square register.</p> PARAMETER DESCRIPTION <code>qubits_side</code> <p>Number of qubits on one side of the square.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef square(\n    cls,\n    qubits_side: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a square register.\n\n    Arguments:\n        qubits_side: Number of qubits on one side of the square.\n    \"\"\"\n    n_points = 4 * (qubits_side - 1)\n\n    def gen_points() -&gt; np.ndarray:\n        rotate_left = np.array([[0.0, -1.0], [1.0, 0.0]])\n        increment = np.array([0.0, 1.0])\n\n        points = [np.array([0.0, 0.0])]\n        counter = 1\n        while len(points) &lt; n_points:\n            points.append(points[-1] + increment)\n\n            counter = (counter + 1) % qubits_side\n            if counter == 0:\n                increment = rotate_left.dot(increment)\n                counter = 1\n        points = np.array(points)  # type: ignore[assignment]\n        points -= np.mean(points, axis=0)\n\n        return points  # type: ignore[return-value]\n\n    graph = nx.grid_2d_graph(n_points, 1, periodic=True)\n    graph = nx.relabel_nodes(graph, {(i, 0): i for i in range(n_points)})\n    values = {i: {\"pos\": point} for i, point in zip(graph.nodes, gen_points())}\n    nx.set_node_attributes(graph, values)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.triangular_lattice","title":"<code>triangular_lattice(n_cells_row, n_cells_col, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a triangular lattice register.</p> <p>Each cell is a triangle made up of three qubits.</p> PARAMETER DESCRIPTION <code>n_cells_row</code> <p>Number of cells in each row.</p> <p> TYPE: <code>int</code> </p> <code>n_cells_col</code> <p>Number of cells in each column.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef triangular_lattice(\n    cls,\n    n_cells_row: int,\n    n_cells_col: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a triangular lattice register.\n\n    Each cell is a triangle made up of three qubits.\n\n    Arguments:\n        n_cells_row: Number of cells in each row.\n        n_cells_col: Number of cells in each column.\n    \"\"\"\n    return cls(triangular_lattice_graph(n_cells_row, n_cells_col), spacing, device_specs)\n</code></pre>"},{"location":"api/serialization/","title":"Serialization","text":""},{"location":"api/serialization/#serialization","title":"Serialization","text":""},{"location":"api/serialization/#qadence.serialization.SerializationModel","title":"<code>SerializationModel(d=dict())</code>  <code>dataclass</code>","text":"<p>A serialization model class to serialize data from <code>QuantumModel</code>s,.</p> <p><code>torch.nn.Module</code> and similar structures. The data included in the serialization logic includes: the <code>AbstractBlock</code> and its children classes, <code>QuantumCircuit</code>, <code>Register</code>, and <code>sympy</code> expressions (including <code>Parameter</code> class from <code>qadence.parameters</code>).</p> <p>A children class must define the <code>value</code> attribute type and how to handle it, since it is the main property for the class to be used by the serialization process. For instance:</p> <pre><code>@dataclass\nclass QuantumCircuitSerialization(SerializationModel):\n    value: QuantumCircuit = dataclass_field(init=False)\n\n    def __post_init__(self) -&gt; None:\n        self.value = (\n            QuantumCircuit._from_dict(self.d)\n            if isinstance(self.d, dict)\n            else self.d\n        )\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.deserialize","title":"<code>deserialize(d, as_torch=False)</code>","text":"<p>Supported Types:</p> <p>AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module Deserializes a dict to one of the supported types.</p> PARAMETER DESCRIPTION <code>d</code> <p>A dict containing a serialized object.</p> <p> TYPE: <code>dict</code> </p> <code>as_torch</code> <p>Whether to transform to torch for the deserialized object.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     AbstractBlock, QuantumCircuit, QuantumModel, Register, torch.nn.Module.</p> <p>Examples: <pre><code>import torch\nfrom qadence import serialize, deserialize, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nblock_dict = serialize(myblock)\nprint(block_dict)\n\n## Lets use myblock in a QuantumCircuit and serialize it.\n\nqc = QuantumCircuit(n_qubits, myblock)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n## Finally, let's wrap it in a QuantumModel\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n# Lets check if the loaded QuantumModel returns the same expectation\nassert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n</code></pre> <pre><code>{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': 'HEA', 'blocks': [{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('533dbc7b-7a8e-4acf-b10f-91213474124d', {'name': 'theta_0', 'expression': \"Parameter('theta_0')\", 'symbols': {'theta_0': {'name': 'theta_0', 'trainable': 'True', 'value': '0.6084912281332678'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('fa1b0d60-19d3-49ea-8267-64432a986f84', {'name': 'theta_1', 'expression': \"Parameter('theta_1')\", 'symbols': {'theta_1': {'name': 'theta_1', 'trainable': 'True', 'value': '0.8684500387338262'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RY', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('81b83019-5222-44ae-a4ab-560e3999ef5b', {'name': 'theta_2', 'expression': \"Parameter('theta_2')\", 'symbols': {'theta_2': {'name': 'theta_2', 'trainable': 'True', 'value': '0.292065869419318'}}})}}, 'noise': None}, {'type': 'RY', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('30623e79-d68b-4420-9209-5419dc386e18', {'name': 'theta_3', 'expression': \"Parameter('theta_3')\", 'symbols': {'theta_3': {'name': 'theta_3', 'trainable': 'True', 'value': '0.9227456102096557'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('5b1e97b4-e0b5-48d0-bbb8-ad64a8160d7a', {'name': 'theta_4', 'expression': \"Parameter('theta_4')\", 'symbols': {'theta_4': {'name': 'theta_4', 'trainable': 'True', 'value': '0.5292873285428984'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('02362c6d-88a9-4269-8439-77933940fabb', {'name': 'theta_5', 'expression': \"Parameter('theta_5')\", 'symbols': {'theta_5': {'name': 'theta_5', 'trainable': 'True', 'value': '0.9014234308376707'}}})}}, 'noise': None}]}]}, {'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'CNOT', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'X', 'qubit_support': (1,), 'tag': None, 'noise': None}], 'noise': None}]}]}]}\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def deserialize(d: dict, as_torch: bool = False) -&gt; SUPPORTED_TYPES:\n    \"\"\"\n    Supported Types:\n\n    AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module\n    Deserializes a dict to one of the supported types.\n\n    Arguments:\n        d (dict): A dict containing a serialized object.\n        as_torch (bool): Whether to transform to torch for the deserialized object.\n    Returns:\n        AbstractBlock, QuantumCircuit, QuantumModel, Register, torch.nn.Module.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence import serialize, deserialize, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    block_dict = serialize(myblock)\n    print(block_dict)\n\n    ## Lets use myblock in a QuantumCircuit and serialize it.\n\n    qc = QuantumCircuit(n_qubits, myblock)\n    qc_dict = serialize(qc)\n    qc_deserialized = deserialize(qc_dict)\n    assert qc == qc_deserialized\n\n    ## Finally, let's wrap it in a QuantumModel\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\n    qm_dict = serialize(qm)\n    qm_deserialized = deserialize(qm_dict)\n    # Lets check if the loaded QuantumModel returns the same expectation\n    assert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n    ```\n    \"\"\"\n    obj: SerializationModel\n    if d.get(\"expression\"):\n        obj = ExpressionSerialization(d)\n    elif d.get(\"block\") and d.get(\"register\"):\n        obj = QuantumCircuitSerialization(d)\n    elif d.get(\"graph\"):\n        obj = RegisterSerialization(d)\n    elif d.get(\"type\"):\n        obj = BlockTypeSerialization(d)\n    else:\n        obj = ModelSerialization(d, as_torch=as_torch)\n    return obj.value\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.load","title":"<code>load(file_path, map_location='cpu')</code>","text":"<p>Same as serialize/deserialize but for storing/loading files.</p> <p>Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register Loads a .json or .pt file to one of the supported types.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>The name of the file.</p> <p> TYPE: <code>str</code> </p> <code>map_location</code> <p>In case of a .pt file, on which device to load the object (cpu,cuda).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cpu'</code> </p> <p>Returns:     A object of type AbstractBlock, QuantumCircuit, QuantumModel, Register.</p> <p>Examples: <pre><code>import torch\nfrom pathlib import Path\nimport os\n\nfrom qadence import save, load, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nqc = QuantumCircuit(n_qubits, myblock)\n# Lets store the circuit in a json file\nsave(qc, '.', 'circ')\nloaded_qc = load(Path('circ.json'))\nqc == loaded_qc\nos.remove('circ.json')\n## Let's wrap it in a QuantumModel and store that\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\nsave(qm, folder= '.',file_name= 'quantum_model')\nqm_loaded = load('quantum_model.json')\nos.remove('quantum_model.json')\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def load(file_path: str | Path, map_location: str = \"cpu\") -&gt; SUPPORTED_TYPES:\n    \"\"\"\n    Same as serialize/deserialize but for storing/loading files.\n\n    Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register\n    Loads a .json or .pt file to one of the supported types.\n\n    Arguments:\n        file_path (str): The name of the file.\n        map_location (str): In case of a .pt file, on which device to load the object (cpu,cuda).\n    Returns:\n        A object of type AbstractBlock, QuantumCircuit, QuantumModel, Register.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from pathlib import Path\n    import os\n\n    from qadence import save, load, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    qc = QuantumCircuit(n_qubits, myblock)\n    # Lets store the circuit in a json file\n    save(qc, '.', 'circ')\n    loaded_qc = load(Path('circ.json'))\n    qc == loaded_qc\n    os.remove('circ.json')\n    ## Let's wrap it in a QuantumModel and store that\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n    save(qm, folder= '.',file_name= 'quantum_model')\n    qm_loaded = load('quantum_model.json')\n    os.remove('quantum_model.json')\n    ```\n    \"\"\"\n    d = {}\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if not os.path.exists(file_path):\n        logger.error(f\"File {file_path} not found.\")\n        raise FileNotFoundError\n    FORMAT = file_extension(file_path)\n    _, _, load_fn, _ = FORMAT_DICT[FORMAT]  # type: ignore[index]\n    try:\n        d = load_fn(file_path, map_location)\n        logger.debug(f\"Successfully loaded {d} from {file_path}.\")\n    except Exception as e:\n        logger.error(f\"Unable to load Object from {file_path} due to {e}\")\n    return deserialize(d)\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.parse_expr_fn","title":"<code>parse_expr_fn(code)</code>","text":"<p>A parsing expressions function that checks whether a given code is valid on.</p> <p>the parsing grammar. The grammar is defined to be compatible with <code>sympy</code> expressions, such as <code>Float('-0.33261030434342942', precision=53)</code>, while avoiding code injection such as <code>2*3</code> or <code>__import__('os').system('ls -la')</code>.</p> PARAMETER DESCRIPTION <code>code</code> <p>code to be parsed and checked.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Boolean indicating whether the code matches the defined grammar or not.</p> Source code in <code>qadence/serialization.py</code> <pre><code>def parse_expr_fn(code: str) -&gt; bool:\n    \"\"\"\n    A parsing expressions function that checks whether a given code is valid on.\n\n    the parsing grammar. The grammar is defined to be compatible with `sympy`\n    expressions, such as `Float('-0.33261030434342942', precision=53)`, while\n    avoiding code injection such as `2*3` or `__import__('os').system('ls -la')`.\n\n    Args:\n        code (str): code to be parsed and checked.\n\n    Returns:\n        Boolean indicating whether the code matches the defined grammar or not.\n    \"\"\"\n\n    parser = _parsing_serialize_expr\n    try:\n        parser.parse(code)\n    except NoMatch:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.save","title":"<code>save(obj, folder, file_name='', format=SerializationFormat.JSON)</code>","text":"<p>Same as serialize/deserialize but for storing/loading files.</p> <p>Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module Saves a qadence object to a json/.pt.</p> PARAMETER DESCRIPTION <code>obj</code> <pre><code>Either AbstractBlock, QuantumCircuit, QuantumModel, Register.\n</code></pre> <p> TYPE: <code>AbstractBlock | QuantumCircuit | QuantumModel | Register</code> </p> <code>file_name</code> <p>The name of the file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>format</code> <p>The type of file to save.</p> <p> TYPE: <code>str</code> DEFAULT: <code>JSON</code> </p> <p>Returns:     None.</p> <p>Examples: <pre><code>import torch\nfrom pathlib import Path\nimport os\n\nfrom qadence import save, load, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nqc = QuantumCircuit(n_qubits, myblock)\n# Lets store the circuit in a json file\nsave(qc, '.', 'circ')\nloaded_qc = load(Path('circ.json'))\nqc == loaded_qc\nos.remove('circ.json')\n## Let's wrap it in a QuantumModel and store that\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\nsave(qm, folder= '.',file_name= 'quantum_model')\nqm_loaded = load('quantum_model.json')\nos.remove('quantum_model.json')\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def save(\n    obj: SUPPORTED_TYPES,\n    folder: str | Path,\n    file_name: str = \"\",\n    format: SerializationFormat = SerializationFormat.JSON,\n) -&gt; None:\n    \"\"\"\n    Same as serialize/deserialize but for storing/loading files.\n\n    Supported types:\n    AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module\n    Saves a qadence object to a json/.pt.\n\n    Arguments:\n        obj (AbstractBlock | QuantumCircuit | QuantumModel | Register):\n                Either AbstractBlock, QuantumCircuit, QuantumModel, Register.\n        file_name (str): The name of the file.\n        format (str): The type of file to save.\n    Returns:\n        None.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from pathlib import Path\n    import os\n\n    from qadence import save, load, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    qc = QuantumCircuit(n_qubits, myblock)\n    # Lets store the circuit in a json file\n    save(qc, '.', 'circ')\n    loaded_qc = load(Path('circ.json'))\n    qc == loaded_qc\n    os.remove('circ.json')\n    ## Let's wrap it in a QuantumModel and store that\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n    save(qm, folder= '.',file_name= 'quantum_model')\n    qm_loaded = load('quantum_model.json')\n    os.remove('quantum_model.json')\n    ```\n    \"\"\"\n    if not isinstance(obj, get_args(SUPPORTED_TYPES)):\n        logger.error(f\"Serialization of object type {type(obj)} not supported.\")\n    folder = Path(folder)\n    if not folder.is_dir():\n        logger.error(NotADirectoryError)\n    if file_name == \"\":\n        file_name = type(obj).__name__\n    try:\n        suffix, save_fn, _, save_params = FORMAT_DICT[format]\n        d = serialize(obj, save_params)\n        file_path = folder / Path(file_name + suffix)\n        save_fn(d, file_path)\n        logger.debug(f\"Successfully saved {obj} from to {folder}.\")\n    except Exception as e:\n        logger.error(f\"Unable to write {type(obj)} to disk due to {e}\")\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.serialize","title":"<code>serialize(obj, save_params=False)</code>","text":"<p>Supported Types:</p> <p>AbstractBlock | QuantumCircuit | QuantumModel | torch.nn.Module | Register | Module Serializes a qadence object to a dictionary.</p> PARAMETER DESCRIPTION <code>obj</code> <p> TYPE: <code>AbstractBlock | QuantumCircuit | QuantumModel | Register | Module</code> </p> <p>Returns:     A dict.</p> <p>Examples: <pre><code>import torch\nfrom qadence import serialize, deserialize, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nblock_dict = serialize(myblock)\nprint(block_dict)\n\n## Lets use myblock in a QuantumCircuit and serialize it.\n\nqc = QuantumCircuit(n_qubits, myblock)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n## Finally, let's wrap it in a QuantumModel\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n# Lets check if the loaded QuantumModel returns the same expectation\nassert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n</code></pre> <pre><code>{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': 'HEA', 'blocks': [{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('bc400449-ea4f-4e2b-acdc-45987ce0249e', {'name': 'theta_0', 'expression': \"Parameter('theta_0')\", 'symbols': {'theta_0': {'name': 'theta_0', 'trainable': 'True', 'value': '0.2915895318800006'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('995a6223-04f7-4f21-b18e-c5b5e0f20aa7', {'name': 'theta_1', 'expression': \"Parameter('theta_1')\", 'symbols': {'theta_1': {'name': 'theta_1', 'trainable': 'True', 'value': '0.22961349503211292'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RY', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('ce566430-e3a6-444f-a6c0-59736ac0f82e', {'name': 'theta_2', 'expression': \"Parameter('theta_2')\", 'symbols': {'theta_2': {'name': 'theta_2', 'trainable': 'True', 'value': '0.024175203816877877'}}})}}, 'noise': None}, {'type': 'RY', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('03fbb49b-4ace-43d5-ad66-baca0bd7bb90', {'name': 'theta_3', 'expression': \"Parameter('theta_3')\", 'symbols': {'theta_3': {'name': 'theta_3', 'trainable': 'True', 'value': '0.5927265369193969'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('fdff4997-c3fb-4992-a4c6-19761ba2af49', {'name': 'theta_4', 'expression': \"Parameter('theta_4')\", 'symbols': {'theta_4': {'name': 'theta_4', 'trainable': 'True', 'value': '0.6333931490419303'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('ea27e52d-dc01-41d8-bfa0-e18465f62995', {'name': 'theta_5', 'expression': \"Parameter('theta_5')\", 'symbols': {'theta_5': {'name': 'theta_5', 'trainable': 'True', 'value': '0.5438656578238705'}}})}}, 'noise': None}]}]}, {'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'CNOT', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'X', 'qubit_support': (1,), 'tag': None, 'noise': None}], 'noise': None}]}]}]}\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def serialize(obj: SUPPORTED_TYPES, save_params: bool = False) -&gt; dict:\n    \"\"\"\n    Supported Types:\n\n    AbstractBlock | QuantumCircuit | QuantumModel | torch.nn.Module | Register | Module\n    Serializes a qadence object to a dictionary.\n\n    Arguments:\n        obj (AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module):\n    Returns:\n        A dict.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence import serialize, deserialize, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    block_dict = serialize(myblock)\n    print(block_dict)\n\n    ## Lets use myblock in a QuantumCircuit and serialize it.\n\n    qc = QuantumCircuit(n_qubits, myblock)\n    qc_dict = serialize(qc)\n    qc_deserialized = deserialize(qc_dict)\n    assert qc == qc_deserialized\n\n    ## Finally, let's wrap it in a QuantumModel\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\n    qm_dict = serialize(qm)\n    qm_deserialized = deserialize(qm_dict)\n    # Lets check if the loaded QuantumModel returns the same expectation\n    assert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n    ```\n    \"\"\"\n    if not isinstance(obj, get_args(SUPPORTED_TYPES)):\n        logger.error(TypeError(f\"Serialization of object type {type(obj)} not supported.\"))\n\n    d: dict = dict()\n    try:\n        if isinstance(obj, core.Expr):\n            symb_dict = dict()\n            expr_dict = {\"name\": str(obj), \"expression\": srepr(obj)}\n            symbs: set[Parameter | core.Basic] = obj.free_symbols\n            if symbs:\n                symb_dict = {\"symbols\": {str(s): s._to_dict() for s in symbs}}\n            d = {**expr_dict, **symb_dict}\n        else:\n            if hasattr(obj, \"_to_dict\"):\n                model_to_dict: Callable = obj._to_dict\n                d = (\n                    model_to_dict(save_params)\n                    if isinstance(obj, torch.nn.Module)\n                    else model_to_dict()\n                )\n            elif hasattr(obj, \"state_dict\"):\n                d = {type(obj).__name__: obj.state_dict()}\n            else:\n                raise ValueError(f\"Cannot serialize object {obj}.\")\n    except Exception as e:\n        logger.error(f\"Serialization of object {obj} failed due to {e}\")\n    return d\n</code></pre>"},{"location":"api/states/","title":"State preparation","text":""},{"location":"api/states/#state-preparation-routines","title":"State Preparation Routines","text":""},{"location":"api/states/#qadence.states.ghz_block","title":"<code>ghz_block(n_qubits)</code>","text":"<p>Generates the abstract ghz state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>A ChainBlock representing the GHZ state.</p> <p>Examples: <pre><code>from qadence.states import ghz_block\n\nblock = ghz_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 CNOT(0, 1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def ghz_block(n_qubits: int) -&gt; ChainBlock:\n    \"\"\"\n    Generates the abstract ghz state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A ChainBlock representing the GHZ state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import ghz_block\n\n    block = ghz_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    cnots = chain(CNOT(i - 1, i) for i in range(1, n_qubits))\n    return chain(H(0), cnots)\n</code></pre>"},{"location":"api/states/#qadence.states.ghz_state","title":"<code>ghz_state(n_qubits, batch_size=1)</code>","text":"<p>Creates a GHZ state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>How many bitstrings to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import ghz_state\n\nprint(ghz_state(n_qubits=2, batch_size=2))\n</code></pre> <pre><code>tensor([[0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j],\n        [0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def ghz_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Creates a GHZ state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): How many bitstrings to use.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import ghz_state\n\n    print(ghz_state(n_qubits=2, batch_size=2))\n    ```\n    \"\"\"\n    norm = 1 / torch.sqrt(torch.tensor(2))\n    return norm * (zero_state(n_qubits, batch_size) + one_state(n_qubits, batch_size))\n</code></pre>"},{"location":"api/states/#qadence.states.is_normalized","title":"<code>is_normalized(wf, atol=NORMALIZATION_ATOL)</code>","text":"<p>Checks if a wave function is normalized.</p> PARAMETER DESCRIPTION <code>wf</code> <p>The wave function as a torch tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>atol</code> <p>The tolerance.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>NORMALIZATION_ATOL</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, is_normalized\n\nprint(is_normalized(uniform_state(2)))\n</code></pre> <pre><code>True\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def is_normalized(wf: Tensor, atol: float = NORMALIZATION_ATOL) -&gt; bool:\n    \"\"\"\n    Checks if a wave function is normalized.\n\n    Arguments:\n        wf (torch.Tensor): The wave function as a torch tensor.\n        atol (float) : The tolerance.\n\n    Returns:\n        A bool.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, is_normalized\n\n    print(is_normalized(uniform_state(2)))\n    ```\n    \"\"\"\n    if wf.dim() == 1:\n        wf = wf.unsqueeze(0)\n    sum_probs: Tensor = (wf.abs() ** 2).sum(dim=1)\n    ones = torch.ones_like(sum_probs)\n    return torch.allclose(sum_probs, ones, rtol=0.0, atol=atol)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/states/#qadence.states.normalize","title":"<code>normalize(wf)</code>","text":"<p>Normalizes a wavefunction or batch of wave functions.</p> PARAMETER DESCRIPTION <code>wf</code> <p>Normalized wavefunctions.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, normalize\n\nprint(normalize(uniform_state(2, 2)))\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j],\n        [0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def normalize(wf: Tensor) -&gt; Tensor:\n    \"\"\"\n    Normalizes a wavefunction or batch of wave functions.\n\n    Arguments:\n        wf (torch.Tensor): Normalized wavefunctions.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, normalize\n\n    print(normalize(uniform_state(2, 2)))\n    ```\n    \"\"\"\n    if wf.dim() == 1:\n        return wf / torch.sqrt((wf.abs() ** 2).sum())\n    else:\n        return wf / torch.sqrt((wf.abs() ** 2).sum(1)).unsqueeze(1)\n</code></pre>"},{"location":"api/states/#qadence.states.one_block","title":"<code>one_block(n_qubits)</code>","text":"<p>Generates the abstract one state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the one state.</p> <p>Examples: <pre><code>from qadence.states import one_block\n\nblock = one_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def one_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract one state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the one state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import one_block\n\n    block = one_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(X, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.one_state","title":"<code>one_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the one state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import one_state\n\nstate = one_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def one_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the one state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): The batch size.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import one_state\n\n    state = one_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    bitstring = \"1\" * n_qubits\n    return _state_from_bitstring(bitstring, batch_size)\n</code></pre>"},{"location":"api/states/#qadence.states.pmf","title":"<code>pmf(wf)</code>","text":"<p>Converts a wave function into a torch Distribution.</p> PARAMETER DESCRIPTION <code>wf</code> <p>The wave function as a torch tensor.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Distribution</code> <p>A torch.distributions.Distribution.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, pmf\n\nprint(pmf(uniform_state(2)).probs)\n</code></pre> <pre><code>tensor([[0.2500, 0.2500, 0.2500, 0.2500]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def pmf(wf: Tensor) -&gt; Distribution:\n    \"\"\"\n    Converts a wave function into a torch Distribution.\n\n    Arguments:\n        wf (torch.Tensor): The wave function as a torch tensor.\n\n    Returns:\n        A torch.distributions.Distribution.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, pmf\n\n    print(pmf(uniform_state(2)).probs)\n    ```\n    \"\"\"\n    return Categorical(torch.abs(torch.pow(wf, 2)))\n</code></pre>"},{"location":"api/states/#qadence.states.product_block","title":"<code>product_block(bitstring)</code>","text":"<p>Creates an abstract product state from a bitstring.</p> PARAMETER DESCRIPTION <code>bitstring</code> <p>A bitstring.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the product state.</p> <p>Examples: <pre><code>from qadence.states import product_block\n\nprint(product_block(\"1100\"))\n</code></pre> <pre><code>KronBlock(0,1,2,3)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u251c\u2500\u2500 I(2)\n\u2514\u2500\u2500 I(3)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def product_block(bitstring: str) -&gt; KronBlock:\n    \"\"\"\n    Creates an abstract product state from a bitstring.\n\n    Arguments:\n        bitstring (str): A bitstring.\n\n    Returns:\n        A KronBlock representing the product state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import product_block\n\n    print(product_block(\"1100\"))\n    ```\n    \"\"\"\n    return _block_from_bitstring(bitstring)\n</code></pre>"},{"location":"api/states/#qadence.states.product_state","title":"<code>product_state(bitstring, batch_size=1, endianness=Endianness.BIG, backend=BackendName.PYQTORCH)</code>","text":"<p>Creates a product state from a bitstring.</p> PARAMETER DESCRIPTION <code>bitstring</code> <p>A bitstring.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1</code> </p> <code>backend</code> <p>The backend to use. Default is \"pyqtorch\".</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import product_state\n\nprint(product_state(\"1100\", backend=\"pyqtorch\"))\nprint(product_state(\"1100\", backend=\"horqrux\"))\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n         1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j\n 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>@singledispatch\ndef product_state(\n    bitstring: str,\n    batch_size: int = 1,\n    endianness: Endianness = Endianness.BIG,\n    backend: BackendName = BackendName.PYQTORCH,\n) -&gt; ArrayLike:\n    \"\"\"\n    Creates a product state from a bitstring.\n\n    Arguments:\n        bitstring (str): A bitstring.\n        batch_size (int) : Batch size.\n        backend (BackendName): The backend to use. Default is \"pyqtorch\".\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import product_state\n\n    print(product_state(\"1100\", backend=\"pyqtorch\"))\n    print(product_state(\"1100\", backend=\"horqrux\"))\n    ```\n    \"\"\"\n    if batch_size:\n        logger.debug(\n            \"The input `batch_size` is going to be deprecated. \"\n            \"For now, default batch_size is set to 1.\"\n        )\n    return run(product_block(bitstring), backend=backend, endianness=endianness)\n</code></pre>"},{"location":"api/states/#qadence.states.rand_bitstring","title":"<code>rand_bitstring(N)</code>","text":"<p>Creates a random bistring.</p> PARAMETER DESCRIPTION <code>N</code> <p>The length of the bitstring.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A string.</p> <p>Examples: <pre><code>from qadence.states import rand_bitstring\n\nprint(rand_bitstring(N=8))\n</code></pre> <pre><code>10000101\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_bitstring(N: int) -&gt; str:\n    \"\"\"\n    Creates a random bistring.\n\n    Arguments:\n        N (int): The length of the bitstring.\n\n    Returns:\n        A string.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_bitstring\n\n    print(rand_bitstring(N=8))\n    ```\n    \"\"\"\n    return \"\".join(str(random.randint(0, 1)) for _ in range(N))\n</code></pre>"},{"location":"api/states/#qadence.states.rand_product_block","title":"<code>rand_product_block(n_qubits)</code>","text":"<p>Creates a block representing a random abstract product state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the product state.</p> <p>Examples: <pre><code>from qadence.states import rand_product_block\n\nprint(rand_product_block(n_qubits=2))\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_product_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Creates a block representing a random abstract product state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the product state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_product_block\n\n    print(rand_product_block(n_qubits=2))\n    ```\n    \"\"\"\n    return product_block(rand_bitstring(n_qubits))\n</code></pre>"},{"location":"api/states/#qadence.states.rand_product_state","title":"<code>rand_product_state(n_qubits, batch_size=1)</code>","text":"<p>Creates a random product state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>How many bitstrings to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import rand_product_state\n\nprint(rand_product_state(n_qubits=2, batch_size=2))\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_product_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Creates a random product state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): How many bitstrings to use.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_product_state\n\n    print(rand_product_state(n_qubits=2, batch_size=2))\n    ```\n    \"\"\"\n    wf_batch = torch.zeros(batch_size, 2**n_qubits, dtype=DTYPE)\n    rand_pos = torch.randint(0, 2**n_qubits, (batch_size,))\n    wf_batch[torch.arange(batch_size), rand_pos] = torch.tensor(1.0 + 0j, dtype=DTYPE)\n    return wf_batch\n</code></pre>"},{"location":"api/states/#qadence.states.random_state","title":"<code>random_state(n_qubits, batch_size=1, backend=BackendName.PYQTORCH, type=StateGeneratorType.HAAR_MEASURE_FAST)</code>","text":"<p>Generates a random state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>backend</code> <p>The backend to use.</p> <p> TYPE: <code>str</code> DEFAULT: <code>PYQTORCH</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>type</code> <p>StateGeneratorType.</p> <p> DEFAULT: <code>HAAR_MEASURE_FAST</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import random_state, StateGeneratorType\nfrom qadence.states import random_state, is_normalized, pmf\nfrom qadence.types import BackendName\nfrom torch.distributions import Distribution\n\n### We have the following options:\nprint([g.value for g in StateGeneratorType])\n\nn_qubits = 2\n# The default is StateGeneratorType.HAAR_MEASURE_FAST\nstate = random_state(n_qubits=n_qubits)\nprint(state)\n\n### Lets initialize a state using random rotations, i.e., StateGeneratorType.RANDOM_ROTATIONS.\nrandom = random_state(n_qubits=n_qubits, type=StateGeneratorType.RANDOM_ROTATIONS)\nprint(random)\n</code></pre> <pre><code>['RandomRotations', 'HaarMeasureFast', 'HaarMeasureSlow']\ntensor([[ 0.2178-0.0496j,  0.3255-0.5266j, -0.2701+0.6294j, -0.0902+0.2993j]])\ntensor([[ 0.9207-0.1467j, -0.0569-0.3572j,  0.0000+0.0000j,  0.0000+0.0000j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def random_state(\n    n_qubits: int,\n    batch_size: int = 1,\n    backend: str = BackendName.PYQTORCH,\n    type: StateGeneratorType = StateGeneratorType.HAAR_MEASURE_FAST,\n) -&gt; Tensor:\n    \"\"\"\n    Generates a random state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        backend (str): The backend to use.\n        batch_size (int): The batch size.\n        type : StateGeneratorType.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import random_state, StateGeneratorType\n    from qadence.states import random_state, is_normalized, pmf\n    from qadence.types import BackendName\n    from torch.distributions import Distribution\n\n    ### We have the following options:\n    print([g.value for g in StateGeneratorType])\n\n    n_qubits = 2\n    # The default is StateGeneratorType.HAAR_MEASURE_FAST\n    state = random_state(n_qubits=n_qubits)\n    print(state)\n\n    ### Lets initialize a state using random rotations, i.e., StateGeneratorType.RANDOM_ROTATIONS.\n    random = random_state(n_qubits=n_qubits, type=StateGeneratorType.RANDOM_ROTATIONS)\n    print(random)\n    ```\n    \"\"\"\n\n    if type == StateGeneratorType.HAAR_MEASURE_FAST:\n        state = concat(tuple(_rand_haar_fast(n_qubits) for _ in range(batch_size)), dim=0)\n    elif type == StateGeneratorType.HAAR_MEASURE_SLOW:\n        state = concat(tuple(_rand_haar_slow(n_qubits) for _ in range(batch_size)), dim=0)\n    elif type == StateGeneratorType.RANDOM_ROTATIONS:\n        state = run(_abstract_random_state(n_qubits, batch_size))  # type: ignore\n    assert all(list(map(is_normalized, state)))\n    return state\n</code></pre>"},{"location":"api/states/#qadence.states.uniform_block","title":"<code>uniform_block(n_qubits)</code>","text":"<p>Generates the abstract uniform state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the uniform state.</p> <p>Examples: <pre><code>from qadence.states import uniform_block\n\nblock = uniform_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 H(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def uniform_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract uniform state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the uniform state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_block\n\n    block = uniform_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(H, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.uniform_state","title":"<code>uniform_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the uniform state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import uniform_state\n\nstate = uniform_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def uniform_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the uniform state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): The batch size.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state\n\n    state = uniform_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    norm = 1 / torch.sqrt(torch.tensor(2**n_qubits))\n    return norm * torch.ones(batch_size, 2**n_qubits, dtype=DTYPE)\n</code></pre>"},{"location":"api/states/#qadence.states.zero_block","title":"<code>zero_block(n_qubits)</code>","text":"<p>Generates the abstract zero state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the zero state.</p> <p>Examples: <pre><code>from qadence.states import zero_block\n\nblock = zero_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 I(0)\n\u2514\u2500\u2500 I(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def zero_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract zero state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the zero state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import zero_block\n\n    block = zero_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(I, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.zero_state","title":"<code>zero_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the zero state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits for which the zero state is to be generated.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size for the zero state.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import zero_state\n\nstate = zero_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def zero_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the zero state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits for which the zero state is to be generated.\n        batch_size (int): The batch size for the zero state.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import zero_state\n\n    state = zero_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    bitstring = \"0\" * n_qubits\n    return _state_from_bitstring(bitstring, batch_size)\n</code></pre>"},{"location":"api/transpile/","title":"Transpilation","text":"<p>Contains functions that operate on blocks and circuits to <code>transpile</code> them to new blocks/circuits.</p>"},{"location":"api/transpile/#qadence.transpile.transpile.transpile","title":"<code>transpile(*fs)</code>","text":"<p><code>AbstractBlock</code> or <code>QuantumCircuit</code> transpilation.</p> <p>Compose functions that accept a circuit/block and returns a circuit/block.</p> PARAMETER DESCRIPTION <code>*fs</code> <p>composable functions that either map blocks to blocks (<code>Callable[[AbstractBlock], AbstractBlock]</code>) or circuits to circuits (<code>Callable[[QuantumCircuit], QuantumCircuit]</code>).</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>Composed function.</p> <p>Examples:</p> <p>Flatten a block of nested chains and krons: <pre><code>from qadence import *\nfrom qadence.transpile import transpile, flatten, scale_primitive_blocks_only\n\nb = chain(2 * chain(chain(X(0), Y(0))), kron(kron(X(0), X(1))))\nprint(b)\n\n# both flatten and scale_primitive_blocks_only are functions that accept and\n# return a block\nt = transpile(flatten, scale_primitive_blocks_only)(b)\nprint(t)\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 [mul: 2] \n\u2502   \u2514\u2500\u2500 ChainBlock(0)\n\u2502       \u2514\u2500\u2500 ChainBlock(0)\n\u2502           \u251c\u2500\u2500 X(0)\n\u2502           \u2514\u2500\u2500 Y(0)\n\u2514\u2500\u2500 KronBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u251c\u2500\u2500 X(0)\n        \u2514\u2500\u2500 X(1)\n\nChainBlock(0,1)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u251c\u2500\u2500 Y(0)\n\u2514\u2500\u2500 KronBlock(0,1)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 X(1)\n</code></pre> </p> <p>We also proved a decorator to easily turn a function <code>Callable[[AbstractBlock], AbstractBlock]</code> into a <code>Callable[[QuantumCircuit], QuantumCircuit]</code> to be used in circuit transpilation. <pre><code>from qadence import *\nfrom qadence.transpile import transpile, blockfn_to_circfn, flatten\n\n# We want to pass this circuit to `transpile` instead of a block,\n# so we need functions that map from a circuit to a circuit.\ncirc = QuantumCircuit(2, chain(chain(X(0), chain(X(1)))))\n\n@blockfn_to_circfn\ndef fn(block):\n    # un-decorated function accepts a block and returns a block\n    return block * block\n\ntransp = transpile(\n    # the decorated function accepts a circuit and returns a circuit\n    fn,\n    # already existing functions can also be decorated\n    blockfn_to_circfn(flatten)\n)\nprint(transp(circ))\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 X(0)\n\u2502   \u2514\u2500\u2500 X(1)\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/transpile/transpile.py</code> <pre><code>def transpile(*fs: Callable) -&gt; Callable:\n    \"\"\"`AbstractBlock` or `QuantumCircuit` transpilation.\n\n    Compose functions that\n    accept a circuit/block and returns a circuit/block.\n\n    Arguments:\n        *fs: composable functions that either map blocks to blocks\n            (`Callable[[AbstractBlock], AbstractBlock]`)\n            or circuits to circuits (`Callable[[QuantumCircuit], QuantumCircuit]`).\n\n    Returns:\n        Composed function.\n\n    Examples:\n\n    Flatten a block of nested chains and krons:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import *\n    from qadence.transpile import transpile, flatten, scale_primitive_blocks_only\n\n    b = chain(2 * chain(chain(X(0), Y(0))), kron(kron(X(0), X(1))))\n    print(b)\n    print() # markdown-exec: hide\n\n    # both flatten and scale_primitive_blocks_only are functions that accept and\n    # return a block\n    t = transpile(flatten, scale_primitive_blocks_only)(b)\n    print(t)\n    ```\n\n    We also proved a decorator to easily turn a function `Callable[[AbstractBlock], AbstractBlock]`\n    into a `Callable[[QuantumCircuit], QuantumCircuit]` to be used in circuit transpilation.\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import *\n    from qadence.transpile import transpile, blockfn_to_circfn, flatten\n\n    # We want to pass this circuit to `transpile` instead of a block,\n    # so we need functions that map from a circuit to a circuit.\n    circ = QuantumCircuit(2, chain(chain(X(0), chain(X(1)))))\n\n    @blockfn_to_circfn\n    def fn(block):\n        # un-decorated function accepts a block and returns a block\n        return block * block\n\n    transp = transpile(\n        # the decorated function accepts a circuit and returns a circuit\n        fn,\n        # already existing functions can also be decorated\n        blockfn_to_circfn(flatten)\n    )\n    print(transp(circ))\n    ```\n    \"\"\"\n    return lambda x: reduce(lambda acc, f: f(acc), reversed(fs), x)\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.chain_single_qubit_ops","title":"<code>chain_single_qubit_ops(block)</code>","text":"<p>Transpile a chain of krons into a kron of chains of single qubit operations.</p> <p>Examples: <pre><code>from qadence import hea\nfrom qadence.transpile.block import chain_single_qubit_ops\n\n# Consider a single HEA layer\nblock = hea(2,1)\nprint(block)\n\n# After applying chain_single_qubit_ops, we get:\nprint(chain_single_qubit_ops(block))\n</code></pre> <pre><code>ChainBlock(0,1) [tag: HEA]\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n\u2502   \u2502   \u2514\u2500\u2500 RX(1) [params: ['theta_1']]\n\u2502   \u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_2']]\n\u2502   \u2502   \u2514\u2500\u2500 RY(1) [params: ['theta_3']]\n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 RX(0) [params: ['theta_4']]\n\u2502       \u2514\u2500\u2500 RX(1) [params: ['theta_5']]\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u2514\u2500\u2500 CNOT(0, 1)\nChainBlock(0,1)\n\u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u251c\u2500\u2500 ChainBlock(0)\n\u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n\u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_2']]\n\u2502   \u2502   \u2514\u2500\u2500 RX(0) [params: ['theta_4']]\n\u2502   \u2514\u2500\u2500 ChainBlock(1)\n\u2502       \u251c\u2500\u2500 RX(1) [params: ['theta_1']]\n\u2502       \u251c\u2500\u2500 RY(1) [params: ['theta_3']]\n\u2502       \u2514\u2500\u2500 RX(1) [params: ['theta_5']]\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u2514\u2500\u2500 CNOT(0, 1)\n</code></pre></p> Source code in <code>qadence/transpile/block.py</code> <pre><code>def chain_single_qubit_ops(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Transpile a chain of krons into a kron of chains of single qubit operations.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import hea\n    from qadence.transpile.block import chain_single_qubit_ops\n\n    # Consider a single HEA layer\n    block = hea(2,1)\n    print(block)\n\n    # After applying chain_single_qubit_ops, we get:\n    print(chain_single_qubit_ops(block))\n    ```\n    \"\"\"\n    if is_chain_of_primitivekrons(block):\n        try:\n            return kron(*map(lambda bs: chain(*bs), zip(*block)))  # type: ignore[misc]\n        except Exception as e:\n            logger.debug(\n                f\"Unable to transpile {block} using chain_single_qubit_ops\\\n                         due to {e}. Returning original circuit.\"\n            )\n            return block\n\n    elif isinstance(block, CompositeBlock):\n        return _construct(type(block), tuple(chain_single_qubit_ops(b) for b in block.blocks))\n    else:\n        return block\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.scale_primitive_blocks_only","title":"<code>scale_primitive_blocks_only(block, scale=None)</code>","text":"<p>Push the scale all the way down into the leaves of the block tree.</p> <p>When given a scaled CompositeBlock consisting of several PrimitiveBlocks.</p> PARAMETER DESCRIPTION <code>block</code> <p>The block to be transpiled.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>scale</code> <p>An optional scale parameter. Only to be used for recursive calls internally.</p> <p> TYPE: <code>Basic</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>A block of the same type where the scales have been moved into the subblocks.</p> <p> TYPE: <code>AbstractBlock</code> </p> <p>Examples:</p> <p>There are two different cases: <code>ChainBlock</code>s/<code>KronBlock</code>s: Only the first subblock needs to be scaled because chains/krons represent multiplications. <pre><code>from qadence import chain, X, RX\nfrom qadence.transpile import scale_primitive_blocks_only\nb = 2 * chain(X(0), RX(0, \"theta\"))\nprint(b)\n# After applying scale_primitive_blocks_only\nprint(scale_primitive_blocks_only(b))\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 ChainBlock(0)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\nChainBlock(0)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 RX(0) [params: ['theta']]\n</code></pre></p> <p><code>AddBlock</code>s: Consider 2 * add(X(0), RX(0, \"theta\")).  The scale needs to be added to all subblocks.  We get add(2 * X(0), 2 * RX(0, \"theta\")). <pre><code>from qadence import add, X, RX\nfrom qadence.transpile import scale_primitive_blocks_only\nb = 2 * add(X(0), RX(0, \"theta\"))\nprint(b)\n# After applying scale_primitive_blocks_only\nprint(scale_primitive_blocks_only(b))\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 AddBlock(0)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\nAddBlock(0)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 [mul: 2.000] \n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\n</code></pre></p> Source code in <code>qadence/transpile/block.py</code> <pre><code>@singledispatch\ndef scale_primitive_blocks_only(block: AbstractBlock, scale: sympy.Basic = None) -&gt; AbstractBlock:\n    \"\"\"Push the scale all the way down into the leaves of the block tree.\n\n    When given a scaled CompositeBlock consisting of several PrimitiveBlocks.\n\n    Arguments:\n        block: The block to be transpiled.\n        scale: An optional scale parameter. Only to be used for recursive calls internally.\n\n    Returns:\n        AbstractBlock: A block of the same type where the scales have been moved into the subblocks.\n\n    Examples:\n\n    There are two different cases:\n    `ChainBlock`s/`KronBlock`s: Only the first subblock needs to be scaled because chains/krons\n    represent multiplications.\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import chain, X, RX\n    from qadence.transpile import scale_primitive_blocks_only\n    b = 2 * chain(X(0), RX(0, \"theta\"))\n    print(b)\n    # After applying scale_primitive_blocks_only\n    print(scale_primitive_blocks_only(b))\n    ```\n\n    `AddBlock`s: Consider 2 * add(X(0), RX(0, \"theta\")).  The scale needs to be added to all\n    subblocks.  We get add(2 * X(0), 2 * RX(0, \"theta\")).\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import add, X, RX\n    from qadence.transpile import scale_primitive_blocks_only\n    b = 2 * add(X(0), RX(0, \"theta\"))\n    print(b)\n    # After applying scale_primitive_blocks_only\n    print(scale_primitive_blocks_only(b))\n    ```\n    \"\"\"\n    raise NotImplementedError(f\"scale_primitive_blocks_only is not implemented for {type(block)}\")\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.set_trainable","title":"<code>set_trainable(blocks, value=True, inplace=True)</code>","text":"<p>Set the trainability of all parameters in a block to a given value.</p> PARAMETER DESCRIPTION <code>blocks</code> <p>Block or list of blocks for which to set the trainable attribute</p> <p> TYPE: <code>AbstractBlock | list[AbstractBlock]</code> </p> <code>value</code> <p>The value of the trainable attribute to assign to the input blocks</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <p>Whether to modify the block(s) in place or not. Currently, only</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>if the <code>inplace</code> argument is set to False, the function will raise  this exception</p> RETURNS DESCRIPTION <code>AbstractBlock | list[AbstractBlock]</code> <p>AbstractBlock | list[AbstractBlock]: the input block or list of blocks with the trainable attribute set to the given value</p> Source code in <code>qadence/transpile/block.py</code> <pre><code>def set_trainable(\n    blocks: AbstractBlock | list[AbstractBlock], value: bool = True, inplace: bool = True\n) -&gt; AbstractBlock | list[AbstractBlock]:\n    \"\"\"Set the trainability of all parameters in a block to a given value.\n\n    Args:\n        blocks (AbstractBlock | list[AbstractBlock]): Block or list of blocks for which\n            to set the trainable attribute\n        value (bool, optional): The value of the trainable attribute to assign to the input blocks\n        inplace (bool, optional): Whether to modify the block(s) in place or not. Currently, only\n\n    Raises:\n        NotImplementedError: if the `inplace` argument is set to False, the function will\n            raise  this exception\n\n    Returns:\n        AbstractBlock | list[AbstractBlock]: the input block or list of blocks with the trainable\n            attribute set to the given value\n    \"\"\"\n\n    if isinstance(blocks, AbstractBlock):\n        blocks = [blocks]\n\n    if inplace:\n        for block in blocks:\n            params: list[sympy.Basic] = parameters(block)\n            for p in params:\n                if not p.is_number:\n                    p.trainable = value\n    else:\n        raise NotImplementedError(\"Not inplace set_trainable is not yet available\")\n\n    return blocks if len(blocks) &gt; 1 else blocks[0]\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.validate","title":"<code>validate(block)</code>","text":"<p>Moves a block from global to local qubit numbers by adding PutBlocks.</p> <p>Reassigns qubit locations appropriately.</p>"},{"location":"api/transpile/#qadence.transpile.block.validate--example","title":"Example","text":"<pre><code>from qadence.blocks import chain\nfrom qadence.operations import X\nfrom qadence.transpile import validate\n\nx = chain(chain(X(0)), chain(X(1)))\nprint(x)\nprint(validate(x))\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 ChainBlock(0)\n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 ChainBlock(1)\n    \u2514\u2500\u2500 X(1)\nChainBlock(0,1)\n\u251c\u2500\u2500 put on (0)\n\u2502   \u2514\u2500\u2500 ChainBlock(0)\n\u2502       \u2514\u2500\u2500 put on (0)\n\u2502           \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 put on (1)\n    \u2514\u2500\u2500 ChainBlock(0)\n        \u2514\u2500\u2500 put on (0)\n            \u2514\u2500\u2500 X(0)\n</code></pre> Source code in <code>qadence/transpile/block.py</code> <pre><code>def validate(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Moves a block from global to local qubit numbers by adding PutBlocks.\n\n    Reassigns qubit locations appropriately.\n\n    # Example\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence.blocks import chain\n    from qadence.operations import X\n    from qadence.transpile import validate\n\n    x = chain(chain(X(0)), chain(X(1)))\n    print(x)\n    print(validate(x))\n    ```\n    \"\"\"\n    vblock: AbstractBlock\n    from qadence.transpile import reassign\n\n    if isinstance(block, ControlBlock):\n        vblock = deepcopy(block)\n        b: AbstractBlock\n        (b,) = block.blocks\n        b = reassign(b, {i: i - min(b.qubit_support) for i in b.qubit_support})\n        b = validate(b)\n        vblock.blocks = (b,)  # type: ignore[assignment]\n\n    elif isinstance(block, CompositeBlock):\n        blocks = []\n        for b in block.blocks:\n            mi, ma = min(b.qubit_support), max(b.qubit_support)\n            nb = reassign(b, {i: i - min(b.qubit_support) for i in b.qubit_support})\n            nb = validate(nb)\n            nb = PutBlock(nb, tuple(range(mi, ma + 1)))\n            blocks.append(nb)\n        try:\n            vblock = _construct(type(block), tuple(blocks))\n        except AssertionError as e:\n            if str(e) == \"Make sure blocks act on distinct qubits!\":\n                vblock = chain(*blocks)\n            else:\n                raise e\n\n    elif isinstance(block, PrimitiveBlock):\n        vblock = deepcopy(block)\n\n    else:\n        raise NotImplementedError\n\n    vblock.tag = block.tag\n    return vblock\n</code></pre>"},{"location":"api/types/","title":"Types","text":""},{"location":"api/types/#qadence-types","title":"Qadence Types","text":""},{"location":"api/types/#qadence.types.TArray","title":"<code>TArray = Union[Iterable, Tensor, np.ndarray]</code>  <code>module-attribute</code>","text":"<p>Union of common array types.</p>"},{"location":"api/types/#qadence.types.TGenerator","title":"<code>TGenerator = Union[Tensor, sympy.Array, sympy.Basic]</code>  <code>module-attribute</code>","text":"<p>Union of torch tensors and numpy arrays.</p>"},{"location":"api/types/#qadence.types.TNumber","title":"<code>TNumber = Union[int, float, complex, np.int64, np.float64]</code>  <code>module-attribute</code>","text":"<p>Union of python and numpy numeric types.</p>"},{"location":"api/types/#qadence.types.TParameter","title":"<code>TParameter = Union[TNumber, Tensor, sympy.Basic, str]</code>  <code>module-attribute</code>","text":"<p>Union of numbers, tensors, and parameter types.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo","title":"<code>AlgoHEvo</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Hamiltonian Evolution algorithms that can be used by the backend.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.EIG","title":"<code>EIG = 'EIG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using Hamiltonian diagonalization.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.EXP","title":"<code>EXP = 'EXP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using torch.matrix_exp on the generator matrix.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.RK4","title":"<code>RK4 = 'RK4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>4th order Runge-Kutta approximation.</p>"},{"location":"api/types/#qadence.types.AnalogNoise","title":"<code>AnalogNoise</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of noise protocol.</p>"},{"location":"api/types/#qadence.types.AnsatzType","title":"<code>AnsatzType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Ansatz types for variational circuits.</p>"},{"location":"api/types/#qadence.types.AnsatzType.HEA","title":"<code>HEA = 'hea'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hardware-efficient ansatz.</p>"},{"location":"api/types/#qadence.types.AnsatzType.IIA","title":"<code>IIA = 'iia'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Identity-Initialised Ansatz.</p>"},{"location":"api/types/#qadence.types.BasisSet","title":"<code>BasisSet</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Basis set for feature maps.</p>"},{"location":"api/types/#qadence.types.BasisSet.CHEBYSHEV","title":"<code>CHEBYSHEV = 'Chebyshev'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Chebyshev polynomials of the first kind.</p>"},{"location":"api/types/#qadence.types.BasisSet.FOURIER","title":"<code>FOURIER = 'Fourier'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fourier basis set.</p>"},{"location":"api/types/#qadence.types.DeviceType","title":"<code>DeviceType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported types of devices for Pulser backend.</p>"},{"location":"api/types/#qadence.types.DeviceType.IDEALIZED","title":"<code>IDEALIZED = 'IdealDevice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Idealized device, least realistic.</p>"},{"location":"api/types/#qadence.types.DeviceType.REALISTIC","title":"<code>REALISTIC = 'RealisticDevice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Device with realistic specs.</p>"},{"location":"api/types/#qadence.types.Endianness","title":"<code>Endianness</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The endianness convention to use.</p>"},{"location":"api/types/#qadence.types.Endianness.BIG","title":"<code>BIG = 'Big'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use Big endianness.</p>"},{"location":"api/types/#qadence.types.Endianness.LITTLE","title":"<code>LITTLE = 'Little'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use little endianness.</p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool","title":"<code>ExperimentTrackingTool</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool.MLFLOW","title":"<code>MLFLOW = 'mlflow'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the ml-flow experiment tracker.</p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool.TENSORBOARD","title":"<code>TENSORBOARD = 'tensorboard'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the tensorboard experiment tracker.</p>"},{"location":"api/types/#qadence.types.FigFormat","title":"<code>FigFormat</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available output formats for exporting visualized circuits to a file.</p>"},{"location":"api/types/#qadence.types.FigFormat.PDF","title":"<code>PDF = 'PDF'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PDF format.</p>"},{"location":"api/types/#qadence.types.FigFormat.PNG","title":"<code>PNG = 'PNG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PNG format.</p>"},{"location":"api/types/#qadence.types.FigFormat.SVG","title":"<code>SVG = 'SVG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>SVG format.</p>"},{"location":"api/types/#qadence.types.GenDAQC","title":"<code>GenDAQC</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The type of interaction for the DAQC transform.</p>"},{"location":"api/types/#qadence.types.GenDAQC.NN","title":"<code>NN = 'NN'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NN</p>"},{"location":"api/types/#qadence.types.GenDAQC.ZZ","title":"<code>ZZ = 'ZZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ZZ</p>"},{"location":"api/types/#qadence.types.InputDiffMode","title":"<code>InputDiffMode</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Derivative modes w.r.t inputs of UFAs.</p>"},{"location":"api/types/#qadence.types.InputDiffMode.AD","title":"<code>AD = 'ad'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Reverse automatic differentiation.</p>"},{"location":"api/types/#qadence.types.InputDiffMode.FD","title":"<code>FD = 'fd'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Central finite differencing.</p>"},{"location":"api/types/#qadence.types.Interaction","title":"<code>Interaction</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Interaction types used in.</p> <ul> <li><code>RydbergDevice</code>.</li> <li><code>hamiltonian_factory</code>.</li> </ul>"},{"location":"api/types/#qadence.types.Interaction.NN","title":"<code>NN = 'NN'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NN-Ising Interaction, N=(I-Z)/2.</p>"},{"location":"api/types/#qadence.types.Interaction.XY","title":"<code>XY = 'XY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>XY Interaction.</p>"},{"location":"api/types/#qadence.types.Interaction.XYZ","title":"<code>XYZ = 'XYZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>XYZ Interaction.</p>"},{"location":"api/types/#qadence.types.Interaction.ZZ","title":"<code>ZZ = 'ZZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ZZ-Ising Interaction.</p>"},{"location":"api/types/#qadence.types.LTSOrder","title":"<code>LTSOrder</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Lie-Trotter-Suzuki approximation order.</p>"},{"location":"api/types/#qadence.types.LTSOrder.BASIC","title":"<code>BASIC = 'BASIC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basic.</p>"},{"location":"api/types/#qadence.types.LTSOrder.ST2","title":"<code>ST2 = 'ST2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ST2.</p>"},{"location":"api/types/#qadence.types.LTSOrder.ST4","title":"<code>ST4 = 'ST4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ST4.</p>"},{"location":"api/types/#qadence.types.LatticeTopology","title":"<code>LatticeTopology</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Lattice topologies to choose from for the register.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.ALL_TO_ALL","title":"<code>ALL_TO_ALL = 'all_to_all'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>All to all- connected lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.ARBITRARY","title":"<code>ARBITRARY = 'arbitrary'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Arbitrarily-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.CIRCLE","title":"<code>CIRCLE = 'circle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Circular lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.HONEYCOMB_LATTICE","title":"<code>HONEYCOMB_LATTICE = 'honeycomb_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Honeycomb-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.LINE","title":"<code>LINE = 'line'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Line-format lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.RECTANGULAR_LATTICE","title":"<code>RECTANGULAR_LATTICE = 'rectangular_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Rectangular-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.SQUARE","title":"<code>SQUARE = 'square'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Square lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.TRIANGULAR_LATTICE","title":"<code>TRIANGULAR_LATTICE = 'triangular_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Triangular-shaped shape.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy","title":"<code>MultivariateStrategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Multivariate strategy for feature maps.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy.PARALLEL","title":"<code>PARALLEL = 'Parallel'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parallel strategy.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy.SERIES","title":"<code>SERIES = 'Series'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Serial strategy.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol","title":"<code>NoiseProtocol()</code>  <code>dataclass</code>","text":"<p>Type of noise protocol.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.ANALOG","title":"<code>ANALOG = AnalogNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied in analog blocks.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.DIGITAL","title":"<code>DIGITAL = DigitalNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied to digital blocks.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.READOUT","title":"<code>READOUT = ReadoutNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied on outputs of quantum programs.</p>"},{"location":"api/types/#qadence.types.ObservableTransform","title":"<code>ObservableTransform</code>","text":"<p>Observable transformation type.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.NONE","title":"<code>NONE = 'none'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>No transformation.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.RANGE","title":"<code>RANGE = 'range'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the given values as min and max.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.SCALE","title":"<code>SCALE = 'scale'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the given values as scale and shift.</p>"},{"location":"api/types/#qadence.types.OpName","title":"<code>OpName</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>A list of all available of digital-analog operations.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGENTANG","title":"<code>ANALOGENTANG = 'AnalogEntanglement'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog entanglement operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGINTERACTION","title":"<code>ANALOGINTERACTION = 'AnalogInteraction'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog interaction operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRX","title":"<code>ANALOGRX = 'AnalogRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RX operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRY","title":"<code>ANALOGRY = 'AnalogRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RY operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRZ","title":"<code>ANALOGRZ = 'AnalogRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RZ operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGSWAP","title":"<code>ANALOGSWAP = 'AnalogSWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog SWAP operation.</p>"},{"location":"api/types/#qadence.types.OpName.CNOT","title":"<code>CNOT = 'CNOT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The CNOT gate.</p>"},{"location":"api/types/#qadence.types.OpName.CPHASE","title":"<code>CPHASE = 'CPHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The controlled PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRX","title":"<code>CRX = 'CRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRY","title":"<code>CRY = 'CRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Controlled RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRZ","title":"<code>CRZ = 'CRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.CSWAP","title":"<code>CSWAP = 'CSWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control SWAP gate.</p>"},{"location":"api/types/#qadence.types.OpName.CZ","title":"<code>CZ = 'CZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The CZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.ENTANGLE","title":"<code>ENTANGLE = 'entangle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The entanglement operation.</p>"},{"location":"api/types/#qadence.types.OpName.H","title":"<code>H = 'H'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Hadamard gate.</p>"},{"location":"api/types/#qadence.types.OpName.HAMEVO","title":"<code>HAMEVO = 'HamEvo'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Hamiltonian Evolution operation.</p>"},{"location":"api/types/#qadence.types.OpName.I","title":"<code>I = 'I'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Identity gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCPHASE","title":"<code>MCPHASE = 'MCPHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRX","title":"<code>MCRX = 'MCRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRY","title":"<code>MCRY = 'MCRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRZ","title":"<code>MCRZ = 'MCRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCZ","title":"<code>MCZ = 'MCZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol CZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.N","title":"<code>N = 'N'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The N = (1/2)(I-Z) operator.</p>"},{"location":"api/types/#qadence.types.OpName.PHASE","title":"<code>PHASE = 'PHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.PROJ","title":"<code>PROJ = 'Projector'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The projector operation.</p>"},{"location":"api/types/#qadence.types.OpName.RX","title":"<code>RX = 'RX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.RY","title":"<code>RY = 'RY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.RZ","title":"<code>RZ = 'RZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.S","title":"<code>S = 'S'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The S gate.</p>"},{"location":"api/types/#qadence.types.OpName.SDAGGER","title":"<code>SDAGGER = 'SDagger'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The S dagger gate.</p>"},{"location":"api/types/#qadence.types.OpName.SWAP","title":"<code>SWAP = 'SWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The SWAP gate.</p>"},{"location":"api/types/#qadence.types.OpName.T","title":"<code>T = 'T'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The T gate.</p>"},{"location":"api/types/#qadence.types.OpName.TDAGGER","title":"<code>TDAGGER = 'TDagger'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The T dagger gate.</p>"},{"location":"api/types/#qadence.types.OpName.TOFFOLI","title":"<code>TOFFOLI = 'Toffoli'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Toffoli gate.</p>"},{"location":"api/types/#qadence.types.OpName.U","title":"<code>U = 'U'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The U gate.</p>"},{"location":"api/types/#qadence.types.OpName.X","title":"<code>X = 'X'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The X gate.</p>"},{"location":"api/types/#qadence.types.OpName.Y","title":"<code>Y = 'Y'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Y gate.</p>"},{"location":"api/types/#qadence.types.OpName.Z","title":"<code>Z = 'Z'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Z gate.</p>"},{"location":"api/types/#qadence.types.OpName.ZERO","title":"<code>ZERO = 'Zero'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The zero gate.</p>"},{"location":"api/types/#qadence.types.OverlapMethod","title":"<code>OverlapMethod</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Overlap Methods to choose from.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.COMPUTE_UNCOMPUTE","title":"<code>COMPUTE_UNCOMPUTE = 'compute_uncompute'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Compute-uncompute.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.EXACT","title":"<code>EXACT = 'exact'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exact.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.HADAMARD_TEST","title":"<code>HADAMARD_TEST = 'hadamard_test'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hadamard-test.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.JENSEN_SHANNON","title":"<code>JENSEN_SHANNON = 'jensen_shannon'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Jensen-shannon.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.SWAP_TEST","title":"<code>SWAP_TEST = 'swap_test'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Swap-test.</p>"},{"location":"api/types/#qadence.types.ParameterType","title":"<code>ParameterType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Parameter types available in qadence.</p>"},{"location":"api/types/#qadence.types.ParameterType.FEATURE","title":"<code>FEATURE = 'Feature'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>FeatureParameters act as input and are not trainable.</p>"},{"location":"api/types/#qadence.types.ParameterType.FIXED","title":"<code>FIXED = 'Fixed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fixed/ constant parameters are neither trainable nor act as input.</p>"},{"location":"api/types/#qadence.types.ParameterType.VARIATIONAL","title":"<code>VARIATIONAL = 'Variational'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>VariationalParameters are trainable.</p>"},{"location":"api/types/#qadence.types.QubitSupportType","title":"<code>QubitSupportType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Qubit support types.</p>"},{"location":"api/types/#qadence.types.QubitSupportType.GLOBAL","title":"<code>GLOBAL = 'global'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use global qubit support.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise","title":"<code>ReadoutNoise</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of readout protocol.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise.CORRELATED","title":"<code>CORRELATED = 'Correlated Readout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using a confusion matrix (2n, 2n) for corrupting bitstrings values.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise.INDEPENDENT","title":"<code>INDEPENDENT = 'Independent Readout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Simple readout protocols where each qubit is corrupted independently.</p>"},{"location":"api/types/#qadence.types.ResultType","title":"<code>ResultType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available data types for generating certain results.</p>"},{"location":"api/types/#qadence.types.ResultType.NUMPY","title":"<code>NUMPY = 'Numpy'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Numpy Array Type.</p>"},{"location":"api/types/#qadence.types.ResultType.STRING","title":"<code>STRING = 'String'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String Type.</p>"},{"location":"api/types/#qadence.types.ResultType.TORCH","title":"<code>TORCH = 'Torch'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Torch Tensor Type.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling","title":"<code>ReuploadScaling</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Scaling for data reuploads in feature maps.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.CONSTANT","title":"<code>CONSTANT = 'Constant'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Constant scaling.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.EXP","title":"<code>EXP = 'Exponential'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exponentially increasing scaling.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.TOWER","title":"<code>TOWER = 'Tower'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Linearly increasing scaling.</p>"},{"location":"api/types/#qadence.types.SerializationFormat","title":"<code>SerializationFormat</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available serialization formats for circuits.</p>"},{"location":"api/types/#qadence.types.SerializationFormat.JSON","title":"<code>JSON = 'JSON'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Json format.</p>"},{"location":"api/types/#qadence.types.SerializationFormat.PT","title":"<code>PT = 'PT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PT format used by Torch.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType","title":"<code>StateGeneratorType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Methods to generate random states.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.HAAR_MEASURE_FAST","title":"<code>HAAR_MEASURE_FAST = 'HaarMeasureFast'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>HaarMeasure.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.HAAR_MEASURE_SLOW","title":"<code>HAAR_MEASURE_SLOW = 'HaarMeasureSlow'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>HaarMeasure non-optimized version.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.RANDOM_ROTATIONS","title":"<code>RANDOM_ROTATIONS = 'RandomRotations'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random Rotations.</p>"},{"location":"api/types/#qadence.types.StrEnum","title":"<code>StrEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"api/types/#qadence.types.StrEnum.__str__","title":"<code>__str__()</code>","text":"<p>Used when dumping enum fields in a schema.</p> Source code in <code>qadence/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Used when dumping enum fields in a schema.\"\"\"\n    ret: str = self.value\n    return ret\n</code></pre>"},{"location":"api/types/#qadence.types.Strategy","title":"<code>Strategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Computing paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.ANALOG","title":"<code>ANALOG = 'Analog'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the analog paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.BDAQC","title":"<code>BDAQC = 'bDAQC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the banged digital-analog QC paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.DIGITAL","title":"<code>DIGITAL = 'Digital'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the digital paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.RYDBERG","title":"<code>RYDBERG = 'Rydberg'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the Rydberg QC paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.SDAQC","title":"<code>SDAQC = 'sDAQC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the step-wise digital-analog QC paradigm.</p>"},{"location":"api/types/#qadence.types.TensorType","title":"<code>TensorType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Tensor Types for converting blocks to tensors.</p>"},{"location":"api/types/#qadence.types.TensorType.DENSE","title":"<code>DENSE = 'Dense'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a block to a dense tensor.</p>"},{"location":"api/types/#qadence.types.TensorType.SPARSE","title":"<code>SPARSE = 'Sparse'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a observable block to a sparse tensor.</p>"},{"location":"api/types/#qadence.types.TensorType.SPARSEDIAGONAL","title":"<code>SPARSEDIAGONAL = 'SparseDiagonal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a diagonal observable block to a sparse diagonal if possible.</p>"},{"location":"api/backends/backend/","title":"Abstract backend","text":""},{"location":"api/backends/backend/#qadence.backend.Backend","title":"<code>Backend(name, supports_ad, support_bp, supports_adjoint, is_remote, with_measurements, native_endianness, engine, with_noise, config)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract class that defines the interface for the backends.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>backend unique string identifier</p> <p> TYPE: <code>BackendName</code> </p> <code>supports_ad</code> <p>whether or not the backend has a native autograd</p> <p> TYPE: <code>bool</code> </p> <code>supports_bp</code> <p>whether or not the backend has a native backprop</p> <p> TYPE: <code>bool</code> </p> <code>supports_adjoint</code> <p>Does the backend support native adjoint differentation.</p> <p> TYPE: <code>bool</code> </p> <code>is_remote</code> <p>whether computations are executed locally or remotely on this backend, useful when using cloud platforms where credentials are needed for example.</p> <p> TYPE: <code>bool</code> </p> <code>with_measurements</code> <p>whether it supports counts or not</p> <p> TYPE: <code>bool</code> </p> <code>with_noise</code> <p>whether to add realistic noise or not</p> <p> TYPE: <code>bool</code> </p> <code>native_endianness</code> <p>The native endianness of the backend</p> <p> TYPE: <code>Endianness</code> </p> <code>engine</code> <p>The underlying (native) automatic differentiation engine of the backend.</p> <p> TYPE: <code>Engine</code> </p>"},{"location":"api/backends/backend/#qadence.backend.Backend.circuit","title":"<code>circuit(circuit)</code>  <code>abstractmethod</code>","text":"<p>Converts an abstract <code>QuantumCircuit</code> to the native backend representation.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A circuit, for example: <code>QuantumCircuit(2, X(0))</code></p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>A converted circuit <code>c</code>. You can access the original, arbstract circuit via <code>c.abstract</code></p> <code>ConvertedCircuit</code> <p>and the converted (or backend native) circuit via <code>c.native</code>.</p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Converts an abstract `QuantumCircuit` to the native backend representation.\n\n    Arguments:\n        circuit: A circuit, for example: `QuantumCircuit(2, X(0))`\n\n    Returns:\n        A converted circuit `c`. You can access the original, arbstract circuit via `c.abstract`\n        and the converted (or backend *native*) circuit via `c.native`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.convert","title":"<code>convert(circuit, observable=None)</code>","text":"<p>Convert an abstract circuit and an optional observable to their native representation.</p> <p>Additionally, this function constructs an embedding function which maps from user-facing parameters to device parameters (read more on parameter embedding here).</p> Source code in <code>qadence/backend.py</code> <pre><code>def convert(\n    self, circuit: QuantumCircuit, observable: list[AbstractBlock] | AbstractBlock | None = None\n) -&gt; Converted:\n    \"\"\"Convert an abstract circuit and an optional observable to their native representation.\n\n    Additionally, this function constructs an embedding function which maps from\n    user-facing parameters to device parameters (read more on parameter embedding\n    [here][qadence.blocks.embedding.embedding]).\n    \"\"\"\n\n    def check_observable(obs_obj: Any) -&gt; AbstractBlock:\n        if isinstance(obs_obj, QubitOperator):\n            from qadence.blocks.manipulate import from_openfermion\n\n            assert len(obs_obj.terms) &gt; 0, \"Make sure to give a non-empty qubit hamiltonian\"\n\n            return from_openfermion(obs_obj)\n\n        elif isinstance(obs_obj, (CompositeBlock, PrimitiveBlock, ScaleBlock)):\n            from qadence.blocks.utils import block_is_qubit_hamiltonian\n\n            assert block_is_qubit_hamiltonian(\n                obs_obj\n            ), \"Make sure the QubitHamiltonian consists only of Pauli operators X, Y, Z, I\"\n            return obs_obj\n        raise TypeError(\n            \"qubit_hamiltonian should be a Pauli-like AbstractBlock or a QubitOperator\"\n        )\n\n    conv_circ = self.circuit(circuit)\n    circ_params, circ_embedding_fn = embedding(\n        conv_circ.abstract.block, self.config._use_gate_params, self.engine\n    )\n    params = circ_params\n    if observable is not None:\n        observable = observable if isinstance(observable, list) else [observable]\n        conv_obs = []\n        obs_embedding_fn_list = []\n\n        for obs in observable:\n            obs = check_observable(obs)\n            c_obs = self.observable(obs, max(circuit.n_qubits, obs.n_qubits))\n            obs_params, obs_embedding_fn = embedding(\n                c_obs.abstract, self.config._use_gate_params, self.engine\n            )\n            params.update(obs_params)\n            obs_embedding_fn_list.append(obs_embedding_fn)\n            conv_obs.append(c_obs)\n\n        def embedding_fn_dict(a: dict, b: dict) -&gt; dict:\n            embedding_dict = circ_embedding_fn(a, b)\n            for o in obs_embedding_fn_list:\n                embedding_dict.update(o(a, b))\n            return embedding_dict\n\n        return Converted(conv_circ, conv_obs, embedding_fn_dict, params)\n\n    def embedding_fn(a: dict, b: dict) -&gt; dict:\n        return circ_embedding_fn(a, b)\n\n    return Converted(conv_circ, None, embedding_fn, params)\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>  <code>abstractmethod</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.observable","title":"<code>observable(observable, n_qubits)</code>  <code>abstractmethod</code>","text":"<p>Converts an abstract observable (which is just an <code>AbstractBlock</code>) to the native backend.</p> <p>representation.</p> PARAMETER DESCRIPTION <code>observable</code> <p>An observable.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits the observable covers. This is typically <code>circuit.n_qubits</code>.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ConvertedObservable</code> <p>A converted observable <code>o</code>. You can access the original, arbstract observable via</p> <code>ConvertedObservable</code> <p><code>o.abstract</code> and the converted (or backend native) observable via <code>o.native</code>.</p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef observable(self, observable: AbstractBlock, n_qubits: int) -&gt; ConvertedObservable:\n    \"\"\"Converts an abstract observable (which is just an `AbstractBlock`) to the native backend.\n\n    representation.\n\n    Arguments:\n        observable: An observable.\n        n_qubits: Number of qubits the observable covers. This is typically `circuit.n_qubits`.\n\n    Returns:\n        A converted observable `o`. You can access the original, arbstract observable via\n        `o.abstract` and the converted (or backend *native*) observable via `o.native`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.run","title":"<code>run(circuit, param_values={}, state=None, endianness=Endianness.BIG, *args, **kwargs)</code>","text":"<p>Run a circuit and return the resulting wave function.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>dict[str, ArrayLike]</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting wavefunction.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>A list of Counter objects where each key represents a bitstring</p> <code>ArrayLike</code> <p>and its value the number of times it has been sampled from the given wave function.</p> Source code in <code>qadence/backend.py</code> <pre><code>def run(\n    self,\n    circuit: ConvertedCircuit,\n    param_values: dict[str, ArrayLike] = {},\n    state: Tensor | None = None,\n    endianness: Endianness = Endianness.BIG,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ArrayLike:\n    \"\"\"Run a circuit and return the resulting wave function.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        endianness: Endianness of the resulting wavefunction.\n\n    Returns:\n        A list of Counter objects where each key represents a bitstring\n        and its value the number of times it has been sampled from the given wave function.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.sample","title":"<code>sample(circuit, param_values={}, n_shots=1000, state=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>  <code>abstractmethod</code>","text":"<p>Sample bit strings.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>n_shots</code> <p>Number of shots to sample.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>An error mitigation protocol to apply.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    circuit: ConvertedCircuit,\n    param_values: dict[str, Tensor] = {},\n    n_shots: int = 1000,\n    state: ArrayLike | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; list[Counter]:\n    \"\"\"Sample bit strings.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        n_shots: Number of shots to sample.\n        state: Initial state.\n        noise: A noise model to use.\n        mitigation: An error mitigation protocol to apply.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration","title":"<code>BackendConfiguration(_use_gate_params=True, use_sparse_observable=False, use_gradient_checkpointing=False, use_single_qubit_composition=False, transpilation_passes=None)</code>  <code>dataclass</code>","text":""},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration.available_options","title":"<code>available_options()</code>","text":"<p>Return as a string the available fields with types of the configuration.</p> RETURNS DESCRIPTION <code>str</code> <p>a string with all the available fields, one per line</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>def available_options(self) -&gt; str:\n    \"\"\"Return as a string the available fields with types of the configuration.\n\n    Returns:\n        str: a string with all the available fields, one per line\n    \"\"\"\n    conf_msg = \"\"\n    for _field in fields(self):\n        if not _field.name.startswith(\"_\"):\n            conf_msg += (\n                f\"Name: {_field.name} - Type: {_field.type} - Default value: {_field.default}\\n\"\n            )\n    return conf_msg\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration.get_param_name","title":"<code>get_param_name(blk)</code>","text":"<p>Return parameter names for the current backend.</p> <p>Depending on which backend is in use this function returns either UUIDs or expressions of parameters.</p> Source code in <code>qadence/backend.py</code> <pre><code>def get_param_name(self, blk: AbstractBlock) -&gt; Tuple[str, ...]:\n    \"\"\"Return parameter names for the current backend.\n\n    Depending on which backend is in use this\n    function returns either UUIDs or expressions of parameters.\n    \"\"\"\n    param_ids: Tuple\n    # FIXME: better type hiearchy?\n    types = (TimeEvolutionBlock, ParametricBlock, ConstantAnalogRotation, InteractionBlock)\n    if not isinstance(blk, types):\n        raise TypeError(f\"Can not infer param name from {type(blk)}\")\n    else:\n        if self._use_gate_params:\n            param_ids = tuple(blk.parameters.uuids())\n        else:\n            param_ids = tuple(map(stringify, blk.parameters.expressions()))\n    return param_ids\n</code></pre>"},{"location":"api/backends/differentiable/","title":"DifferentiableBackend","text":""},{"location":"api/backends/differentiable/#qadence.engines.torch.differentiable_backend.DifferentiableBackend","title":"<code>DifferentiableBackend(backend, diff_mode=DiffMode.AD, **psr_args)</code>","text":"<p>               Bases: <code>DifferentiableBackend</code></p> <p>A class which wraps a QuantumBackend with the automatic differentation engine TORCH.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of the QuantumBackend type perform execution.</p> <p> TYPE: <code>Backend</code> </p> <code>diff_mode</code> <p>A differentiable mode supported by the differentiation engine.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>**psr_args</code> <p>Arguments that will be passed on to <code>DifferentiableExpectation</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>{}</code> </p> Source code in <code>qadence/engines/torch/differentiable_backend.py</code> <pre><code>def __init__(\n    self,\n    backend: QuantumBackend,\n    diff_mode: DiffMode = DiffMode.AD,\n    **psr_args: int | float | None,\n) -&gt; None:\n    super().__init__(backend=backend, engine=Engine.TORCH, diff_mode=diff_mode)\n    self.psr_args = psr_args\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.torch.differentiable_backend.DifferentiableBackend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>observable</code> <p>A converted observable as returned by <code>backend.observable</code>.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>The error mitigation to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/engines/torch/differentiable_backend.py</code> <pre><code>def expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        observable: A converted observable as returned by `backend.observable`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: The error mitigation to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    observable = observable if isinstance(observable, list) else [observable]\n    differentiable_expectation = DifferentiableExpectation(\n        backend=self.backend,\n        circuit=circuit,\n        observable=observable,\n        param_values=param_values,\n        state=state,\n        measurement=measurement,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n\n    if self.diff_mode == DiffMode.AD:\n        expectation = differentiable_expectation.ad\n    elif self.diff_mode == DiffMode.ADJOINT:\n        expectation = differentiable_expectation.adjoint\n    else:\n        try:\n            fns = get_gpsr_fns()\n            psr_fn = fns[self.diff_mode]\n        except KeyError:\n            raise ValueError(f\"{self.diff_mode} differentiation mode is not supported\")\n        expectation = partial(differentiable_expectation.psr, psr_fn=psr_fn, **self.psr_args)\n    return expectation()\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.jax.differentiable_backend.DifferentiableBackend","title":"<code>DifferentiableBackend(backend, diff_mode=DiffMode.AD, **psr_args)</code>","text":"<p>               Bases: <code>DifferentiableBackend</code></p> <p>A class which wraps a QuantumBackend with the automatic differentation engine JAX.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of the QuantumBackend type perform execution.</p> <p> TYPE: <code>Backend</code> </p> <code>diff_mode</code> <p>A differentiable mode supported by the differentiation engine.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>**psr_args</code> <p>Arguments that will be passed on to <code>DifferentiableExpectation</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>{}</code> </p> Source code in <code>qadence/engines/jax/differentiable_backend.py</code> <pre><code>def __init__(\n    self,\n    backend: Backend,\n    diff_mode: DiffMode = DiffMode.AD,\n    **psr_args: int | float | None,\n) -&gt; None:\n    super().__init__(backend=backend, engine=Engine.JAX, diff_mode=diff_mode)\n    self.psr_args = psr_args\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.jax.differentiable_backend.DifferentiableBackend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>observable</code> <p>A converted observable as returned by <code>backend.observable</code>.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>The error mitigation to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/engines/jax/differentiable_backend.py</code> <pre><code>def expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        observable: A converted observable as returned by `backend.observable`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: The error mitigation to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    observable = observable if isinstance(observable, list) else [observable]\n\n    if self.diff_mode == DiffMode.AD:\n        expectation = self.backend.expectation(circuit, observable, param_values, state)\n    else:\n        expectation = DifferentiableExpectation(\n            backend=self.backend,\n            circuit=circuit,\n            observable=observable,\n            param_values=param_values,\n            state=state,\n            measurement=measurement,\n            noise=noise,\n            mitigation=mitigation,\n            endianness=endianness,\n        ).psr()\n    return expectation\n</code></pre>"},{"location":"api/backends/pulser/","title":"Pulser","text":"<p>The Pulser backend features a basic integration with the pulse-level programming interface Pulser. This backend offers for now few simple operations which are translated into a valid, non time-dependent pulse sequence. In particular, one has access to:</p> <ul> <li>analog rotations: <code>AnalogRx</code> and <code>AnalogRy</code> blocks</li> <li>free evolution blocks (basically no pulse, just interaction): <code>AnalogWait</code> block</li> <li>a block for creating entangled states: <code>AnalogEntanglement</code></li> <li>digital rotation <code>Rx</code> and <code>Ry</code></li> </ul>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend.Backend","title":"<code>Backend(name=BackendName.PULSER, supports_ad=False, support_bp=False, supports_adjoint=False, is_remote=False, with_measurements=True, native_endianness=Endianness.BIG, engine=Engine.TORCH, with_noise=False, config=Configuration())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Backend</code></p> <p>The Pulser backend.</p>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend.create_register","title":"<code>create_register(register)</code>","text":"<p>Convert Qadence Register to Pulser Register.</p> Source code in <code>qadence/backends/pulser/backend.py</code> <pre><code>def create_register(register: Register) -&gt; PulserRegister:\n    \"\"\"Convert Qadence Register to Pulser Register.\"\"\"\n    coords = np.array(list(register.coords.values()))\n    return PulserRegister.from_coordinates(coords)\n</code></pre>"},{"location":"api/backends/pyqtorch/","title":"PyQTorch","text":"<p>Fast differentiable statevector emulator based on PyTorch. The code is open source, hosted on Github and maintained by Pasqal.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend","title":"<code>Backend(name=BackendName.PYQTORCH, supports_ad=True, support_bp=True, supports_adjoint=True, is_remote=False, with_measurements=True, native_endianness=Endianness.BIG, engine=Engine.TORCH, with_noise=False, config=Configuration())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Backend</code></p> <p>PyQTorch backend.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend.circuit","title":"<code>circuit(circuit)</code>","text":"<p>Return the converted circuit.</p> <p>Note that to get a representation with noise, noise should be passed within the config.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Original circuit</p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>ConvertedCircuit instance for backend.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Return the converted circuit.\n\n    Note that to get a representation with noise, noise\n    should be passed within the config.\n\n    Args:\n        circuit (QuantumCircuit): Original circuit\n\n    Returns:\n        ConvertedCircuit: ConvertedCircuit instance for backend.\n    \"\"\"\n    passes = self.config.transpilation_passes\n    if passes is None:\n        passes = default_passes(self.config)\n\n    original_circ = circuit\n    if len(passes) &gt; 0:\n        circuit = transpile(*passes)(circuit)\n    # Setting noise in the circuit.\n    if self.config.noise:\n        set_noise(circuit, self.config.noise)\n\n    ops = convert_block(circuit.block, n_qubits=circuit.n_qubits, config=self.config)\n    readout_noise = (\n        convert_readout_noise(circuit.n_qubits, self.config.noise)\n        if self.config.noise\n        else None\n    )\n    if self.config.dropout_probability == 0:\n        native = pyq.QuantumCircuit(\n            circuit.n_qubits,\n            ops,\n            readout_noise,\n        )\n    else:\n        native = pyq.DropoutQuantumCircuit(\n            circuit.n_qubits,\n            ops,\n            readout_noise,\n            dropout_prob=self.config.dropout_probability,\n            dropout_mode=self.config.dropout_mode,\n        )\n    return ConvertedCircuit(native=native, abstract=circuit, original=original_circ)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend.convert","title":"<code>convert(circuit, observable=None)</code>","text":"<p>Convert an abstract circuit and an optional observable to their native representation.</p> <p>Additionally, this function constructs an embedding function which maps from user-facing parameters to device parameters (read more on parameter embedding here).</p> Source code in <code>qadence/backend.py</code> <pre><code>def convert(\n    self, circuit: QuantumCircuit, observable: list[AbstractBlock] | AbstractBlock | None = None\n) -&gt; Converted:\n    \"\"\"Convert an abstract circuit and an optional observable to their native representation.\n\n    Additionally, this function constructs an embedding function which maps from\n    user-facing parameters to device parameters (read more on parameter embedding\n    [here][qadence.blocks.embedding.embedding]).\n    \"\"\"\n\n    def check_observable(obs_obj: Any) -&gt; AbstractBlock:\n        if isinstance(obs_obj, QubitOperator):\n            from qadence.blocks.manipulate import from_openfermion\n\n            assert len(obs_obj.terms) &gt; 0, \"Make sure to give a non-empty qubit hamiltonian\"\n\n            return from_openfermion(obs_obj)\n\n        elif isinstance(obs_obj, (CompositeBlock, PrimitiveBlock, ScaleBlock)):\n            from qadence.blocks.utils import block_is_qubit_hamiltonian\n\n            assert block_is_qubit_hamiltonian(\n                obs_obj\n            ), \"Make sure the QubitHamiltonian consists only of Pauli operators X, Y, Z, I\"\n            return obs_obj\n        raise TypeError(\n            \"qubit_hamiltonian should be a Pauli-like AbstractBlock or a QubitOperator\"\n        )\n\n    conv_circ = self.circuit(circuit)\n    circ_params, circ_embedding_fn = embedding(\n        conv_circ.abstract.block, self.config._use_gate_params, self.engine\n    )\n    params = circ_params\n    if observable is not None:\n        observable = observable if isinstance(observable, list) else [observable]\n        conv_obs = []\n        obs_embedding_fn_list = []\n\n        for obs in observable:\n            obs = check_observable(obs)\n            c_obs = self.observable(obs, max(circuit.n_qubits, obs.n_qubits))\n            obs_params, obs_embedding_fn = embedding(\n                c_obs.abstract, self.config._use_gate_params, self.engine\n            )\n            params.update(obs_params)\n            obs_embedding_fn_list.append(obs_embedding_fn)\n            conv_obs.append(c_obs)\n\n        def embedding_fn_dict(a: dict, b: dict) -&gt; dict:\n            embedding_dict = circ_embedding_fn(a, b)\n            for o in obs_embedding_fn_list:\n                embedding_dict.update(o(a, b))\n            return embedding_dict\n\n        return Converted(conv_circ, conv_obs, embedding_fn_dict, params)\n\n    def embedding_fn(a: dict, b: dict) -&gt; dict:\n        return circ_embedding_fn(a, b)\n\n    return Converted(conv_circ, None, embedding_fn, params)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_block_and_readout_noises","title":"<code>set_block_and_readout_noises(circuit, noise, config)</code>","text":"<p>Add noise on blocks and readout on circuit.</p> <p>We first start by adding noise to the abstract blocks. Then we do a conversion to their native representation. Finally, we add readout.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>noise</code> <p>Noise to add.</p> <p> TYPE: <code>NoiseHandler | None</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_block_and_readout_noises(\n    circuit: ConvertedCircuit, noise: NoiseHandler | None, config: Configuration\n) -&gt; None:\n    \"\"\"Add noise on blocks and readout on circuit.\n\n    We first start by adding noise to the abstract blocks. Then we do a conversion to their\n    native representation. Finally, we add readout.\n\n    Args:\n        circuit (ConvertedCircuit): Input circuit.\n        noise (NoiseHandler | None): Noise to add.\n    \"\"\"\n    if noise:\n        set_noise(circuit, noise)\n        set_noise_abstract_to_native(circuit, config)\n        set_readout_noise(circuit, noise)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_noise_abstract_to_native","title":"<code>set_noise_abstract_to_native(circuit, config)</code>","text":"<p>Set noise in native blocks from the abstract ones with noise.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input converted circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_noise_abstract_to_native(circuit: ConvertedCircuit, config: Configuration) -&gt; None:\n    \"\"\"Set noise in native blocks from the abstract ones with noise.\n\n    Args:\n        circuit (ConvertedCircuit): Input converted circuit.\n    \"\"\"\n    ops = convert_block(circuit.abstract.block, n_qubits=circuit.native.n_qubits, config=config)\n    circuit.native = pyq.QuantumCircuit(circuit.native.n_qubits, ops, circuit.native.readout_noise)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_readout_noise","title":"<code>set_readout_noise(circuit, noise)</code>","text":"<p>Set readout noise in place in native.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input converted circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>noise</code> <p>Noise.</p> <p> TYPE: <code>NoiseHandler | None</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_readout_noise(circuit: ConvertedCircuit, noise: NoiseHandler) -&gt; None:\n    \"\"\"Set readout noise in place in native.\n\n    Args:\n        circuit (ConvertedCircuit):  Input converted circuit.\n        noise (NoiseHandler | None): Noise.\n    \"\"\"\n    readout = convert_readout_noise(circuit.abstract.n_qubits, noise)\n    if readout:\n        circuit.native.readout_noise = readout\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration","title":"<code>Configuration(_use_gate_params=True, use_sparse_observable=False, use_gradient_checkpointing=False, use_single_qubit_composition=False, transpilation_passes=None, algo_hevo=AlgoHEvo.EXP, ode_solver=SolverType.DP5_SE, n_steps_hevo=100, loop_expectation=False, noise=None, dropout_probability=0.0, dropout_mode=DropoutMode.ROTATIONAL)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BackendConfiguration</code></p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.algo_hevo","title":"<code>algo_hevo: AlgoHEvo = AlgoHEvo.EXP</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine which kind of Hamiltonian evolution algorithm to use.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.dropout_mode","title":"<code>dropout_mode: DropoutMode = DropoutMode.ROTATIONAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of quantum dropout to perform.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.dropout_probability","title":"<code>dropout_probability: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantum dropout probability (0 means no dropout).</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.loop_expectation","title":"<code>loop_expectation: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When computing batches of expectation values, only allocate one wavefunction.</p> <p>Loop over the batch of parameters to only allocate a single wavefunction at any given time.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.n_steps_hevo","title":"<code>n_steps_hevo: int = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default number of steps for the Hamiltonian evolution.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.noise","title":"<code>noise: NoiseHandler | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NoiseHandler containing readout noise applied in backend.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.ode_solver","title":"<code>ode_solver: SolverType = SolverType.DP5_SE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine which ODE solver to use for time-dependent blocks.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.use_gradient_checkpointing","title":"<code>use_gradient_checkpointing: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use gradient checkpointing.</p> <p>Recommended for higher-order optimization tasks.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.use_single_qubit_composition","title":"<code>use_single_qubit_composition: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Composes chains of single qubit gates into a single matmul if possible.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.supported_gates","title":"<code>supported_gates = list(set(OpName.list()) - set([OpName.TDAGGER]))</code>  <code>module-attribute</code>","text":"<p>The set of supported gates.</p> <p>Tdagger is currently not supported.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_block","title":"<code>convert_block(block, n_qubits=None, config=None)</code>","text":"<p>Convert block to native Pyqtorch representation.</p> PARAMETER DESCRIPTION <code>block</code> <p>Block to convert.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Backend configuration instance. Defaults to None.</p> <p> TYPE: <code>Configuration</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>For non supported blocks.</p> RETURNS DESCRIPTION <code>Sequence[Module | Tensor | str | Expr]</code> <p>Sequence[Module | Tensor | str | sympy.Expr]: List of native operations.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_block(\n    block: AbstractBlock,\n    n_qubits: int = None,\n    config: Configuration = None,\n) -&gt; Sequence[Module | Tensor | str | sympy.Expr]:\n    \"\"\"Convert block to native Pyqtorch representation.\n\n    Args:\n        block (AbstractBlock): Block to convert.\n        n_qubits (int, optional): Number of qubits. Defaults to None.\n        config (Configuration, optional): Backend configuration instance. Defaults to None.\n\n    Raises:\n        NotImplementedError: For non supported blocks.\n\n    Returns:\n        Sequence[Module | Tensor | str | sympy.Expr]: List of native operations.\n    \"\"\"\n    if isinstance(block, (Tensor, str, sympy.Expr)):  # case for hamevo generators\n        if isinstance(block, Tensor):\n            block = block.permute(1, 2, 0)  # put batch size in the back\n        return [block]\n    qubit_support = block.qubit_support\n    if n_qubits is None:\n        n_qubits = max(qubit_support) + 1\n\n    if config is None:\n        config = Configuration()\n\n    noise: NoiseHandler | None = None\n    if hasattr(block, \"noise\") and block.noise:\n        noise = convert_digital_noise(block.noise)\n\n    if isinstance(block, ScaleBlock):\n        scaled_ops = convert_block(block.block, n_qubits, config)\n        scale = extract_parameter(block, config=config)\n\n        # replace underscore by dot when underscore is between two numbers in string\n        if isinstance(scale, str):\n            scale = replace_underscore_floats(scale)\n\n        if isinstance(scale, str) and not config._use_gate_params:\n            param = sympy_to_pyq(sympy.parse_expr(scale))\n        else:\n            param = scale\n\n        return [pyq.Scale(pyq.Sequence(scaled_ops), param)]\n\n    elif isinstance(block, TimeEvolutionBlock):\n        duration = block.duration  # type: ignore [attr-defined]\n        if getattr(block.generator, \"is_time_dependent\", False):\n            config._use_gate_params = False\n            duration = config.get_param_name(block)[1]\n            generator = convert_block(block.generator, config=config)[0]  # type: ignore [arg-type]\n        elif isinstance(block.generator, sympy.Basic):\n            generator = config.get_param_name(block)[1]\n\n        elif isinstance(block.generator, Tensor):\n            m = block.generator.to(dtype=cdouble)\n            generator = convert_block(\n                MatrixBlock(\n                    m,\n                    qubit_support=qubit_support,\n                    check_unitary=False,\n                    check_hermitian=True,\n                )\n            )[0]\n        else:\n            generator = convert_block(block.generator, n_qubits, config)[0]  # type: ignore[arg-type]\n        time_param = config.get_param_name(block)[0]\n\n        # convert noise operators here\n        noise_operators: list = [\n            convert_block(noise_block, config=config)[0] for noise_block in block.noise_operators\n        ]\n        if len(noise_operators) &gt; 0:\n            # squeeze batch size for noise operators\n            noise_operators = [\n                pyq_op.tensor(full_support=qubit_support).squeeze(-1) for pyq_op in noise_operators\n            ]\n\n        return [\n            pyq.HamiltonianEvolution(\n                qubit_support=qubit_support,\n                generator=generator,\n                time=time_param,\n                cache_length=0,\n                duration=duration,\n                solver=config.ode_solver,\n                steps=config.n_steps_hevo,\n                noise_operators=noise_operators,\n            )\n        ]\n\n    elif isinstance(block, MatrixBlock):\n        return [pyq.primitives.Primitive(block.matrix, block.qubit_support, noise=noise)]\n    elif isinstance(block, CompositeBlock):\n        ops = list(flatten(*(convert_block(b, n_qubits, config) for b in block.blocks)))\n        if isinstance(block, AddBlock):\n            return [pyq.Add(ops)]  # add\n        elif is_single_qubit_chain(block) and config.use_single_qubit_composition:\n            return [pyq.Merge(ops)]  # for chains of single qubit ops on the same qubit\n        else:\n            return [pyq.Sequence(ops)]  # for kron and chain\n    elif isinstance(block, tuple(non_unitary_gateset)):\n        if isinstance(block, ProjectorBlock):\n            projector = getattr(pyq, block.name)\n            if block.name == OpName.N:\n                return [projector(target=qubit_support, noise=noise)]\n            else:\n                return [\n                    projector(\n                        qubit_support=qubit_support,\n                        ket=block.ket,\n                        bra=block.bra,\n                        noise=noise,\n                    )\n                ]\n        else:\n            return [getattr(pyq, block.name)(qubit_support[0])]\n    elif isinstance(block, tuple(single_qubit_gateset)):\n        pyq_cls = getattr(pyq, block.name)\n        if isinstance(block, ParametricBlock):\n            if isinstance(block, U):\n                op = pyq_cls(\n                    qubit_support[0],\n                    *config.get_param_name(block),\n                    noise=noise,\n                )\n            else:\n                param = extract_parameter(block, config)\n                op = pyq_cls(qubit_support[0], param, noise=noise)\n        else:\n            op = pyq_cls(qubit_support[0], noise=noise)  # type: ignore [attr-defined]\n        return [op]\n    elif isinstance(block, tuple(two_qubit_gateset)):\n        pyq_cls = getattr(pyq, block.name)\n        if isinstance(block, ParametricBlock):\n            op = pyq_cls(\n                qubit_support[0],\n                qubit_support[1],\n                extract_parameter(block, config),\n                noise=noise,\n            )\n        else:\n            op = pyq_cls(\n                qubit_support[0], qubit_support[1], noise=noise  # type: ignore [attr-defined]\n            )\n        return [op]\n    elif isinstance(block, tuple(three_qubit_gateset) + tuple(multi_qubit_gateset)):\n        block_name = block.name[1:] if block.name.startswith(\"M\") else block.name\n        pyq_cls = getattr(pyq, block_name)\n        if isinstance(block, ParametricBlock):\n            op = pyq_cls(\n                qubit_support[:-1],\n                qubit_support[-1],\n                extract_parameter(block, config),\n                noise=noise,\n            )\n        else:\n            if \"CSWAP\" in block_name:\n                op = pyq_cls(\n                    qubit_support[:-2], qubit_support[-2:], noise=noise  # type: ignore [attr-defined]\n                )\n            else:\n                op = pyq_cls(\n                    qubit_support[:-1], qubit_support[-1], noise=noise  # type: ignore [attr-defined]\n                )\n        return [op]\n    else:\n        raise NotImplementedError(\n            f\"Non supported operation of type {type(block)}. \"\n            \"In case you are trying to run an `AnalogBlock`, make sure you \"\n            \"specify the `device_specs` in your `Register` first.\"\n        )\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_digital_noise","title":"<code>convert_digital_noise(noise)</code>","text":"<p>Convert the digital noise into pyqtorch NoiseProtocol.</p> PARAMETER DESCRIPTION <code>noise</code> <p>Noise to convert.</p> <p> TYPE: <code>NoiseHandler</code> </p> RETURNS DESCRIPTION <code>NoiseProtocol | None</code> <p>pyq.noise.NoiseProtocol | None: Pyqtorch native noise protocol if there are any digital noise protocols.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_digital_noise(noise: NoiseHandler) -&gt; pyq.noise.NoiseProtocol | None:\n    \"\"\"Convert the digital noise into pyqtorch NoiseProtocol.\n\n    Args:\n        noise (NoiseHandler): Noise to convert.\n\n    Returns:\n        pyq.noise.NoiseProtocol | None: Pyqtorch native noise protocol\n            if there are any digital noise protocols.\n    \"\"\"\n    digital_part = noise.filter(NoiseProtocol.DIGITAL)\n    if digital_part is None:\n        return None\n    return pyq.noise.NoiseProtocol(\n        [\n            pyq.noise.NoiseProtocol(proto, option.get(\"error_probability\"))\n            for proto, option in zip(digital_part.protocol, digital_part.options)\n        ]\n    )\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_readout_noise","title":"<code>convert_readout_noise(n_qubits, noise)</code>","text":"<p>Convert the readout noise into pyqtorch ReadoutNoise.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Number of qubits</p> <p> TYPE: <code>int</code> </p> <code>noise</code> <p>Noise to convert.</p> <p> TYPE: <code>NoiseHandler</code> </p> RETURNS DESCRIPTION <code>ReadoutNoise | None</code> <p>pyq.noise.ReadoutNoise | None: Pyqtorch native ReadoutNoise instance if readout is is noise.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_readout_noise(n_qubits: int, noise: NoiseHandler) -&gt; pyq.noise.ReadoutNoise | None:\n    \"\"\"Convert the readout noise into pyqtorch ReadoutNoise.\n\n    Args:\n        n_qubits (int): Number of qubits\n        noise (NoiseHandler):  Noise to convert.\n\n    Returns:\n        pyq.noise.ReadoutNoise | None: Pyqtorch native ReadoutNoise instance\n            if readout is is noise.\n    \"\"\"\n    readout_part = noise.filter(NoiseProtocol.READOUT)\n    if readout_part is None:\n        return None\n\n    if readout_part.protocol[0] == NoiseProtocol.READOUT.INDEPENDENT:\n        return pyq.noise.ReadoutNoise(n_qubits, **readout_part.options[0])\n    else:\n        return pyq.noise.CorrelatedReadoutNoise(**readout_part.options[0])\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.extract_parameter","title":"<code>extract_parameter(block, config)</code>","text":"<p>Extract the parameter as string or its tensor value.</p> PARAMETER DESCRIPTION <code>block</code> <p>Block to extract parameter from.</p> <p> TYPE: <code>ScaleBlock | ParametricBlock</code> </p> <code>config</code> <p>Configuration instance.</p> <p> TYPE: <code>Configuration</code> </p> RETURNS DESCRIPTION <code>str | Tensor</code> <p>str | Tensor: Parameter value or symbol.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def extract_parameter(block: ScaleBlock | ParametricBlock, config: Configuration) -&gt; str | Tensor:\n    \"\"\"Extract the parameter as string or its tensor value.\n\n    Args:\n        block (ScaleBlock | ParametricBlock): Block to extract parameter from.\n        config (Configuration): Configuration instance.\n\n    Returns:\n        str | Tensor: Parameter value or symbol.\n    \"\"\"\n    if not block.is_parametric:\n        tensor_val = tensor([block.parameters.parameter], dtype=complex64)\n        return (\n            tensor([block.parameters.parameter], dtype=float64)\n            if torch.all(tensor_val.imag == 0)\n            else tensor_val\n        )\n\n    return config.get_param_name(block)[0]\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.replace_underscore_floats","title":"<code>replace_underscore_floats(s)</code>","text":"<p>Replace underscores with periods for all floats in given string.</p> <p>Needed for correct parsing of string by sympy parser.</p> PARAMETER DESCRIPTION <code>s</code> <p>string expression</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>transformed string expression</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def replace_underscore_floats(s: str) -&gt; str:\n    \"\"\"Replace underscores with periods for all floats in given string.\n\n    Needed for correct parsing of string by sympy parser.\n\n    Args:\n        s (str): string expression\n\n    Returns:\n        str: transformed string expression\n    \"\"\"\n\n    # Regular expression to match floats written with underscores instead of dots\n    float_with_underscore_pattern = r\"\"\"\n        (?&lt;!\\w)            # Negative lookbehind to ensure not part of a word\n        -?                 # Optional negative sign\n        \\d+                # One or more digits (before underscore)\n        _                  # The underscore acting as decimal separator\n        \\d+                # One or more digits (after underscore)\n        ([eE][-+]?\\d+)?    # Optional exponent part for scientific notation\n        (?!\\w)             # Negative lookahead to ensure not part of a word\n    \"\"\"\n\n    # Function to replace the underscore with a dot\n    def underscore_to_dot(match: re.Match) -&gt; Any:\n        return match.group(0).replace(\"_\", \".\")\n\n    # Compile the regular expression\n    pattern = re.compile(float_with_underscore_pattern, re.VERBOSE)\n\n    return pattern.sub(underscore_to_dot, s)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.sympy_to_pyq","title":"<code>sympy_to_pyq(expr)</code>","text":"<p>Convert sympy expression to pyqtorch ConcretizedCallable object.</p> PARAMETER DESCRIPTION <code>expr</code> <p>sympy expression</p> <p> TYPE: <code>Expr</code> </p> RETURNS DESCRIPTION <code>ConcretizedCallable</code> <p>expression encoded as ConcretizedCallable</p> <p> TYPE: <code>ConcretizedCallable | Tensor</code> </p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def sympy_to_pyq(expr: sympy.Expr) -&gt; ConcretizedCallable | Tensor:\n    \"\"\"Convert sympy expression to pyqtorch ConcretizedCallable object.\n\n    Args:\n        expr (sympy.Expr): sympy expression\n\n    Returns:\n        ConcretizedCallable: expression encoded as ConcretizedCallable\n    \"\"\"\n\n    # base case - independent argument\n    if len(expr.args) == 0:\n        try:\n            res = torch.as_tensor(float(expr))\n        except Exception as e:\n            res = str(expr)\n\n            if \"/\" in res:  # Found a rational\n                res = torch.as_tensor(float(sympy.Rational(res).evalf()))\n        return res\n\n    # Recursively iterate through current function arguments\n    all_results = []\n    for arg in expr.args:\n        res = sympy_to_pyq(arg)\n        all_results.append(res)\n\n    # deal with multi-argument (&gt;2) sympy functions: converting to nested\n    # ConcretizedCallable objects\n    if len(all_results) &gt; 2:\n\n        def fn(x: str | ConcretizedCallable, y: str | ConcretizedCallable) -&gt; Callable:\n            return partial(ConcretizedCallable, call_name=SYMPY_TO_PYQ_MAPPING[expr.func])(  # type: ignore [no-any-return]\n                abstract_args=[x, y]\n            )\n\n        concretized_callable = reduce(fn, all_results)\n    else:\n        concretized_callable = ConcretizedCallable(SYMPY_TO_PYQ_MAPPING[expr.func], all_results)\n    return concretized_callable\n</code></pre>"},{"location":"content/backends/","title":"Backends","text":"<p>Backends allow execution of Qadence abstract quantum circuits. They could be chosen from a variety of simulators, emulators and hardware and can enable circuit differentiability. The primary way to interact and configure a backend is via the high-level API <code>QuantumModel</code>.</p> <p>Not all backends are equivalent</p> <p>Not all backends support the same set of operations, especially while executing analog blocks. Qadence will throw descriptive errors in such cases.</p>"},{"location":"content/backends/#execution-backends","title":"Execution backends","text":"<p>PyQTorch: An efficient, large-scale simulator designed for quantum machine learning, seamlessly integrated with the popular PyTorch deep learning framework for automatic differentiability. It also offers analog computing for time-(in)dependent pulses. See <code>PyQTorchBackend</code>.</p> <p>Pulser: A Python library for pulse-level/analog control of neutral atom devices. Execution via QuTiP. See <code>PulserBackend</code>.</p> <p>More: Proprietary Qadence extensions provide more high-performance backends based on tensor networks or differentiation engines. For more enquiries, please contact: <code>info@pasqal.com</code>.</p>"},{"location":"content/backends/#differentiation-backend","title":"Differentiation backend","text":"<p>The <code>DifferentiableBackend</code> class enables different differentiation modes for the given backend. This can be chosen from two types:</p> <ul> <li>Automatic differentiation (AD): available for PyTorch based backends (PyQTorch).</li> <li>Parameter Shift Rules (PSR): available for all backends. See this section for more information on differentiability and PSR.</li> </ul> <p>In practice, only a <code>diff_mode</code> should be provided in the <code>QuantumModel</code>. Please note that <code>diff_mode</code> defaults to <code>None</code>:</p> <pre><code>import sympy\nimport torch\nfrom qadence import Parameter, RX, RZ, Z, CNOT, QuantumCircuit, QuantumModel, chain, BackendName, DiffMode\n\nx = Parameter(\"x\", trainable=False)\ny = Parameter(\"y\", trainable=False)\nfm = chain(\n    RX(0, 3 * x),\n    RX(0, x),\n    RZ(1, sympy.exp(y)),\n    RX(0, 3.14),\n    RZ(1, \"theta\")\n)\n\nansatz = CNOT(0, 1)\nblock = chain(fm, ansatz)\n\ncircuit = QuantumCircuit(2, block)\n\nobservable = Z(0)\n\n# DiffMode.GPSR is available for any backend.\n# DiffMode.AD is only available for natively differentiable backends.\nmodel = QuantumModel(circuit, observable, backend=BackendName.PYQTORCH, diff_mode=DiffMode.GPSR)\n\n# Get some values for the feature parameters.\nvalues = {\"x\": (x := torch.tensor([0.5], requires_grad=True)), \"y\": torch.tensor([0.1])}\n\n# Compute expectation.\nexp = model.expectation(values)\n\n# Differentiate the expectation wrt x.\ndexp_dx = torch.autograd.grad(exp, x, torch.ones_like(exp))\n</code></pre> <pre><code>dexp_dx = (tensor([3.6398]),)\n</code></pre>"},{"location":"content/backends/#low-level-backend_factory-interface","title":"Low-level <code>backend_factory</code> interface","text":"<p>Every backend in Qadence inherits from the abstract <code>Backend</code> class: <code>Backend</code> and implement the following methods:</p> <ul> <li><code>run</code>: propagate the initial state according to the quantum circuit and return the final wavefunction object.</li> <li><code>sample</code>: sample from a circuit.</li> <li><code>expectation</code>: computes the expectation of a circuit given an observable.</li> <li><code>convert</code>: convert the abstract <code>QuantumCircuit</code> object to its backend-native representation including a backend specific parameter embedding function.</li> </ul> <p>Backends are purely functional objects which take as input the values for the circuit parameters and return the desired output from a call to a method. In order to use a backend directly, embedded parameters must be supplied as they are returned by the backend specific embedding function.</p> <p>Here is a simple demonstration of the use of the PyQTorch backend to execute a circuit in non-differentiable mode:</p> <pre><code>from qadence import QuantumCircuit, FeatureParameter, RX, RZ, CNOT, hea, chain\n\n# Construct a feature map.\nx = FeatureParameter(\"x\")\nz = FeatureParameter(\"y\")\nfm = chain(RX(0, 3 * x), RZ(1, z), CNOT(0, 1))\n\n# Construct a circuit with an hardware-efficient ansatz.\ncircuit = QuantumCircuit(3, fm, hea(3,1))\n</code></pre> <p>The abstract <code>QuantumCircuit</code> can now be converted to its native representation via the PyQTorch backend.</p> <pre><code>from qadence import backend_factory\n\n# Use only PyQtorch in non-differentiable mode:\nbackend = backend_factory(\"pyqtorch\")\n\n# The `Converted` object\n# (contains a `ConvertedCircuit` with the original and native representation)\nconv = backend.convert(circuit)\n</code></pre> <pre><code>conv.circuit.original = ChainBlock(0,1,2)\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 RX(0) [params: ['3*x']]\n\u2502   \u251c\u2500\u2500 RZ(1) [params: ['y']]\n\u2502   \u2514\u2500\u2500 CNOT(0, 1)\n\u2514\u2500\u2500 ChainBlock(0,1,2) [tag: HEA]\n    \u251c\u2500\u2500 ChainBlock(0,1,2)\n    \u2502   \u251c\u2500\u2500 KronBlock(0,1,2)\n    \u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n    \u2502   \u2502   \u251c\u2500\u2500 RX(1) [params: ['theta_1']]\n    \u2502   \u2502   \u2514\u2500\u2500 RX(2) [params: ['theta_2']]\n    \u2502   \u251c\u2500\u2500 KronBlock(0,1,2)\n    \u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_3']]\n    \u2502   \u2502   \u251c\u2500\u2500 RY(1) [params: ['theta_4']]\n    \u2502   \u2502   \u2514\u2500\u2500 RY(2) [params: ['theta_5']]\n    \u2502   \u2514\u2500\u2500 KronBlock(0,1,2)\n    \u2502       \u251c\u2500\u2500 RX(0) [params: ['theta_6']]\n    \u2502       \u251c\u2500\u2500 RX(1) [params: ['theta_7']]\n    \u2502       \u2514\u2500\u2500 RX(2) [params: ['theta_8']]\n    \u2514\u2500\u2500 ChainBlock(0,1,2)\n        \u251c\u2500\u2500 KronBlock(0,1)\n        \u2502   \u2514\u2500\u2500 CNOT(0, 1)\n        \u2514\u2500\u2500 KronBlock(1,2)\n            \u2514\u2500\u2500 CNOT(1, 2)\nconv.circuit.native = QuantumCircuit(\n  (operations): ModuleList(\n    (0): Sequence(\n      (operations): ModuleList(\n        (0): Sequence(\n          (operations): ModuleList(\n            (0): RX(target: (0,), param: c71ae995-1761-43d7-882b-f66d63db78dd)\n            (1): RZ(target: (1,), param: 95c60058-8e2e-4c35-996c-596be26ad63d)\n            (2): CNOT(control: (0,), target: (1,))\n          )\n        )\n        (1): Sequence(\n          (operations): ModuleList(\n            (0): Sequence(\n              (operations): ModuleList(\n                (0): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (0,), param: 3ac4fc11-c297-443d-9395-6a5f6221ca8f)\n                    (1): RY(target: (0,), param: 3479ca8f-8931-43a5-9c3d-d4b657b0abd4)\n                    (2): RX(target: (0,), param: 641f7a15-ba03-4f1c-8c73-86a47f0f78c1)\n                  )\n                )\n                (1): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (1,), param: 8ee4f763-67d8-4603-a0ef-fa9ea4be097e)\n                    (1): RY(target: (1,), param: d328c437-9f6a-4bf6-a504-9bb659401819)\n                    (2): RX(target: (1,), param: e9c3a962-7234-43ff-ab80-4a256b52bac2)\n                  )\n                )\n                (2): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (2,), param: bf63768e-28e3-4ecb-af10-8854a4b911a1)\n                    (1): RY(target: (2,), param: 751cec43-9046-4482-93b8-8fc9a1e8b244)\n                    (2): RX(target: (2,), param: c90e9e3c-00a1-42f9-a3ac-bdb07fac277b)\n                  )\n                )\n              )\n            )\n            (1): Sequence(\n              (operations): ModuleList(\n                (0): Sequence(\n                  (operations): ModuleList(\n                    (0): CNOT(control: (0,), target: (1,))\n                  )\n                )\n                (1): Sequence(\n                  (operations): ModuleList(\n                    (0): CNOT(control: (1,), target: (2,))\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n)\n</code></pre> <p>Additionally, <code>Converted</code> contains all fixed and variational parameters, as well as an embedding function which accepts feature parameters to construct a dictionary of circuit native parameters. These are needed as each backend uses a different representation of the circuit parameters:</p> <pre><code>import torch\n\n# Contains fixed parameters and variational (from the HEA)\nconv.params\n\ninputs = {\"x\": torch.tensor([1., 1.]), \"y\":torch.tensor([2., 2.])}\n\n# get all circuit parameters (including feature params)\nembedded = conv.embedding_fn(conv.params, inputs)\n</code></pre> <pre><code>conv.params = {\n  theta_3: tensor([0.0027], requires_grad=True)\n  theta_7: tensor([0.0292], requires_grad=True)\n  theta_8: tensor([0.0008], requires_grad=True)\n  theta_0: tensor([0.6596], requires_grad=True)\n  theta_2: tensor([0.0555], requires_grad=True)\n  theta_5: tensor([0.8874], requires_grad=True)\n  theta_1: tensor([0.2985], requires_grad=True)\n  theta_4: tensor([0.7404], requires_grad=True)\n  theta_6: tensor([0.4904], requires_grad=True)\n}\nembedded = {\n  c71ae995-1761-43d7-882b-f66d63db78dd: tensor([3., 3.], grad_fn=&lt;ViewBackward0&gt;)\n  95c60058-8e2e-4c35-996c-596be26ad63d: tensor([2., 2.])\n  3ac4fc11-c297-443d-9395-6a5f6221ca8f: tensor([0.6596], grad_fn=&lt;ViewBackward0&gt;)\n  3479ca8f-8931-43a5-9c3d-d4b657b0abd4: tensor([0.0027], grad_fn=&lt;ViewBackward0&gt;)\n  641f7a15-ba03-4f1c-8c73-86a47f0f78c1: tensor([0.4904], grad_fn=&lt;ViewBackward0&gt;)\n  8ee4f763-67d8-4603-a0ef-fa9ea4be097e: tensor([0.2985], grad_fn=&lt;ViewBackward0&gt;)\n  d328c437-9f6a-4bf6-a504-9bb659401819: tensor([0.7404], grad_fn=&lt;ViewBackward0&gt;)\n  e9c3a962-7234-43ff-ab80-4a256b52bac2: tensor([0.0292], grad_fn=&lt;ViewBackward0&gt;)\n  bf63768e-28e3-4ecb-af10-8854a4b911a1: tensor([0.0555], grad_fn=&lt;ViewBackward0&gt;)\n  751cec43-9046-4482-93b8-8fc9a1e8b244: tensor([0.8874], grad_fn=&lt;ViewBackward0&gt;)\n  c90e9e3c-00a1-42f9-a3ac-bdb07fac277b: tensor([0.0008], grad_fn=&lt;ViewBackward0&gt;)\n}\n</code></pre> <p>With the embedded parameters, <code>QuantumModel</code> methods are accessible:</p> <pre><code>output = backend.run(conv.circuit, embedded)\nprint(f\"{output = }\")\n</code></pre> <pre><code>output = tensor([[ 0.1880-0.1455j,  0.0843-0.0756j, -0.0936+0.1829j, -0.2237+0.3690j,\n         -0.6136-0.3550j, -0.3036-0.1475j,  0.0729+0.1027j,  0.1371+0.2262j],\n        [ 0.1880-0.1455j,  0.0843-0.0756j, -0.0936+0.1829j, -0.2237+0.3690j,\n         -0.6136-0.3550j, -0.3036-0.1475j,  0.0729+0.1027j,  0.1371+0.2262j]],\n       grad_fn=&lt;TBackward0&gt;)\n</code></pre>"},{"location":"content/backends/#lower-level-the-backend-representation","title":"Lower-level: the <code>Backend</code> representation","text":"<p>If there is a requirement to work with a specific backend, it is possible to access directly the native circuit. For example, should one wish to use PyQtorch noise features directly instead of using the <code>NoiseHandler</code> interface from Qadence:</p> <pre><code>from pyqtorch.noise import Depolarizing\n\ninputs = {\"x\": torch.rand(1), \"y\":torch.rand(1)}\nembedded = conv.embedding_fn(conv.params, inputs)\n\n# Define a noise channel on qubit 0\nnoise = Depolarizing(0, error_probability=0.1)\n\n# Add noise to circuit\nconv.circuit.native.operations.append(noise)\n</code></pre> <p>When running With noise, one can see that the output is a density matrix:</p> <pre><code>density_result = backend.run(conv.circuit, embedded)\nprint(density_result.shape)\n</code></pre> <pre><code>torch.Size([1, 8, 8])\n</code></pre>"},{"location":"content/block_system/","title":"Block system","text":"<p>Quantum programs in Qadence are constructed using a block-system, with an emphasis on composability of primitive blocks to obtain larger, composite blocks. This functional approach is different from other frameworks which follow a more object-oriented way to construct circuits and express programs.</p>"},{"location":"content/block_system/#primitive-blocks","title":"Primitive blocks","text":"<p>A <code>PrimitiveBlock</code> represents a digital or an analog time-evolution quantum operation applied to a qubit support. Programs can always be decomposed down into a sequence of <code>PrimitiveBlock</code> elements.</p> <p>Two canonical examples of digital primitive blocks are the parametrized <code>RX</code> and the <code>CNOT</code> gates:</p> <pre><code>from qadence import chain, RX, CNOT\n\nrx = RX(0, 0.5)\ncnot = CNOT(0, 1)\n\nblock = chain(rx, cnot)\n</code></pre> %3 912751ed362c4272b0a13a4e51bef45d 0 448596d7246546449078a739826fdc2d RX(0.5) 912751ed362c4272b0a13a4e51bef45d--448596d7246546449078a739826fdc2d 046c9cdd7a8d4bd3ab76753b4153efb5 1 c0440b1ad47a4e78ab54bba40519dde9 448596d7246546449078a739826fdc2d--c0440b1ad47a4e78ab54bba40519dde9 093673f8bdc747028a050faf01b8f9f0 c0440b1ad47a4e78ab54bba40519dde9--093673f8bdc747028a050faf01b8f9f0 594edd46a26645f9be22c5d3fdd03157 ecf35acef84f4414ab1429e9c32733f3 046c9cdd7a8d4bd3ab76753b4153efb5--ecf35acef84f4414ab1429e9c32733f3 a542d7c7cc1147b48ae2fc17b09d6c32 X ecf35acef84f4414ab1429e9c32733f3--a542d7c7cc1147b48ae2fc17b09d6c32 a542d7c7cc1147b48ae2fc17b09d6c32--c0440b1ad47a4e78ab54bba40519dde9 a542d7c7cc1147b48ae2fc17b09d6c32--594edd46a26645f9be22c5d3fdd03157 <p>A list of all available primitive operations can be found here.</p> How to visualize blocks <p>There are two ways to display blocks in a Python interpreter: either as a tree in ASCII format using <code>print</code>:</p> <pre><code>from qadence import X, Y, kron\n\nkron_block = kron(X(0), Y(1))\nprint(kron_block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 Y(1)\n</code></pre> <p>Or using the visualization package:</p> <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nkron_block = kron(X(0), Y(1))\n# display(kron_block)\n</code></pre> %3 d5a1cb4a8a2f4202be600334a9327552 0 069ac1381c0842b9bbeec85efadae184 X d5a1cb4a8a2f4202be600334a9327552--069ac1381c0842b9bbeec85efadae184 2e48ee099167485aa578193fbb4fddaf 1 368370a9100d453fa545b58aabe57f98 069ac1381c0842b9bbeec85efadae184--368370a9100d453fa545b58aabe57f98 c3861004942841dfa1bd39aa65dcef2c 95aea282043a45ee95684e51d8a7eb67 Y 2e48ee099167485aa578193fbb4fddaf--95aea282043a45ee95684e51d8a7eb67 95aea282043a45ee95684e51d8a7eb67--c3861004942841dfa1bd39aa65dcef2c"},{"location":"content/block_system/#composite-blocks","title":"Composite Blocks","text":"<p>Programs can be expressed by composing blocks to result in a larger <code>CompositeBlock</code> using three fundamental operations: chain, kron, and add.</p> <ul> <li>chain applies a set of blocks in sequence, which can have overlapping qubit supports, and results in a <code>ChainBlock</code> type. It is akin to applying a matrix product of the sub-blocks, and can also be used with the <code>*</code> operator.</li> <li>kron applies a set of blocks in parallel, requiring disjoint qubit support, and results in a <code>KronBlock</code> type. This is akin to applying a tensor product of the sub-blocks, and can also be used with the <code>@</code> operator.</li> <li>add performs a direct sum of the operators, and results in an <code>AddBlock</code> type. Blocks constructed this way are typically non-unitary, as is the case for Hamiltonians which can be constructed through sums of Pauli strings. Addition can also be performed directly with the <code>+</code> operator.</li> </ul> <pre><code>from qadence import X, Y, chain, kron\n\nchain_0 = chain(X(0), Y(0))\nchain_1 = chain(X(1), Y(1))\n\nkron_block = kron(chain_0, chain_1)\n</code></pre> %3 4eabd43c3e464418bcd871489382fe12 0 5a8c8041c3f14ed8b7b4440d4a696fe0 X 4eabd43c3e464418bcd871489382fe12--5a8c8041c3f14ed8b7b4440d4a696fe0 3a8b10d6c4314642a8cbd1462368b4e8 1 a93aeb0c55c144ea82095f5559470d8f Y 5a8c8041c3f14ed8b7b4440d4a696fe0--a93aeb0c55c144ea82095f5559470d8f 6ff96597cb3c43bcbb716ba7bfd5bf6c a93aeb0c55c144ea82095f5559470d8f--6ff96597cb3c43bcbb716ba7bfd5bf6c 0f659067651043948a104a23ebf03dcc 44a6aa82e56b4880b96947d6747bfc13 X 3a8b10d6c4314642a8cbd1462368b4e8--44a6aa82e56b4880b96947d6747bfc13 ac785394c3e244b9979f8dff87efb3fb Y 44a6aa82e56b4880b96947d6747bfc13--ac785394c3e244b9979f8dff87efb3fb ac785394c3e244b9979f8dff87efb3fb--0f659067651043948a104a23ebf03dcc <p>All composition functions support list comprehension syntax. Below we exemplify the creation of an XY Hamiltonian for qubits laid out on a line.</p> <pre><code>from qadence import X, Y, add\n\ndef xy_int(i: int, j: int):\n    return (1/2) * (X(i)@X(j) + Y(i)@Y(j))\n\nn_qubits = 3\n\nxy_ham = add(xy_int(i, i+1) for i in range(n_qubits-1))\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 AddBlock(0,1)\n\u2502       \u251c\u2500\u2500 KronBlock(0,1)\n\u2502       \u2502   \u251c\u2500\u2500 X(0)\n\u2502       \u2502   \u2514\u2500\u2500 X(1)\n\u2502       \u2514\u2500\u2500 KronBlock(0,1)\n\u2502           \u251c\u2500\u2500 Y(0)\n\u2502           \u2514\u2500\u2500 Y(1)\n\u2514\u2500\u2500 [mul: 0.500] \n    \u2514\u2500\u2500 AddBlock(1,2)\n        \u251c\u2500\u2500 KronBlock(1,2)\n        \u2502   \u251c\u2500\u2500 X(1)\n        \u2502   \u2514\u2500\u2500 X(2)\n        \u2514\u2500\u2500 KronBlock(1,2)\n            \u251c\u2500\u2500 Y(1)\n            \u2514\u2500\u2500 Y(2)\n</code></pre> <p>Qadence blocks can be directly translated to matrix form by calling <code>block.tensor()</code>. Note that first dimension is the batch dimension, following PyTorch conventions. This becomes relevant if the block are parameterized and batched input values are passed, as we will see later.</p> <pre><code>from qadence import X, Y\n\nxy = (1/2) * (X(0)@X(1) + Y(0)@Y(1))\n\nprint(xy.tensor().real)\n</code></pre> <pre><code>tensor([[[0., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [0., 1., 0., 0.],\n         [0., 0., 0., 0.]]])\n</code></pre> <p>For a final example of the flexibility of functional block composition, below is an implementation of the Quantum Fourier Transform on an arbitrary qubit support.</p> <pre><code>from qadence import H, CPHASE, PI, chain, kron\n\ndef qft_layer(qs: tuple, l: int):\n    cphases = chain(CPHASE(qs[j], qs[l], PI/2**(j-l)) for j in range(l+1, len(qs)))\n    return H(qs[l]) * cphases\n\ndef qft(qs: tuple):\n    return chain(qft_layer(qs, l) for l in range(len(qs)))\n</code></pre> %3 06bcb0ec51f64923b95d6d1a72a5d884 0 0888d71d6cb942aca576c539138c5164 H 06bcb0ec51f64923b95d6d1a72a5d884--0888d71d6cb942aca576c539138c5164 fe9f1f3e954548fba4253ae5836d1a09 1 f35b75dd98bf4fedb61946747cd91ed5 PHASE(1.571) 0888d71d6cb942aca576c539138c5164--f35b75dd98bf4fedb61946747cd91ed5 54f5ada9bfb24f7390753c9e027a2c09 PHASE(0.785) f35b75dd98bf4fedb61946747cd91ed5--54f5ada9bfb24f7390753c9e027a2c09 129ebf291ce643988299128cedcb1a47 f35b75dd98bf4fedb61946747cd91ed5--129ebf291ce643988299128cedcb1a47 e53afd8d1ca2497c854c08e79f9fec78 54f5ada9bfb24f7390753c9e027a2c09--e53afd8d1ca2497c854c08e79f9fec78 a13ee1a6beef468d853b35e3866568c2 54f5ada9bfb24f7390753c9e027a2c09--a13ee1a6beef468d853b35e3866568c2 d3e6f5955b86458480b05e91d8c0af28 e53afd8d1ca2497c854c08e79f9fec78--d3e6f5955b86458480b05e91d8c0af28 068d96e1c3f54b8c86d964f9c130f848 d3e6f5955b86458480b05e91d8c0af28--068d96e1c3f54b8c86d964f9c130f848 69f347935d634da6b5d7bced0921d7f2 068d96e1c3f54b8c86d964f9c130f848--69f347935d634da6b5d7bced0921d7f2 dd71c715c76f497588df9abfc569d3e7 6c9b490857d044b09ca701d5cc9e0a24 fe9f1f3e954548fba4253ae5836d1a09--6c9b490857d044b09ca701d5cc9e0a24 0c259dbc804d4e2e995d76597b0b040b 2 6c9b490857d044b09ca701d5cc9e0a24--129ebf291ce643988299128cedcb1a47 3d6a3d09f7b142eda2986f91c6c652d4 129ebf291ce643988299128cedcb1a47--3d6a3d09f7b142eda2986f91c6c652d4 d6d2ffe694b34273aa3b8eba71a2815b H 3d6a3d09f7b142eda2986f91c6c652d4--d6d2ffe694b34273aa3b8eba71a2815b 87dd5d3ffbb04c7ca94eebc040242f5f PHASE(1.571) d6d2ffe694b34273aa3b8eba71a2815b--87dd5d3ffbb04c7ca94eebc040242f5f 19a1208719554abeb169e1bf860ace2a 87dd5d3ffbb04c7ca94eebc040242f5f--19a1208719554abeb169e1bf860ace2a 0e7bdcdd76a749aba8a48550554c851d 87dd5d3ffbb04c7ca94eebc040242f5f--0e7bdcdd76a749aba8a48550554c851d 19a1208719554abeb169e1bf860ace2a--dd71c715c76f497588df9abfc569d3e7 f5588ba449584d13bd361259397b7974 0dceb24d86724834bdb3bdd79c10b7b7 0c259dbc804d4e2e995d76597b0b040b--0dceb24d86724834bdb3bdd79c10b7b7 a01eb7124f64488b95c76fa5137bf4ff 0dceb24d86724834bdb3bdd79c10b7b7--a01eb7124f64488b95c76fa5137bf4ff a01eb7124f64488b95c76fa5137bf4ff--a13ee1a6beef468d853b35e3866568c2 85fbec1149b74e74be5880b744e2f4a5 a13ee1a6beef468d853b35e3866568c2--85fbec1149b74e74be5880b744e2f4a5 85fbec1149b74e74be5880b744e2f4a5--0e7bdcdd76a749aba8a48550554c851d 229a0bbba3f84a7f9ace08bcb77bd607 H 0e7bdcdd76a749aba8a48550554c851d--229a0bbba3f84a7f9ace08bcb77bd607 229a0bbba3f84a7f9ace08bcb77bd607--f5588ba449584d13bd361259397b7974 <p>Other functionalities are directly built in the block system. For example, the inverse operation can be created with the <code>dagger()</code> method.</p> <pre><code>qft_inv = qft((0, 1, 2)).dagger()\n</code></pre> %3 3b887fc0c9704c71b38e948eb9385b63 0 20eeb1b1e125483885ddff36f7979ed7 3b887fc0c9704c71b38e948eb9385b63--20eeb1b1e125483885ddff36f7979ed7 70231bd3ad174cde97e30809454eca5a 1 d5c3a3258a11409e91bc5914502ee282 20eeb1b1e125483885ddff36f7979ed7--d5c3a3258a11409e91bc5914502ee282 c5c9d08b2c364acdbbfc60ff3b29467c d5c3a3258a11409e91bc5914502ee282--c5c9d08b2c364acdbbfc60ff3b29467c dd634cf7b1ad45f6936fdaeb6c6ffe8a PHASE(-0.785) c5c9d08b2c364acdbbfc60ff3b29467c--dd634cf7b1ad45f6936fdaeb6c6ffe8a 6cae0f14d44a4cbbbb9d878017d592ae PHASE(-1.571) dd634cf7b1ad45f6936fdaeb6c6ffe8a--6cae0f14d44a4cbbbb9d878017d592ae 0ee8f01f030445f8822a4070f71af2d9 dd634cf7b1ad45f6936fdaeb6c6ffe8a--0ee8f01f030445f8822a4070f71af2d9 a431ffa49a23424d9cd02d6e8448c8a0 H 6cae0f14d44a4cbbbb9d878017d592ae--a431ffa49a23424d9cd02d6e8448c8a0 7c341fdf413b4761acaf637db0ad1c8a 6cae0f14d44a4cbbbb9d878017d592ae--7c341fdf413b4761acaf637db0ad1c8a cbfeda05ce8e4b968e484f869ff0748c a431ffa49a23424d9cd02d6e8448c8a0--cbfeda05ce8e4b968e484f869ff0748c 95a3406b353149a3a4e07b6f76eea399 42bbf990e1824cc791295add8192018d 70231bd3ad174cde97e30809454eca5a--42bbf990e1824cc791295add8192018d cd2684587cfe4fd59558ed4fe40f6d32 2 51a09b4284154984a7e8aad700070c34 PHASE(-1.571) 42bbf990e1824cc791295add8192018d--51a09b4284154984a7e8aad700070c34 4e347f7d43b2409aab663657f8de1959 H 51a09b4284154984a7e8aad700070c34--4e347f7d43b2409aab663657f8de1959 44177f0e66a04c53be6aba7c0f5e4448 51a09b4284154984a7e8aad700070c34--44177f0e66a04c53be6aba7c0f5e4448 76b0680f6b32455e9bbc63a553e27b7d 4e347f7d43b2409aab663657f8de1959--76b0680f6b32455e9bbc63a553e27b7d 76b0680f6b32455e9bbc63a553e27b7d--7c341fdf413b4761acaf637db0ad1c8a bc6df54394514b17be5869d8d8d6342f 7c341fdf413b4761acaf637db0ad1c8a--bc6df54394514b17be5869d8d8d6342f bc6df54394514b17be5869d8d8d6342f--95a3406b353149a3a4e07b6f76eea399 24a2ce6bc723466f8546c30ce25bbf1c cdca996c7cf64147bcdfad4daa8ef057 H cd2684587cfe4fd59558ed4fe40f6d32--cdca996c7cf64147bcdfad4daa8ef057 cdca996c7cf64147bcdfad4daa8ef057--44177f0e66a04c53be6aba7c0f5e4448 6973a28b50df4eeb923469e088399e51 44177f0e66a04c53be6aba7c0f5e4448--6973a28b50df4eeb923469e088399e51 6973a28b50df4eeb923469e088399e51--0ee8f01f030445f8822a4070f71af2d9 8534f493727b4034a048cb31b3e43d71 0ee8f01f030445f8822a4070f71af2d9--8534f493727b4034a048cb31b3e43d71 afd0167764de4421b7a50397d4cddc39 8534f493727b4034a048cb31b3e43d71--afd0167764de4421b7a50397d4cddc39 afd0167764de4421b7a50397d4cddc39--24a2ce6bc723466f8546c30ce25bbf1c"},{"location":"content/block_system/#digital-analog-composition","title":"Digital-analog composition","text":"<p>In Qadence, analog operations are first-class citizens. An analog operation is one whose unitary is best described by the evolution of some hermitian generator, or Hamiltonian, acting on an arbitrary number of qubits. Qadence provides the <code>HamEvo</code> class to initialize analog operations. For a time-independent generator \\(\\mathcal{H}\\) and some time variable \\(t\\), <code>HamEvo(H, t)</code> represents the evolution operator \\(\\exp(-i\\mathcal{H}t)\\).</p> <p>Analog operations constitute a generalization of digital operations, and all digital operations can also be represented as the evolution of some hermitian generator. For example, the <code>RX</code> gate is the evolution of <code>X</code>.</p> <pre><code>from qadence import X, RX, HamEvo, PI\nfrom torch import allclose\n\nangle = PI/2\n\nblock_digital = RX(0, angle)\n\nblock_analog = HamEvo(0.5*X(0), angle)\n\nprint(allclose(block_digital.tensor(), block_analog.tensor()))\n</code></pre> <pre><code>True\n</code></pre> <p>As seen in the previous section, arbitrary Hamiltonians can be constructed using Pauli operators. Their evolution can be combined with other arbitrary digital operations and incorporated into any quantum program.</p> <pre><code>from qadence import X, Y, RX, HamEvo\nfrom qadence import add, kron, PI\n\ndef xy_int(i: int, j: int):\n    return (1/2) * (X(i)@X(j) + Y(i)@Y(j))\n\nn_qubits = 3\n\nxy_ham = add(xy_int(i, i+1) for i in range(n_qubits-1))\n\nanalog_evo = HamEvo(xy_ham, 1.0)\n\ndigital_block = kron(RX(i, i*PI/2) for i in range(n_qubits))\n\nprogram = digital_block * analog_evo * digital_block\n</code></pre> %3 cluster_58a8ccbe676743afb3a9d6cc2838053f 49d213041a274b45beacefc4958c7361 0 50b35187b6034d529a56830f122f6d0f RX(0.0) 49d213041a274b45beacefc4958c7361--50b35187b6034d529a56830f122f6d0f f4bbba66fcd649d8bb4d290de5a78cb5 1 2e013268ecf14ac09b0d588924462393 HamEvo 50b35187b6034d529a56830f122f6d0f--2e013268ecf14ac09b0d588924462393 adf471619dd4430c9d68a1369011813d RX(0.0) 2e013268ecf14ac09b0d588924462393--adf471619dd4430c9d68a1369011813d 1351250ff9a141c9881de4b1d89b1b40 adf471619dd4430c9d68a1369011813d--1351250ff9a141c9881de4b1d89b1b40 9f9ae8ff122643a9a9ab5b8a9383c7c8 e955f9c898a446c08eec7f9ceb7082b7 RX(1.571) f4bbba66fcd649d8bb4d290de5a78cb5--e955f9c898a446c08eec7f9ceb7082b7 f1d1902874c24f1fae39e270ed692431 2 b562b5494f9946c8bb52db6ce85da834 t = 1.000 e955f9c898a446c08eec7f9ceb7082b7--b562b5494f9946c8bb52db6ce85da834 fc1a986a1f98468c9633e43586450cfc RX(1.571) b562b5494f9946c8bb52db6ce85da834--fc1a986a1f98468c9633e43586450cfc fc1a986a1f98468c9633e43586450cfc--9f9ae8ff122643a9a9ab5b8a9383c7c8 8dfb3e9678ac462d8c6015d03aeeb334 32065c6c5bb14117904ef0ac20bc2c00 RX(3.142) f1d1902874c24f1fae39e270ed692431--32065c6c5bb14117904ef0ac20bc2c00 12fbc49fac534ecda6312cbb274d1264 32065c6c5bb14117904ef0ac20bc2c00--12fbc49fac534ecda6312cbb274d1264 c0eb84c6e7884cdcb7875c47973b727c RX(3.142) 12fbc49fac534ecda6312cbb274d1264--c0eb84c6e7884cdcb7875c47973b727c c0eb84c6e7884cdcb7875c47973b727c--8dfb3e9678ac462d8c6015d03aeeb334"},{"location":"content/block_system/#block-execution","title":"Block execution","text":"<p>To quickly run block operations and access wavefunctions, samples or expectation values of observables, one can use the convenience functions <code>run</code>, <code>sample</code> and <code>expectation</code>.</p> <pre><code>from qadence import kron, add, H, Z, run, sample, expectation\n\nn_qubits = 2\n\n# Prepares a uniform state\nh_block = kron(H(i) for i in range(n_qubits))\n\nwf = run(h_block)\n\nxs = sample(h_block, n_shots=1000)\n\nobs = add(Z(i) for i in range(n_qubits))\nex = expectation(h_block, obs)\n</code></pre> <pre><code>wf = tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\nxs = [OrderedCounter({'01': 264, '11': 257, '00': 249, '10': 230})]\nex = tensor([[0.]])\n</code></pre>"},{"location":"content/block_system/#execution-via-quantumcircuit-and-quantummodel","title":"Execution via <code>QuantumCircuit</code> and <code>QuantumModel</code>","text":"<p>More fine-grained control and better performance is provided via the high-level <code>QuantumModel</code> abstraction. Quantum programs in Qadence are constructed in two steps:</p> <ol> <li>Build a <code>QuantumCircuit</code> which ties together a composite block and a register.</li> <li>Define a <code>QuantumModel</code> which differentiates, compiles and executes the circuit.</li> </ol> <p>Execution of more complex Qadence programs will be explored in the next tutorials.</p>"},{"location":"content/block_system/#adding-noise-to-gates","title":"Adding noise to gates","text":"<p>It is possible to add noise to gates. Please refer to the noise tutorial here.</p>"},{"location":"content/hamiltonians/","title":"Constructing arbitrary Hamiltonians","text":"<p>At the heart of digital-analog quantum computing is the description and execution of analog blocks, which represent a set of interacting qubits under some interaction Hamiltonian. For this purpose, Qadence relies on the <code>hamiltonian_factory</code> function to create arbitrary Hamiltonian blocks to be used as generators of <code>HamEvo</code> or as observables to be measured.</p>"},{"location":"content/hamiltonians/#arbitrary-all-to-all-hamiltonians","title":"Arbitrary all-to-all Hamiltonians","text":"<p>Arbitrary all-to-all interaction Hamiltonians can be easily created by passing the number of qubits in the first argument. The type of <code>interaction</code> can be chosen from the available ones in the <code>Interaction</code> enum type.</p> <pre><code>from qadence import hamiltonian_factory\nfrom qadence import N, X, Y, Z\nfrom qadence import Interaction\n\nn_qubits = 3\n\nhamilt = hamiltonian_factory(n_qubits, interaction=Interaction.ZZ)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(2)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 Z(1)\n        \u2514\u2500\u2500 Z(2)\n</code></pre> <p>Alternatively, a custom interaction function can also be defined. The input should be two integer indices \\(i\\) and \\(j\\) and it should return a composition of pauli terms representing the interaction between qubits \\(i\\) and \\(j\\):</p> <pre><code>def custom_int(i: int, j: int):\n    return X(i) @ X(j) + Y(i) @ Y(j)\n\nn_qubits = 2\n\nhamilt = hamiltonian_factory(n_qubits, interaction=custom_int)\n</code></pre> <pre><code>AddBlock(0,1)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 AddBlock(0,1)\n        \u251c\u2500\u2500 KronBlock(0,1)\n        \u2502   \u251c\u2500\u2500 X(0)\n        \u2502   \u2514\u2500\u2500 X(1)\n        \u2514\u2500\u2500 KronBlock(0,1)\n            \u251c\u2500\u2500 Y(0)\n            \u2514\u2500\u2500 Y(1)\n</code></pre> <p>Single-qubit terms can also be added by passing the respective operator directly to the <code>detuning</code> argument. For example, the total magnetization is commonly used as an observable to be measured:</p> <pre><code>total_mag = hamiltonian_factory(n_qubits, detuning = Z)\n</code></pre> <pre><code>AddBlock(0,1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 Z(0)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 Z(1)\n</code></pre> <p>For further customization, arbitrary coefficients can be passed as arrays to the <code>interaction_strength</code> and <code>detuning_strength</code> arguments for the two-qubits and single-qubit terms respectively.</p> <pre><code>n_qubits = 3\n\nhamilt = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.ZZ,\n    detuning=Z,\n    interaction_strength=[0.5, 0.2, 0.1],\n    detuning_strength=[0.1, 0.5, -0.3]\n)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 0.100] \n\u2502   \u2514\u2500\u2500 Z(0)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: -0.30] \n\u2502   \u2514\u2500\u2500 Z(2)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: 0.200] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(2)\n\u2514\u2500\u2500 [mul: 0.100] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 Z(1)\n        \u2514\u2500\u2500 Z(2)\n</code></pre> <p>Ordering interaction strengths matters</p> <p>When passing interaction strengths as an array, the ordering must be identical to the one obtained from the <code>edges</code> property of a Qadence <code>Register</code>:</p> <pre><code>from qadence import Register\n\nprint(Register(n_qubits).edges)\n</code></pre> <pre><code>[(0, 1), (0, 2), (1, 2)]\n</code></pre> <p>For one more example, let's create a transverse-field Ising model,</p> <pre><code>n_qubits = 4\nn_edges = int(0.5 * n_qubits * (n_qubits - 1))\n\nz_terms = [1.0] * n_qubits\nzz_terms = [2.0] * n_edges\n\nzz_ham = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.ZZ,\n    detuning=Z,\n    interaction_strength=zz_terms,\n    detuning_strength=z_terms\n)\n\nx_terms = [-1.0] * n_qubits\nx_ham = hamiltonian_factory(n_qubits, detuning = X, detuning_strength = x_terms)\n\ntransverse_ising = zz_ham + x_ham\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 AddBlock(0,1,2,3)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(0)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(1)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(3)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(1)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(3)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502   \u2502       \u251c\u2500\u2500 Z(1)\n\u2502   \u2502       \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(1,3)\n\u2502   \u2502       \u251c\u2500\u2500 Z(1)\n\u2502   \u2502       \u2514\u2500\u2500 Z(3)\n\u2502   \u2514\u2500\u2500 [mul: 2.000] \n\u2502       \u2514\u2500\u2500 KronBlock(2,3)\n\u2502           \u251c\u2500\u2500 Z(2)\n\u2502           \u2514\u2500\u2500 Z(3)\n\u2514\u2500\u2500 AddBlock(0,1,2,3)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(0)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(1)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(2)\n    \u2514\u2500\u2500 [mul: -1.00] \n        \u2514\u2500\u2500 X(3)\n</code></pre> <p>Random interaction coefficients</p> <p>Random interaction coefficients can be chosen between -1 and 1 by simply passing <code>random_strength = True</code> instead of <code>detuning_strength</code> and <code>interaction_strength</code>.</p>"},{"location":"content/hamiltonians/#arbitrary-hamiltonian-topologies","title":"Arbitrary Hamiltonian topologies","text":"<p>Arbitrary interaction topologies can be created using the Qadence <code>Register</code>. Simply pass the register with the desired topology as the first argument to the <code>hamiltonian_factory</code>:</p> <pre><code>from qadence import Register\n\nreg = Register.square(qubits_side=2)\n\nsquare_hamilt = hamiltonian_factory(reg, interaction=Interaction.NN)\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(3)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(2)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(2,3)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(3)\n</code></pre>"},{"location":"content/hamiltonians/#adding-variational-parameters","title":"Adding variational parameters","text":"<p>Finally, fully parameterized Hamiltonians can be created by passing a string to the strength arguments, and used to prefix the name of the variational parameters.</p> <pre><code>n_qubits = 3\n\nnn_ham = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.NN,\n    detuning=N,\n    interaction_strength=\"c\",\n    detuning_strength=\"d\"\n)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: d_0] \n\u2502   \u2514\u2500\u2500 N(0)\n\u251c\u2500\u2500 [mul: d_1] \n\u2502   \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: d_2] \n\u2502   \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: c_01] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: c_02] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(2)\n\u2514\u2500\u2500 [mul: c_12] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 N(1)\n        \u2514\u2500\u2500 N(2)\n</code></pre> <p>Alternatively, fully customizable sympy functions can be passed in an array using the Qadence parameters. Furthermore, the <code>use_all_node_pairs = True</code> option can be passed so that interactions are created for every single node pair in the register, irrespectively of the topology of the edges. This is useful for creating Hamiltonians that depend on qubit distance.</p> <pre><code>from qadence import VariationalParameter, Register\n\n# Square register of 4 qubits with a dimensionless distance of 8.0\nreg = Register.square(2, spacing = 8.0)\n\n# Get the distances between all pairs of qubits\ndistance_dict = reg.distances\n\n# Create interaction strength with variational parameter and 1/r term\nstrength_list = []\nfor node_pair in reg.all_node_pairs:\n    param = VariationalParameter(\"x\" + f\"_{node_pair[0]}{node_pair[1]}\")\n    dist_factor = reg.distances[node_pair]\n    strength_list.append(param / dist_factor)\n\nnn_ham = hamiltonian_factory(\n    reg,\n    interaction=Interaction.NN,\n    interaction_strength=strength_list,\n    use_all_node_pairs=True,\n)\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 [mul: 0.125*x_01] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: 0.088*x_02] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: 0.125*x_03] \n\u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(3)\n\u251c\u2500\u2500 [mul: 0.125*x_12] \n\u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: 0.088*x_13] \n\u2502   \u2514\u2500\u2500 KronBlock(1,3)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(3)\n\u2514\u2500\u2500 [mul: 0.125*x_23] \n    \u2514\u2500\u2500 KronBlock(2,3)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(3)\n</code></pre>"},{"location":"content/overlap/","title":"Wavefunction overlaps","text":"<p>Qadence offers convenience functions for computing the overlap between the wavefunctions generated by two quantum circuits \\(U\\) and \\(W\\) as:</p> \\[ S = |\\langle \\psi_U | \\psi_W \\rangle|^2 \\quad \\textrm{where} \\quad \\psi_U = U|\\psi_0\\rangle \\] <p>Here is an example on how to compute the overlap between two very simple parametric circuits consisting of a single <code>RX</code> rotation on different qubits. The overlap is expected to be non-zero only when the rotation angle is different from \\(\\pi \\; \\textrm{mod}\\; 2\\pi\\) for both rotations:</p> <pre><code>import numpy as np\nfrom torch import tensor\nfrom qadence import Overlap, OverlapMethod, QuantumCircuit, H, RX, X, FeatureParameter, hea, PI\n\n\n# Create two quantum circuits\n# with a single qubit rotation on two random qubits\nn_qubits = 4\nqubits = np.random.choice(n_qubits, n_qubits, replace=False)\n\nphi = FeatureParameter(\"phi\")\ncircuit_bra = QuantumCircuit(n_qubits, RX(qubits[0], phi))\n\npsi = FeatureParameter(\"psi\")\ncircuit_ket = QuantumCircuit(n_qubits, RX(qubits[1], psi))\n\n# Values for the feature parameters\nvalues_bra = {\"phi\": tensor([PI / 2, PI])}\nvalues_ket = {\"psi\": tensor([PI / 2, PI])}\n\n# Calculate overlap by assigning values to the given bra and ket circuits\novrlp = Overlap(circuit_bra, circuit_ket)\novrlp = ovrlp(bra_param_values=values_bra, ket_param_values=values_ket)\n</code></pre> <pre><code>Overlap with exact method:\n tensor([[2.5000e-01, 1.8747e-33],\n        [1.8747e-33, 1.4058e-65]])\n</code></pre> <p>The <code>Overlap</code> class above inherits from <code>QuantumModel</code> and is executed through its inherited forward method for the given input parameter values. By default, the overlap is computed exactly by performing the dot product of the wavefunction propagated from bra and ket circuits.</p> <p>However, it is possible to choose a different method from the <code>OverlapMethod</code> enumeration to be passed via the <code>overlap_method</code> argument in the <code>Overlap</code> initializer. Currently, one can choose from:</p> <ul> <li><code>EXACT</code>: exact computation using the wavefunction matrix representation. Does not work with real devices since it assumes access to the complete qubit system wavefunction.</li> <li><code>COMPUTE_UNCOMPUTE</code>: exact or sampling-based computation using bra \\(U\\) and ket \\(W^{\\dagger}\\) unitaries.</li> <li><code>SWAP_TEST</code>: exact or sampling-based computation using the SWAP test method.</li> <li><code>HADAMARD_TEST</code>: exact or sampling-based computation using the Hadamard test method.</li> <li><code>JENSEN_SHANNON</code>: compute the overlap using the Jensen-Shannon divergence of the two probability distributions obtained by sampling the propagated circuits. This will yield a different result than the other methods.</li> </ul> <p>All methods (except for the <code>EXACT</code> method) take an optional <code>n_shots</code> argument which can be used to perform shot-based calculations.</p> <p>Warning</p> <p>If you select a finite number of shots, the overlap is not differentiable. Therefore, it cannot be used as output of a quantum model if gradients are required.</p> <pre><code># Calculate overlap with SWAP test\novrlp = Overlap(circuit_bra, circuit_ket, method=OverlapMethod.SWAP_TEST)\novrlp_ha = ovrlp(values_bra, values_ket)\n\n# Calculate overlap with SWAP test\n# using a finite number of shots\novrlp = Overlap(circuit_bra, circuit_ket, method=OverlapMethod.SWAP_TEST)\novrlp_ha = ovrlp(values_bra, values_ket, n_shots=10_000)\n</code></pre> <pre><code>Overlap with SWAP test:\n tensor([[ 2.5000e-01, -3.3307e-16],\n        [-3.3307e-16, -4.4409e-16]])\nOverlap with SWAP test with finite number of shots:\n tensor([[0.2408, 0.0084],\n        [0.0024, 0.0104]])\n</code></pre>"},{"location":"content/parameters/","title":"Parametric programs","text":"<p>Qadence provides a flexible parameter system built on top of Sympy. Parameters can be of different types:</p> <ul> <li>Fixed parameter: a constant with a fixed, non-trainable value (e.g. \\(\\dfrac{\\pi}{2}\\)).</li> <li>Variational parameter: a trainable parameter which will be automatically picked up by the optimizer.</li> <li>Feature parameter: a non-trainable parameter which can be used to pass input values.</li> </ul>"},{"location":"content/parameters/#fixed-parameters","title":"Fixed parameters","text":"<p>Passing fixed parameters to blocks can be done by simply passing a Python numeric type or a <code>torch.Tensor</code>.</p> <pre><code>import torch\nfrom qadence import RX, run, PI\n\nwf = run(RX(0, torch.tensor(PI)))\n\nwf = run(RX(0, PI))\n</code></pre> <pre><code>wf = tensor([[6.1232e-17+0.j, 0.0000e+00-1.j]])\nwf = tensor([[6.1232e-17+0.j, 0.0000e+00-1.j]])\n</code></pre>"},{"location":"content/parameters/#variational-parameters","title":"Variational parameters","text":"<p>To parametrize a block a <code>VariationalParameter</code> instance is required. In most cases Qadence also accepts a Python string, which will be used to automatically initialize a <code>VariationalParameter</code>:</p> <pre><code>from qadence import RX, run, VariationalParameter\n\nblock = RX(0, VariationalParameter(\"theta\"))\nblock = RX(0, \"theta\")  # Equivalent\n\nwf = run(block)\n</code></pre> <pre><code>wf = tensor([[0.9979+0.0000j, 0.0000-0.0648j]])\n</code></pre> <p>By calling <code>run</code>, a random value for <code>\"theta\"</code> is initialized at execution. In a <code>QuantumModel</code>, variational parameters are stored in the underlying model parameter dictionary.</p>"},{"location":"content/parameters/#feature-parameters","title":"Feature parameters","text":"<p>A <code>FeatureParameter</code> type can also be used. It requires an input value or a batch of values. In most cases, Qadence accepts a <code>values</code> dictionary to set the input of feature parameters.</p> <pre><code>from torch import tensor\nfrom qadence import RX, PI, run, FeatureParameter\n\nblock = RX(0, FeatureParameter(\"phi\"))\n\nwf = run(block, values = {\"phi\": tensor([PI, PI/2])})\n</code></pre> <pre><code>wf = tensor([[6.1232e-17+0.0000j, 0.0000e+00-1.0000j],\n        [7.0711e-01+0.0000j, 0.0000e+00-0.7071j]])\n</code></pre> <p>Since a batch of input values was passed, the <code>run</code> function returns a batch of output states. Note that <code>FeatureParameter(\"x\")</code> and <code>VariationalParameter(\"x\")</code> are simply aliases for <code>Parameter(\"x\", trainable = False)</code> and <code>Parameter(\"x\", trainable = True)</code>.</p>"},{"location":"content/parameters/#multiparameter-expressions-and-analog-integration","title":"Multiparameter expressions and analog integration","text":"<p>The integration with Sympy becomes useful when one wishes to write arbitrary parameter compositions. Parameters can also be used as scaling coefficients in the block system, which is essential when defining arbitrary analog operations.</p> <pre><code>from torch import tensor\nfrom qadence import RX, Z, HamEvo, PI\nfrom qadence import VariationalParameter, FeatureParameter, run\nfrom sympy import sin\n\ntheta, phi = VariationalParameter(\"theta\"), FeatureParameter(\"phi\")\n\n# Arbitrary parameter composition\nexpr = PI * sin(theta + phi)\n\n# Use as unitary gate arguments\ngate = RX(0, expr)\n\n# Or as scaling coefficients for Hermitian operators\nh_op = expr * (Z(0) @ Z(1))\n\nwf = run(gate * HamEvo(h_op, 1.0), values = {\"phi\": tensor(PI)})\n</code></pre> <pre><code>wf = tensor([[-0.2498+0.1812j,  0.0000+0.0000j,  0.5584-0.7700j,  0.0000+0.0000j]])\n</code></pre>"},{"location":"content/parameters/#parameter-redundancy","title":"Parameter redundancy","text":"<p>Parameters are uniquely defined by their name and redundancy is allowed in composite blocks to assign the same value to different blocks. This is useful, for example, when defining layers of rotation gates typically used as feature maps.</p> <pre><code>from torch import tensor\nfrom qadence import RY, PI, run, kron, FeatureParameter\n\nn_qubits = 3\n\nparam = FeatureParameter(\"phi\")\n\nblock = kron(RY(i, (i+1) * param) for i in range(n_qubits))\n\nwf = run(block, values = {\"phi\": tensor(PI)})\n</code></pre> <pre><code>wf = tensor([[ 1.1248e-32+0.j,  6.1232e-17+0.j, -1.3775e-48+0.j, -7.4988e-33+0.j,\n          1.8370e-16+0.j,  1.0000e+00+0.j, -2.2496e-32+0.j, -1.2246e-16+0.j]])\n</code></pre>"},{"location":"content/parameters/#parametrized-circuits","title":"Parametrized circuits","text":"<p>Let's look at a final example of an arbitrary composition of digital and analog parameterized blocks:</p> <pre><code>import sympy\nfrom qadence import RX, RY, RZ, CNOT, CPHASE, Z, HamEvo\nfrom qadence import run, chain, add, kron, FeatureParameter, VariationalParameter, PI\n\nn_qubits = 3\n\nphi = FeatureParameter(\"\u03a6\")\ntheta = VariationalParameter(\"\u03b8\")\n\nrotation_block = kron(\n    RX(0, phi/theta),\n    RY(1, theta*2),\n    RZ(2, sympy.cos(phi))\n)\ndigital_entangler = CNOT(0, 1) * CPHASE(1, 2, PI)\n\nhamiltonian = add(theta * (Z(i) @ Z(i+1)) for i in range(n_qubits-1))\n\nanalog_evo = HamEvo(hamiltonian, phi)\n\nprogram = chain(rotation_block, digital_entangler, analog_evo)\n</code></pre> %3 cluster_782de171f85040e49d93075e59118b64 2e6ff3aeda064f86a94e1de91d5d8233 0 2547012b16c6452493aeac5dc7744efb RX(\u03a6/\u03b8) 2e6ff3aeda064f86a94e1de91d5d8233--2547012b16c6452493aeac5dc7744efb 4a967131ec69411e8ec64265fc26f695 1 d4d32140d5c7407484ee97dc15319a9b 2547012b16c6452493aeac5dc7744efb--d4d32140d5c7407484ee97dc15319a9b a8508667106b4819a29de35b7a7f5ccd d4d32140d5c7407484ee97dc15319a9b--a8508667106b4819a29de35b7a7f5ccd b7fe1515e76a4b4db82d1cd3f3a1d5db HamEvo a8508667106b4819a29de35b7a7f5ccd--b7fe1515e76a4b4db82d1cd3f3a1d5db 4ef805f98cf8443caa2acf4a19ee3b39 b7fe1515e76a4b4db82d1cd3f3a1d5db--4ef805f98cf8443caa2acf4a19ee3b39 a5385c2886d342bf91d89732fa7c1ac9 5b53fc10f5914e22bd34eb51109bf609 RY(2*\u03b8) 4a967131ec69411e8ec64265fc26f695--5b53fc10f5914e22bd34eb51109bf609 90d3dbd25bb544b78585c517f32dd90e 2 21d18fcd0e4d4da3b0ffaad8d982709f X 5b53fc10f5914e22bd34eb51109bf609--21d18fcd0e4d4da3b0ffaad8d982709f 21d18fcd0e4d4da3b0ffaad8d982709f--d4d32140d5c7407484ee97dc15319a9b 0354b279584c4457acbbd1bdadd13b2d 21d18fcd0e4d4da3b0ffaad8d982709f--0354b279584c4457acbbd1bdadd13b2d 5336f135bf6540c3a7b6ecedc5ff414e t = \u03a6 0354b279584c4457acbbd1bdadd13b2d--5336f135bf6540c3a7b6ecedc5ff414e 5336f135bf6540c3a7b6ecedc5ff414e--a5385c2886d342bf91d89732fa7c1ac9 578c5f1237e84bbb80db80bdcf51274a 6a1f8580b1964c7b978da6dc0f5fc77c RZ(cos(\u03a6)) 90d3dbd25bb544b78585c517f32dd90e--6a1f8580b1964c7b978da6dc0f5fc77c 66d43924e0c84d2e906b693b68cece00 6a1f8580b1964c7b978da6dc0f5fc77c--66d43924e0c84d2e906b693b68cece00 83876653505046b8b0614b54dc203a9e PHASE(3.142) 66d43924e0c84d2e906b693b68cece00--83876653505046b8b0614b54dc203a9e 83876653505046b8b0614b54dc203a9e--0354b279584c4457acbbd1bdadd13b2d a157253efba2411da768b24c13a931dc 83876653505046b8b0614b54dc203a9e--a157253efba2411da768b24c13a931dc a157253efba2411da768b24c13a931dc--578c5f1237e84bbb80db80bdcf51274a <p>Please note the different colors for the parametrization with different types. The default palette assigns blue for <code>VariationalParameter</code>, green for <code>FeatureParameter</code>, orange for numeric values, and shaded red for non-parametric gates.</p>"},{"location":"content/qml_constructors/","title":"Quantum machine learning constructors","text":"<p>Besides the arbitrary Hamiltonian constructors, Qadence also provides a complete set of program constructors useful for digital-analog quantum machine learning programs.</p>"},{"location":"content/qml_constructors/#feature-maps","title":"Feature maps","text":"<p>The <code>feature_map</code> function can easily create several types of data-encoding blocks. The two main types of feature maps use a Fourier basis or a Chebyshev basis.</p> <pre><code>from qadence import feature_map, BasisSet, chain\nfrom qadence.draw import display\n\nn_qubits = 3\n\nfourier_fm = feature_map(n_qubits, fm_type=BasisSet.FOURIER)\n\nchebyshev_fm = feature_map(n_qubits, fm_type=BasisSet.CHEBYSHEV)\n\nblock = chain(fourier_fm, chebyshev_fm)\n</code></pre> %3 cluster_af3e56141cd442a78660fc8f2244ebd7 Constant Chebyshev FM cluster_9d6c4b6abacd4c448588583810050a0c Constant Fourier FM c8f7ed78de80406d8cb160c01090a25a 0 0783c55ebd954a94a46ff7a07ffc85c6 RX(phi) c8f7ed78de80406d8cb160c01090a25a--0783c55ebd954a94a46ff7a07ffc85c6 dc522e54e3ad4c9a97d9e19d50aebeb3 1 281c7fadfb464e18873e531556236a54 RX(acos(phi)) 0783c55ebd954a94a46ff7a07ffc85c6--281c7fadfb464e18873e531556236a54 c3c8093e1529480f91d8654d4099176b 281c7fadfb464e18873e531556236a54--c3c8093e1529480f91d8654d4099176b 881b705a8d2f4f9f8e369f9f4b917ea7 2b91dc6b9e404961895bcda7222732e9 RX(phi) dc522e54e3ad4c9a97d9e19d50aebeb3--2b91dc6b9e404961895bcda7222732e9 3ee05afdef554d6184e2e8811eff934c 2 2a9d7ad6e7834437988c40d6666413e5 RX(acos(phi)) 2b91dc6b9e404961895bcda7222732e9--2a9d7ad6e7834437988c40d6666413e5 2a9d7ad6e7834437988c40d6666413e5--881b705a8d2f4f9f8e369f9f4b917ea7 d879d3a68e42408095fcb7e30359c1e6 0e1ece74feb04dc6a326a4c8b2bef20f RX(phi) 3ee05afdef554d6184e2e8811eff934c--0e1ece74feb04dc6a326a4c8b2bef20f 2e318393f93f43c3bac4dee0bc960f17 RX(acos(phi)) 0e1ece74feb04dc6a326a4c8b2bef20f--2e318393f93f43c3bac4dee0bc960f17 2e318393f93f43c3bac4dee0bc960f17--d879d3a68e42408095fcb7e30359c1e6 <p>A custom encoding function can also be passed with <code>sympy</code></p> <pre><code>from sympy import asin, Function\n\nn_qubits = 3\n\n# Using a pre-defined sympy Function\ncustom_fm_0 = feature_map(n_qubits, fm_type=asin)\n\n# Creating a custom function\ndef custom_fn(x):\n    return asin(x) + x**2\n\ncustom_fm_1 = feature_map(n_qubits, fm_type=custom_fn)\n\nblock = chain(custom_fm_0, custom_fm_1)\n</code></pre> %3 cluster_0b1a9bf4403a462b816d21cdca26213a Constant &lt;function custom_fn at 0x7efc804dfd00&gt; FM cluster_661a908fe71847378971da202732a848 Constant asin FM c5b94cb361dc44c4941a2ac1c27098c9 0 b30aca662a6043bb8e6e79db7fbe464d RX(asin(phi)) c5b94cb361dc44c4941a2ac1c27098c9--b30aca662a6043bb8e6e79db7fbe464d dc2a035b81e44576aa1287db5dfe2232 1 549cf6f2f60248949b29c0b4bbe892be RX(phi**2 + asin(phi)) b30aca662a6043bb8e6e79db7fbe464d--549cf6f2f60248949b29c0b4bbe892be c9ddeabb6cd1427da33e32cbdb12ac8e 549cf6f2f60248949b29c0b4bbe892be--c9ddeabb6cd1427da33e32cbdb12ac8e 2f5c53f59e434816a29bbf38b9d85a7b 2606f830346e4279b9075b9fb6139fb8 RX(asin(phi)) dc2a035b81e44576aa1287db5dfe2232--2606f830346e4279b9075b9fb6139fb8 eae9ce69d00f4d15bd1b4f583e130ed0 2 bc65b6944c5c49068cad8dab3855dfea RX(phi**2 + asin(phi)) 2606f830346e4279b9075b9fb6139fb8--bc65b6944c5c49068cad8dab3855dfea bc65b6944c5c49068cad8dab3855dfea--2f5c53f59e434816a29bbf38b9d85a7b 55e0dc42dbd444c3adca4190fb034c5a 3a5d6f2a348f48faaf767b0efdaff7b3 RX(asin(phi)) eae9ce69d00f4d15bd1b4f583e130ed0--3a5d6f2a348f48faaf767b0efdaff7b3 02279e1c176046159098b7c785cadc88 RX(phi**2 + asin(phi)) 3a5d6f2a348f48faaf767b0efdaff7b3--02279e1c176046159098b7c785cadc88 02279e1c176046159098b7c785cadc88--55e0dc42dbd444c3adca4190fb034c5a <p>Furthermore, the <code>reupload_scaling</code> argument can be used to change the scaling applied to each qubit in the support of the feature map. The default scalings can be chosen from the <code>ReuploadScaling</code> enumeration.</p> <pre><code>from qadence import ReuploadScaling\nfrom qadence.draw import display\n\nn_qubits = 5\n\n# Default constant value\nfm_constant = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT)\n\n# Linearly increasing scaling\nfm_tower = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.TOWER)\n\n# Exponentially increasing scaling\nfm_exp = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.EXP)\n\nblock = chain(fm_constant, fm_tower, fm_exp)\n</code></pre> %3 cluster_067eb07035f344bbbd3d97e68e94f59d Exponential Fourier FM cluster_7ed1a849c1e84d22a064059e14c14a94 Constant Fourier FM cluster_b35bf537b8db4610812336c4f82ece61 Tower Fourier FM 09e7e47f1c5c42b1a67228f243429b2d 0 967d83e2a8c24c098c1bea0ff0a1f1ea RX(phi) 09e7e47f1c5c42b1a67228f243429b2d--967d83e2a8c24c098c1bea0ff0a1f1ea 4d3c4e5dbc3b4073b34cea40e859069e 1 8c7b0ca487f04758a9ef0436dc00fd48 RX(1.0*phi) 967d83e2a8c24c098c1bea0ff0a1f1ea--8c7b0ca487f04758a9ef0436dc00fd48 1973d9483cc94d879736e9f16ebfe2d3 RX(1.0*phi) 8c7b0ca487f04758a9ef0436dc00fd48--1973d9483cc94d879736e9f16ebfe2d3 f7d779aa13ac492f8f6976f602e3d829 1973d9483cc94d879736e9f16ebfe2d3--f7d779aa13ac492f8f6976f602e3d829 6c1b4de338604f10965befc3f8be44df 46b25e3e9b7246f0a096a98a46356aa8 RX(phi) 4d3c4e5dbc3b4073b34cea40e859069e--46b25e3e9b7246f0a096a98a46356aa8 e06a799a052a433fa7908d1e30297371 2 e79e65a33ff540a491025b306cd8c6cc RX(2.0*phi) 46b25e3e9b7246f0a096a98a46356aa8--e79e65a33ff540a491025b306cd8c6cc 25c3cb81eee444c583b5239f592d173c RX(2.0*phi) e79e65a33ff540a491025b306cd8c6cc--25c3cb81eee444c583b5239f592d173c 25c3cb81eee444c583b5239f592d173c--6c1b4de338604f10965befc3f8be44df 67bbe2c1bfbe4bdeb8ab6be5310a091a 9e67cb1f5b1d4809b159274cbd5b22e8 RX(phi) e06a799a052a433fa7908d1e30297371--9e67cb1f5b1d4809b159274cbd5b22e8 c8dab028f8d6400d8c345a14569c5c1c 3 af4c22cf63654abe95be1da3aa65b6c2 RX(3.0*phi) 9e67cb1f5b1d4809b159274cbd5b22e8--af4c22cf63654abe95be1da3aa65b6c2 3474c7efe6914b7e9f06d429a9b6c84f RX(4.0*phi) af4c22cf63654abe95be1da3aa65b6c2--3474c7efe6914b7e9f06d429a9b6c84f 3474c7efe6914b7e9f06d429a9b6c84f--67bbe2c1bfbe4bdeb8ab6be5310a091a 60a4224c1b4844aa9836e3a77fa8e7a2 29d52f6f6a3f47d3a6a7d7fc15bdc6ea RX(phi) c8dab028f8d6400d8c345a14569c5c1c--29d52f6f6a3f47d3a6a7d7fc15bdc6ea 36d8e650bc894307811e9ce6790563e1 4 d547e07182fb4f3f8f42670dc5759dea RX(4.0*phi) 29d52f6f6a3f47d3a6a7d7fc15bdc6ea--d547e07182fb4f3f8f42670dc5759dea 83de174622224d3699bb2c6e6e320dc0 RX(8.0*phi) d547e07182fb4f3f8f42670dc5759dea--83de174622224d3699bb2c6e6e320dc0 83de174622224d3699bb2c6e6e320dc0--60a4224c1b4844aa9836e3a77fa8e7a2 83405de9b4fa473db2d0a00a68f7cebc 88725873cbd14fdcb1843599d1264e96 RX(phi) 36d8e650bc894307811e9ce6790563e1--88725873cbd14fdcb1843599d1264e96 078c099c1997450696f2ee52e372cad5 RX(5.0*phi) 88725873cbd14fdcb1843599d1264e96--078c099c1997450696f2ee52e372cad5 af43b874bebd467b84eb045303cf1ce4 RX(16.0*phi) 078c099c1997450696f2ee52e372cad5--af43b874bebd467b84eb045303cf1ce4 af43b874bebd467b84eb045303cf1ce4--83405de9b4fa473db2d0a00a68f7cebc <p>A custom scaling can also be defined with a function with an <code>int</code> input and <code>int</code> or <code>float</code> output.</p> <pre><code>n_qubits = 5\n\ndef custom_scaling(i: int) -&gt; int | float:\n    \"\"\"Sqrt(i+1)\"\"\"\n    return (i+1) ** (0.5)\n\n# Custom scaling function\nfm_custom = feature_map(n_qubits, fm_type=BasisSet.CHEBYSHEV, reupload_scaling=custom_scaling)\n</code></pre> %3 867e909ef88c4840b0d5bcd2201cdbd8 0 d650aff87b3144069aa25b90034f9b2b RX(1.0*acos(phi)) 867e909ef88c4840b0d5bcd2201cdbd8--d650aff87b3144069aa25b90034f9b2b a9fafe1032a24454bfaa456864d737a4 1 5ac3677715e648eebabae949dddee60d d650aff87b3144069aa25b90034f9b2b--5ac3677715e648eebabae949dddee60d 87c257ca86214bcabacba84755734b87 76abc66cf22d4eefb9456b0811d8a697 RX(1.414*acos(phi)) a9fafe1032a24454bfaa456864d737a4--76abc66cf22d4eefb9456b0811d8a697 4bfa237acc2a45b3a02f5ba0d3a00c57 2 76abc66cf22d4eefb9456b0811d8a697--87c257ca86214bcabacba84755734b87 086a10439ea24838aa966d1392778ac1 04945455029645e1b6882170ba462c96 RX(1.732*acos(phi)) 4bfa237acc2a45b3a02f5ba0d3a00c57--04945455029645e1b6882170ba462c96 310bac8259664d6c9745d3e9ca57e46c 3 04945455029645e1b6882170ba462c96--086a10439ea24838aa966d1392778ac1 79ea74f00b3d47dda43827716a22cd42 19822ee5f07e4b4c9ee399894ddfdeb6 RX(2.0*acos(phi)) 310bac8259664d6c9745d3e9ca57e46c--19822ee5f07e4b4c9ee399894ddfdeb6 dc4d9fb683e146148f597eb437de8769 4 19822ee5f07e4b4c9ee399894ddfdeb6--79ea74f00b3d47dda43827716a22cd42 cee0c63db9f34900843f13d1ebb2fd7b f92682b4d27a4ebf907f0249894a427d RX(2.236*acos(phi)) dc4d9fb683e146148f597eb437de8769--f92682b4d27a4ebf907f0249894a427d f92682b4d27a4ebf907f0249894a427d--cee0c63db9f34900843f13d1ebb2fd7b <p>To add a trainable parameter that multiplies the feature parameter inside the encoding function, simply pass a <code>param_prefix</code> string:</p> <pre><code>n_qubits = 5\n\nfm_trainable = feature_map(\n    n_qubits,\n    fm_type=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.EXP,\n    param_prefix = \"w\",\n)\n</code></pre> %3 d986edae0b5142f59a05a9c1d2638e01 0 e21ca3f3f515425eb4dc1ac11566cd92 RX(1.0*phi*w\u2080) d986edae0b5142f59a05a9c1d2638e01--e21ca3f3f515425eb4dc1ac11566cd92 31f06d0a34b14c96b46dd68ae8512ac2 1 9dab312386ef4e69a8c91ce3c3f13961 e21ca3f3f515425eb4dc1ac11566cd92--9dab312386ef4e69a8c91ce3c3f13961 7b2b2ae7ec6c4dfd887af7602c443a91 7e6a8dbabcc24de4a7888c596c27fa44 RX(2.0*phi*w\u2081) 31f06d0a34b14c96b46dd68ae8512ac2--7e6a8dbabcc24de4a7888c596c27fa44 67814ad4f9f44a5997c1ce06412a548f 2 7e6a8dbabcc24de4a7888c596c27fa44--7b2b2ae7ec6c4dfd887af7602c443a91 ebba9fb87a1241178322202fa8a60d85 12664bf8dbb745e3b2ad7a0f275e8ec0 RX(4.0*phi*w\u2082) 67814ad4f9f44a5997c1ce06412a548f--12664bf8dbb745e3b2ad7a0f275e8ec0 e7c5639eb01e4437986f16708e2d9b67 3 12664bf8dbb745e3b2ad7a0f275e8ec0--ebba9fb87a1241178322202fa8a60d85 76cc849f82334b33bab1c08fa2f35e11 2dfffe4478ed4b80a272473f25a0b58b RX(8.0*phi*w\u2083) e7c5639eb01e4437986f16708e2d9b67--2dfffe4478ed4b80a272473f25a0b58b 403191ffb4134462b8a6c6a58adca4b6 4 2dfffe4478ed4b80a272473f25a0b58b--76cc849f82334b33bab1c08fa2f35e11 9bf2ade8fcbd4e55bbbe312383e71803 6e6158e788db4666828e404d199d17ce RX(16.0*phi*w\u2084) 403191ffb4134462b8a6c6a58adca4b6--6e6158e788db4666828e404d199d17ce 6e6158e788db4666828e404d199d17ce--9bf2ade8fcbd4e55bbbe312383e71803 <p>Note that for the Fourier feature map, the encoding function is simply \\(f(x)=x\\). For other cases, like the Chebyshev <code>acos()</code> encoding, the trainable parameter may cause the feature value to be outside the domain of the encoding function. This will eventually be fixed by adding range constraints to trainable parameters in Qadence.</p> <p>A full description of the remaining arguments can be found in the <code>feature_map</code> API reference. We provide an example below.</p> <pre><code>from qadence import RY\n\nn_qubits = 5\n\n# Custom scaling function\nfm_full = feature_map(\n    n_qubits = n_qubits,\n    support = tuple(reversed(range(n_qubits))), # Reverse the qubit support to run the scaling from bottom to top\n    param = \"x\", # Change the name of the parameter\n    op = RY, # Change the rotation gate between RX, RY, RZ or PHASE\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.EXP,\n    feature_range = (-1.0, 2.0), # Range from which the input data comes from\n    target_range = (1.0, 3.0), # Range the encoder assumes as the natural range\n    multiplier = 5.0, # Extra multiplier, which can also be a Parameter\n    param_prefix = \"w\", # Add trainable parameters\n)\n</code></pre> %3 94a996a2048743a98bcb64c3e81ef2ea 0 99a416e7f3314c7baeac1b3c4647841f RY(80.0*acos(w\u2084*(0.667*x + 1.667))) 94a996a2048743a98bcb64c3e81ef2ea--99a416e7f3314c7baeac1b3c4647841f 909fa36dad04444b82d563756c85395f 1 b3ef937774ca42d9857fb86ea7fc0294 99a416e7f3314c7baeac1b3c4647841f--b3ef937774ca42d9857fb86ea7fc0294 6ce0d2d937324f65b56ac3aa37326ebc 2615da7c0d7248d58be035c4029ec49e RY(40.0*acos(w\u2083*(0.667*x + 1.667))) 909fa36dad04444b82d563756c85395f--2615da7c0d7248d58be035c4029ec49e a3876910a3c54e739c8171bdd045d40a 2 2615da7c0d7248d58be035c4029ec49e--6ce0d2d937324f65b56ac3aa37326ebc d106750114bd45bbb3575aaf4d49bfc7 aba45be9943340588ece3e2cd6d76671 RY(20.0*acos(w\u2082*(0.667*x + 1.667))) a3876910a3c54e739c8171bdd045d40a--aba45be9943340588ece3e2cd6d76671 2300b57d018847c5ae2efecc7665bd7f 3 aba45be9943340588ece3e2cd6d76671--d106750114bd45bbb3575aaf4d49bfc7 fe6de3b1e2264bf4b88e237df1f8fedf 3c58e842b44146e3b2bc61ade5c4f54b RY(10.0*acos(w\u2081*(0.667*x + 1.667))) 2300b57d018847c5ae2efecc7665bd7f--3c58e842b44146e3b2bc61ade5c4f54b 100de8a15c5c489caa6b2098a6571609 4 3c58e842b44146e3b2bc61ade5c4f54b--fe6de3b1e2264bf4b88e237df1f8fedf 9ea97abefce248659e1d1db3d506e87a dee04d3c244b4884959402a0bf67a0d8 RY(5.0*acos(w\u2080*(0.667*x + 1.667))) 100de8a15c5c489caa6b2098a6571609--dee04d3c244b4884959402a0bf67a0d8 dee04d3c244b4884959402a0bf67a0d8--9ea97abefce248659e1d1db3d506e87a"},{"location":"content/qml_constructors/#hardware-efficient-ansatz","title":"Hardware-efficient ansatz","text":"<p>Ansatze blocks for quantum machine-learning are typically built following the Hardware-Efficient Ansatz formalism (HEA). Both fully digital and digital-analog HEAs can easily be built with the <code>hea</code> function. By default, the digital version is returned:</p> <pre><code>from qadence import hea\nfrom qadence.draw import display\n\nn_qubits = 3\ndepth = 2\n\nansatz = hea(n_qubits, depth)\n</code></pre> %3 a19bcb9ff8fb4d5e99cc2e7ca265bac2 0 a466b6126cc7475b85cfc1cd58eb5a2b RX(theta\u2080) a19bcb9ff8fb4d5e99cc2e7ca265bac2--a466b6126cc7475b85cfc1cd58eb5a2b cd3b7f3bd3ba433cb23b2d25e0684a77 1 77d1eb372d604d709a6c9ae929a4e2a4 RY(theta\u2083) a466b6126cc7475b85cfc1cd58eb5a2b--77d1eb372d604d709a6c9ae929a4e2a4 45c4766c75924adfb736fa1a0c069509 RX(theta\u2086) 77d1eb372d604d709a6c9ae929a4e2a4--45c4766c75924adfb736fa1a0c069509 24cff01931854c228515efb8860c5c66 45c4766c75924adfb736fa1a0c069509--24cff01931854c228515efb8860c5c66 8347bbf6d861443c90972052d45bb20d 24cff01931854c228515efb8860c5c66--8347bbf6d861443c90972052d45bb20d 456c7e87cd004094966282b9b50693bc RX(theta\u2089) 8347bbf6d861443c90972052d45bb20d--456c7e87cd004094966282b9b50693bc fb98ca9a878e43fc8ab8978183ec28bb RY(theta\u2081\u2082) 456c7e87cd004094966282b9b50693bc--fb98ca9a878e43fc8ab8978183ec28bb ff57fe8c327a4a6ca127ae0f6ba6d759 RX(theta\u2081\u2085) fb98ca9a878e43fc8ab8978183ec28bb--ff57fe8c327a4a6ca127ae0f6ba6d759 1592fc25e61a4d82a86e463eb5c1f961 ff57fe8c327a4a6ca127ae0f6ba6d759--1592fc25e61a4d82a86e463eb5c1f961 da4a72567a404a6a9061714ffccce1f1 1592fc25e61a4d82a86e463eb5c1f961--da4a72567a404a6a9061714ffccce1f1 5fd12941b27f48d9b6128bd9f78acc91 da4a72567a404a6a9061714ffccce1f1--5fd12941b27f48d9b6128bd9f78acc91 d2e577c38fa345328f44f3b73abadcee 6b156678eb804b1aa612796312437014 RX(theta\u2081) cd3b7f3bd3ba433cb23b2d25e0684a77--6b156678eb804b1aa612796312437014 d5f4327f616c48a9bf16bd61eba84690 2 27d7517f358143f1b5c05e0205b2dbe4 RY(theta\u2084) 6b156678eb804b1aa612796312437014--27d7517f358143f1b5c05e0205b2dbe4 336bed1dab984562a63773113c991a10 RX(theta\u2087) 27d7517f358143f1b5c05e0205b2dbe4--336bed1dab984562a63773113c991a10 3ea9b733bbc04c2a9111e8bef541054e X 336bed1dab984562a63773113c991a10--3ea9b733bbc04c2a9111e8bef541054e 3ea9b733bbc04c2a9111e8bef541054e--24cff01931854c228515efb8860c5c66 d0044852c1b14ff1a9e0d50b2687cfda 3ea9b733bbc04c2a9111e8bef541054e--d0044852c1b14ff1a9e0d50b2687cfda 29cfd44d44204093b6dcb742a0354e0d RX(theta\u2081\u2080) d0044852c1b14ff1a9e0d50b2687cfda--29cfd44d44204093b6dcb742a0354e0d 494fa83eb9364c26a0a30b4b4d16636c RY(theta\u2081\u2083) 29cfd44d44204093b6dcb742a0354e0d--494fa83eb9364c26a0a30b4b4d16636c eb48f472d5754e39af59e7c2db8f31be RX(theta\u2081\u2086) 494fa83eb9364c26a0a30b4b4d16636c--eb48f472d5754e39af59e7c2db8f31be f7a7a7b5958a4b2a8e5dea75a13218ee X eb48f472d5754e39af59e7c2db8f31be--f7a7a7b5958a4b2a8e5dea75a13218ee f7a7a7b5958a4b2a8e5dea75a13218ee--1592fc25e61a4d82a86e463eb5c1f961 7d39af92869c408cb50dec06b4a7fb8e f7a7a7b5958a4b2a8e5dea75a13218ee--7d39af92869c408cb50dec06b4a7fb8e 7d39af92869c408cb50dec06b4a7fb8e--d2e577c38fa345328f44f3b73abadcee 2eb007e99f5544f884efef62c6db1249 008c2c2adffb49acb73b8beab7503b39 RX(theta\u2082) d5f4327f616c48a9bf16bd61eba84690--008c2c2adffb49acb73b8beab7503b39 8d5b8cd594aa4c4d9410c629625ea691 RY(theta\u2085) 008c2c2adffb49acb73b8beab7503b39--8d5b8cd594aa4c4d9410c629625ea691 f26426c018154aae92b4130270acbd1d RX(theta\u2088) 8d5b8cd594aa4c4d9410c629625ea691--f26426c018154aae92b4130270acbd1d e43984e18cb34c8cad83a71fca30432c f26426c018154aae92b4130270acbd1d--e43984e18cb34c8cad83a71fca30432c 452075d807134e0089ab5006ee2a4863 X e43984e18cb34c8cad83a71fca30432c--452075d807134e0089ab5006ee2a4863 452075d807134e0089ab5006ee2a4863--d0044852c1b14ff1a9e0d50b2687cfda 1fb0cfd2c2414ab5b3379247562eabed RX(theta\u2081\u2081) 452075d807134e0089ab5006ee2a4863--1fb0cfd2c2414ab5b3379247562eabed 93b1257740804413926a47a51433705d RY(theta\u2081\u2084) 1fb0cfd2c2414ab5b3379247562eabed--93b1257740804413926a47a51433705d 59868f8ec2734b0baceb97ec653c6b7c RX(theta\u2081\u2087) 93b1257740804413926a47a51433705d--59868f8ec2734b0baceb97ec653c6b7c eeb899e197e243e6b0d2636c99d1613f 59868f8ec2734b0baceb97ec653c6b7c--eeb899e197e243e6b0d2636c99d1613f e26ce398e99e46458c35f7c83712a463 X eeb899e197e243e6b0d2636c99d1613f--e26ce398e99e46458c35f7c83712a463 e26ce398e99e46458c35f7c83712a463--7d39af92869c408cb50dec06b4a7fb8e e26ce398e99e46458c35f7c83712a463--2eb007e99f5544f884efef62c6db1249 <p>As seen above, the rotation layers are automatically parameterized, and the prefix <code>\"theta\"</code> can be changed with the <code>param_prefix</code> argument.</p> <p>Furthermore, both the single-qubit rotations and the two-qubit entangler can be customized with the <code>operations</code> and <code>entangler</code> argument. The operations can be passed as a list of single-qubit rotations, while the entangler should be either <code>CNOT</code>, <code>CZ</code>, <code>CRX</code>, <code>CRY</code>, <code>CRZ</code> or <code>CPHASE</code>.</p> <pre><code>from qadence import RX, RY, CPHASE\n\nansatz = hea(\n    n_qubits=n_qubits,\n    depth=depth,\n    param_prefix=\"phi\",\n    operations=[RX, RY, RX],\n    entangler=CPHASE\n)\n</code></pre> %3 73e5aeefdfa04a49bc14026ec8273f94 0 94a513cd8fd447ce9b7d82de42a05092 RX(phi\u2080) 73e5aeefdfa04a49bc14026ec8273f94--94a513cd8fd447ce9b7d82de42a05092 13d183ed8bac4f4186f9733c88c6d90c 1 ad9cd0bb07144fe9988682298b79646e RY(phi\u2083) 94a513cd8fd447ce9b7d82de42a05092--ad9cd0bb07144fe9988682298b79646e 5e5f00aed0a241a983e8a296b8ccffc1 RX(phi\u2086) ad9cd0bb07144fe9988682298b79646e--5e5f00aed0a241a983e8a296b8ccffc1 5fe67263df044b7bb8e9783a81ff0002 5e5f00aed0a241a983e8a296b8ccffc1--5fe67263df044b7bb8e9783a81ff0002 0b3ef280c4f142f9b352e4d4af74f492 5fe67263df044b7bb8e9783a81ff0002--0b3ef280c4f142f9b352e4d4af74f492 5430e4e82b36435e9be22096a478495c RX(phi\u2089) 0b3ef280c4f142f9b352e4d4af74f492--5430e4e82b36435e9be22096a478495c 5a3eba02e7bc4d5e86f3b2ec10040800 RY(phi\u2081\u2082) 5430e4e82b36435e9be22096a478495c--5a3eba02e7bc4d5e86f3b2ec10040800 b81ace1b5ae7427f842978343f5f9a17 RX(phi\u2081\u2085) 5a3eba02e7bc4d5e86f3b2ec10040800--b81ace1b5ae7427f842978343f5f9a17 12d139db85e446bdb94ba04a4485034b b81ace1b5ae7427f842978343f5f9a17--12d139db85e446bdb94ba04a4485034b e4de6605bd67486ca624fc009f06aa3e 12d139db85e446bdb94ba04a4485034b--e4de6605bd67486ca624fc009f06aa3e 6046025463df48c8aedc0140acb4431a e4de6605bd67486ca624fc009f06aa3e--6046025463df48c8aedc0140acb4431a 3a8969c314494e84bae1eb806ca908fa c32b0caf91a54dcd8a2e2e2ec288e67b RX(phi\u2081) 13d183ed8bac4f4186f9733c88c6d90c--c32b0caf91a54dcd8a2e2e2ec288e67b f068de7cf484490baddc52675f944910 2 35d4857dce6748a5a9b7123bdaecf8d0 RY(phi\u2084) c32b0caf91a54dcd8a2e2e2ec288e67b--35d4857dce6748a5a9b7123bdaecf8d0 f14ab730333d47558672832b8693659b RX(phi\u2087) 35d4857dce6748a5a9b7123bdaecf8d0--f14ab730333d47558672832b8693659b 705af651fe0d46068aaa7493cc83cc65 PHASE(phi_ent\u2080) f14ab730333d47558672832b8693659b--705af651fe0d46068aaa7493cc83cc65 705af651fe0d46068aaa7493cc83cc65--5fe67263df044b7bb8e9783a81ff0002 65372742fabf4215b81aac443a61d210 705af651fe0d46068aaa7493cc83cc65--65372742fabf4215b81aac443a61d210 3c7c14e12aa94e36a639faadacb25b59 RX(phi\u2081\u2080) 65372742fabf4215b81aac443a61d210--3c7c14e12aa94e36a639faadacb25b59 7dfbf2e6c85045b6afce8c0e6ca8c5f4 RY(phi\u2081\u2083) 3c7c14e12aa94e36a639faadacb25b59--7dfbf2e6c85045b6afce8c0e6ca8c5f4 0b143649c5234272b384846cd5885c0a RX(phi\u2081\u2086) 7dfbf2e6c85045b6afce8c0e6ca8c5f4--0b143649c5234272b384846cd5885c0a 934c0f23b5d04df09fdd542bd66cb014 PHASE(phi_ent\u2082) 0b143649c5234272b384846cd5885c0a--934c0f23b5d04df09fdd542bd66cb014 934c0f23b5d04df09fdd542bd66cb014--12d139db85e446bdb94ba04a4485034b 86038bfc67fe45efa783eec3ebddcd2b 934c0f23b5d04df09fdd542bd66cb014--86038bfc67fe45efa783eec3ebddcd2b 86038bfc67fe45efa783eec3ebddcd2b--3a8969c314494e84bae1eb806ca908fa f7666cbd8d814e92ae68db13288b8c2f c52fc99099d44686be09d299c857a13b RX(phi\u2082) f068de7cf484490baddc52675f944910--c52fc99099d44686be09d299c857a13b c03e8e91615041c6902b611f5976afbb RY(phi\u2085) c52fc99099d44686be09d299c857a13b--c03e8e91615041c6902b611f5976afbb 246af20e5c2440be8a9c42b47f32eb10 RX(phi\u2088) c03e8e91615041c6902b611f5976afbb--246af20e5c2440be8a9c42b47f32eb10 e1ecb9eaf09c41b3a705e60ab30a2cb1 246af20e5c2440be8a9c42b47f32eb10--e1ecb9eaf09c41b3a705e60ab30a2cb1 c6c4f7add237416586f163e7130015aa PHASE(phi_ent\u2081) e1ecb9eaf09c41b3a705e60ab30a2cb1--c6c4f7add237416586f163e7130015aa c6c4f7add237416586f163e7130015aa--65372742fabf4215b81aac443a61d210 2292857dd6d64a209fdd7a072a7f73e6 RX(phi\u2081\u2081) c6c4f7add237416586f163e7130015aa--2292857dd6d64a209fdd7a072a7f73e6 707dcefd0ae7463f809da20b5cabbc8d RY(phi\u2081\u2084) 2292857dd6d64a209fdd7a072a7f73e6--707dcefd0ae7463f809da20b5cabbc8d ed69b4255141476b98688cc99dbf5efb RX(phi\u2081\u2087) 707dcefd0ae7463f809da20b5cabbc8d--ed69b4255141476b98688cc99dbf5efb 13ea4963a53348009b9733298dee98f8 ed69b4255141476b98688cc99dbf5efb--13ea4963a53348009b9733298dee98f8 dd883aa4ea694735bc22b60cc16e27bd PHASE(phi_ent\u2083) 13ea4963a53348009b9733298dee98f8--dd883aa4ea694735bc22b60cc16e27bd dd883aa4ea694735bc22b60cc16e27bd--86038bfc67fe45efa783eec3ebddcd2b dd883aa4ea694735bc22b60cc16e27bd--f7666cbd8d814e92ae68db13288b8c2f <p>Having a truly hardware-efficient ansatz means that the entangling operation can be chosen according to each device's native interactions. Besides digital operations, in Qadence it is also possible to build digital-analog HEAs with the entanglement produced by the natural evolution of a set of interacting qubits, as natively implemented in neutral atom devices. As with other digital-analog functions, this can be controlled with the <code>strategy</code> argument which can be chosen from the <code>Strategy</code> enum type. Currently, only <code>Strategy.DIGITAL</code> and <code>Strategy.SDAQC</code> are available. By default, calling <code>strategy = Strategy.SDAQC</code> will use a global entangling Hamiltonian with Ising-like \\(NN\\) interactions and constant interaction strength,</p> <pre><code>from qadence import Strategy\n\nansatz = hea(\n    n_qubits,\n    depth=depth,\n    strategy=Strategy.SDAQC\n)\n</code></pre> %3 cluster_87900bf3084549df90058b6af0fe2c3b cluster_c5d7dc6cf9b7418fbb635ec71e935fc4 4ecb9e2670c2440d978a0b59320c35d6 0 783d89cbe95d4436b50ffe4b72f624f7 RX(theta\u2080) 4ecb9e2670c2440d978a0b59320c35d6--783d89cbe95d4436b50ffe4b72f624f7 7f76d29700ce40f8818391caf956467f 1 c9366e4d723f42cbacf433095e630e85 RY(theta\u2083) 783d89cbe95d4436b50ffe4b72f624f7--c9366e4d723f42cbacf433095e630e85 237824318fe54492a9570fcc3da39c4b RX(theta\u2086) c9366e4d723f42cbacf433095e630e85--237824318fe54492a9570fcc3da39c4b be3f821cc4f74041b6bcbabb5a760b26 HamEvo 237824318fe54492a9570fcc3da39c4b--be3f821cc4f74041b6bcbabb5a760b26 e3c456fcf2824f3ab286d0ffb9a759a5 RX(theta\u2089) be3f821cc4f74041b6bcbabb5a760b26--e3c456fcf2824f3ab286d0ffb9a759a5 199f015c8d2741a482552ed11e3ad7b0 RY(theta\u2081\u2082) e3c456fcf2824f3ab286d0ffb9a759a5--199f015c8d2741a482552ed11e3ad7b0 62f7ad11c78b40ceacac705b2b16e4f5 RX(theta\u2081\u2085) 199f015c8d2741a482552ed11e3ad7b0--62f7ad11c78b40ceacac705b2b16e4f5 e4613a3a47914a8fa81a8e81e4ba0e18 HamEvo 62f7ad11c78b40ceacac705b2b16e4f5--e4613a3a47914a8fa81a8e81e4ba0e18 f435f710297b41f3b14697f9f57284a9 e4613a3a47914a8fa81a8e81e4ba0e18--f435f710297b41f3b14697f9f57284a9 b915da0d01a846ce926cdbcad04e63a6 09c1237b4ff9464eb71ceffd29e4f807 RX(theta\u2081) 7f76d29700ce40f8818391caf956467f--09c1237b4ff9464eb71ceffd29e4f807 f99cdbf8602149dea1afcf349421c76a 2 dcd2268b04d140c1847ff4a35c2bf099 RY(theta\u2084) 09c1237b4ff9464eb71ceffd29e4f807--dcd2268b04d140c1847ff4a35c2bf099 6867a66dcd4340418df23c5f3c91341c RX(theta\u2087) dcd2268b04d140c1847ff4a35c2bf099--6867a66dcd4340418df23c5f3c91341c 15e6ad2b8a3444048b2c07447108e0a0 t = theta_t\u2080 6867a66dcd4340418df23c5f3c91341c--15e6ad2b8a3444048b2c07447108e0a0 743d9e18a9b84a91a70cdf86f8921b56 RX(theta\u2081\u2080) 15e6ad2b8a3444048b2c07447108e0a0--743d9e18a9b84a91a70cdf86f8921b56 471c4aa6369149518dd7d065938caa7c RY(theta\u2081\u2083) 743d9e18a9b84a91a70cdf86f8921b56--471c4aa6369149518dd7d065938caa7c abb27f276ebd4bf8b89706753daf5207 RX(theta\u2081\u2086) 471c4aa6369149518dd7d065938caa7c--abb27f276ebd4bf8b89706753daf5207 c3db926d95ab43829b9677b568c4243c t = theta_t\u2081 abb27f276ebd4bf8b89706753daf5207--c3db926d95ab43829b9677b568c4243c c3db926d95ab43829b9677b568c4243c--b915da0d01a846ce926cdbcad04e63a6 237ccd9df0b04af29ee664223d232ae1 8aa828f5dbf1441abbf66207508f0e74 RX(theta\u2082) f99cdbf8602149dea1afcf349421c76a--8aa828f5dbf1441abbf66207508f0e74 cc28d7e86c264a36bc9f81cdc6a614cc RY(theta\u2085) 8aa828f5dbf1441abbf66207508f0e74--cc28d7e86c264a36bc9f81cdc6a614cc bb562adddc394e408c647f54a560e72d RX(theta\u2088) cc28d7e86c264a36bc9f81cdc6a614cc--bb562adddc394e408c647f54a560e72d b12842d8dfea4e7ea4552f8d8be41102 bb562adddc394e408c647f54a560e72d--b12842d8dfea4e7ea4552f8d8be41102 0e2e687b5e4c4faa932761f8ae10fa70 RX(theta\u2081\u2081) b12842d8dfea4e7ea4552f8d8be41102--0e2e687b5e4c4faa932761f8ae10fa70 79e70324824547b98831a9313f51d7f3 RY(theta\u2081\u2084) 0e2e687b5e4c4faa932761f8ae10fa70--79e70324824547b98831a9313f51d7f3 d0406c510c44493180291eeaccd90a58 RX(theta\u2081\u2087) 79e70324824547b98831a9313f51d7f3--d0406c510c44493180291eeaccd90a58 93c93d82a912435dabd42bedd3ea5b1e d0406c510c44493180291eeaccd90a58--93c93d82a912435dabd42bedd3ea5b1e 93c93d82a912435dabd42bedd3ea5b1e--237ccd9df0b04af29ee664223d232ae1 <p>Note that, by default, only the time-parameter is automatically parameterized when building a digital-analog HEA. However, as described in the Hamiltonians tutorial, arbitrary interaction Hamiltonians can be easily built with the <code>hamiltonian_factory</code> function, with both customized or fully parameterized interactions, and these can be directly passed as the <code>entangler</code> for a customizable digital-analog HEA.</p> <pre><code>from qadence import hamiltonian_factory, Interaction, N, Register, hea\n\n# Build a parameterized neutral-atom Hamiltonian following a honeycomb_lattice:\nregister = Register.honeycomb_lattice(1, 1)\n\nentangler = hamiltonian_factory(\n    register,\n    interaction=Interaction.NN,\n    detuning=N,\n    interaction_strength=\"e\",\n    detuning_strength=\"n\"\n)\n\n# Build a fully parameterized Digital-Analog HEA:\nn_qubits = register.n_qubits\ndepth = 2\n\nansatz = hea(\n    n_qubits=register.n_qubits,\n    depth=depth,\n    operations=[RX, RY, RX],\n    entangler=entangler,\n    strategy=Strategy.SDAQC\n)\n</code></pre> %3 cluster_eebf1ca7e2d74e7da44bc1ceccc26d33 cluster_0c2c0baf0ccc4dd7839b3476a4978de1 cdc74d5e967f46528632b070f6b83202 0 1da6530c99604354affe56a806da159f RX(theta\u2080) cdc74d5e967f46528632b070f6b83202--1da6530c99604354affe56a806da159f ec6b0ce77b584d078bc2b34e6c210a51 1 832c66f1be9b4e16a8237f141187842c RY(theta\u2086) 1da6530c99604354affe56a806da159f--832c66f1be9b4e16a8237f141187842c 70cb8340665049859d9d0573df6ff835 RX(theta\u2081\u2082) 832c66f1be9b4e16a8237f141187842c--70cb8340665049859d9d0573df6ff835 eb657600b3a94791a5bf057b7aaf9d8f 70cb8340665049859d9d0573df6ff835--eb657600b3a94791a5bf057b7aaf9d8f f60f8a1bb2314e10b980138d0a333949 RX(theta\u2081\u2088) eb657600b3a94791a5bf057b7aaf9d8f--f60f8a1bb2314e10b980138d0a333949 9683997f5d8d448a8ae1396b523b62c7 RY(theta\u2082\u2084) f60f8a1bb2314e10b980138d0a333949--9683997f5d8d448a8ae1396b523b62c7 83450ec34b384f0189b3bae8fb0c9654 RX(theta\u2083\u2080) 9683997f5d8d448a8ae1396b523b62c7--83450ec34b384f0189b3bae8fb0c9654 6d98cdccadcc4506926ffc3bab9a3fc0 83450ec34b384f0189b3bae8fb0c9654--6d98cdccadcc4506926ffc3bab9a3fc0 084bbcf035864a079c4940efa1af3e2c 6d98cdccadcc4506926ffc3bab9a3fc0--084bbcf035864a079c4940efa1af3e2c f7955c065b3d48c5bf175e6227658285 1d18da3dc0d840dfa084daa6b444b9f2 RX(theta\u2081) ec6b0ce77b584d078bc2b34e6c210a51--1d18da3dc0d840dfa084daa6b444b9f2 519752a7f0fc4a22bb18c1e9709ffc9e 2 cbd5290256144897bc283f5c985cb9ff RY(theta\u2087) 1d18da3dc0d840dfa084daa6b444b9f2--cbd5290256144897bc283f5c985cb9ff 32773f62fe3f4386936682b178dbafe0 RX(theta\u2081\u2083) cbd5290256144897bc283f5c985cb9ff--32773f62fe3f4386936682b178dbafe0 37b3d99d6a7e490a99cb560513c150a8 32773f62fe3f4386936682b178dbafe0--37b3d99d6a7e490a99cb560513c150a8 4d760e144a6d4d69ab40060bbaa309e3 RX(theta\u2081\u2089) 37b3d99d6a7e490a99cb560513c150a8--4d760e144a6d4d69ab40060bbaa309e3 20c0c4b249034602bb4eef1e0021d76e RY(theta\u2082\u2085) 4d760e144a6d4d69ab40060bbaa309e3--20c0c4b249034602bb4eef1e0021d76e 4d49ca2848ae469eb121c7c7f95f3950 RX(theta\u2083\u2081) 20c0c4b249034602bb4eef1e0021d76e--4d49ca2848ae469eb121c7c7f95f3950 ba7cacd98a0744f9b026aa5223840a50 4d49ca2848ae469eb121c7c7f95f3950--ba7cacd98a0744f9b026aa5223840a50 ba7cacd98a0744f9b026aa5223840a50--f7955c065b3d48c5bf175e6227658285 7e10f80bd1824f688a31e3bba43a8ed5 ba215b70132845aa8f32a5570a4cb6dd RX(theta\u2082) 519752a7f0fc4a22bb18c1e9709ffc9e--ba215b70132845aa8f32a5570a4cb6dd 9020425fe5f743f39e0eeac634e871f6 3 452d9e69e6b240b6ae34b36ca50c7598 RY(theta\u2088) ba215b70132845aa8f32a5570a4cb6dd--452d9e69e6b240b6ae34b36ca50c7598 cc323fa364d647018076b4d769db6975 RX(theta\u2081\u2084) 452d9e69e6b240b6ae34b36ca50c7598--cc323fa364d647018076b4d769db6975 9aa542ed9e9d4692bb6efa3f748f19e5 HamEvo cc323fa364d647018076b4d769db6975--9aa542ed9e9d4692bb6efa3f748f19e5 2e1a42f09e384c7eb435ce7191c8f0a5 RX(theta\u2082\u2080) 9aa542ed9e9d4692bb6efa3f748f19e5--2e1a42f09e384c7eb435ce7191c8f0a5 408a3bf66b9440f6a9dfd62485c1ca28 RY(theta\u2082\u2086) 2e1a42f09e384c7eb435ce7191c8f0a5--408a3bf66b9440f6a9dfd62485c1ca28 9f05726d717341f58c679633770f8f42 RX(theta\u2083\u2082) 408a3bf66b9440f6a9dfd62485c1ca28--9f05726d717341f58c679633770f8f42 3bb2ef0e25f74e168b840784f2cd3a68 HamEvo 9f05726d717341f58c679633770f8f42--3bb2ef0e25f74e168b840784f2cd3a68 3bb2ef0e25f74e168b840784f2cd3a68--7e10f80bd1824f688a31e3bba43a8ed5 90b6f022f4564c4795282d0937333229 43ef04dc6a5e4253a59d7dee169695b1 RX(theta\u2083) 9020425fe5f743f39e0eeac634e871f6--43ef04dc6a5e4253a59d7dee169695b1 73b72ecf2a0549cc8aa8897f50421f11 4 0a97810c04f14472a25edf0cddf2bc10 RY(theta\u2089) 43ef04dc6a5e4253a59d7dee169695b1--0a97810c04f14472a25edf0cddf2bc10 7ee4e0a638f949c094cde916e66cb8a2 RX(theta\u2081\u2085) 0a97810c04f14472a25edf0cddf2bc10--7ee4e0a638f949c094cde916e66cb8a2 fc13a780745d46088773ccf046c43aa5 t = theta_t\u2080 7ee4e0a638f949c094cde916e66cb8a2--fc13a780745d46088773ccf046c43aa5 9f444d5cd110408ab37054208df59189 RX(theta\u2082\u2081) fc13a780745d46088773ccf046c43aa5--9f444d5cd110408ab37054208df59189 42f830f558cc4d1a91c0b5f4116302e0 RY(theta\u2082\u2087) 9f444d5cd110408ab37054208df59189--42f830f558cc4d1a91c0b5f4116302e0 2edce6fa705a401cbbf97f4eb5cca29e RX(theta\u2083\u2083) 42f830f558cc4d1a91c0b5f4116302e0--2edce6fa705a401cbbf97f4eb5cca29e bb236b2eea794fce8ad1b43a1adb4f6b t = theta_t\u2081 2edce6fa705a401cbbf97f4eb5cca29e--bb236b2eea794fce8ad1b43a1adb4f6b bb236b2eea794fce8ad1b43a1adb4f6b--90b6f022f4564c4795282d0937333229 fe06fa3cc57b4707be4e93dd7ede61d8 4b5bd2d567a44034959798e7f6159bb8 RX(theta\u2084) 73b72ecf2a0549cc8aa8897f50421f11--4b5bd2d567a44034959798e7f6159bb8 e637df5ce83e49a2b3e5a6a9239b32eb 5 c331ebf78d2f475c9f693384ba031c34 RY(theta\u2081\u2080) 4b5bd2d567a44034959798e7f6159bb8--c331ebf78d2f475c9f693384ba031c34 1770629ccf8e49eaa59b2a84467f4365 RX(theta\u2081\u2086) c331ebf78d2f475c9f693384ba031c34--1770629ccf8e49eaa59b2a84467f4365 21f585b97476415587fa04dd19322420 1770629ccf8e49eaa59b2a84467f4365--21f585b97476415587fa04dd19322420 2e6f90625ca84a0796dd49bac1014409 RX(theta\u2082\u2082) 21f585b97476415587fa04dd19322420--2e6f90625ca84a0796dd49bac1014409 6478cd759bdb4a66899e28730d7d3729 RY(theta\u2082\u2088) 2e6f90625ca84a0796dd49bac1014409--6478cd759bdb4a66899e28730d7d3729 0c9c4dc42fca42b4b867421ddbaca68e RX(theta\u2083\u2084) 6478cd759bdb4a66899e28730d7d3729--0c9c4dc42fca42b4b867421ddbaca68e ecfeb29711df4b0eadac674434041506 0c9c4dc42fca42b4b867421ddbaca68e--ecfeb29711df4b0eadac674434041506 ecfeb29711df4b0eadac674434041506--fe06fa3cc57b4707be4e93dd7ede61d8 ee304360bcaf4519a47b20f60b5b5596 eb4c9814d3ce446eb604736b83af5eaf RX(theta\u2085) e637df5ce83e49a2b3e5a6a9239b32eb--eb4c9814d3ce446eb604736b83af5eaf 9b629e54e0cc48fdaef6a78db152fcd9 RY(theta\u2081\u2081) eb4c9814d3ce446eb604736b83af5eaf--9b629e54e0cc48fdaef6a78db152fcd9 f95d80069a234b1f9383fb810509f272 RX(theta\u2081\u2087) 9b629e54e0cc48fdaef6a78db152fcd9--f95d80069a234b1f9383fb810509f272 cdad3b155bb1456d99e9ad158786acbc f95d80069a234b1f9383fb810509f272--cdad3b155bb1456d99e9ad158786acbc 207c328350e1444d8f429b5261b9e2ab RX(theta\u2082\u2083) cdad3b155bb1456d99e9ad158786acbc--207c328350e1444d8f429b5261b9e2ab 4d1ea298ef3540efa8aa3ca737ee0864 RY(theta\u2082\u2089) 207c328350e1444d8f429b5261b9e2ab--4d1ea298ef3540efa8aa3ca737ee0864 ffb76daec2174398a7722277e8b201ea RX(theta\u2083\u2085) 4d1ea298ef3540efa8aa3ca737ee0864--ffb76daec2174398a7722277e8b201ea dfd5b6bccec64808bc1306f3f1db884e ffb76daec2174398a7722277e8b201ea--dfd5b6bccec64808bc1306f3f1db884e dfd5b6bccec64808bc1306f3f1db884e--ee304360bcaf4519a47b20f60b5b5596"},{"location":"content/qml_constructors/#identity-initialized-ansatz","title":"Identity-initialized ansatz","text":"<p>It is widely known that parametrized quantum circuits are characterized by barren plateaus, where the gradient becomes exponentially small in the number of qubits. Here we include one of many techniques that have been proposed in recent years to mitigate this effect and facilitate <code>QNN</code>s training: Grant et al. showed that initializing the weights of a <code>QNN</code> so that each block of the circuit evaluates to identity reduces the effect of barren plateaus in the initial stage of training. In a similar fashion to <code>hea</code>, such circuit can be created via calling the associated function, <code>identity_initialized_ansatz</code>:</p> <pre><code>from qadence.constructors import identity_initialized_ansatz\nfrom qadence.draw import display\n\nn_qubits = 3\ndepth = 2\n\nansatz = identity_initialized_ansatz(n_qubits, depth)\n</code></pre> %3 cluster_d2a530c5c7754d249ede83301cc503cf BPMA-1 cluster_f1a8824a02c34517915c336d033031ac BPMA-0 8a808c65d4aa4a4f97e45269b405fb0c 0 5317c940ba2f4314a30d8d4a03ccfef8 RX(iia_\u03b1\u2080\u2080) 8a808c65d4aa4a4f97e45269b405fb0c--5317c940ba2f4314a30d8d4a03ccfef8 8981c0971f854e91a1afbea1e388145e 1 a8dcde1f6e214897af445e59d36a6cb6 RY(iia_\u03b1\u2080\u2083) 5317c940ba2f4314a30d8d4a03ccfef8--a8dcde1f6e214897af445e59d36a6cb6 8cedf52ab834467d8b084d39fa30408a a8dcde1f6e214897af445e59d36a6cb6--8cedf52ab834467d8b084d39fa30408a ee76ca6db6c14108a075b233ebe32639 8cedf52ab834467d8b084d39fa30408a--ee76ca6db6c14108a075b233ebe32639 9c7be0f93ca042199f693759d06a4445 RX(iia_\u03b3\u2080\u2080) ee76ca6db6c14108a075b233ebe32639--9c7be0f93ca042199f693759d06a4445 9be6fa070f634a47b8fd431b0b54bb6c 9c7be0f93ca042199f693759d06a4445--9be6fa070f634a47b8fd431b0b54bb6c 7b4a98e2cae048b4bb40d620f3ec0861 9be6fa070f634a47b8fd431b0b54bb6c--7b4a98e2cae048b4bb40d620f3ec0861 41d45a80c0a34337aa131263dda7d2cd RY(iia_\u03b2\u2080\u2083) 7b4a98e2cae048b4bb40d620f3ec0861--41d45a80c0a34337aa131263dda7d2cd 37f25d318fd74bf2bc26d9207b0fb6f1 RX(iia_\u03b2\u2080\u2080) 41d45a80c0a34337aa131263dda7d2cd--37f25d318fd74bf2bc26d9207b0fb6f1 2cd1ff38896d4dccbcd7c4f35544bf3b RX(iia_\u03b1\u2081\u2080) 37f25d318fd74bf2bc26d9207b0fb6f1--2cd1ff38896d4dccbcd7c4f35544bf3b eb268809ecec4c7ead59de6b6e7062a8 RY(iia_\u03b1\u2081\u2083) 2cd1ff38896d4dccbcd7c4f35544bf3b--eb268809ecec4c7ead59de6b6e7062a8 6199da861a6d4cb5be035dae3601b3fa eb268809ecec4c7ead59de6b6e7062a8--6199da861a6d4cb5be035dae3601b3fa 205abe5c6265422ea898c07801b578ff 6199da861a6d4cb5be035dae3601b3fa--205abe5c6265422ea898c07801b578ff fafb7d21cdcf47168f01c189b8a79ea1 RX(iia_\u03b3\u2081\u2080) 205abe5c6265422ea898c07801b578ff--fafb7d21cdcf47168f01c189b8a79ea1 29b24cff29254f2aa7a5ca5d72ad4872 fafb7d21cdcf47168f01c189b8a79ea1--29b24cff29254f2aa7a5ca5d72ad4872 9bf167862ed449cba2b2dba712b27923 29b24cff29254f2aa7a5ca5d72ad4872--9bf167862ed449cba2b2dba712b27923 62ecfc2785ef4875992460ce709e0cdd RY(iia_\u03b2\u2081\u2083) 9bf167862ed449cba2b2dba712b27923--62ecfc2785ef4875992460ce709e0cdd b1cc91c0cdfa4be4861403a53d88c106 RX(iia_\u03b2\u2081\u2080) 62ecfc2785ef4875992460ce709e0cdd--b1cc91c0cdfa4be4861403a53d88c106 04e46c826ae6495685f5bf6832cf7077 b1cc91c0cdfa4be4861403a53d88c106--04e46c826ae6495685f5bf6832cf7077 c5a371ab0a0a4583b821b71b3e5adddc 9342594b479048d99cfe4255b711af13 RX(iia_\u03b1\u2080\u2081) 8981c0971f854e91a1afbea1e388145e--9342594b479048d99cfe4255b711af13 0479c43b732145238a869ec8a72bbfed 2 194e0bb01f40413c8ba446d698a134bf RY(iia_\u03b1\u2080\u2084) 9342594b479048d99cfe4255b711af13--194e0bb01f40413c8ba446d698a134bf 9d0da7551ee247329d3bda4145ed6fd9 X 194e0bb01f40413c8ba446d698a134bf--9d0da7551ee247329d3bda4145ed6fd9 9d0da7551ee247329d3bda4145ed6fd9--8cedf52ab834467d8b084d39fa30408a 6744a2ef0cea409b91b8f83f4043de2a 9d0da7551ee247329d3bda4145ed6fd9--6744a2ef0cea409b91b8f83f4043de2a aa9f8eb2d9e14c3e9052eed2cc30848f RX(iia_\u03b3\u2080\u2081) 6744a2ef0cea409b91b8f83f4043de2a--aa9f8eb2d9e14c3e9052eed2cc30848f cbd3699f04644a6a99b41d6153190279 aa9f8eb2d9e14c3e9052eed2cc30848f--cbd3699f04644a6a99b41d6153190279 886f8ceda1c741a2b755989da6ce2ce6 X cbd3699f04644a6a99b41d6153190279--886f8ceda1c741a2b755989da6ce2ce6 886f8ceda1c741a2b755989da6ce2ce6--7b4a98e2cae048b4bb40d620f3ec0861 279964b88919417d899d798e79a89491 RY(iia_\u03b2\u2080\u2084) 886f8ceda1c741a2b755989da6ce2ce6--279964b88919417d899d798e79a89491 76d9f781e0434a5cb7d87746a3cad6cb RX(iia_\u03b2\u2080\u2081) 279964b88919417d899d798e79a89491--76d9f781e0434a5cb7d87746a3cad6cb 58a92efa9e5d4d01b7ea146a2f5d684c RX(iia_\u03b1\u2081\u2081) 76d9f781e0434a5cb7d87746a3cad6cb--58a92efa9e5d4d01b7ea146a2f5d684c 186a8d14ae334dd8b53dde5dca1410fc RY(iia_\u03b1\u2081\u2084) 58a92efa9e5d4d01b7ea146a2f5d684c--186a8d14ae334dd8b53dde5dca1410fc e3213edbaafa4891b4e7af12984676ae X 186a8d14ae334dd8b53dde5dca1410fc--e3213edbaafa4891b4e7af12984676ae e3213edbaafa4891b4e7af12984676ae--6199da861a6d4cb5be035dae3601b3fa 7b8b107cebf746f38346906556c6394a e3213edbaafa4891b4e7af12984676ae--7b8b107cebf746f38346906556c6394a 74dcf638e5a7487da1832dbf08bca58d RX(iia_\u03b3\u2081\u2081) 7b8b107cebf746f38346906556c6394a--74dcf638e5a7487da1832dbf08bca58d 03c1df20d2264d9ca4cd7c47712233cc 74dcf638e5a7487da1832dbf08bca58d--03c1df20d2264d9ca4cd7c47712233cc 7af7294ae57d481e8846d098ee6963d1 X 03c1df20d2264d9ca4cd7c47712233cc--7af7294ae57d481e8846d098ee6963d1 7af7294ae57d481e8846d098ee6963d1--9bf167862ed449cba2b2dba712b27923 da0043ec684549b49e254f606f93e2b8 RY(iia_\u03b2\u2081\u2084) 7af7294ae57d481e8846d098ee6963d1--da0043ec684549b49e254f606f93e2b8 632d25833ea84ac480399f11302ca0a1 RX(iia_\u03b2\u2081\u2081) da0043ec684549b49e254f606f93e2b8--632d25833ea84ac480399f11302ca0a1 632d25833ea84ac480399f11302ca0a1--c5a371ab0a0a4583b821b71b3e5adddc 9feac6380d9947838cf01b79171770ea 0e690bfbf29e434fb8f17a5b2adf25fa RX(iia_\u03b1\u2080\u2082) 0479c43b732145238a869ec8a72bbfed--0e690bfbf29e434fb8f17a5b2adf25fa ab4c0ef093844365976a6e31f515d50b RY(iia_\u03b1\u2080\u2085) 0e690bfbf29e434fb8f17a5b2adf25fa--ab4c0ef093844365976a6e31f515d50b 510924a3e6d54a9b99f91af657cfd6c0 ab4c0ef093844365976a6e31f515d50b--510924a3e6d54a9b99f91af657cfd6c0 a4edb68ba65041bbac129f5470f2756c X 510924a3e6d54a9b99f91af657cfd6c0--a4edb68ba65041bbac129f5470f2756c a4edb68ba65041bbac129f5470f2756c--6744a2ef0cea409b91b8f83f4043de2a c5f0a3e6cf404add90d614669bc8b160 RX(iia_\u03b3\u2080\u2082) a4edb68ba65041bbac129f5470f2756c--c5f0a3e6cf404add90d614669bc8b160 b066f3e9ad3040c99bcae376ec6a8e98 X c5f0a3e6cf404add90d614669bc8b160--b066f3e9ad3040c99bcae376ec6a8e98 b066f3e9ad3040c99bcae376ec6a8e98--cbd3699f04644a6a99b41d6153190279 562b0646d2424471bb3056c912ce7f99 b066f3e9ad3040c99bcae376ec6a8e98--562b0646d2424471bb3056c912ce7f99 d9fb7a7d819c413cbf16ec315f99963d RY(iia_\u03b2\u2080\u2085) 562b0646d2424471bb3056c912ce7f99--d9fb7a7d819c413cbf16ec315f99963d 78fd524f18bd4859badce6fdd4db1980 RX(iia_\u03b2\u2080\u2082) d9fb7a7d819c413cbf16ec315f99963d--78fd524f18bd4859badce6fdd4db1980 43203b5470be41bdb8e120b2d84323da RX(iia_\u03b1\u2081\u2082) 78fd524f18bd4859badce6fdd4db1980--43203b5470be41bdb8e120b2d84323da 8f39655001a44e7c8e3e11be90af0786 RY(iia_\u03b1\u2081\u2085) 43203b5470be41bdb8e120b2d84323da--8f39655001a44e7c8e3e11be90af0786 050c4e2b37734bc994207aabf414fe26 8f39655001a44e7c8e3e11be90af0786--050c4e2b37734bc994207aabf414fe26 be39b04ac0444572b36e70e48643bac9 X 050c4e2b37734bc994207aabf414fe26--be39b04ac0444572b36e70e48643bac9 be39b04ac0444572b36e70e48643bac9--7b8b107cebf746f38346906556c6394a a055b876bc644d0e9aee9e241b2309a0 RX(iia_\u03b3\u2081\u2082) be39b04ac0444572b36e70e48643bac9--a055b876bc644d0e9aee9e241b2309a0 226b783b2dec4d9b8ff894d119e3e363 X a055b876bc644d0e9aee9e241b2309a0--226b783b2dec4d9b8ff894d119e3e363 226b783b2dec4d9b8ff894d119e3e363--03c1df20d2264d9ca4cd7c47712233cc 678a0cc4e5f44aaf9abc35e1a7fd69cd 226b783b2dec4d9b8ff894d119e3e363--678a0cc4e5f44aaf9abc35e1a7fd69cd aee8af2dfc4a4c878ab086ffd79f711f RY(iia_\u03b2\u2081\u2085) 678a0cc4e5f44aaf9abc35e1a7fd69cd--aee8af2dfc4a4c878ab086ffd79f711f 7398f25e330549848050a32d995547c2 RX(iia_\u03b2\u2081\u2082) aee8af2dfc4a4c878ab086ffd79f711f--7398f25e330549848050a32d995547c2 7398f25e330549848050a32d995547c2--9feac6380d9947838cf01b79171770ea"},{"location":"content/quantummodels/","title":"Quantum models","text":"<p>A quantum program can be expressed and executed using the <code>QuantumModel</code> type. It serves three primary purposes:</p> <p>Parameter handling: by conveniently handling and embedding the two parameter types that Qadence supports: feature and variational (see more details in the previous section).</p> <p>Differentiability: by enabling a differentiable backend that supports two differentiable modes: automatic differentiation (AD) and parameter shift rules (PSR). The former is used general differentiation in statevector simulators based on PyTorch and JAX. The latter is a quantum specific method used to differentiate gate parameters, and is enabled for all backends.</p> <p>Execution: by defining which backend the program is expected to be executed on. Qadence supports circuit compilation to the native backend representation.</p> <p>Backends</p> <p>The goal is for quantum models to be executed seemlessly on a number of different purpose backends: simulators, emulators or real hardware. By default, Qadence executes on the PyQTorch backend which implements a state vector simulator. Currently, this is the most feature rich backend. The Pulser backend is being developed, and currently supports a more limited set of functionalities (pulse sequences on programmable neutral atom arrays). The Horqrux backend, built on JAX, is also available, but currently not supported with the <code>QuantumModel</code> interface. For more information see the backend section.</p> <p>The base <code>QuantumModel</code> exposes the following methods:</p> <ul> <li><code>QuantumModel.run()</code>: To extract the wavefunction after circuit execution. Not supported by all backends.</li> <li><code>QuantumModel.sample()</code>: Sample a bitstring from the resulting quantum state after circuit execution. Supported by all backends.</li> <li><code>QuantumModel.expectation()</code>: Compute the expectation value of an observable.</li> </ul> <p>Every <code>QuantumModel</code> is an instance of a <code>torch.nn.Module</code> that enables differentiability for its <code>expectation</code> method. For statevector simulators, AD also works for the statevector itself.</p> <p>To construct a <code>QuantumModel</code>, the program block must first be initialized into a <code>QuantumCircuit</code> instance by combining it with a <code>Register</code>. An integer number can also be passed for the total number of qubits, which instantiates a <code>Register</code> automatically. The qubit register also includes topological information on the qubit layout, essential for digital-analog computations. However, we will explore that in a later tutorial. For now, let's construct a simple parametrized quantum circuit.</p> <pre><code>from qadence import QuantumCircuit, RX, RY, chain, kron\nfrom qadence import FeatureParameter, VariationalParameter\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, theta), RY(1, theta)),\n    kron(RX(0, phi), RY(1, phi)),\n)\n\ncircuit = QuantumCircuit(2, block)\nunique_params = circuit.unique_parameters\n</code></pre> <pre><code>unique_params = [theta, phi]\n</code></pre> <p>The model can then be instantiated. Similarly to the direct execution functions shown in the previous tutorial, the <code>run</code>, <code>sample</code> and <code>expectation</code> methods are available directly from the model.</p> <pre><code>import torch\nfrom qadence import QuantumModel, PI, Z\n\nobservable = Z(0) + Z(1)\n\nmodel = QuantumModel(circuit, observable)\n\nvalues = {\"phi\": torch.tensor([PI, PI/2])}\n\nwf = model.run(values)\nxs = model.sample(values, n_shots=100)\nex = model.expectation(values)\n</code></pre> <pre><code>wf = tensor([[ 0.0091+0.0000j, -0.0949+0.0000j,  0.0000+0.0949j,  0.0000-0.9909j],\n        [ 0.4051+0.0000j,  0.4909+0.0000j,  0.0000-0.4909j,  0.0000-0.5949j]])\nxs = [OrderedCounter({'11': 98, '01': 1, '10': 1}), OrderedCounter({'11': 36, '10': 28, '01': 21, '00': 15})]\nex = tensor([[-1.9637],\n        [-0.3794]])\n</code></pre> <p>By default, the <code>forward</code> method of <code>QuantumModel</code> calls <code>model.run()</code>. To define custom quantum models, the best way is to inherit from <code>QuantumModel</code> and override the <code>forward</code> method, as typically done with custom PyTorch Modules.</p> <p>The <code>QuantumModel</code> class provides convenience methods to manipulate parameters. Being a <code>torch.nn.Module</code>, all torch methods are also available.</p> <pre><code># To pass onto a torch optimizer\nparameter_generator = model.parameters()\n\n# Number of variational parameters\nnum_vparams = model.num_vparams\n\n# Dictionary to easily inspect variational parameter values\nvparams_values = model.vparams\n</code></pre> <pre><code>vparams_values = OrderedDict([('theta', tensor([0.1909]))])\n</code></pre>"},{"location":"content/quantummodels/#model-output","title":"Model output","text":"<p>The output of a quantum model is typically encoded in the measurement of an expectation value. In Qadence, one way to customize the number of outputs is by batching the number of observables at model creation by passing a list of blocks.</p> <pre><code>from torch import tensor\nfrom qadence import chain, kron, VariationalParameter, FeatureParameter\nfrom qadence import QuantumModel, QuantumCircuit, PI, Z, RX, CNOT\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, phi), RX(1, phi)),\n    CNOT(0, 1)\n)\n\ncircuit = QuantumCircuit(2, block)\n\nmodel = QuantumModel(circuit, [Z(0), Z(0) + Z(1)])\n\nvalues = {\"phi\": tensor(PI)}\n\nex = model.expectation(values)\n</code></pre> <pre><code>ex = tensor([[-1.0000e+00, -7.4988e-33]])\n</code></pre> <p>As mentioned in the previous tutorial, blocks can also be arbitrarily parameterized through multiplication, which allows the inclusion of trainable parameters in the definition of the observable.</p> <pre><code>from qadence import I, Z\n\na = VariationalParameter(\"a\")\nb = VariationalParameter(\"b\")\n\n# Magnetization with a trainable shift and scale\nobservable = a * I(0) + b * Z(0)\n\nmodel = QuantumModel(circuit, observable)\n</code></pre>"},{"location":"content/quantummodels/#quantum-neural-network-qnn","title":"Quantum Neural Network (QNN)","text":"<p>The <code>QNN</code> is a subclass of the <code>QuantumModel</code> geared towards quantum machine learning and parameter optimisation. See the quantum machine learning section section or the <code>QNN</code> API reference for more detailed information. There are three main differences in interface when compared with the <code>QuantumModel</code>:</p> <ul> <li>It is initialized with a list of the input parameter names, and then supports direct <code>torch.Tensor</code> inputs instead of the values dictionary shown above. The ordering of the input values should respect the order given in the input names.</li> <li>Passing an observable is mandatory.</li> <li>The <code>forward</code> method calls <code>model.expectation()</code>.</li> </ul> <pre><code>from torch import tensor\nfrom qadence import chain, kron, VariationalParameter, FeatureParameter\nfrom qadence import QNN, QuantumCircuit, PI, Z, RX, RY, CNOT\n\ntheta = FeatureParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, phi), RX(1, phi)),\n    kron(RY(0, theta), RY(1, theta)),\n    CNOT(0, 1)\n)\n\ncircuit = QuantumCircuit(2, block)\nobservable = Z(0) + Z(1)\n\nmodel = QNN(circuit, observable, inputs = [\"phi\", \"theta\"])\n\n# \"phi\" = PI, PI/2, \"theta\" = 0.0, 1.0\nvalues = tensor([[PI, 0.0], [PI/2, 1.0]])\n\nex = model(values)\n</code></pre> <pre><code>ex = tensor([[-7.4988e-33],\n        [ 1.1102e-16]])\n</code></pre>"},{"location":"content/register/","title":"Quantum registers","text":"<p>In Qadence, quantum programs can be executed by specifying the layout of a register of resources as a lattice. Built-in <code>Register</code> types can be used or constructed for arbitrary topologies. Common register topologies are available and illustrated in the plot below.</p> 2024-12-05T14:30:49.142107 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"content/register/#building-and-drawing-registers","title":"Building and drawing registers","text":"<p>Built-in topologies are directly accessible in the <code>Register</code> methods:</p> <pre><code>from qadence import Register\n\nreg = Register.all_to_all(n_qubits = 4)\nreg_line = Register.line(n_qubits = 4)\nreg_circle = Register.circle(n_qubits = 4)\nreg_squre = Register.square(qubits_side = 2)\nreg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\nreg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\nreg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n</code></pre> <p>The <code>Register</code> class builds on top of the NetworkX <code>Graph</code>, and the graphs can be visualized with the <code>reg.draw()</code> method</p> <p>Qubit coordinates are saved as node properties in the underlying NetworkX graph, but can be accessed directly with the <code>coords</code> property.</p> <p><pre><code>reg = Register.square(2)\nprint(reg.coords)\n</code></pre> <pre><code>{0: (0.5, -0.5), 1: (0.5, 0.5), 2: (-0.5, 0.5), 3: (-0.5, -0.5)}\n</code></pre>  By default, the coords are scaled such that the minimum distance between any two qubits is 1, unless the register is created directly from specific coordinates as shown below. The <code>spacing</code> argument can be used to set the minimum spacing. The <code>rescale_coords</code> method can be used to create a new register by rescaling the coordinates of an already created register.</p> <pre><code>scaled_reg_1 = Register.square(2, spacing = 4.0)\nscaled_reg_2 = reg.rescale_coords(scaling = 4.0)\nprint(scaled_reg_1.coords)\nprint(scaled_reg_2.coords)\n</code></pre> <pre><code>{0: (2.0, -2.0), 1: (2.0, 2.0), 2: (-2.0, 2.0), 3: (-2.0, -2.0)}\n{0: (2.0, -2.0), 1: (2.0, 2.0), 2: (-2.0, 2.0), 3: (-2.0, -2.0)}\n</code></pre> <p>The distance between qubits can also be directly accessed with the <code>distances</code> and <code>edge_distances</code> properties.</p> <pre><code>print(reg.distances)\nprint(reg.edge_distances)\n</code></pre> <pre><code>Distance between all qubit pairs:\n{(0, 1): 1.0, (0, 2): 1.4142135623730951, (0, 3): 1.0, (1, 2): 1.0, (1, 3): 1.4142135623730951, (2, 3): 1.0}\nDistance between qubits connect by an edge in the graph\n{(0, 1): 1.0, (0, 3): 1.0, (1, 2): 1.0, (2, 3): 1.0}\n</code></pre> <p>By calling the <code>Register</code> directly, either the number of nodes or a specific graph can be given as input. If passing a custom graph directly, the node positions will not be defined automatically, and should be previously saved in the <code>\"pos\"</code> node property. If not, <code>reg.coords</code> will return empty tuples and all distances will be 0.</p> <pre><code>import networkx as nx\n\n# Same as Register.all_to_all(n_qubits = 2):\nreg = Register(2)\n\n# Register from a custom graph:\ngraph = nx.complete_graph(3)\n\n# Set node positions, in this case a simple line:\nfor i, node in enumerate(graph.nodes):\n    graph.nodes[node][\"pos\"] = (1.0 * i, 0.0)\n\nreg = Register(graph)\n\nprint(reg.distances)\n</code></pre> <pre><code>{(0, 1): 1.0, (0, 2): 2.0, (1, 2): 1.0}\n</code></pre> <p>Alternatively, arbitrarily shaped registers can also be constructed by providing the node coordinates. In this case, there will be no edges automatically created in the connectivity graph.</p> <pre><code>import numpy as np\nfrom qadence import Register, PI\n\nreg = Register.from_coordinates(\n    [(x, np.sin(x)) for x in np.linspace(0, 2*PI, 10)]\n)\n\nreg.draw(show=False)\n</code></pre> 2024-12-05T14:30:49.398611 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/ <p>Units for qubit coordinates</p> <p>In general, Qadence makes no assumption about the units for qubit coordinates and distances. However, if used in the context of a Hamiltonian coefficient, care should be taken by the user to guarantee the quantity \\(H.t\\) is dimensionless for exponentiation in the PyQTorch backend, where it is assumed that \\(\\hbar = 1\\). For registers passed to the Pulser backend, coordinates are in \\(\\mu \\textrm{m}\\).</p>"},{"location":"content/register/#connectivity-graphs","title":"Connectivity graphs","text":"<p>Register topology is often assumed in digital simulations to be an all-to-all qubit connectivity. When running on real devices that enable the digital-analog computing paradigm, qubit interactions must be specified either by specifying distances between qubits, or by defining edges in the register connectivity graph.</p> <p>The abstract graph nodes and edges are accessible for direct usage.</p> <pre><code>from qadence import Register\n\nreg = Register.rectangular_lattice(2,3)\n</code></pre> <pre><code>reg.nodes = NodeView((0, 1, 2, 3, 4, 5))\nreg.edges = EdgeView([(0, 2), (0, 1), (1, 3), (2, 4), (2, 3), (3, 5), (4, 5)])\n</code></pre> <p>There is also an <code>all_node_pairs</code> property for convenience:</p> <pre><code>print(reg.all_node_pairs)\n</code></pre> <pre><code>[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n</code></pre> <p>More details about the usage of <code>Register</code> types in the digital-analog paradigm can be found in the digital-analog basics section.</p>"},{"location":"content/serializ_and_prep/","title":"Serialization","text":"<p>Qadence offers convenience functions for serializing and deserializing any quantum program. This is useful for storing quantum programs and sending them for execution over the network via an API.</p> <p>Note</p> <p>Qadence currently uses a custom JSON serialization as interchange format. Support for QASM format for digital quantum programs is currently under consideration.</p> <ul> <li><code>serialize/deserialize</code>: serialize and deserialize a Qadence object into a dictionary</li> <li><code>save/load</code>: save and load a Qadence object to a file with one of the supported   formats. Currently, these are <code>.json</code> and the PyTorch-compatible <code>.pt</code> format.</li> </ul> <p>Let's start with serialization into a dictionary.</p> <pre><code>import torch\nfrom qadence import QuantumCircuit, QuantumModel, DiffMode\nfrom qadence import chain, hamiltonian_factory, feature_map, hea, Z\nfrom qadence.serialization import serialize, deserialize\n\nn_qubits = 4\n\nmy_block = chain(feature_map(n_qubits, param=\"x\"), hea(n_qubits, depth=2))\nobs = hamiltonian_factory(n_qubits, detuning=Z)\n\n# Use the block defined above to create a quantum circuit\n# serialize/deserialize it\nqc = QuantumCircuit(n_qubits, my_block)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n# Let's wrap it in a QuantumModel\n# and serialize it\nqm = QuantumModel(qc, obs, diff_mode=DiffMode.AD)\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n\n# Check if the loaded QuantumModel returns the same expectation\nvalues = {\"x\": torch.rand(10)}\nassert torch.allclose(qm.expectation(values=values), qm_deserialized.expectation(values=values))\n</code></pre> <p>Finally, we can save the quantum circuit and the model with the two supported formats.</p> <pre><code>from qadence.serialization import serialize, deserialize, save, load, SerializationFormat\n\nqc_fname = \"circuit\"\nsave(qc, folder=\".\", file_name=qc_fname, format=SerializationFormat.PT)\nloaded_qc = load(f\"{qc_fname}.pt\")\nassert qc == loaded_qc\n\nqm_fname = \"model\"\nsave(qm, folder=\".\", file_name=qm_fname, format=SerializationFormat.JSON)\nmodel = load(f\"{qm_fname}.json\")\nassert isinstance(model, QuantumModel)\n</code></pre>"},{"location":"content/state_conventions/","title":"State Conventions","text":"<p>Here is an overview of the state conventions used in Qadence together with practical examples.</p>"},{"location":"content/state_conventions/#qubit-register-order","title":"Qubit register order","text":"<p>Qubit registers in quantum computing are often indexed in increasing or decreasing order from left to right. In Qadence, the convention is qubit indexation in increasing order. For example, a register of four qubits in bra-ket notation reads:</p> \\[|q_0, q_1, q_2, q_3\\rangle\\] <p>Furthermore, when displaying a quantum circuit, qubits are ordered from top to bottom.</p>"},{"location":"content/state_conventions/#basis-state-order","title":"Basis state order","text":"<p>Basis state ordering refers to how basis states are ordered when considering the conversion from bra-ket notation to the standard linear algebra basis. In Qadence, basis states are ordered in the following manner:</p> \\[ \\begin{align} |00\\rangle = [1, 0, 0, 0]^T\\\\ |01\\rangle = [0, 1, 0, 0]^T\\\\ |10\\rangle = [0, 0, 1, 0]^T\\\\ |11\\rangle = [0, 0, 0, 1]^T \\end{align} \\]"},{"location":"content/state_conventions/#endianness","title":"Endianness","text":"<p>Endianness refers to the storage convention for binary information (in bytes) in a classical memory register. In quantum computing, information is either stored in bits or in qubits. The most commonly used conventions are:</p> <ul> <li>A big-endian system stores the most significant bit of a binary word at the smallest memory address.</li> <li>A little-endian system stores the least significant bit of a binary word at the smallest memory address.</li> </ul> <p>Given the register convention in Qadence, the integer \\(2\\) written in binary big-endian as \\(10\\) can be encoded in a qubit register in both big-endian as \\(|10\\rangle\\) or little-endian as \\(|01\\rangle\\).</p> <p>The convention for Qadence is big-endian.</p>"},{"location":"content/state_conventions/#quantum-states","title":"Quantum states","text":"<p>In practical scenarios, conventions regarding register order, basis state order and endianness are very much intertwined, and identical results can be obtained by fixing or varying any of them. In Qadence, we assume that qubit ordering and basis state ordering is fixed, and allow an <code>endianness</code> argument that can be passed to control the expected result. Here are a few examples:</p> <p>A simple and direct way to exemplify the endianness convention is using convenience functions for state preparation.</p> <p>Bitstring convention as inputs</p> <p>When a bitstring is passed as input to a function for state preparation, it has to be understood in big-endian convention.</p> <pre><code>from qadence import Endianness, product_state\n\n# The state |10&gt;, the 3rd basis state.\nstate_big = product_state(\"10\", endianness=Endianness.BIG) # or just \"Big\"\n\n# The state |01&gt;, the 2nd basis state.\nstate_little = product_state(\"10\", endianness=Endianness.LITTLE) # or just \"Little\"\n</code></pre> <pre><code>State in big endian = tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]])\nState in little endian = tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> <p>Here, a bitword expressed as a Python string to encode the integer 2 in big-endian is used to create the respective basis state in both conventions. However, note that the same results can be obtained by fixing the endianness convention as big-endian (thus creating the state \\(|10\\rangle\\) in both cases), and changing the basis state ordering. A similar argument holds for fixing both endianness and basis state ordering and simply changing the qubit index order.</p> <p>Another example where endianness directly comes into play is when measuring a register. A big- or little-endian measurement will choose the first or the last qubit, respectively, as the most significant bit. Let's see this in an example:</p> <pre><code>from qadence import I, H, sample\n\n# Create superposition state: |00&gt; + |01&gt; (normalized)\nblock = I(0) @ H(1)  # Identity on qubit 0, Hadamard on qubit 1\n\n# Generate bitword samples following both conventions\n# Samples \"00\" and \"01\"\nresult_big = sample(block, endianness=Endianness.BIG)\n# Samples \"00\" and \"10\"\nresult_little = sample(block, endianness=Endianness.LITTLE)\n</code></pre> <pre><code>Sample in big endian = [OrderedCounter({'00': 52, '01': 48})]\nSample in little endian = [Counter({'00': 53, '10': 47})]\n</code></pre> <p>In Qadence, endianness can be flipped for many relevant objects:</p> <pre><code>from qadence import invert_endianness\n\n# Equivalent to sampling in little-endian.\nflip_big_sample = invert_endianness(result_big)\n\n# Equivalent to a state created in little-endian.\nflip_big_state = invert_endianness(state_big)\n</code></pre> <pre><code>Flipped sample = [Counter({'00': 52, '10': 48})]\nFlipped state = tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre>"},{"location":"content/state_conventions/#quantum-operations","title":"Quantum operations","text":"<p>When looking at the matricial form of quantum operations, the usage of the term endianness becomes slightly abusive. To exemplify, we may consider the <code>CNOT</code> operation with <code>control = 0</code> and <code>target = 1</code>. This operation is often described with two different matrices:</p> \\[ \\text{CNOT(0, 1)} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\qquad \\text{or} \\qquad \\text{CNOT(0, 1)} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] <p>The difference can be easily explained either by considering a different ordering of the qubit indices, or a different ordering of the basis states. In Qadence, both can be retrieved through the <code>endianness</code> argument:</p> <pre><code>from qadence import block_to_tensor, CNOT\n\nmatrix_big = block_to_tensor(CNOT(0, 1), endianness=Endianness.BIG)\nmatrix_little = block_to_tensor(CNOT(0, 1), endianness=Endianness.LITTLE)\n</code></pre> <pre><code>CNOT matrix in big endian =\n\ntensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]])\n\nCNOT matrix in little endian =\n\ntensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]]])\n</code></pre>"},{"location":"content/state_conventions/#backends","title":"Backends","text":"<p>An important part of having clear state conventions is that we need to make sure our results are consistent accross different computational backends, which may have their own conventions. In Qadence, this is taken care of automatically: by calling operations for different backends, the result is expected to be equivalent up to qubit ordering.</p> <pre><code>from qadence import BackendName, RX, run, sample, PI\n\n# RX(PI/4) on qubit 1\nn_qubits = 2\nop = RX(1, PI/4)\n</code></pre> <pre><code>Same sampling order in big endian:\n\nOn PyQTorch = [OrderedCounter({'00': 91, '01': 9})]\nOn Pulser = [Counter({'00': 88, '01': 12})]\n\nSame wavefunction order:\n\nOn PyQTorch = tensor([[0.9239+0.0000j, 0.0000-0.3827j, 0.0000+0.0000j, 0.0000+0.0000j]])\nOn Pulser = tensor([[0.9241+0.0000j, 0.0000-0.3822j, 0.0000+0.0000j, 0.0000+0.0000j]])\n</code></pre>"},{"location":"content/state_init/","title":"State initialization","text":"<p>Qadence offers convenience routines for preparing initial quantum states. These routines are divided into two approaches:</p> <ul> <li>As a dense matrix.</li> <li>From a suitable quantum circuit. This is available for every backend and it should be added in front of the desired quantum circuit to simulate.</li> </ul> <p>Let's illustrate the usage of the state preparation routine.</p> <pre><code>from qadence import random_state, product_state, is_normalized, StateGeneratorType\n\n# Random initial state.\n# the default `type` is StateGeneratorType.HaarMeasureFast\nstate = random_state(n_qubits=2, type=StateGeneratorType.RANDOM_ROTATIONS)\n\n# Check the normalization.\nassert is_normalized(state)\n\n# Product state from a given bitstring.\n# NB: Qadence follows the big endian convention.\nstate = product_state(\"01\")\n</code></pre> <pre><code>Random initial state generated with rotations:\n\nstate = [0.09573453-0.9954069j 0.        +0.j        0.        +0.j\n 0.        +0.j       ]\n\nProduct state corresponding to bitstring '01':\n\nstate = [0.+0.j 1.+0.j 0.+0.j 0.+0.j]\n</code></pre> <p>Now we see how to generate the product state corresponding to the one above with a suitable quantum circuit.</p> <p><pre><code>from qadence import product_block, tag, hea, QuantumCircuit\nfrom qadence.draw import display\n\nstate_prep_block = product_block(\"01\")\n# display(state_prep_block)\n\n# Let's now prepare a circuit.\nn_qubits = 4\n\nstate_prep_block = product_block(\"0001\")\ntag(state_prep_block, \"Prep block\")\n\ncircuit_block = tag(hea(n_qubits, depth = 2), \"Circuit block\")\n\nqc_with_state_prep = QuantumCircuit(n_qubits, state_prep_block, circuit_block)\n</code></pre> %3 cluster_3de77a67644d4105ac98fa64342a15a3 Circuit block cluster_67a64a38358c47079cab219d209041b4 Prep block d4b7821d6f5b4e7db6edc0155868df25 0 33c9d7a12eab4260adbbdfe51a4347c0 d4b7821d6f5b4e7db6edc0155868df25--33c9d7a12eab4260adbbdfe51a4347c0 8b7eb1f9bcad446a95adb09988cec23c 1 5779ab83efac4bc9b5a6345cf2f43fcc RX(theta\u2080) 33c9d7a12eab4260adbbdfe51a4347c0--5779ab83efac4bc9b5a6345cf2f43fcc bb2448fadc06461292079f367da45cc7 RY(theta\u2084) 5779ab83efac4bc9b5a6345cf2f43fcc--bb2448fadc06461292079f367da45cc7 71750456dcef470f90f46a729808637c RX(theta\u2088) bb2448fadc06461292079f367da45cc7--71750456dcef470f90f46a729808637c 53e46d67d383428fb0a8e7d5d9b1a534 71750456dcef470f90f46a729808637c--53e46d67d383428fb0a8e7d5d9b1a534 4104fa1a69874264b36bb02eba801eb0 53e46d67d383428fb0a8e7d5d9b1a534--4104fa1a69874264b36bb02eba801eb0 df41fa98f8ca4161b05b72cc5b016cc4 RX(theta\u2081\u2082) 4104fa1a69874264b36bb02eba801eb0--df41fa98f8ca4161b05b72cc5b016cc4 800f7c32646e439cac571738f233bc6b RY(theta\u2081\u2086) df41fa98f8ca4161b05b72cc5b016cc4--800f7c32646e439cac571738f233bc6b 542f24b3fd1d443c94300ac196b093ca RX(theta\u2082\u2080) 800f7c32646e439cac571738f233bc6b--542f24b3fd1d443c94300ac196b093ca dbdd71b338af4a1bb89b12b90c0ba4e5 542f24b3fd1d443c94300ac196b093ca--dbdd71b338af4a1bb89b12b90c0ba4e5 3c3c496516d84e9db60a201de4838e6a dbdd71b338af4a1bb89b12b90c0ba4e5--3c3c496516d84e9db60a201de4838e6a 66699e4525444a18a5c878f5f4941bca 3c3c496516d84e9db60a201de4838e6a--66699e4525444a18a5c878f5f4941bca 0be9a1d4569f4d2a9e2058b7460f3f08 77d6419237664f37a357af2fbd5daa8a 8b7eb1f9bcad446a95adb09988cec23c--77d6419237664f37a357af2fbd5daa8a c07f563c93d34bc7be0777d30a8a44d3 2 701affa5f20f4d63844a474e69a63f23 RX(theta\u2081) 77d6419237664f37a357af2fbd5daa8a--701affa5f20f4d63844a474e69a63f23 c7ee152c3a4e45cc81161584906f0c16 RY(theta\u2085) 701affa5f20f4d63844a474e69a63f23--c7ee152c3a4e45cc81161584906f0c16 897012ca02ae4e24b41ae30edadf42a2 RX(theta\u2089) c7ee152c3a4e45cc81161584906f0c16--897012ca02ae4e24b41ae30edadf42a2 2a22edec0c0947269571b77b2062d867 X 897012ca02ae4e24b41ae30edadf42a2--2a22edec0c0947269571b77b2062d867 2a22edec0c0947269571b77b2062d867--53e46d67d383428fb0a8e7d5d9b1a534 747920d2c1a147969916b1ae299cb0f3 2a22edec0c0947269571b77b2062d867--747920d2c1a147969916b1ae299cb0f3 57060540824142e08c2dbfea962b6466 RX(theta\u2081\u2083) 747920d2c1a147969916b1ae299cb0f3--57060540824142e08c2dbfea962b6466 9a5fabe90695421b8ed6e87e602f8a08 RY(theta\u2081\u2087) 57060540824142e08c2dbfea962b6466--9a5fabe90695421b8ed6e87e602f8a08 9218f12971254f0f9080d5a819b52e83 RX(theta\u2082\u2081) 9a5fabe90695421b8ed6e87e602f8a08--9218f12971254f0f9080d5a819b52e83 666ed3e8167c4731adf980da73cedd05 X 9218f12971254f0f9080d5a819b52e83--666ed3e8167c4731adf980da73cedd05 666ed3e8167c4731adf980da73cedd05--dbdd71b338af4a1bb89b12b90c0ba4e5 446e99050f5548cc94f1824ca6f981df 666ed3e8167c4731adf980da73cedd05--446e99050f5548cc94f1824ca6f981df 446e99050f5548cc94f1824ca6f981df--0be9a1d4569f4d2a9e2058b7460f3f08 3ca2235bdfbd4177a614c71c58196701 309da3c4756b4cbcba2f8bb0c25fc69c c07f563c93d34bc7be0777d30a8a44d3--309da3c4756b4cbcba2f8bb0c25fc69c 03c1eb036a044425ba6b7f21cbbc4765 3 69c3cebef3954af3b38f323022d6a643 RX(theta\u2082) 309da3c4756b4cbcba2f8bb0c25fc69c--69c3cebef3954af3b38f323022d6a643 2090c9c347c740e8a333f3d528cc1288 RY(theta\u2086) 69c3cebef3954af3b38f323022d6a643--2090c9c347c740e8a333f3d528cc1288 c318b607f62f447681e2a0ab0d9ab2d4 RX(theta\u2081\u2080) 2090c9c347c740e8a333f3d528cc1288--c318b607f62f447681e2a0ab0d9ab2d4 6ca12fa3f3294cefab0abe2ceda6e785 c318b607f62f447681e2a0ab0d9ab2d4--6ca12fa3f3294cefab0abe2ceda6e785 055b00e778a3414dba840d5a667bba2d X 6ca12fa3f3294cefab0abe2ceda6e785--055b00e778a3414dba840d5a667bba2d 055b00e778a3414dba840d5a667bba2d--747920d2c1a147969916b1ae299cb0f3 e9aae1dbe2f04190ba472685f46c386e RX(theta\u2081\u2084) 055b00e778a3414dba840d5a667bba2d--e9aae1dbe2f04190ba472685f46c386e c81313fe288340338abeb5749e182bcd RY(theta\u2081\u2088) e9aae1dbe2f04190ba472685f46c386e--c81313fe288340338abeb5749e182bcd c5d341da95be48afa67052bc83be0507 RX(theta\u2082\u2082) c81313fe288340338abeb5749e182bcd--c5d341da95be48afa67052bc83be0507 e260e1622e1b487fa29c77fb7d43adbb c5d341da95be48afa67052bc83be0507--e260e1622e1b487fa29c77fb7d43adbb ca213fd5be514beebf02cdbc854c39ac X e260e1622e1b487fa29c77fb7d43adbb--ca213fd5be514beebf02cdbc854c39ac ca213fd5be514beebf02cdbc854c39ac--446e99050f5548cc94f1824ca6f981df ca213fd5be514beebf02cdbc854c39ac--3ca2235bdfbd4177a614c71c58196701 503be18a74c74662a49c5c32d8deb575 1ffada0a7eb7450b802b066426610cda X 03c1eb036a044425ba6b7f21cbbc4765--1ffada0a7eb7450b802b066426610cda 3750ad05c4314af2850c79deb4420eda RX(theta\u2083) 1ffada0a7eb7450b802b066426610cda--3750ad05c4314af2850c79deb4420eda 839cf81a5e714aa78b97de7786d16234 RY(theta\u2087) 3750ad05c4314af2850c79deb4420eda--839cf81a5e714aa78b97de7786d16234 1db055bf32244110a5bd5f9911b7b54c RX(theta\u2081\u2081) 839cf81a5e714aa78b97de7786d16234--1db055bf32244110a5bd5f9911b7b54c 4b0eec023e4443ae8d403afec4a5f89b X 1db055bf32244110a5bd5f9911b7b54c--4b0eec023e4443ae8d403afec4a5f89b 4b0eec023e4443ae8d403afec4a5f89b--6ca12fa3f3294cefab0abe2ceda6e785 012acfa364684e798b1753130be973ce 4b0eec023e4443ae8d403afec4a5f89b--012acfa364684e798b1753130be973ce 237cc420448645648cbe032f08e14f17 RX(theta\u2081\u2085) 012acfa364684e798b1753130be973ce--237cc420448645648cbe032f08e14f17 79f3392fab1b49f6951a24aea3aae4fc RY(theta\u2081\u2089) 237cc420448645648cbe032f08e14f17--79f3392fab1b49f6951a24aea3aae4fc c40653847eaa48fdb007c0640a05334b RX(theta\u2082\u2083) 79f3392fab1b49f6951a24aea3aae4fc--c40653847eaa48fdb007c0640a05334b 3fe06d00f8ae41f582c704de4695eb5f X c40653847eaa48fdb007c0640a05334b--3fe06d00f8ae41f582c704de4695eb5f 3fe06d00f8ae41f582c704de4695eb5f--e260e1622e1b487fa29c77fb7d43adbb 853568f893b64cada1d13bea1cfcb065 3fe06d00f8ae41f582c704de4695eb5f--853568f893b64cada1d13bea1cfcb065 853568f893b64cada1d13bea1cfcb065--503be18a74c74662a49c5c32d8deb575  Several standard quantum states can be conveniently initialized in Qadence, both in statevector form as well as in block form as shown in following.</p>"},{"location":"content/state_init/#state-vector-initialization","title":"State vector initialization","text":"<p>Qadence offers a number of constructor functions for state vector preparation.</p> <pre><code>from qadence import uniform_state, zero_state, one_state\n\nn_qubits = 3\nbatch_size = 2\n\nuniform_state = uniform_state(n_qubits, batch_size)\nzero_state = zero_state(n_qubits, batch_size)\none_state = one_state(n_qubits, batch_size)\n</code></pre> <pre><code>Uniform state = \n\ntensor([[0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,\n         0.3536+0.j],\n        [0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,\n         0.3536+0.j]])\nZero state = \n\ntensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\nOne state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> <p>As already seen, product states can be easily created, even in batches:</p> <pre><code>from qadence import product_state, rand_product_state\n\n# From a bitsring \"100\"\nprod_state = product_state(\"100\", batch_size)\n\n# Or a random product state\nrand_state = rand_product_state(n_qubits, batch_size)\n</code></pre> <pre><code>Product state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n\nRandom state = \n\ntensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> <p>Creating a GHZ state:</p> <pre><code>from qadence import ghz_state\n\nghz = ghz_state(n_qubits, batch_size)\n</code></pre> <pre><code>GHZ state = \n\ntensor([[0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j,\n         0.7071+0.j],\n        [0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j,\n         0.7071+0.j]])\n</code></pre> <p>Creating a random state uniformly sampled from a Haar measure:</p> <pre><code>from qadence import random_state\n\nrand_haar_state = random_state(n_qubits, batch_size)\n</code></pre> <pre><code>Random state from Haar = \n\ntensor([[ 0.1174-0.2038j,  0.1194+0.0970j,  0.4173-0.3010j,  0.0464-0.0913j,\n          0.1557-0.1457j,  0.3139+0.2603j,  0.1850+0.1393j, -0.6106-0.0869j],\n        [ 0.1436-0.4136j, -0.0725+0.1590j,  0.1971-0.0148j, -0.0967-0.0249j,\n         -0.3791-0.4295j,  0.1268-0.0350j, -0.3676-0.0611j,  0.1942-0.4546j]])\n</code></pre> <p>Custom initial states can then be passed to either <code>run</code>, <code>sample</code> and <code>expectation</code> through the <code>state</code> argument</p> <pre><code>from qadence import random_state, product_state, CNOT, run\n\ninit_state = product_state(\"10\")\nfinal_state = run(CNOT(0, 1), state=init_state)\n</code></pre> <pre><code>Final state = tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre>"},{"location":"content/state_init/#block-initialization","title":"Block initialization","text":"<p>Not all backends support custom statevector initialization, however previous utility functions have their counterparts to initialize the respective blocks:</p> <pre><code>from qadence import uniform_block, one_block\n\nn_qubits = 3\n\nuniform_block = uniform_block(n_qubits)\n\none_block = one_block(n_qubits)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 H(0)\n\u251c\u2500\u2500 H(1)\n\u2514\u2500\u2500 H(2)\nKronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> <p>Similarly, for product states:</p> <pre><code>from qadence import product_block, rand_product_block\n\nproduct_block = product_block(\"100\")\n\nrand_product_block = rand_product_block(n_qubits)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 I(1)\n\u2514\u2500\u2500 I(2)\nKronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 I(1)\n\u2514\u2500\u2500 I(2)\n</code></pre> <p>And GHZ states:</p> <pre><code>from qadence import ghz_block\n\nghz_block = ghz_block(n_qubits)\n</code></pre> <pre><code>ChainBlock(0,1,2)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 ChainBlock(0,1,2)\n    \u251c\u2500\u2500 CNOT(0, 1)\n    \u2514\u2500\u2500 CNOT(1, 2)\n</code></pre> <p>Initial state blocks can simply be chained at the start of a given circuit.</p>"},{"location":"content/state_init/#utility-functions","title":"Utility functions","text":"<p>Some state vector utility functions are also available. We can easily create the probability mass function of a given statevector using <code>torch.distributions.Categorical</code></p> <pre><code>from qadence import random_state, pmf\n\nn_qubits = 3\n\nstate = random_state(n_qubits)\ndistribution = pmf(state)\n</code></pre> <pre><code>Categorical(probs: torch.Size([1, 8]))\n</code></pre> <p>We can also check if a state is normalized:</p> <pre><code>from qadence import random_state, is_normalized\n\nstate = random_state(n_qubits)\nprint(is_normalized(state))\n</code></pre> <pre><code>True\n</code></pre> <p>Or normalize a state:</p> <pre><code>import torch\nfrom qadence import normalize, is_normalized\n\nstate = torch.tensor([[1, 1, 1, 1]], dtype = torch.cdouble)\nprint(normalize(state))\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre>"},{"location":"content/time_dependent/","title":"Time-dependent generators","text":"<p>For use cases when the Hamiltonian of the system is time-dependent, Qadence provides a special parameter <code>TimeParameter(\"t\")</code> that denotes the explicit time dependence. Using this time parameter, one can define a parameterized block acting as the generator passed to <code>HamEvo</code> that encapsulates the required time dependence function.</p>"},{"location":"content/time_dependent/#noiseless-time-dependent-hamiltonian-evolution","title":"Noiseless time-dependent Hamiltonian evolution","text":"<pre><code>from qadence import X, Y, HamEvo, TimeParameter, FeatureParameter, run\nfrom pyqtorch.utils import SolverType\nimport torch\n\n# Simulation parameters\node_solver = SolverType.DP5_SE  # time-dependent Schrodinger equation solver method\nn_steps_hevo = 500  # integration time steps used by solver\n\n# Define block parameters\nt = TimeParameter(\"t\")\nomega_param = FeatureParameter(\"omega\")\n\n# Arbitrarily compose a time-dependent generator\ntd_generator = omega_param * (t * X(0) + t**2 * Y(1))\n\n# Create parameterized HamEvo block\nhamevo = HamEvo(td_generator, t)\n</code></pre> <p>Note that when using <code>HamEvo</code> with a time-dependent generator, the actual time parameter that was used to construct the generator must be passed for the second argument <code>parameter</code>.</p> <p>By default, the code above will initialize an internal parameter <code>FeatureParameter(\"duration\")</code> in the <code>HamEvo</code>. Alternatively, the <code>duration</code> argument can be used to rename this parameter, or to pass a fixed value directly. If no fixed value is passed, it must then be set in the <code>values</code> dictionary at runtime.</p> <p>Future improvements</p> <p>Currently it is only possible to pass a single value for the duration, and the only result obtained will be the one corresponding to the state at end of the integration. In the future we will change the interface to allow directly passing some array of save values to obtain expectation values or statevectors at intermediate steps during the evolution.</p> <pre><code>values = {\"omega\": torch.tensor(10.0), \"duration\": torch.tensor(1.0)}\n\nconfig = {\"ode_solver\": ode_solver, \"n_steps_hevo\": n_steps_hevo}\n\nout_state = run(hamevo, values = values, configuration = config)\n\nprint(out_state)\n</code></pre> <pre><code>tensor([[-0.2785+0.0000j, -0.0541+0.0000j,  0.0000-0.9414j,  0.0000-0.1827j]])\n</code></pre> <p>Note that Qadence makes no assumption on units. The unit of passed duration value \\(\\tau\\) must be aligned with the units of other parameters in the time-dependent generator so that the integral of generator \\(\\overset{\\tau}{\\underset{0}{\\int}}\\mathcal{\\hat{H}}(t){\\rm d}t\\) is dimensionless.</p>"},{"location":"content/time_dependent/#noisy-time-dependent-hamiltonian-evolution","title":"Noisy time-dependent Hamiltonian evolution","text":"<p>To perform noisy time-dependent Hamiltonian evolution, one needs to pass a list of noise operators to the <code>noise_operators</code> argument in <code>HamEvo</code>. They correspond to the jump operators used within the time-dependent Schrodinger equation solver method <code>SolverType.DP5_ME</code>:</p> <pre><code>from qadence import X, Y, HamEvo, TimeParameter, FeatureParameter, run\nfrom pyqtorch.utils import SolverType\nimport torch\n\n# Simulation parameters\node_solver = SolverType.DP5_ME  # time-dependent Schrodinger equation solver method\nn_steps_hevo = 500  # integration time steps used by solver\n\n# Define block parameters\nt = TimeParameter(\"t\")\nomega_param = FeatureParameter(\"omega\")\n\n# Arbitrarily compose a time-dependent generator\ntd_generator = omega_param * (t * X(0) + t**2 * Y(1))\n\n# Create parameterized HamEvo block\nnoise_operators = [X(i) for i in td_generator.qubit_support]\nhamevo = HamEvo(td_generator, t, noise_operators = noise_operators)\n\nvalues = {\"omega\": torch.tensor(10.0), \"duration\": torch.tensor(1.0)}\n\nconfig = {\"ode_solver\": ode_solver, \"n_steps_hevo\": n_steps_hevo}\n\nout_state = run(hamevo, values = values, configuration = config)\n\nprint(out_state)\n</code></pre>   DensityMatrix([[[0.2734+0.0000j, 0.0297+0.0000j, 0.0000-0.0227j, 0.0000-0.0025j],                 [0.0297+0.0000j, 0.1698+0.0000j, 0.0000-0.0025j, 0.0000-0.0141j],                 [0.0000+0.0227j, 0.0000+0.0025j, 0.3435+0.0000j, 0.0373+0.0000j],                 [0.0000+0.0025j, 0.0000+0.0141j, 0.0373+0.0000j, 0.2133+0.0000j]]])    <p>Noise operators definition</p> <p>Note it is not possible to define <code>noise_operators</code> with parametric operators. If you want to do so, we recommend obtaining the tensors via run and set <code>noise_operators</code> using <code>MatrixBlock</code>. Also, <code>noise_operators</code> should have the same or a subset of the qubit support of the <code>HamEvo</code> instance.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"getting_started/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"getting_started/CONTRIBUTING/","title":"How to contribute","text":"<p>We're grateful for your interest in participating in Qadence. Please follow our guidelines to ensure a smooth contribution process.</p>"},{"location":"getting_started/CONTRIBUTING/#reporting-an-issue-or-proposing-a-feature","title":"Reporting an issue or proposing a feature","text":"<p>Your course of action will depend on your objective, but generally, you should start by creating an issue. If you've discovered a bug or have a feature you'd like to see added to qadence, feel free to create an issue on qadence's GitHub issue tracker. Here are some steps to take:</p> <ol> <li>Quickly search the existing issues using relevant keywords to ensure your issue hasn't been addressed already.</li> <li> <p>If your issue is not listed, create a new one. Try to be as detailed and clear as possible in your description.</p> </li> <li> <p>If you're merely suggesting an improvement or reporting a bug, that's already excellent! We thank you for it. Your issue will be listed and, hopefully, addressed at some point.</p> </li> <li>However, if you're willing to be the one solving the issue, that would be even better! In such instances, you would proceed by preparing a Pull Request.</li> </ol>"},{"location":"getting_started/CONTRIBUTING/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>We're excited that you're eager to contribute to Qadence. To contribute, fork the <code>main</code> branch of qadence repository and once you are satisfied with your feature and all the tests pass create a Pull Request.</p> <p>Here's the process for making a contribution:</p> <p>Click the \"Fork\" button at the upper right corner of the repo page to create a new GitHub repo at <code>https://github.com/USERNAME/qadence</code>, where <code>USERNAME</code> is your GitHub ID. Then, <code>cd</code> into the directory where you want to place your new fork and clone it:</p> <pre><code>git clone https://github.com/USERNAME/qadence.git\n</code></pre> <p>Next, navigate to your new qadence fork directory and mark the main qadence repository as the <code>upstream</code>:</p> <pre><code>git remote add upstream https://github.com/pasqal-io/qadence.git\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>We recommended to use <code>hatch</code> for managing environments:</p> <p>To develop within qadence, use: <pre><code>pip install hatch\nhatch -v shell\n</code></pre></p> <p>To run qadence tests, use:</p> <pre><code>hatch -e tests run test\n</code></pre> <p>If you don't want to use <code>hatch</code>, you can use the environment manager of your choice (e.g. Conda) and execute the following:</p> <pre><code>pip install pytest\npip install -e .\npytest\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#useful-things-for-your-workflow-linting-and-testing","title":"Useful things for your workflow: linting and testing","text":"<p>Use <code>pre-commit</code> to lint your code and run the unit tests before pushing a new commit.</p> <p>Using <code>hatch</code>, it's simply:</p> <pre><code>hatch -e tests run pre-commit run --all-files\nhatch -e tests run test\n</code></pre> <p>Our CI/CD pipeline will also test if the documentation can be built correctly. To test it locally, please run:</p> <pre><code>hatch -e docs run mkdocs build --clean --strict\n</code></pre> <p>Without <code>hatch</code>, <code>pip</code> install those libraries first: \"mkdocs\", \"mkdocs-material\", \"mkdocstrings\", \"mkdocstrings-python\", \"mkdocs-section-index\", \"mkdocs-jupyter\", \"mkdocs-exclude\", \"markdown-exec\"</p> <p>And then:</p> <pre><code> mkdocs build --clean --strict\n</code></pre>"},{"location":"getting_started/LICENSE/","title":"Apache License","text":"<p>Version 2.0, January 2004</p> <p>http://www.apache.org/licenses/</p>"},{"location":"getting_started/LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"getting_started/LICENSE/#1-definitions","title":"1. Definitions:","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"getting_started/LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"getting_started/LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"getting_started/LICENSE/#4-redistribution","title":"4. Redistribution.","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ul> <li> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> </li> <li> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> </li> <li> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> </li> <li> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> </li> </ul> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"getting_started/LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions.","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"getting_started/LICENSE/#6-trademarks","title":"6. Trademarks.","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"getting_started/LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty.","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"getting_started/LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability.","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"getting_started/LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability.","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"getting_started/LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":""},{"location":"getting_started/LICENSE/#appendix-how-to-apply-the-apache-license-to-your-work","title":"APPENDIX: How to apply the Apache License to your work.","text":"<p>To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!)  The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>Qadence is fully tested on Linux/MacOS operating systems. For Windows users, we recommend using WSL2 to install a Linux distribution of choice.</p>"},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>Qadence can be installed from PyPI with <code>pip</code> as follows:</p> <pre><code>pip install qadence\n</code></pre> <p>By default, this will also install PyQTorch, a differentiable state vector simulator which serves as the main numerical backend for Qadence.</p> <p>It is possible to install additional backends and the circuit visualization library using the following extras:</p> <ul> <li><code>visualization</code>: to display quantum circuits.</li> <li><code>pulser</code>: the Pulser backend for composing, simulating and executing pulse sequences for neutral-atom quantum devices (in development).</li> </ul> <p>To install other backends or the visualization tool, please use:</p> <pre><code>pip install \"qadence[pulser, visualization]\"\n</code></pre> <p>Note</p> <p>In order to correctly install the <code>visualization</code> extra, the <code>graphviz</code> package needs to be installed in your system:</p> <pre><code># on Ubuntu\nsudo apt install graphviz\n\n# on MacOS\nbrew install graphviz\n\n# via conda\nconda install python-graphviz\n</code></pre>"},{"location":"getting_started/installation/#install-from-source","title":"Install from source","text":"<p>We recommend to use the <code>hatch</code> environment manager to install <code>qadence</code> from source:</p> <pre><code>python -m pip install hatch\n\n# get into a shell with all the dependencies\npython -m hatch shell\n\n# run a command within the virtual environment with all the dependencies\npython -m hatch run python my_script.py\n</code></pre> <p>Warning</p> <p><code>hatch</code> will not combine nicely with other environment managers such Conda. If you want to use Conda, install it from source using <code>pip</code>:</p> <pre><code># within the Conda environment\npython -m pip install -e .\n</code></pre>"},{"location":"getting_started/installation/#citation","title":"Citation","text":"<p>If you use Qadence for a publication, we kindly ask you to cite our work using the following BibTex entry:</p> <pre><code>@article{qadence2024pasqal,\n  title = {Qadence: a differentiable interface for digital-analog programs.},\n  author={Dominik Seitz and Niklas Heim and Jo\u00e3o P. Moutinho and Roland Guichard and Vytautas Abramavicius and Aleksander Wennersteen and Gert-Jan Both and Anton Quelle and Caroline de Groot and Gergana V. Velikova and Vincent E. Elfving and Mario Dagrada},\n  journal={arXiv:2401.09915},\n  url = {https://github.com/pasqal-io/qadence},\n  year = {2024}\n}\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section is undergoing changes and most of the information here will be reorganized.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/advanced_tutorials/","title":"Advanced Tutorials","text":"<p>In this section, advanced programming concepts and implementations in Qadence are examplified.</p>"},{"location":"tutorials/advanced_tutorials/custom-models/","title":"Custom quantum models","text":"<p>In <code>qadence</code>, the <code>QuantumModel</code> is the central class point for executing <code>QuantumCircuit</code>s.  The idea of a <code>QuantumModel</code> is to decouple the backend execution from the management of circuit parameters and desired quantum computation output.</p> <p>In the following, we create a custom <code>QuantumModel</code> instance which introduces some additional optimizable parameters: *  an adjustable scaling factor in front of the observable to measured *  adjustable scale and shift factors to be applied to the model output before returning the result</p> <p>This can be easily done using PyTorch flexible model definition, and it will automatically work with the rest of <code>qadence</code> infrastructure.</p> <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit\n\n\nclass CustomQuantumModel(QuantumModel):\n\n    def __init__(self, circuit: QuantumCircuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\"):\n        super().__init__(circuit, observable=observable, backend=backend, diff_mode=diff_mode)\n\n        self.n_qubits = circuit.n_qubits\n\n        # define some additional parameters which will scale and shift (variationally) the\n        # output of the QuantumModel\n        # you can use all torch machinery for building those\n        self.scale_out = torch.nn.Parameter(torch.ones(1))\n        self.shift_out = torch.nn.Parameter(torch.ones(1))\n\n    # override the forward pass of the model\n    # the forward pass is the output of your QuantumModel and in this case\n    # it's the (scaled) expectation value of the total magnetization with\n    # a variable coefficient in front\n    def forward(self, values: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n\n        # scale the observable\n        res = self.expectation(values)\n\n        # scale and shift the result before returning\n        return self.shift_out + res * self.scale_out\n</code></pre> <p>The custom model can be used like any other <code>QuantumModel</code>: <pre><code>from qadence import Parameter, RX, CNOT, QuantumCircuit\nfrom qadence import chain, kron, hamiltonian_factory, Z\nfrom sympy import acos\n\ndef quantum_circuit(n_qubits):\n\n    x = Parameter(\"x\", trainable=False)\n    fm = kron(RX(i, acos(x) * (i+1)) for i in range(n_qubits))\n\n    ansatz = kron(RX(i, f\"theta{i}\") for i in range(n_qubits))\n    ansatz = chain(ansatz, CNOT(0, n_qubits-1))\n\n    block = chain(fm, ansatz)\n    block.tag = \"circuit\"\n    return QuantumCircuit(n_qubits, block)\n\nn_qubits = 4\nbatch_size = 10\ncircuit = quantum_circuit(n_qubits)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)  # Total magnetization\n\nmodel = CustomQuantumModel(circuit, observable, backend=\"pyqtorch\")\n\nvalues = {\"x\": torch.rand(batch_size)}\nres = model(values)\nprint(\"Model output: \", res)\nassert len(res) == batch_size\n</code></pre> <pre><code>Model output:  tensor([[-0.1365],\n        [-0.3117],\n        [-0.2471],\n        [-0.0576],\n        [-0.1891],\n        [-0.2995],\n        [-0.2878],\n        [-0.2408],\n        [-0.2614],\n        [-0.2065]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> </p>"},{"location":"tutorials/advanced_tutorials/custom-models/#quantum-model-with-wavefunction-overlaps","title":"Quantum model with wavefunction overlaps","text":"<p><code>QuantumModel</code>'s can also use different quantum operations in their forward pass, such as wavefunction overlaps described here. Beware that the resulting overlap tensor has to be differentiable to apply gradient-based optimization. This is only applicable to the <code>\"EXACT\"</code> overlap method.</p> <p>Here we show how to use overlap calculation when fitting a parameterized quantum circuit to act as a standard Hadamard gate.</p> <pre><code>from qadence import RY, RX, H, Overlap\n\n# create a quantum model which acts as an Hadamard gate after training\nclass LearnHadamard(QuantumModel):\n    def __init__(\n        self,\n        train_circuit: QuantumCircuit,\n        target_circuit: QuantumCircuit,\n        backend=\"pyqtorch\",\n    ):\n        super().__init__(circuit=train_circuit, backend=backend)\n        self.overlap_fn = Overlap(train_circuit, target_circuit, backend=backend, method=\"exact\", diff_mode='ad')\n\n    def forward(self):\n        return self.overlap_fn()\n\n    # compute the wavefunction of the associated train circuit\n    def wavefunction(self):\n        return model.overlap_fn.run({})\n\n\ntrain_circuit = QuantumCircuit(1, chain(RX(0, \"phi\"), RY(0, \"theta\")))\ntarget_circuit = QuantumCircuit(1, H(0))\n\nmodel = LearnHadamard(train_circuit, target_circuit)\n\n# get the overlap between model and target circuit wavefunctions\nprint(model())\n</code></pre> <pre><code>tensor([[0.7502]], grad_fn=&lt;UnsqueezeBackward0&gt;)\n</code></pre> <p>This model can then be trained with the standard Qadence helper functions.</p> <pre><code>from qadence import run\nfrom qadence.ml_tools import Trainer, TrainConfig\nTrainer.set_use_grad(True)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n\ndef loss_fn(model: LearnHadamard, _unused) -&gt; tuple[torch.Tensor, dict]:\n    loss = criterion(torch.tensor([[1.0]]), model())\n    return loss, {}\n\nconfig = TrainConfig(max_iter=2500)\ntrainer = Trainer(\n    model, optimizer, config, loss_fn\n)\nmodel, optimizer = trainer.fit()\n\nwf_target = run(target_circuit)\nassert torch.allclose(wf_target, model.wavefunction(), atol=1e-2)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/advanced_tutorials/differentiability/","title":"Differentiability","text":"<p>Many application in quantum computing and quantum machine learning more specifically requires the differentiation of a quantum circuit with respect to its parameters.</p> <p>In Qadence, we perform quantum computations via the <code>QuantumModel</code> interface. The derivative of the outputs of quantum models with respect to feature and variational parameters in the quantum circuit can be implemented in Qadence with two different modes:</p> <ul> <li>Automatic differentiation (AD) mode <sup>1</sup>. This mode allows to differentiation both <code>run()</code> and <code>expectation()</code> methods of the <code>QuantumModel</code> and it is the fastest available differentiation method. Under the hood, it is based on the PyTorch autograd engine wrapped by the <code>DifferentiableBackend</code> class. This mode is not working on quantum devices.</li> <li>Generalized parameter shift rule (GPSR) mode. This is general implementation of the well known parameter  shift rule algorithm <sup>2</sup> which works for arbitrary quantum operations <sup>3</sup>. This mode is only applicable to  the <code>expectation()</code> method of <code>QuantumModel</code> but it is compatible with execution or quantum devices.</li> </ul>"},{"location":"tutorials/advanced_tutorials/differentiability/#automatic-differentiation","title":"Automatic differentiation","text":"<p>Automatic differentiation <sup>1</sup> is a procedure to derive a complex function defined as a sequence of elementary mathematical operations in the form of a computer program. Automatic differentiation is a cornerstone of modern machine learning and a crucial ingredient of its recent successes. In its so-called reverse mode, it follows this sequence of operations in reverse order by systematically applying the chain rule to recover the exact value of derivative. Reverse mode automatic differentiation is implemented in Qadence leveraging the PyTorch <code>autograd</code> engine.</p> <p>Only available via the PyQTorch or Horqrux backends</p> <p>Currently, automatic differentiation mode is only available when the <code>pyqtorch</code> or <code>horqrux</code> backends are selected.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#generalized-parameter-shift-rule","title":"Generalized parameter shift rule","text":"<p>The generalized parameter shift rule implementation in Qadence was introduced in <sup>3</sup>. Here the standard parameter shift rules, which only works for quantum operations whose generator has a single gap in its eigenvalue spectrum, was generalized to work with arbitrary generators of quantum operations.</p> <p>For this, we define the differentiable function as quantum expectation value</p> \\[ f(x) = \\left\\langle 0\\right|\\hat{U}^{\\dagger}(x)\\hat{C}\\hat{U}(x)\\left|0\\right\\rangle \\] <p>where \\(\\hat{U}(x)={\\rm exp}{\\left( -i\\frac{x}{2}\\hat{G}\\right)}\\) is the quantum evolution operator with generator \\(\\hat{G}\\) representing the structure of the underlying quantum circuit and \\(\\hat{C}\\) is the cost operator. Then using the eigenvalue spectrum \\(\\left\\{ \\lambda_n\\right\\}\\) of the generator \\(\\hat{G}\\) we calculate the full set of corresponding unique non-zero spectral gaps \\(\\left\\{ \\Delta_s\\right\\}\\) (differences between eigenvalues). It can be shown that the final expression of derivative of \\(f(x)\\) is then given by the following expression:</p> <p>\\(\\begin{equation} \\frac{{\\rm d}f\\left(x\\right)}{{\\rm d}x}=\\overset{S}{\\underset{s=1}{\\sum}}\\Delta_{s}R_{s}, \\end{equation}\\)</p> <p>where \\(S\\) is the number of unique non-zero spectral gaps and \\(R_s\\) are real quantities that are solutions of a system of linear equations</p> <p>\\(\\begin{equation} \\begin{cases} F_{1} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{1}\\Delta_{s}}{2}\\right)R_{s},\\\\ F_{2} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{2}\\Delta_{s}}{2}\\right)R_{s},\\\\  &amp; ...\\\\ F_{S} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{M}\\Delta_{s}}{2}\\right)R_{s}. \\end{cases} \\end{equation}\\)</p> <p>Here \\(F_s=f(x+\\delta_s)-f(x-\\delta_s)\\) denotes the difference between values of functions evaluated at shifted arguments \\(x\\pm\\delta_s\\).</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#adjoint-differentiation","title":"Adjoint Differentiation","text":"<p>Qadence also offers a memory-efficient, non-device compatible alternative to automatic differentation, called 'Adjoint Differentiation' <sup>4</sup> and allows for precisely calculating the gradients of variational parameters in O(P) time and using O(1) state-vectors. Adjoint Differentation is currently only supported by the Torch Engine and allows for first-order derivatives only.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#usage","title":"Usage","text":""},{"location":"tutorials/advanced_tutorials/differentiability/#basics","title":"Basics","text":"<p>In Qadence, the differentiation modes can be selected via the <code>diff_mode</code> argument of the QuantumModel class. It either accepts a <code>DiffMode</code>(<code>DiffMode.GSPR</code>, <code>DiffMode.AD</code> or <code>DiffMode.ADJOINT</code>) or a string (<code>\"gpsr\"\"</code>, <code>\"ad\"</code> or <code>\"adjoint\"</code>). The code in the box below shows how to create <code>QuantumModel</code> instances with all available differentiation modes.</p> <pre><code>from qadence import (FeatureParameter, RX, Z, hea, chain,\n                    hamiltonian_factory, QuantumCircuit,\n                    QuantumModel, BackendName, DiffMode)\nimport torch\n\nn_qubits = 2\n\n# Define a symbolic parameter to differentiate with respect to\nx = FeatureParameter(\"x\")\n\nblock = chain(hea(n_qubits, 1), RX(0, x))\n\n# create quantum circuit\ncircuit = QuantumCircuit(n_qubits, block)\n\n# create total magnetization cost operator\nobs = hamiltonian_factory(n_qubits, detuning=Z)\n\n# create models with AD, ADJOINT and GPSR differentiation engines\nmodel_ad = QuantumModel(circuit, obs,\n                        backend=BackendName.PYQTORCH,\n                        diff_mode=DiffMode.AD)\nmodel_adjoint = QuantumModel(circuit, obs,\n                        backend=BackendName.PYQTORCH,\n                        diff_mode=DiffMode.ADJOINT)\nmodel_gpsr = QuantumModel(circuit, obs,\n                          backend=BackendName.PYQTORCH,\n                          diff_mode=DiffMode.GPSR)\n\n# Create concrete values for the parameter we want to differentiate with respect to\nxs = torch.linspace(0, 2*torch.pi, 100, requires_grad=True)\nvalues = {\"x\": xs}\n\n# calculate function f(x)\nexp_val_ad = model_ad.expectation(values)\nexp_val_adjoint = model_adjoint.expectation(values)\nexp_val_gpsr = model_gpsr.expectation(values)\n\n# calculate derivative df/dx using the PyTorch\n# autograd engine\ndexpval_x_ad = torch.autograd.grad(\n    exp_val_ad, values[\"x\"], torch.ones_like(exp_val_ad), create_graph=True\n)[0]\ndexpval_x_adjoint = torch.autograd.grad(\n    exp_val_adjoint, values[\"x\"], torch.ones_like(exp_val_ad), create_graph=True\n)[0]\ndexpval_x_gpsr = torch.autograd.grad(\n    exp_val_gpsr, values[\"x\"], torch.ones_like(exp_val_gpsr), create_graph=True\n)[0]\n</code></pre> <p>We can plot the resulting derivatives and see that in both cases they coincide.</p> <pre><code>import matplotlib.pyplot as plt\n\n# plot f(x) and df/dx derivatives calculated using AD ,ADJOINT and GPSR\n# differentiation engines\nfig, ax = plt.subplots()\nax.scatter(xs.detach().numpy(),\n           exp_val_ad.detach().numpy(),\n           label=\"f(x)\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_ad.detach().numpy(),\n           label=\"df/dx AD\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_adjoint.detach().numpy(),\n           label=\"df/dx ADJOINT\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_gpsr.detach().numpy(),\n           s=5,\n           label=\"df/dx GPSR\")\nplt.legend()\n</code></pre> 2024-12-05T14:31:07.500601 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/advanced_tutorials/differentiability/#low-level-control-on-the-shift-values","title":"Low-level control on the shift values","text":"<p>In order to get a finer control over the GPSR differentiation engine we can use the low-level Qadence API to define a <code>DifferentiableBackend</code>.</p> <pre><code>from qadence.engines.torch import DifferentiableBackend\nfrom qadence.backends.pyqtorch import Backend as PyQBackend\n\n# define differentiable quantum backend\nquantum_backend = PyQBackend()\nconv = quantum_backend.convert(circuit, obs)\npyq_circ, pyq_obs, embedding_fn, params = conv\ndiff_backend = DifferentiableBackend(quantum_backend, diff_mode=DiffMode.GPSR, shift_prefac=0.2)\n\n# calculate function f(x)\nexpval = diff_backend.expectation(pyq_circ, pyq_obs, embedding_fn(params, values))\n</code></pre> <p>Here we passed an additional argument <code>shift_prefac</code> to the <code>DifferentiableBackend</code> instance that governs the magnitude of shifts \\(\\delta\\equiv\\alpha\\delta^\\prime\\) shown in equation (2) above. In this relation \\(\\delta^\\prime\\) is set internally and \\(\\alpha\\) is the value passed by <code>shift_prefac</code> and the resulting shift value \\(\\delta\\) is then used in all the following GPSR calculations.</p> <p>Tuning parameter \\(\\alpha\\) is useful to improve results when the generator \\(\\hat{G}\\) or the quantum operation is a dense matrix, for example a complex <code>HamEvo</code> operation; if many entries of this matrix are sufficiently larger than 0 the operation is equivalent to a strongly interacting system. In such case parameter \\(\\alpha\\) should be gradually lowered in order to achieve exact derivative values.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#low-level-differentiation-of-qadence-circuits-using-jax","title":"Low-level differentiation of qadence circuits using JAX","text":"<p>For users interested in using the <code>JAX</code> engine instead, we show how to run and differentiate qadence programs using the <code>horqrux</code> backend under qadence examples.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#references","title":"References","text":"<ol> <li> <p>A. G. Baydin et al., Automatic Differentiation in Machine Learning: a Survey \u21a9\u21a9</p> </li> <li> <p>Schuld et al., Evaluating analytic gradients on quantum hardware (2018). \u21a9</p> </li> <li> <p>Kyriienko et al., General quantum circuit differentiation rules \u21a9\u21a9</p> </li> <li> <p>Tyson et al., Efficient calculation of gradients in classical simulations of variational quantum algorithms \u21a9</p> </li> </ol>"},{"location":"tutorials/advanced_tutorials/profiling-and-debugging/","title":"Profiling and debugging on CUDA devices","text":"<p>For this to work, you'll have to have the right to access perf counters on your machine (for example with sudo or inside a Docker container). See: https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/index.html.</p> <pre><code>$ pip install nvidia-pyindex\n$ pip install nvidia-dlprof[pytorch]\n</code></pre> <p>Make sure that your entrypoint is an executable script. That means that it must start with a she-bang on the top line, e.g. <pre><code>#!/bin/python\n</code></pre> and have execution rights <pre><code>$ chmod +x your_script.py\n</code></pre></p> <p>Lastly it's recommended to add <pre><code>import nvidia_dlprof_pytorch_nvtx\nnvidia_dlprof_pytorch_nvtx.init()\n</code></pre> To your script in the beginning enable extra annotations of PyTorch functions.</p> <p>You can then use dlprof to profile. <pre><code>dlprof --mode=pytorch your_script.py\n</code></pre></p> <pre><code>PYQ_LOG_LEVEL=info QADENCE_LOG_LEVEL=debug dlprof --mode=pytorch examples/backends/differentiable_backend.py\n</code></pre> <p>For example to achieve this through Docker we can start a session in the shell, also mounting our local Qadence version and PyQTorch (Both optional, but you probably need to mount your script at least) <pre><code>$ docker run --rm --gpus=1 --shm-size=1g --ulimit memlock=-1 \\\n --ulimit stack=67108864 -it -p8000:8000 -v./:/opt/qadence -v ../PyQ:/opt/pyqtorch pytorch:24.02-py3 bash\n</code></pre> (You may need to jump through extra hoops to make Docker access the GPUs if you have error messages like <code>docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].</code>)</p> <p>After this you should have a shell inside the container and you can <pre><code>root@2a85826c4e7b:/workspace# cd /opt/qadence/\nroot@2a85826c4e7b:/opt/qadence# pip3 install -e .\nroot@2a85826c4e7b:/opt/qadence# pip3 install -e ../pyqtorch/\nroot@2a85826c4e7b:/opt/qadence# pip3 install nvidia-dlprof[pytorch]\nroot@2a85826c4e7b:/opt/qadence# PYQ_LOG_LEVEL=debug QADENCE_LOG_LEVEL=debug dlprof --mode=pytorch --nsys_opts=\"-t cuda,nvtx,cublas,cusparse,cusparse-verbose,cublas-verbose --force-overwrite true\" examples/models/quantum_model.py\n</code></pre></p> <p>Where we have <code>--force-overwrite true</code> to always store the latest profiling result (hence you must rename the file) if you wish to keep several. We add <code>cublas,cusparse,cusparse-verbose,cublas-verbose</code> do get more details about the numerical backend pacakges being used.</p>"},{"location":"tutorials/advanced_tutorials/projectors/","title":"Projector blocks","text":"<p>This section introduces the <code>ProjectorBlock</code> as an implementation for the quantum mechanical projection operation onto the subspace spanned by \\(|a\\rangle\\): \\(\\mathbb{\\hat{P}}=|a\\rangle \\langle a|\\). It evaluates the outer product for bras and kets expressed as bitstrings for a given qubit support. They have to possess matching lengths.</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence.operations import Projector  # Projector as an operation.\n\n# Define a projector for |1&gt; onto the qubit labelled 0.\nprojector_block = Projector(ket=\"1\", bra=\"1\", qubit_support=0)\n\n# As any block, the matrix representation can be retrieved.\nprojector_matrix = block_to_tensor(projector_block)\n</code></pre> <pre><code>projector matrix = tensor([[[0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j]]])\n</code></pre> <p>Other standard operations are expressed as projectors in Qadence. For instance, the number operator is the projector onto the 1-subspace, \\(N=|1\\rangle\\langle 1|\\).</p> <p>In fact, projectors can be used to compose any arbitrary operator. For example, the <code>CNOT</code> can be defined as \\(\\textrm{CNOT}(i,j)=|0\\rangle\\langle 0|_i\\otimes \\mathbb{I}_j+|1\\rangle\\langle 1|_i\\otimes X_j\\) and we can compare its matrix representation with the native one in Qadence:</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence import kron, I, X, CNOT\n\n# Define a projector for |0&gt; onto the qubit labelled 0.\nprojector0 = Projector(ket=\"0\", bra=\"0\", qubit_support=0)\n\n# Define a projector for |1&gt; onto the qubit labelled 0.\nprojector1 = Projector(ket=\"1\", bra=\"1\", qubit_support=0)\n\n# Construct the projector controlled CNOT.\nprojector_cnot = kron(projector0, I(1)) + kron(projector1, X(1))\n\n# Get the underlying unitary.\nprojector_cnot_matrix = block_to_tensor(projector_cnot)\n\n# Qadence CNOT unitary.\nqadence_cnot_matrix = block_to_tensor(CNOT(0,1))\n</code></pre> <pre><code>projector cnot matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]])\nqadence cnot matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Another example is the canonical SWAP unitary that can be defined as \\(SWAP=|00\\rangle\\langle 00|+|01\\rangle\\langle 10|+|10\\rangle\\langle 01|+|11\\rangle\\langle 11|\\). Indeed, it can be shown that their matricial representations are again identical:</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence import SWAP\n\n# Define all projectors.\nprojector00 = Projector(ket=\"00\", bra=\"00\", qubit_support=(0, 1))\nprojector01 = Projector(ket=\"01\", bra=\"10\", qubit_support=(0, 1))\nprojector10 = Projector(ket=\"10\", bra=\"01\", qubit_support=(0, 1))\nprojector11 = Projector(ket=\"11\", bra=\"11\", qubit_support=(0, 1))\n\n# Construct the SWAP gate.\nprojector_swap = projector00 + projector10 + projector01 + projector11\n\n# Get the underlying unitary.\nprojector_swap_matrix = block_to_tensor(projector_swap)\n\n# Qadence SWAP unitary.\nqadence_swap_matrix = block_to_tensor(SWAP(0,1))\n</code></pre> <pre><code>projector swap matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]])\nqadence swap matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]], grad_fn=&lt;UnsafeViewBackward0&gt;)\n</code></pre> <p>Warning</p> <p>Projectors are non-unitary operators, only supported by the PyQTorch backend.</p> <p>To examplify this point, let's run some non-unitary computation involving projectors.</p> <pre><code>from qadence import chain, run\nfrom qadence.operations import H, CNOT\n\n# Define a projector for |1&gt; onto the qubit labelled 1.\nprojector_block = Projector(ket=\"1\", bra=\"1\", qubit_support=1)\n\n# Some non-unitary computation.\nnon_unitary_block = chain(H(0), CNOT(0,1), projector_block)\n\n# Projected wavefunction becomes unnormalized\nprojected_wf = run(non_unitary_block)  # Run on PyQTorch.\n</code></pre> <pre><code>projected_wf = tensor([[0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j]])\n</code></pre>"},{"location":"tutorials/development/architecture/","title":"Architecture and sharp bits","text":"<p>Qadence as a software library mixes functional and object-oriented programming. We do that by maintaining core objects and operating on them with functions.</p> <p>Furthermore, Qadence strives at keeping the lower level abstraction layers for automatic differentiation and quantum computation fully stateless while only the frontend layer which is the main user-facing interface is stateful.</p> <p>Code design philosopy</p> <p>Functional, stateless core with object-oriented, stateful user interface.</p>"},{"location":"tutorials/development/architecture/#abstraction-layers","title":"Abstraction layers","text":"<p>In Qadence there are 4 main objects spread across 3 different levels of abstraction:</p> <ul> <li> <p>Frontend layer: The user facing layer and encompasses two objects:</p> <ul> <li><code>QuantumCircuit</code>: A class representing an abstract quantum   circuit not tight not any particular framework. Parameters are represented symbolically using   <code>sympy</code> expressions.</li> <li><code>QuantumModel</code>: The models are higher-level abstraction   providing an interface for executing different kinds of common quantum computing models such   quantum neural networks (QNNs), quantum kernels etc.</li> </ul> </li> <li> <p>Differentiation layer: Intermediate layer has the purpose of integrating quantum   computation with a given automatic differentiation engine. It is meant to be purely stateless and   contains one object:</p> <ul> <li><code>DifferentiableBackend</code>:   An abstract class whose concrete implementation wraps a quantum backend and make it   automatically differentiable using different engines (e.g. PyTorch or Jax).   Note, that today only PyTorch is supported but there is plan to add also a Jax   differentiable backend which will require some changes in the base class implementation.</li> </ul> </li> <li> <p>Quantum layer: The lower-level layer which directly interfaces with quantum emulators   and processing units. It is meant to be purely stateless and it contains one base object which is   specialized for each supported backend:</p> <ul> <li><code>Backend</code>: An abstract class whose concrete implementation   enables the execution of quantum circuit with a variety of quantum backends (normally non   automatically differentiable by default) such as PyQTorch, or Pulser.</li> </ul> </li> </ul>"},{"location":"tutorials/development/architecture/#main-components","title":"Main components","text":""},{"location":"tutorials/development/architecture/#quantumcircuit","title":"<code>QuantumCircuit</code>","text":"<p>We consider <code>QuantumCircuit</code> to be an abstract object, i.e. it is not tied to any backend. However, it blocks are even more abstract. This is because we consider <code>QuantumCircuit</code>s \"real\", whereas the blocks are largely considered just syntax.</p> <p>Unitary <code>QuantumCircuits</code> (this encompasses digital, or gate-based, circuits as well as analog circuits) are constructed by [<code>PrimitiveBlocks</code>] using a syntax that allows you to execute them in sequence, dubbed <code>ChainBlock</code> in the code, or in parallel (i.e. at the same time) where applicable, dubbed <code>KronBlock</code> in the code. Notice that this differs from other packages by providing more control of the layout of the circuit than conventional packages like Qiskit, and from Yao where the blocks are the primary type.</p>"},{"location":"tutorials/development/architecture/#quantummodel","title":"<code>QuantumModel</code>","text":"<p><code>QuantumModel</code>s are meant to be the main entry point for quantum computations in <code>qadence</code>. In general, they take one or more quantum circuit as input and they wrap all the necessary boiler plate code to make the circuit executable and differentiable on the chosen backend.</p> <p>Models are meant to be specific for a certain kind of quantum problem or algorithm and you can easily create new ones starting from the base class <code>QuantumModel</code>, as explained in the custom model tutorial. Currently, Qadence offers a <code>QNN</code> model class which provides convenient methods to work with quantum neural networks with multi-dimensional inputs and outputs.</p>"},{"location":"tutorials/development/architecture/#differentiablebackend","title":"<code>DifferentiableBackend</code>","text":"<p>The differentiable backend is a thin wrapper which takes as input a <code>QuantumCircuit</code> instance and a chosen quantum backend and make the circuit execution routines (expectation value, overalap, etc.) differentiable. Qadence offers both a PyTorch and Jax differentiation engine.</p>"},{"location":"tutorials/development/architecture/#quantum-backend","title":"Quantum <code>Backend</code>","text":"<p>For execution the primary object is the <code>Backend</code>. Backends maintain the same user-facing interface, and internally connects to other libraries to execute circuits. Those other libraries can execute the code on QPUs and local or cloud-based emulators. The <code>Backends</code> use PyTorch tensors to represent data and leverages PyTorchs autograd to help compute derivatives of circuits.</p>"},{"location":"tutorials/development/architecture/#symbolic-parameters","title":"Symbolic parameters","text":"<p>To illustrate how parameters work in Qadence, let's consider the following simple block composed of just two rotations:</p> <pre><code>import sympy\nfrom qadence import Parameter, RX\n\nparam = Parameter(\"phi\", trainable=False)\nblock = RX(0, param) * RX(1, sympy.acos(param))\n</code></pre> <p>The rotation angles assigned to <code>RX</code> (and to any Qadence quantum operation) are defined as arbitrary expressions of <code>Parameter</code>'s. <code>Parameter</code> is a subclass of <code>sympy.Symbol</code>, thus fully interoperable with it.</p> <p>To assign values of the parameter <code>phi</code> in a quantum model, one should use a dictionary containing the a key with parameter name and the corresponding values values:</p> <pre><code>import torch\nfrom qadence import run\n\nvalues = {\"phi\": torch.rand(10)}\nwf = run(block, values=values)\n</code></pre> <p>This is the only interface for parameter assignment exposed to the user. Under the hood, parameters applied to every quantum operation are identified in different ways:</p> <ul> <li> <p>By default, with a stringified version of the <code>sympy</code> expression supplied to the quantum operation. Notice that multiple operations can have the same expression.</p> </li> <li> <p>In certain case, e.g. for constructing parameter shift rules, one must access a unique identifier of the parameter for each quantum operation. Therefore, Qadence also creates unique identifiers for each parametrized operation (see the <code>ParamMap</code> class).</p> </li> </ul> <p>By default, when one constructs a new backend, the parameter identifiers are the <code>sympy</code> expressions which are used when converting an abstract block into a native circuit for the chosen backend. However, one can use the unique identifiers as parameter names by setting the private flag <code>_use_gate_params</code> to <code>True</code> in the backend configuration <code>BackendConfiguration</code>. This is automatically set when PSR differentiation is selected (see next section for more details).</p> <p>You can see the logic for choosing the parameter identifier in <code>get_param_name</code>.</p>"},{"location":"tutorials/development/architecture/#differentiation-with-parameter-shift-rules-psr","title":"Differentiation with parameter shift rules (PSR)","text":"<p>In Qadence, parameter shift rules are applied by implementing a custom <code>torch.autograd.Function</code> class for PyTorch and the <code>custom_vjp</code> in the Jax Engine, respectively.</p> <p>A custom PyTorch <code>Function</code> looks like this:</p> <pre><code>import torch\nfrom torch.autograd import Function\n\nclass CustomFunction(Function):\n\n    # forward pass implementation giving the output of the module\n    @staticmethod\n    def forward(ctx, inputs: torch.Tensor, params: torch.Tensor):\n        ctx.save_for_backward(inputs, params)\n        ...\n\n    # backward pass implementation giving the derivative of the module\n    # with respect to the parameters. This must return the whole vector-jacobian\n    # product to integrate within the autograd engine\n    @staticmethod\n    def backward(ctx, grad_output: torch.Tensor):\n        inputs, params = ctx.saved_tensors\n        ...\n</code></pre> <p>The class <code>PSRExpectation</code> under <code>qadence.engines.torch.differentiable_expectation</code> implements parameter shift rules for all parameters using a custom function as the one above. There are a few implementation details to keep in mind if you want to modify the PSR code:</p> <ul> <li> <p>PyTorch <code>Function</code> only works with tensor arguments. Parameters in Qadence are passed around as   dictionaries with parameter names as keys and current parameter values (tensors)   as values. This works for both variational and feature parameters. However, the <code>Function</code> class   only work with PyTorch tensors as input, not dictionaries. Therefore, the forward pass of   <code>PSRExpectation</code> accepts one argument <code>param_keys</code> with the   parameter keys and a variadic positional argument <code>param_values</code> with the parameter values one by   one. The dictionary is reconstructed within the <code>forward()</code> pass body.</p> </li> <li> <p>Higher-order derivatives with PSR. Higher-order PSR derivatives can be tricky. Parameter shift   rules calls, under the hood, the <code>QuantumBackend</code> expectation value routine that usually yield a   non-differentiable output. Therefore, a second call to the backward pass would not work. However,   Qadence employs a very simple trick to make higher-order derivatives work: instead of using   directly the expectation value of the quantum backend, the PSR backward pass uses the PSR forward   pass itself as expectation value function (see the code below). In this way, multiple calls to the   backward pass are allowed since the <code>expectation_fn</code> routine is always differentiable by   definition. Notice that this implementation is simple but suboptimal since, in some corner cases,   higher-order derivates might include some repeated terms that, with this implementation, are   always recomputed.</p> </li> </ul> <pre><code># expectation value used in the PSR backward pass\ndef expectation_fn(params: dict[str, Tensor]) -&gt; Tensor:\n    return PSRExpectation.apply(\n        ctx.expectation_fn,\n        ctx.param_psrs,\n        params.keys(),\n        *params.values(),\n    )\n</code></pre> <ul> <li> <p>Operation parameters must be uniquely identified for PSR to work. Parameter shift rules work at the level of individual quantum operations. This means that, given a parameter <code>x</code>, one needs to sum the contributions from shifting the parameter values of all the operation where the parameter <code>x</code> appears. When constructing the PSR rules, one must access a unique parameter identifier for each operation even if the corresponding user-facing parameter is the same. Therefore, when PSR differentiation is selected, the flag <code>_use_gate_params</code> is automatically set to <code>True</code> in the backend configuration <code>BackendConfiguration</code> (see previous section).</p> </li> <li> <p>PSR must not be applied to observable parameters. In Qadence, Pauli observables can also be parametrized. However, the tunable parameters of observables are purely classical and should not be included in the differentiation with PSRs. However, the quantum expectation value depends on them, thus they still need to enter into the PSR evaluation. To solve this issue, the code sets the <code>requires_grad</code> attribute of all observable parameters to <code>False</code> when constructing the PSRs for the circuit as in the snippet below:</p> </li> </ul> <pre><code>for obs in observable:\n    for param_id, _ in uuid_to_eigen(obs).items():\n        param_to_psr[param_id] = lambda x: torch.tensor([0.0], requires_grad=False)\n</code></pre>"},{"location":"tutorials/development/draw/","title":"<code>qadence.draw</code> example plots","text":"<p>Mostly for quick, manual checking of correct plotting output.</p> <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\n</code></pre> %3 d47aea37daa04eea933be0bed2353a47 0 4e639d8982b2453f92d9fea922ddd4f6 X d47aea37daa04eea933be0bed2353a47--4e639d8982b2453f92d9fea922ddd4f6 1757ea5764fe4ab2b48540b067a6553b 1 3df8422716434f5ba94ee598e97efbbc 4e639d8982b2453f92d9fea922ddd4f6--3df8422716434f5ba94ee598e97efbbc 3963760928ec4f8888f8b33a738b5e36 beb2de416d464124b0aa9ecb2ce46bc9 Y 1757ea5764fe4ab2b48540b067a6553b--beb2de416d464124b0aa9ecb2ce46bc9 beb2de416d464124b0aa9ecb2ce46bc9--3963760928ec4f8888f8b33a738b5e36 <pre><code>from qadence import X, Y, chain\nfrom qadence.draw import display\n\nb = chain(X(0), Y(0))\n</code></pre> %3 14e6b97d31034802959985a2776ec568 0 e853f0fa1d364df98b01cda10c0909f7 X 14e6b97d31034802959985a2776ec568--e853f0fa1d364df98b01cda10c0909f7 aac0e1a200c04509b6aa96e841eed4f3 Y e853f0fa1d364df98b01cda10c0909f7--aac0e1a200c04509b6aa96e841eed4f3 8713defdc97d4afb857733c1fc36bd7f aac0e1a200c04509b6aa96e841eed4f3--8713defdc97d4afb857733c1fc36bd7f <pre><code>from qadence import X, Y, chain\nfrom qadence.draw import display\n\nb = chain(X(0), Y(1))\n</code></pre> %3 7a5e1133c5af49c68466979472796099 0 0245ed2dfe284611a88bd9ef357a1383 X 7a5e1133c5af49c68466979472796099--0245ed2dfe284611a88bd9ef357a1383 862060ad46c94020a25073c9b93dfb16 1 5e6b3d5b85f04fc392b453460d1c04d3 0245ed2dfe284611a88bd9ef357a1383--5e6b3d5b85f04fc392b453460d1c04d3 f8c07355ec2649fa9b5699097ffc040b 5e6b3d5b85f04fc392b453460d1c04d3--f8c07355ec2649fa9b5699097ffc040b bca12943d252413c8769aea65185feb5 6a69ac0a72cf423f822a59e1fb26478e 862060ad46c94020a25073c9b93dfb16--6a69ac0a72cf423f822a59e1fb26478e be3c85caf809489d9558cd3adcec4f20 Y 6a69ac0a72cf423f822a59e1fb26478e--be3c85caf809489d9558cd3adcec4f20 be3c85caf809489d9558cd3adcec4f20--bca12943d252413c8769aea65185feb5 <pre><code>from qadence import X, Y, add\nfrom qadence.draw import display\n\nb = add(X(0), Y(1), X(2))\n</code></pre> %3 cluster_8f57b2c6d3cf4ac9a93d7bc37a8e22f8 912eda2ef3a44c5686c188977c705a1a 0 ff61117951df453f84c63c9cd745f212 912eda2ef3a44c5686c188977c705a1a--ff61117951df453f84c63c9cd745f212 2792c539f6b94eb484c2a8e91035f3af 1 d1761326192640a6b42c00ab221970f3 ff61117951df453f84c63c9cd745f212--d1761326192640a6b42c00ab221970f3 5c32a93669854db29e4f5fd965ccf7f9 847ae861812549d3bee8a35fea842024 AddBlock 2792c539f6b94eb484c2a8e91035f3af--847ae861812549d3bee8a35fea842024 1b65cd194e114af99130724570757dbb 2 847ae861812549d3bee8a35fea842024--5c32a93669854db29e4f5fd965ccf7f9 78fb70d5438941c8b44c87e158a27093 4593720fcc4a493abd35dc94999f514b 1b65cd194e114af99130724570757dbb--4593720fcc4a493abd35dc94999f514b 4593720fcc4a493abd35dc94999f514b--78fb70d5438941c8b44c87e158a27093 <pre><code>from qadence import CNOT, RX, HamEvo, X, Y, Z, chain, kron\n\nrx = kron(RX(3,0.5), RX(2, \"x\"))\nrx.tag = \"rx\"\ngen = chain(Z(i) for i in range(4))\n\n# `chain` puts things in sequence\nblock = chain(\n    kron(X(0), Y(1), rx),\n    CNOT(2,3),\n    HamEvo(gen, 10)\n)\n</code></pre> %3 cluster_71628a0ad70d4560b4114d3083677daa cluster_3bfa3966dacc44a29d4ee0f050d866b4 rx 093d7a1bc0a1438f82bffa18ce642ef9 0 9473c5e019bf453b916733f34d7aaa98 X 093d7a1bc0a1438f82bffa18ce642ef9--9473c5e019bf453b916733f34d7aaa98 ca76932d521d47c88d4ceb3e92e7b43c 1 cc31eb6fb6f34399b8d9575ee43d5e54 9473c5e019bf453b916733f34d7aaa98--cc31eb6fb6f34399b8d9575ee43d5e54 da7dc8e74b6a4c9987f49b0904616947 cc31eb6fb6f34399b8d9575ee43d5e54--da7dc8e74b6a4c9987f49b0904616947 da9cea2ace2141a9a15a73c8c3d81ded da7dc8e74b6a4c9987f49b0904616947--da9cea2ace2141a9a15a73c8c3d81ded bc376563e50d4a52a9f8e285ab7d55c9 d379292a63404720b79e62587b82dffa Y ca76932d521d47c88d4ceb3e92e7b43c--d379292a63404720b79e62587b82dffa bd71aa567d54496a891d49e4b7bf5e44 2 fc1ecbd7525645a995a3564d62d6113a d379292a63404720b79e62587b82dffa--fc1ecbd7525645a995a3564d62d6113a c9105ce58f794be183b8d64e89634b71 HamEvo fc1ecbd7525645a995a3564d62d6113a--c9105ce58f794be183b8d64e89634b71 c9105ce58f794be183b8d64e89634b71--bc376563e50d4a52a9f8e285ab7d55c9 d18460e9bb5546ec85412856adac0aa6 b32d606990ca4b868246c5c8a9f8e788 RX(x) bd71aa567d54496a891d49e4b7bf5e44--b32d606990ca4b868246c5c8a9f8e788 81542e7abbaf4f64b89733c1235fedf1 3 e3d8fdda57e14ea4af8cf4c6c24f334b b32d606990ca4b868246c5c8a9f8e788--e3d8fdda57e14ea4af8cf4c6c24f334b 3506271134a1414e86242a14b2bf0f5c t = 10 e3d8fdda57e14ea4af8cf4c6c24f334b--3506271134a1414e86242a14b2bf0f5c 3506271134a1414e86242a14b2bf0f5c--d18460e9bb5546ec85412856adac0aa6 fcf6c8c2a9ed4fc3a17fe5ac9f8294a1 059d3790973e4e4abc0c8729c800adfe RX(0.5) 81542e7abbaf4f64b89733c1235fedf1--059d3790973e4e4abc0c8729c800adfe 786f912665fc49838586cc32bc298cb8 X 059d3790973e4e4abc0c8729c800adfe--786f912665fc49838586cc32bc298cb8 786f912665fc49838586cc32bc298cb8--e3d8fdda57e14ea4af8cf4c6c24f334b acd80b4b7fc446f993b5ff9892942bd7 786f912665fc49838586cc32bc298cb8--acd80b4b7fc446f993b5ff9892942bd7 acd80b4b7fc446f993b5ff9892942bd7--fcf6c8c2a9ed4fc3a17fe5ac9f8294a1 <pre><code>from qadence import feature_map, hea, chain\n\nblock = chain(feature_map(4, reupload_scaling=\"Tower\"), hea(4,2))\n</code></pre> %3 cluster_d7a06e8ec7fb41118747b5c41a1f0927 HEA cluster_794824abf2764fecaabfa70603381395 Tower Fourier FM 73c76903829e4a909ab589718e90a3b5 0 55a9769715974f02a085d677b04b6c69 RX(1.0*phi) 73c76903829e4a909ab589718e90a3b5--55a9769715974f02a085d677b04b6c69 98af634a0bc3494b9c80e59246907887 1 f961cb490fcc429e8dba510193631003 RX(theta\u2080) 55a9769715974f02a085d677b04b6c69--f961cb490fcc429e8dba510193631003 81c8a6eb5afa4b0da4463e4942300e78 RY(theta\u2084) f961cb490fcc429e8dba510193631003--81c8a6eb5afa4b0da4463e4942300e78 f18703551108477ca217969f452cabc2 RX(theta\u2088) 81c8a6eb5afa4b0da4463e4942300e78--f18703551108477ca217969f452cabc2 359eff4bf36a453eb66bed984634e097 f18703551108477ca217969f452cabc2--359eff4bf36a453eb66bed984634e097 51875b43d343495c9170f259cd3d13f6 359eff4bf36a453eb66bed984634e097--51875b43d343495c9170f259cd3d13f6 94d3aecca81941ac9b8ed96f8b443517 RX(theta\u2081\u2082) 51875b43d343495c9170f259cd3d13f6--94d3aecca81941ac9b8ed96f8b443517 c426917eadcc4b5db1908b63b6f9d710 RY(theta\u2081\u2086) 94d3aecca81941ac9b8ed96f8b443517--c426917eadcc4b5db1908b63b6f9d710 283e374628fd4ff68ab157503dd7c516 RX(theta\u2082\u2080) c426917eadcc4b5db1908b63b6f9d710--283e374628fd4ff68ab157503dd7c516 41fdbfa2706c4e3fb1c9ab83dde46f49 283e374628fd4ff68ab157503dd7c516--41fdbfa2706c4e3fb1c9ab83dde46f49 b4d8ee9212cf4e94929faf23ebd4d91b 41fdbfa2706c4e3fb1c9ab83dde46f49--b4d8ee9212cf4e94929faf23ebd4d91b 4c2a9b6a1952435aabef84b83c70e987 b4d8ee9212cf4e94929faf23ebd4d91b--4c2a9b6a1952435aabef84b83c70e987 3bcbe4e2f6984a76b4a10aa8588ccb4f 2a60adfc24344cdaad94048cdf18c4dc RX(2.0*phi) 98af634a0bc3494b9c80e59246907887--2a60adfc24344cdaad94048cdf18c4dc 61d6734e27cc4a5dadf7274eaa1ff9dd 2 f51d8196fb504aa2ab11096cfea9a32d RX(theta\u2081) 2a60adfc24344cdaad94048cdf18c4dc--f51d8196fb504aa2ab11096cfea9a32d 863b547bc98046e38a03a8bafac584bc RY(theta\u2085) f51d8196fb504aa2ab11096cfea9a32d--863b547bc98046e38a03a8bafac584bc 3c2dea4cdd564cbfa1c9bf211833f17c RX(theta\u2089) 863b547bc98046e38a03a8bafac584bc--3c2dea4cdd564cbfa1c9bf211833f17c 37ee48c1593d47f0a446e67fb8425945 X 3c2dea4cdd564cbfa1c9bf211833f17c--37ee48c1593d47f0a446e67fb8425945 37ee48c1593d47f0a446e67fb8425945--359eff4bf36a453eb66bed984634e097 f00d6e733330419797794a353611b741 37ee48c1593d47f0a446e67fb8425945--f00d6e733330419797794a353611b741 6e016e1064f14a29931deb1bbd284c3c RX(theta\u2081\u2083) f00d6e733330419797794a353611b741--6e016e1064f14a29931deb1bbd284c3c a7e9b48348b744baaaa5514551bc9705 RY(theta\u2081\u2087) 6e016e1064f14a29931deb1bbd284c3c--a7e9b48348b744baaaa5514551bc9705 d8fad4550759406991908862d9116e9b RX(theta\u2082\u2081) a7e9b48348b744baaaa5514551bc9705--d8fad4550759406991908862d9116e9b a4b001e6918c42029e06113282c9cc1b X d8fad4550759406991908862d9116e9b--a4b001e6918c42029e06113282c9cc1b a4b001e6918c42029e06113282c9cc1b--41fdbfa2706c4e3fb1c9ab83dde46f49 39a574d675da4aab82a385284eb8895c a4b001e6918c42029e06113282c9cc1b--39a574d675da4aab82a385284eb8895c 39a574d675da4aab82a385284eb8895c--3bcbe4e2f6984a76b4a10aa8588ccb4f e73eaffd2f93449cb9d921acd9369c0c 3d800aed0d9c4d4fb936a8eee5908769 RX(3.0*phi) 61d6734e27cc4a5dadf7274eaa1ff9dd--3d800aed0d9c4d4fb936a8eee5908769 f4a0ee8aff2c419fb55e97ce48f76bfb 3 ccffefcebc514e69aa57fc4b3175b735 RX(theta\u2082) 3d800aed0d9c4d4fb936a8eee5908769--ccffefcebc514e69aa57fc4b3175b735 bcb05a62094b4003ab8ee264665f64d9 RY(theta\u2086) ccffefcebc514e69aa57fc4b3175b735--bcb05a62094b4003ab8ee264665f64d9 37896cad05c04d6798f8b8e528111c32 RX(theta\u2081\u2080) bcb05a62094b4003ab8ee264665f64d9--37896cad05c04d6798f8b8e528111c32 0be5ae4a9f794fb9a96e91cab43f659c 37896cad05c04d6798f8b8e528111c32--0be5ae4a9f794fb9a96e91cab43f659c 424c337b8a0746aaa650e05a2c5adf15 X 0be5ae4a9f794fb9a96e91cab43f659c--424c337b8a0746aaa650e05a2c5adf15 424c337b8a0746aaa650e05a2c5adf15--f00d6e733330419797794a353611b741 2aacf4bcc9ff42c5b1afdbdbdbc1f76b RX(theta\u2081\u2084) 424c337b8a0746aaa650e05a2c5adf15--2aacf4bcc9ff42c5b1afdbdbdbc1f76b 750b50f435e04d058eec6e052f4b2a7d RY(theta\u2081\u2088) 2aacf4bcc9ff42c5b1afdbdbdbc1f76b--750b50f435e04d058eec6e052f4b2a7d 6ee24874b8414211bc0b898e6a44f8f1 RX(theta\u2082\u2082) 750b50f435e04d058eec6e052f4b2a7d--6ee24874b8414211bc0b898e6a44f8f1 33168da092e34660a9e8102c179b6c4c 6ee24874b8414211bc0b898e6a44f8f1--33168da092e34660a9e8102c179b6c4c 9b80a699f2a4468ebb62f68e0edf9cb1 X 33168da092e34660a9e8102c179b6c4c--9b80a699f2a4468ebb62f68e0edf9cb1 9b80a699f2a4468ebb62f68e0edf9cb1--39a574d675da4aab82a385284eb8895c 9b80a699f2a4468ebb62f68e0edf9cb1--e73eaffd2f93449cb9d921acd9369c0c 7e1623afa7a74a9984b993c1e9827214 c141c49e563445229089893c447f7c8b RX(4.0*phi) f4a0ee8aff2c419fb55e97ce48f76bfb--c141c49e563445229089893c447f7c8b 07e3ddb860f8442e9407d249163a4253 RX(theta\u2083) c141c49e563445229089893c447f7c8b--07e3ddb860f8442e9407d249163a4253 58e9cc1b5b1745d2912e3f086c3d75ba RY(theta\u2087) 07e3ddb860f8442e9407d249163a4253--58e9cc1b5b1745d2912e3f086c3d75ba 2e0f53463b7440e99bc02f91e181d2b7 RX(theta\u2081\u2081) 58e9cc1b5b1745d2912e3f086c3d75ba--2e0f53463b7440e99bc02f91e181d2b7 a0b6b0bfb7434e6796d0beb218d8bac7 X 2e0f53463b7440e99bc02f91e181d2b7--a0b6b0bfb7434e6796d0beb218d8bac7 a0b6b0bfb7434e6796d0beb218d8bac7--0be5ae4a9f794fb9a96e91cab43f659c 8bbf8f1c1d2943ed9897b4173cfdfc65 a0b6b0bfb7434e6796d0beb218d8bac7--8bbf8f1c1d2943ed9897b4173cfdfc65 dc40fec9e6c142ddb272c2db413d0e06 RX(theta\u2081\u2085) 8bbf8f1c1d2943ed9897b4173cfdfc65--dc40fec9e6c142ddb272c2db413d0e06 2f6e5abc31e24ac7b2361abcef760233 RY(theta\u2081\u2089) dc40fec9e6c142ddb272c2db413d0e06--2f6e5abc31e24ac7b2361abcef760233 b1cdeded32de4a59b412e3e759d0e3cc RX(theta\u2082\u2083) 2f6e5abc31e24ac7b2361abcef760233--b1cdeded32de4a59b412e3e759d0e3cc 9bd054aa5dcc465bad98f0b4b602497b X b1cdeded32de4a59b412e3e759d0e3cc--9bd054aa5dcc465bad98f0b4b602497b 9bd054aa5dcc465bad98f0b4b602497b--33168da092e34660a9e8102c179b6c4c 5b768dad2e324b47b8f9b0892a55ef49 9bd054aa5dcc465bad98f0b4b602497b--5b768dad2e324b47b8f9b0892a55ef49 5b768dad2e324b47b8f9b0892a55ef49--7e1623afa7a74a9984b993c1e9827214 <pre><code>from qadence import QuantumModel, QuantumCircuit, total_magnetization, hea\n\nmodel = QuantumModel(QuantumCircuit(3, hea(3,2)), total_magnetization(3))\n</code></pre> %3 cluster_663f44cb3f9d4f96870c6a946e0adf83 Obs. cluster_ab8ea034a1004bcca0e2fb70a27ffd10 cluster_d52270d6533443e9b50ed245e50206fb HEA e2218267a3264f539bf91441c2f29fb8 0 d9fe4cebc9f6426ea680302ec4cb33a4 RX(theta\u2080) e2218267a3264f539bf91441c2f29fb8--d9fe4cebc9f6426ea680302ec4cb33a4 c9b8f28e010e48e784ae0a103625840f 1 c17ca36cdf3d4becaafb8f9ec8089211 RY(theta\u2083) d9fe4cebc9f6426ea680302ec4cb33a4--c17ca36cdf3d4becaafb8f9ec8089211 d23f81d347674a1faab33b386e200003 RX(theta\u2086) c17ca36cdf3d4becaafb8f9ec8089211--d23f81d347674a1faab33b386e200003 378b533ea04b4001b2ec6cd80206bf68 d23f81d347674a1faab33b386e200003--378b533ea04b4001b2ec6cd80206bf68 77f5893a21384c288c872c37d70c5981 378b533ea04b4001b2ec6cd80206bf68--77f5893a21384c288c872c37d70c5981 eb73aeaf09e746dba9c95dc32e16f35d RX(theta\u2089) 77f5893a21384c288c872c37d70c5981--eb73aeaf09e746dba9c95dc32e16f35d 31c7ec4a408249fab92012cb7c8cf71e RY(theta\u2081\u2082) eb73aeaf09e746dba9c95dc32e16f35d--31c7ec4a408249fab92012cb7c8cf71e 4194968a0c754560b388449689247068 RX(theta\u2081\u2085) 31c7ec4a408249fab92012cb7c8cf71e--4194968a0c754560b388449689247068 4e7898f1e4c14df0b2dd81c4bf846f5e 4194968a0c754560b388449689247068--4e7898f1e4c14df0b2dd81c4bf846f5e c5eeb8d4e0ad41eab3674e0503991ff7 4e7898f1e4c14df0b2dd81c4bf846f5e--c5eeb8d4e0ad41eab3674e0503991ff7 7110cc26b38c48e6a1e57d2b188460de c5eeb8d4e0ad41eab3674e0503991ff7--7110cc26b38c48e6a1e57d2b188460de 8c82c9f613534048827b53ea142940fb 7110cc26b38c48e6a1e57d2b188460de--8c82c9f613534048827b53ea142940fb b66d87039e3a4337a959a3844cfbb510 39a9de3cc2284c1a85b9f71a309b2984 RX(theta\u2081) c9b8f28e010e48e784ae0a103625840f--39a9de3cc2284c1a85b9f71a309b2984 ee2faf7fab4f4f82a2db4126cebaaa6a 2 2cf49464beff4b64a500455b1679dc76 RY(theta\u2084) 39a9de3cc2284c1a85b9f71a309b2984--2cf49464beff4b64a500455b1679dc76 acaaa01258274ca3b945de76a8b59259 RX(theta\u2087) 2cf49464beff4b64a500455b1679dc76--acaaa01258274ca3b945de76a8b59259 f85fdbc1447348d09ec4476c77941c80 X acaaa01258274ca3b945de76a8b59259--f85fdbc1447348d09ec4476c77941c80 f85fdbc1447348d09ec4476c77941c80--378b533ea04b4001b2ec6cd80206bf68 3f3dd2646a7b47f8b85ab88b459ab891 f85fdbc1447348d09ec4476c77941c80--3f3dd2646a7b47f8b85ab88b459ab891 a08864c1519c43949142d5cbf655fec9 RX(theta\u2081\u2080) 3f3dd2646a7b47f8b85ab88b459ab891--a08864c1519c43949142d5cbf655fec9 d68104e2b7994e668f9c15529d8886f8 RY(theta\u2081\u2083) a08864c1519c43949142d5cbf655fec9--d68104e2b7994e668f9c15529d8886f8 2d367697b58f4e1f872901d0054e6f7b RX(theta\u2081\u2086) d68104e2b7994e668f9c15529d8886f8--2d367697b58f4e1f872901d0054e6f7b 6389723b3d9246248c8ac450b46431e5 X 2d367697b58f4e1f872901d0054e6f7b--6389723b3d9246248c8ac450b46431e5 6389723b3d9246248c8ac450b46431e5--4e7898f1e4c14df0b2dd81c4bf846f5e 23ebf522463247569d8cec4576f14f19 6389723b3d9246248c8ac450b46431e5--23ebf522463247569d8cec4576f14f19 16730dcb5f3646b189db4844a002824c AddBlock 23ebf522463247569d8cec4576f14f19--16730dcb5f3646b189db4844a002824c 16730dcb5f3646b189db4844a002824c--b66d87039e3a4337a959a3844cfbb510 fe32cffc6b28422395f273fb11785f9a 3113e567b24240a582093c43eab805a6 RX(theta\u2082) ee2faf7fab4f4f82a2db4126cebaaa6a--3113e567b24240a582093c43eab805a6 7b8b4388fb1d4606830dfd80ebe5bcf5 RY(theta\u2085) 3113e567b24240a582093c43eab805a6--7b8b4388fb1d4606830dfd80ebe5bcf5 3de25035d85d44eea953fee6077541b5 RX(theta\u2088) 7b8b4388fb1d4606830dfd80ebe5bcf5--3de25035d85d44eea953fee6077541b5 99d556bacb344f82b1a3d33b6a994de7 3de25035d85d44eea953fee6077541b5--99d556bacb344f82b1a3d33b6a994de7 6b80ef7db8b54302b607721d0a3f8088 X 99d556bacb344f82b1a3d33b6a994de7--6b80ef7db8b54302b607721d0a3f8088 6b80ef7db8b54302b607721d0a3f8088--3f3dd2646a7b47f8b85ab88b459ab891 e991de26657245f78bfd14de7c552dc1 RX(theta\u2081\u2081) 6b80ef7db8b54302b607721d0a3f8088--e991de26657245f78bfd14de7c552dc1 a7b7b30150084677ab53bee6bf2dcde2 RY(theta\u2081\u2084) e991de26657245f78bfd14de7c552dc1--a7b7b30150084677ab53bee6bf2dcde2 994ea9913e1f468cb6efd97cf35f0816 RX(theta\u2081\u2087) a7b7b30150084677ab53bee6bf2dcde2--994ea9913e1f468cb6efd97cf35f0816 3692a8d22d124e5a9cb16fc8d78151be 994ea9913e1f468cb6efd97cf35f0816--3692a8d22d124e5a9cb16fc8d78151be c22cc1e6d44e4b9ebe2d3b580f771bf1 X 3692a8d22d124e5a9cb16fc8d78151be--c22cc1e6d44e4b9ebe2d3b580f771bf1 c22cc1e6d44e4b9ebe2d3b580f771bf1--23ebf522463247569d8cec4576f14f19 8737ece6a0ac40489ee74b99edb09182 c22cc1e6d44e4b9ebe2d3b580f771bf1--8737ece6a0ac40489ee74b99edb09182 8737ece6a0ac40489ee74b99edb09182--fe32cffc6b28422395f273fb11785f9a <pre><code>from qadence import *\n\nb = chain(SWAP(0,1), SWAP(0,3))\n</code></pre> %3 05682f29323b4e62b56a7a442433eb0d 0 9beac7eeb47d43269545bb9057e60da6 05682f29323b4e62b56a7a442433eb0d--9beac7eeb47d43269545bb9057e60da6 9e355244aa2f4b60b1bb0fc168fe3d1a 1 c397cd336d6e492591ff0c1abe0d85be cfc33d7b2bb64944a217921135de6ce6 9beac7eeb47d43269545bb9057e60da6--cfc33d7b2bb64944a217921135de6ce6 24f87a8c64b7473c9856a94221c1919c c397cd336d6e492591ff0c1abe0d85be--24f87a8c64b7473c9856a94221c1919c be59cf25bdd049c99d26b3f6ecb9a7ca 57e9525858c0428f9e1ee24d0d563cdd 24f87a8c64b7473c9856a94221c1919c--57e9525858c0428f9e1ee24d0d563cdd 8e1108873cf940ffa38fd919be6b31f2 be59cf25bdd049c99d26b3f6ecb9a7ca--8e1108873cf940ffa38fd919be6b31f2 38b2ae92c0d24c3eb01e8b0ef39b752e 65cff32bfc4c44949e2f1d7bc33570bb 9e355244aa2f4b60b1bb0fc168fe3d1a--65cff32bfc4c44949e2f1d7bc33570bb 76016eccc9944d8bb8b0d0c97732e97b 2 65cff32bfc4c44949e2f1d7bc33570bb--c397cd336d6e492591ff0c1abe0d85be 9230b063d1e54e9fa2dd3f726a6c70cb cfc33d7b2bb64944a217921135de6ce6--9230b063d1e54e9fa2dd3f726a6c70cb b69c6b0981a24770bb43f964b7f16255 9230b063d1e54e9fa2dd3f726a6c70cb--b69c6b0981a24770bb43f964b7f16255 b69c6b0981a24770bb43f964b7f16255--38b2ae92c0d24c3eb01e8b0ef39b752e 0adc71a6323e40aeb97811230571ac60 819d17e930cc4401ae68d9b2f67f25d7 76016eccc9944d8bb8b0d0c97732e97b--819d17e930cc4401ae68d9b2f67f25d7 0561c5a26aa34fcfaa0ca3c68dd06252 3 aede8b018faf4288a941b8eaa6f9ae6e 819d17e930cc4401ae68d9b2f67f25d7--aede8b018faf4288a941b8eaa6f9ae6e 413e859b89884478b78afd726bbefbb4 aede8b018faf4288a941b8eaa6f9ae6e--413e859b89884478b78afd726bbefbb4 07ef109f4581488cba36033e69172366 413e859b89884478b78afd726bbefbb4--07ef109f4581488cba36033e69172366 07ef109f4581488cba36033e69172366--0adc71a6323e40aeb97811230571ac60 258b05f652b7419c9dfac7b89c43ef1e 1473ce94bf43411c9fbf44e5496fbd93 0561c5a26aa34fcfaa0ca3c68dd06252--1473ce94bf43411c9fbf44e5496fbd93 a904eda9aeb24ad8b356d8f2fd4581d3 1473ce94bf43411c9fbf44e5496fbd93--a904eda9aeb24ad8b356d8f2fd4581d3 758695331ee2484a877f469e25d2031b a904eda9aeb24ad8b356d8f2fd4581d3--758695331ee2484a877f469e25d2031b 758695331ee2484a877f469e25d2031b--be59cf25bdd049c99d26b3f6ecb9a7ca 57e9525858c0428f9e1ee24d0d563cdd--258b05f652b7419c9dfac7b89c43ef1e <pre><code>from qadence import *\n\nb = chain(CPHASE(0, 1, 0.5), CPHASE(0, 2, 0.5), CPHASE(0, 3, 0.5))\n</code></pre> %3 627c2cdf6d624f949b4953566b71f75b 0 0844121030cf4d38ac7f3e661a06efa0 627c2cdf6d624f949b4953566b71f75b--0844121030cf4d38ac7f3e661a06efa0 95bdb64179934fa1951139bf460802ce 1 133ab730faea41ec9dfc556d014a4d1a 0844121030cf4d38ac7f3e661a06efa0--133ab730faea41ec9dfc556d014a4d1a ab74f87fb1664bf6909070781b40e434 133ab730faea41ec9dfc556d014a4d1a--ab74f87fb1664bf6909070781b40e434 3f09cb14ec874d8b8b006d9f212694ca ab74f87fb1664bf6909070781b40e434--3f09cb14ec874d8b8b006d9f212694ca 5f8cf2b2304f45429d2deea2868779f6 f0d23555af1c4dbaa19e0db6894377e7 PHASE(0.5) 95bdb64179934fa1951139bf460802ce--f0d23555af1c4dbaa19e0db6894377e7 4b01c0a1395a4bc689948273ccacc2e9 2 f0d23555af1c4dbaa19e0db6894377e7--0844121030cf4d38ac7f3e661a06efa0 a5bb9217b91a45fda7e12c2e3652d390 f0d23555af1c4dbaa19e0db6894377e7--a5bb9217b91a45fda7e12c2e3652d390 80f90d80920c4a92b682d501427b5221 a5bb9217b91a45fda7e12c2e3652d390--80f90d80920c4a92b682d501427b5221 80f90d80920c4a92b682d501427b5221--5f8cf2b2304f45429d2deea2868779f6 f5933d000c3f4a18b30f9e95bb76f240 8600edab32c5427881713dc1ba493219 4b01c0a1395a4bc689948273ccacc2e9--8600edab32c5427881713dc1ba493219 6f687fa998444334b8784d414fb856d4 3 d1ebdcf04320477b9c24be662b3291ae PHASE(0.5) 8600edab32c5427881713dc1ba493219--d1ebdcf04320477b9c24be662b3291ae d1ebdcf04320477b9c24be662b3291ae--133ab730faea41ec9dfc556d014a4d1a ef748554c8814b32ab564c321d0ee303 d1ebdcf04320477b9c24be662b3291ae--ef748554c8814b32ab564c321d0ee303 ef748554c8814b32ab564c321d0ee303--f5933d000c3f4a18b30f9e95bb76f240 36d5b0b3824244b9a0c2ca72d08d55c1 bbb760250165489799df45e5bcd12efb 6f687fa998444334b8784d414fb856d4--bbb760250165489799df45e5bcd12efb a5172aa8d34a498fa8d34b12c01730de bbb760250165489799df45e5bcd12efb--a5172aa8d34a498fa8d34b12c01730de f06e4fe8847440ad9cd824251f691921 PHASE(0.5) a5172aa8d34a498fa8d34b12c01730de--f06e4fe8847440ad9cd824251f691921 f06e4fe8847440ad9cd824251f691921--ab74f87fb1664bf6909070781b40e434 f06e4fe8847440ad9cd824251f691921--36d5b0b3824244b9a0c2ca72d08d55c1"},{"location":"tutorials/development/draw/#developer-documentation","title":"Developer documentation","text":"<p>This section contains examples in pure graphviz that can be used to understand roughly what is done in the actual drawing backend.</p> <pre><code>import graphviz\n\nfont_name = \"Sans-Serif\"\nfont_size = \"8\"\n\ngraph_attr = {\n    \"rankdir\": \"LR\",  # LR = left to right, TB = top to bottom\n    \"nodesep\": \"0.1\",  # In inches, tells distance between nodes without edges\n    \"compound\": \"true\",  # Needed to draw properly edges in hamevo when content is hidden\n    \"splines\": \"false\",  # Needed to draw control gates vertical lines one over the other\n}  # These are the default values for graphs\n\nnode_attr = {\n    \"shape\": \"box\",  # 'box' for normal nodes, 'point' for control gates or 'plaintext' for starting nodes (the qubit label).\n    \"style\": \"rounded\",  # Unfortunately we can't specify the radius of the rounded, at least for this version\n    \"fontname\": font_name,\n    \"fontsize\": font_size,\n    \"width\": \"0.1\",  # In inches, it doesn't get tinier than the label font.\n    \"height\": \"0.1\"  # In inches, it doesn't get tinier than the label font.\n}  # These are the defaults values that can be overridden at node declaration.\n\ndefault_cluster_attr = {\n    \"fontname\": font_name,\n    \"fontsize\": font_size,\n    \"labelloc\": \"b\",  # location of cluster label. b as bottom, t as top\n    \"style\": \"rounded\"\n} # These are the defaults values that can be overridden at sub graph declaration\n\nhamevo_cluster_attr = {\n    \"label\": \"HamEvo(t=10)\"\n}\nhamevo_cluster_attr.update(default_cluster_attr)\n\nh = graphviz.Graph(graph_attr=graph_attr, node_attr=node_attr)\nh.node(\"Hello World!\")\nh\n</code></pre> <pre><code>\n</code></pre> <pre><code># Define graph\nh = graphviz.Graph(node_attr=node_attr, graph_attr=graph_attr)\n\n# Add start and end nodes\nfor i in range(4):\n    h.node(f's{i}', shape=\"plaintext\", label=f'{i}', group=f\"{i}\")\n    h.node(f'e{i}', style='invis', group=f\"{i}\")\n\n# Add nodes\nh.node('X', group=\"0\")\nh.node('Y', group=\"1\")\n\n# Add hamevo and its nodes\nhamevo = graphviz.Graph(name='cluster_hamevo', graph_attr=hamevo_cluster_attr)\nfor i in range(4):\n    hamevo.node(f'z{i}', shape=\"box\", style=\"invis\", label=f'{i}', group=f\"{i}\")\nh.subgraph(hamevo)\n\n# Add rx gates cluster and its nodes\ncluster_attr = {\"label\": \"RX gates\"}\ncluster_attr.update(default_cluster_attr)\ncluster = graphviz.Graph(name=\"cluster_0\", graph_attr=cluster_attr)\ncluster.node('RX(x)', group=\"2\")\ncluster.node('RX(0.5)', group=\"3\")\nh.subgraph(cluster)\n\nh.node('cnot0', label='', shape='point', width='0.1', group='0')\nh.node('cnot1', label='X', group='1')\nh.node('cnot2', label='', shape='point', width='0.1', group='2')\nh.node('cnot3', label='', shape='point', width='0.1', group='3')\n\n# Add edges\nh.edge('s0', 'X')\nh.edge('X', 'cnot0')\nh.edge('cnot0', 'z0', lhead='cluster_hamevo')\nh.edge('z0', 'e0', ltail='cluster_hamevo')\nh.edge('s1', 'Y')\nh.edge('Y', 'cnot1')\nh.edge('cnot1', 'z1', lhead='cluster_hamevo')\nh.edge('z1', 'e1', ltail='cluster_hamevo')\nh.edge('s2', 'RX(x)')\nh.edge('RX(x)', 'cnot2')\nh.edge('cnot2', 'z2', lhead='cluster_hamevo')\nh.edge('z2', 'e2', ltail='cluster_hamevo')\nh.edge('s3', 'RX(0.5)')\nh.edge('RX(0.5)', 'cnot3')\nh.edge('cnot3', 'z3', lhead='cluster_hamevo')\nh.edge('z3', 'e3', ltail='cluster_hamevo')\nh.edge('cnot1', 'cnot0', constraint='false')  # constraint: false is needed to draw vertical edges\nh.edge('cnot1', 'cnot2', constraint='false')  # constraint: false is needed to draw vertical edges\nh.edge('cnot1', 'cnot3', constraint='false')  # constraint: false is needed to draw vertical edges\nh\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/development/draw/#example-of-cluster-of-clusters","title":"Example of cluster of clusters","text":"<pre><code># Define graph\nh = graphviz.Graph(node_attr=node_attr, graph_attr=graph_attr)\n\n# Define start and end nodes\nfor i in range(4):\n    h.node(f's{i}', shape=\"plaintext\", label=f'{i}', group=f\"{i}\")\n    h.node(f'e{i}', style='invis', group=f\"{i}\")\n\n# Define outer cluster\ncluster_attr = {\"label\": \"Outer cluster\"}\ncluster_attr.update(default_cluster_attr)\nouter_cluster = graphviz.Graph(name=\"cluster_outer\", graph_attr=cluster_attr)\n\n# Define inner cluster 1 and its nodes\ncluster_attr = {\"label\": \"Inner cluster 1\"}\ncluster_attr.update(default_cluster_attr)\ninner1_cluster = graphviz.Graph(name=\"cluster_inner1\", graph_attr=cluster_attr)\ninner1_cluster.node(\"a0\", group=\"0\")\ninner1_cluster.node(\"a1\", group=\"1\")\nouter_cluster.subgraph(inner1_cluster)\n\n# Define inner cluster 2 and its nodes\ncluster_attr = {\"label\": \"Inner cluster 2\"}\ncluster_attr.update(default_cluster_attr)\ninner2_cluster = graphviz.Graph(name=\"cluster_inner2\", graph_attr=cluster_attr)\ninner2_cluster.node(\"a2\", group=\"2\")\ninner2_cluster.node(\"a3\", group=\"3\")\nouter_cluster.subgraph(inner2_cluster)\n\n# This has to be done here, after inner clusters definitions\nh.subgraph(outer_cluster)\n\n# Define more nodes\nfor i in range(4):\n    h.node(f\"b{i}\", group=f\"{i}\")\n\nfor i in range(4):\n    h.edge(f's{i}', f'a{i}')\n    h.edge(f'a{i}', f'b{i}')\n    h.edge(f'b{i}', f'e{i}')\n\nh\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/digital_analog_qc/","title":"Digital-Analog Quantum Computation","text":"<p>Digital-analog quantum computation (DAQC) is a universal quantum computing paradigm<sup>1</sup>, based on two primary computations:</p> <ul> <li>Fast single-qubit operations (digital).</li> <li>Multi-partite entangling operations acting on all qubits (analog).</li> </ul> <p>A promising quantum computing platform for the implementation of the DAQC paradigm is neutral-atoms, where both these computations are realizable.</p>"},{"location":"tutorials/digital_analog_qc/#digital-analog-emulation","title":"Digital-analog emulation","text":"<p>Qadence simplifies the execution of DAQC programs on either emulated or real devices by providing a simplified interface for customizing interactions and interfacing with pulse-level programming in <code>Pulser</code><sup>3</sup>.</p>"},{"location":"tutorials/digital_analog_qc/#digital-analog-transformation","title":"Digital-analog transformation","text":"<p>Furthermore, the essence of digital-analog computation is the ability to represent any analog operation, i.e. any arbitrary Hamiltonian, using an auxiliary device-amenable Hamiltonian, such as the ubiquitous Ising model<sup>2</sup>. This is at the core of the DAQC implementation in Qadence.</p>"},{"location":"tutorials/digital_analog_qc/#execution-on-rydberg-atom-arrays-with-restriced-addressability","title":"Execution on Rydberg atom arrays with restriced addressability","text":"<p>Finally, Qadence offers some convenience constructors and interfaces to execute programs compatible with a DAQC flavor featuring only a restricted access to individual qubit addressability with always-on interaction. This regime is common in currently available neutral atom quantum computers.</p>"},{"location":"tutorials/digital_analog_qc/#references","title":"References","text":"<ol> <li> <p>Dodd et al., Universal quantum computation and simulation using any entangling Hamiltonian and local unitaries, PRA 65, 040301 (2002). \u21a9</p> </li> <li> <p>Pulser: An open-source package for the design of pulse sequences in programmable neutral-atom arrays \u21a9</p> </li> <li> <p>Parra-Rodriguez et al., Digital-Analog Quantum Computation, PRA 101, 022305 (2020). \u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/analog-basics/","title":"Basic operations on neutral-atoms","text":"<p>Warning</p> <p>The digital-analog emulation framework is under construction and more changes to the interface may still occur.</p> <p>Qadence includes primitives for the construction of programs implemented on a set of interacting qubits. The goal is to build digital-analog programs that better represent the reality of interacting qubit platforms, such as neutral-atoms, while maintaining a simplified interface for users coming from a digital quantum computing background that may not be as familiar with pulse-level programming.</p> <p>To build the intuition for the interface in Qadence, it is important to go over some of the underlying physics. We can write a general Hamiltonian for a set of \\(n\\) interacting qubits as</p> \\[ \\mathcal{H} = \\sum_{i=0}^{n-1}\\left(\\mathcal{H}^\\text{d}_{i}(t) + \\sum_{j&lt;i}\\mathcal{H}^\\text{int}_{ij}\\right), \\] <p>where the driving Hamiltonian \\(\\mathcal{H}^\\text{d}_{i}\\) describes the pulses used to control single-qubit rotations, and the interaction Hamiltonian \\(\\mathcal{H}^\\text{int}_{ij}\\) describes the natural interaction between qubits.</p>"},{"location":"tutorials/digital_analog_qc/analog-basics/#rydberg-atoms","title":"Rydberg atoms","text":"<p>For the purpose of digital-analog emulation of neutral-atom systems in Qadence, we now consider a simplified time-independent global driving Hamiltonian, written as</p> \\[ \\mathcal{H}^\\text{d}_{i} = \\frac{\\Omega}{2}\\left(\\cos(\\phi) X_i - \\sin(\\phi) Y_i \\right) - \\delta N_i \\] <p>where \\(\\Omega\\) is the Rabi frequency, \\(\\delta\\) is the detuning, \\(\\phi\\) is the phase, \\(X_i\\) and \\(Y_i\\) are the standard Pauli operators, and \\(N_i=\\frac{1}{2}(I_i-Z_i)\\) is the number operator. This Hamiltonian allows arbitrary global single-qubit rotations to be written, meaning that the values set for \\((\\Omega,\\phi,\\delta)\\) are the same accross the qubit support.</p> <p>For the interaction term, Rydberg atoms typically allow both an Ising and an XY mode of operation. For now, we focus on the Ising interaction, where the Hamiltonian is written as</p> \\[ \\mathcal{H}^\\text{int}_{ij} = \\frac{C_6}{r_{ij}^6}N_iN_j \\] <p>where \\(r_{ij}\\) is the distance between atoms \\(i\\) and \\(j\\), and \\(C_6\\) is a coefficient depending on the specific Rydberg level of the excited state used in the computational logic states. A typical value for rydberg level of 60 is \\(C_6\\approx 866~[\\text{rad} . \\mu \\text{m}^6 / \\text{ns}]\\).</p> <p>For a given register of atoms prepared in some spatial coordinates, the Hamiltonians described will generate the dynamics of some unitary operation as</p> \\[ U(t, \\Omega, \\delta, \\phi) = \\exp(-i\\mathcal{H}t) \\] <p>where we specify the final parameter \\(t\\), the duration of the operation.</p> <p>Qadence uses the following units for user-specified parameters:</p> <ul> <li>Rabi frequency and detuning \\(\\Omega\\), \\(\\delta\\): \\([\\text{rad}/\\mu \\text{s}]\\)</li> <li>Phase \\(\\phi\\): \\([\\text{rad}]\\)</li> <li>Duration \\(t\\): \\([\\text{ns}]\\)</li> <li>Atom coordinates: \\([\\mu \\text{m}]\\)</li> </ul>"},{"location":"tutorials/digital_analog_qc/analog-basics/#in-practice","title":"In practice","text":"<p>Given the Hamiltonian description in the previous section, we will now go over a few examples of the standard operations available in Qadence.</p>"},{"location":"tutorials/digital_analog_qc/analog-basics/#arbitrary-rotation","title":"Arbitrary rotation","text":"<p>To start, we will exemplify the a general rotation on a set of atoms. To create an arbitrary register of atoms, we refer the user to the register creation tutorial. Below, we create a line register of three qubits with a separation of \\(8~\\mu\\text{m}\\). This is a typical value used in combination with a standard experimental setup of neutral atoms such that the interaction term in the Hamiltonian can effectively be used for computations.</p> <pre><code>from qadence import Register\n\nreg = Register.line(3, spacing=8.0)  # Atom spacing in \u03bcm\n</code></pre> <p>Currently, the most general rotation operation uses the <code>AnalogRot</code> operation, which essentially implements \\(U(t, \\Omega, \\delta, \\phi)\\) defined above.</p> <pre><code>from qadence import AnalogRot, PI\n\nrot_op = AnalogRot(\n    duration = 500., # [ns]\n    omega = PI, # [rad/\u03bcs]\n    delta = PI, # [rad/\u03bcs]\n    phase = PI, # [rad]\n)\n</code></pre> <p>Note that in the code above a specific qubit support is not defined. By default this operation applies a global rotation on all qubits. We can define a circuit using the 3-qubit register and run it in the pyqtorch backend:</p> <pre><code>from qadence import BackendName, run\n\nwf = run(reg, rot_op, backend = BackendName.PYQTORCH)\n\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4248-0.2411j, -0.1687+0.3156j, -0.1696+0.2676j, -0.2040-0.2671j,\n         -0.1687+0.3156j,  0.0014-0.2721j, -0.2040-0.2671j,  0.3034-0.1130j]])\n</code></pre> Under the hood of AnalogRot      To be fully explicit about what goes on under the hood of `AnalogRot`, we can look at the example     code below.      <pre><code>from qadence import BackendName, HamEvo, X, Y, N, add, run, PI\nfrom qadence.analog.constants import C6_DICT\nfrom math import cos, sin\n\n# Following the 3-qubit register above\nn_qubits = 3\ndx = 8.0\n\n# Parameters used in the AnalogRot\nduration = 500.\nomega = PI\ndelta = PI\nphase = PI\n\n# Building the terms in the driving Hamiltonian\nh_x = (omega / 2) * cos(phase) * add(X(i) for i in range(n_qubits))\nh_y = (-1.0 * omega / 2) * sin(phase) * add(Y(i) for i in range(n_qubits))\nh_n = -1.0 * delta * add(N(i) for i in range(n_qubits))\n\n# Building the interaction Hamiltonian\n\n# Dictionary of coefficient values for each Rydberg level, which is 60 by default\nc_6 = C6_DICT[60]\n\nh_int = c_6 * (\n    1/(dx**6) * (N(0)@N(1)) +\n    1/(dx**6) * (N(1)@N(2)) +\n    1/((2*dx)**6) * (N(0)@N(2))\n)\n\nhamiltonian = h_x + h_y + h_n + h_int\n\n# Convert duration to \u00b5s due to the units of the Hamiltonian\nexplicit_rot = HamEvo(hamiltonian, duration / 1000)\n\nwf = run(n_qubits, explicit_rot, backend = BackendName.PYQTORCH)\n\n# We get the same final wavefunction\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4248-0.2411j, -0.1687+0.3156j, -0.1696+0.2676j, -0.2040-0.2671j,\n         -0.1687+0.3156j,  0.0014-0.2721j, -0.2040-0.2671j,  0.3034-0.1130j]])\n</code></pre> <p>When sending the <code>AnalogRot</code> operation to the pyqtorch backend, Qadence automatically builds the correct Hamiltonian and the corresponding <code>HamEvo</code> operation with the added qubit interactions, as shown explicitly in the minimized section above. However, this operation is also supported in the Pulser backend, where the correct pulses are automatically created.</p> <pre><code>wf = run(\n    reg,\n    rot_op,\n    backend = BackendName.PULSER,\n)\n\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4254-0.2408j, -0.1688+0.3157j, -0.1698+0.2678j, -0.2044-0.2666j,\n         -0.1688+0.3157j,  0.0010-0.2721j, -0.2044-0.2666j,  0.3024-0.1138j]])\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#rx-ry-rz-rotations","title":"RX / RY / RZ rotations","text":"<p>The <code>AnalogRot</code> provides full control over the parameters of \\(\\mathcal{H}^\\text{d}\\), but users coming from a digital quantum computing background may be more familiar with the standard <code>RX</code>, <code>RY</code> and <code>RZ</code> rotations, also available in Qadence. For the emulated analog interface, Qadence provides alternative <code>AnalogRX</code>, <code>AnalogRY</code> and <code>AnalogRZ</code> operations which call <code>AnalogRot</code> under the hood to represent the rotations accross the respective axis.</p> <p>For a given angle of rotation \\(\\theta\\) provided to each of these operations, currently a set of hardcoded assumptions are made on the tunable Hamiltonian parameters:</p> \\[ \\begin{aligned} \\text{RX}:&amp; \\quad \\Omega = \\pi, \\quad \\delta = 0, \\quad \\phi = 0, \\quad t = (\\theta/\\Omega)\\times 10^3 \\\\ \\text{RY}:&amp; \\quad \\Omega = \\pi, \\quad \\delta = 0, \\quad \\phi = -\\pi/2, \\quad t = (\\theta/\\Omega)\\times 10^3 \\\\ \\text{RZ}:&amp; \\quad \\Omega = 0, \\quad \\delta = \\pi, \\quad \\phi = 0, \\quad t = (\\theta/\\delta)\\times 10^3 \\\\ \\end{aligned} \\] <p>Note that the \\(\\text{RZ}\\) operation as defined above includes a global phase compared to the standard \\(\\text{RZ}\\) rotation since it evolves \\(\\exp\\left(-i\\frac{\\theta}{2}\\frac{I-Z}{2}\\right)\\) instead of \\(\\exp\\left(-i\\frac{\\theta}{2}Z\\right)\\) given the detuning operator in \\(\\mathcal{H}^\\text{d}\\).</p> <p>Warning</p> <p>As shown above, the values of \\(\\Omega\\) and \\(\\delta\\) are currently hardcoded in these operators, and the effective angle of rotation is controlled by varying the duration of the evolution. Currently, the best way to overcome this is to use <code>AnalogRot</code> directly, but more general and convenient options will be provided soon in an improved interface.</p> <p>Below we exemplify the usage of <code>AnalogRX</code>:</p> <pre><code>from qadence import Register, BackendName\nfrom qadence import RX, AnalogRX, random_state, equivalent_state, kron, run, PI\n\nn_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\n# Rotation angle\ntheta = PI\n\n# Analog rotation using the Rydberg Hamiltonian\nrot_analog = AnalogRX(angle = theta)\n\n# Equivalent full-digital global rotation\nrot_digital = kron(RX(i, theta) for i in range(n_qubits))\n\n# Some random initial state\ninit_state = random_state(n_qubits)\n\n# Compare the final state using the full digital and the AnalogRX\nwf_analog_pyq = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\n\nwf_digital_pyq = run(\n    reg,\n    rot_digital,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_digital_pyq, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  False\n</code></pre> <p>As we can see, running a global <code>RX</code> or the <code>AnalogRX</code> does not result in equivalent states at the end, given that the digital <code>RX</code> operation does not include the interaction between the qubits. By setting <code>dx</code> very high in the code above the interaction will be less significant and the results will match.</p> <p>However, if we compare with the Pulser backend, we see that the results for <code>AnalogRX</code> are consistent with the expected results from a real device:</p> <pre><code>wf_analog_pulser = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PULSER,\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_analog_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#evolving-the-interaction-term","title":"Evolving the interaction term","text":"<p>Finally, besides applying specific qubit rotations, we can also choose to evolve only the interaction term \\(\\mathcal{H}^\\text{int}\\), equivalent to setting \\(\\Omega = \\delta = \\phi = 0\\). To do so, Qadence provides the function <code>AnalogInteraction</code> which does exactly this.</p> <pre><code>from qadence import Register, BackendName, random_state, equivalent_state, AnalogInteraction, run\n\nn_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\nduration = 1000.\nop = AnalogInteraction(duration = duration)\n\ninit_state = random_state(n_qubits)\n\nwf_pyq = run(reg, op, state = init_state, backend = BackendName.PYQTORCH)\nwf_pulser = run(reg, op, state = init_state, backend = BackendName.PULSER)\n\nbool_equiv = equivalent_state(wf_pyq, wf_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#device-specifications-in-qadence","title":"Device specifications in Qadence","text":"<p>As a way to control other specifications of the interacting Rydberg atoms, Qadence provides a <code>RydbergDevice</code> class, which is currently used for both the pyqtorch and the pulser backends. Below we initialize a Rydberg device showcasing all the possible options.</p> <pre><code>from qadence import RydbergDevice, DeviceType, Interaction, PI\n\ndevice_specs = RydbergDevice(\n    interaction=Interaction.NN, # Or Interaction.XY, supported only for pyqtorch\n    rydberg_level=60, # Integer value affecting the C_6 coefficient\n    coeff_xy=3700.00, # C_3 coefficient for the XY interaction\n    max_detuning=2 * PI * 4, # Max value for delta, currently only used in pulser\n    max_amp=2 * PI * 3, # Max value for omega, currently only used in pulser\n    pattern=None, # Semi-local addressing pattern, see the relevant tutorial\n    type=DeviceType.IDEALIZED, # Pulser device to which the qadence device is converted in that backend\n)\n</code></pre> <p>The values above are the defaults when simply running <code>device_specs = RydbergDevice()</code>. The convenience wrappers <code>IdealDevice()</code> or <code>RealisticDevice()</code> can also be used which simply change the <code>type</code> for the Pulser backend, but also allow an <code>AddressingPattern</code> passed in the <code>pattern</code> argument (see the relevant tutorial here).</p> <p>Warning</p> <p>Currently, the options above are not fully integrated in both backends and this class should mostly be used if a user wishes to experiment with a different <code>rydberg_level</code>, or to change the device type for the pulser backend.</p> <p>Planned features to add to the RydbergDevice include the definition of custom interaction functions, the control of other drive Hamiltonian parameters so that \\(\\Omega\\), \\(\\delta\\) and \\(\\phi\\) are not hardcoded when doing analog rotations, and the usage of the <code>max_detuning</code> and <code>max_amp</code> to control those respective parameters when training models in the pyqtorch backend.</p> <p>Finally, to change a given simulation, the device specifications are integrated in the Qadence <code>Register</code>. By default, all registers initialize an <code>IdealDevice()</code> under the hood. Below we run a quick test for a different rydberg level.</p> <pre><code>from qadence import Register, BackendName, random_state, equivalent_state, run\nfrom qadence import AnalogRX, RydbergDevice, PI\n\ndevice_specs = RydbergDevice(rydberg_level = 70)\n\nn_qubits_side = 2\nreg = Register.square(\n    n_qubits_side,\n    spacing = 8.0,\n    device_specs = device_specs\n)\n\nrot_analog = AnalogRX(angle = PI)\n\ninit_state = random_state(n_qubits = 4)\n\nwf_analog_pyq = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\nwf_analog_pulser = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PULSER\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_analog_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#technical-details","title":"Technical details","text":"<p>Warning</p> <p>The details described here are relevant in the current version but will be lifted soon for the next version of the emulated analog interface.</p> <p>In the previous section we have exemplified the main ingredients of the current user-facing functionalities of the emulated analog interface, and in the next tutorial on Quantum Circuit Learning we will exmplify its usage in a simple QML example. Here we specify some extra details of this interface.</p> <p>In the block system, all analog rotation operators initialize a <code>ConstantAnalogRotation</code> block, while the <code>AnalogInteraction</code> operation initializes an <code>InteractionBlock</code>. As we have shown, by default, these blocks use a global qubit support, which can be passed explicitly by setting <code>qubit_support = QubitSupportType.GLOBAL</code>. However, composing blocks using <code>kron</code> with local qubit supports and different durations is not allowed.</p> <pre><code>from qadence import AnalogRX, AnalogRY, Register, kron\n\ndx = 8.0\nreg = Register.from_coordinates([(0, 0), (dx, 0)])\n\n# Does not work (the angle affects the duration, as seen above):\nrot_0 = AnalogRX(angle = 1.0, qubit_support = (0,))\nrot_1 = AnalogRY(angle = 2.0, qubit_support = (1,))\n\ntry:\n    block = kron(rot_0, rot_1)\nexcept ValueError as error:\n    print(\"Error:\", error)\n\n# Works:\nrot_0 = AnalogRX(angle = 1.0, qubit_support = (0,))\nrot_1 = AnalogRY(angle = 1.0, qubit_support = (1,))\n\nblock = kron(rot_0, rot_1)\n</code></pre> <pre><code>Error: Kron'ed blocks have to have same duration.\n</code></pre> <p>Using <code>chain</code> is only supported between analog blocks with global qubit support:</p> <pre><code>from qadence import chain\n\nrot_0 = AnalogRX(angle = 1.0, qubit_support = \"global\")\nrot_1 = AnalogRY(angle = 2.0, qubit_support = \"global\")\n\nblock = chain(rot_0, rot_1)\n</code></pre> <p>The restrictions above only apply to the analog blocks, and analog and digital blocks can currently be composed.</p> <pre><code>from qadence import RX\n\nrot_0 = AnalogRX(angle = 1.0, qubit_support = \"global\")\nrot_1 = AnalogRY(angle = 2.0, qubit_support = (0,))\nrot_digital = RX(1, 1.0)\n\nblock_0 = chain(rot_0, rot_digital)\nblock_1 = kron(rot_1, rot_digital)\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-blocks-qcl/","title":"Fitting a function with analog blocks","text":"<p>Analog blocks can be parametrized in the usual Qadence manner. Like any other parameters, they can be optimized. The next snippet exemplifies the creation of an analog and parameterized ansatz to fit a simple function. First, define a register and feature map block. We again use a default spacing of \\(8~\\mu\\text{m}\\) as done in the basic tutorial.</p> <pre><code>from qadence import Register, FeatureParameter, chain\nfrom qadence import AnalogRX, AnalogRY, AnalogRZ, AnalogInteraction\nfrom sympy import acos\n\n# Line register\nn_qubits = 2\nregister = Register.line(n_qubits, spacing = 8.0)\n\n# The input feature x for the circuit to learn f(x)\nx = FeatureParameter(\"x\")\n\n# Feature map with a few global analog rotations\nfm = chain(\n    AnalogRX(x),\n    AnalogRY(2*x),\n    AnalogRZ(3*x),\n)\n</code></pre> <p>Next, we define the ansatz with parameterized rotations.</p> <pre><code>from qadence import hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel, BackendName, DiffMode\nfrom qadence import VariationalParameter\n\nt_0 = 1000. * VariationalParameter(\"t_0\")\nt_1 = 1000. * VariationalParameter(\"t_1\")\nt_2 = 1000. * VariationalParameter(\"t_2\")\n\n# Creating the ansatz with parameterized rotations and wait time\nansatz = chain(\n    AnalogRX(\"tht_0\"),\n    AnalogRY(\"tht_1\"),\n    AnalogRZ(\"tht_2\"),\n    AnalogInteraction(t_0),\n    AnalogRX(\"tht_3\"),\n    AnalogRY(\"tht_4\"),\n    AnalogRZ(\"tht_5\"),\n    AnalogInteraction(t_1),\n    AnalogRX(\"tht_6\"),\n    AnalogRY(\"tht_7\"),\n    AnalogRZ(\"tht_8\"),\n    AnalogInteraction(t_2),\n)\n</code></pre> <p>We define the measured observable as the total magnetization, and build the <code>QuantumModel</code>.</p> <pre><code># Total magnetization observable\nobservable = hamiltonian_factory(n_qubits, detuning = Z)\n\n# Defining the circuit and observable\ncircuit = QuantumCircuit(register, fm, ansatz)\n\nmodel = QuantumModel(\n    circuit,\n    observable = observable,\n    backend = BackendName.PYQTORCH,\n    diff_mode = DiffMode.AD\n)\n</code></pre> <p>Now we can define the function to fit as well as our training and test data.</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Function to fit:\ndef f(x):\n    return x**2\n\nx_test = torch.linspace(-1.0, 1.0, steps=100)\ny_test = f(x_test)\n\nx_train = torch.linspace(-1.0, 1.0, steps=10)\ny_train = f(x_train)\n\n# Initial prediction from the model, to be visualized later\ny_pred_initial = model.expectation({\"x\": x_test}).detach()\n</code></pre> <p>Finally we define a simple loss function and training loop.</p> <pre><code>mse_loss = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(x_train, y_train):\n    out = model.expectation({\"x\": x_train})\n    loss = mse_loss(out.squeeze(), y_train)\n    return loss\n\nn_epochs = 200\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_fn(x_train, y_train)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>And with the model trained we can plot the final results.</p> <pre><code>y_pred_final = model.expectation({\"x\": x_test}).detach()\n\nplt.plot(x_test, y_pred_initial, label = \"Initial prediction\")\nplt.plot(x_test, y_pred_final, label = \"Final prediction\")\nplt.scatter(x_train, y_train, label = \"Training points\")\n</code></pre> 2024-12-05T14:31:31.285614 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/analog-qubo/","title":"Solve a QUBO problem","text":"<p>In this notebook, we solve a quadratic unconstrained binary optimization (QUBO) problem with Qadence. QUBOs are very popular combinatorial optimization problems with a wide range of applications. Here, we solve the problem using the QAOA <sup>1</sup> variational algorithm by embedding the QUBO problem weights onto a register as standard for neutral atom quantum devices.</p> <p>Additional background information on QUBOs can be found here, directly solved using the pulse-level interface Pulser.</p>"},{"location":"tutorials/digital_analog_qc/analog-qubo/#define-and-solve-qubo","title":"Define and solve QUBO","text":"Pre-requisite: optimal register coordinates for embedding the QUBO problem <p>A basic ingredient for solving a QUBO problem with a neutral atom device is to embed the problem onto the atomic register. In short, embedding algorithms cast the problem onto a graph mapped onto the register by optimally finding atomic coordinates. A discussion on the embedding algorithms is beyond the scope of this tutorial and a simplified version taken from here is added below.</p> <pre><code>import numpy as np\nimport numpy.typing as npt\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist, squareform\nfrom qadence import RydbergDevice\n\ndef qubo_register_coords(Q: np.ndarray, device: RydbergDevice) -&gt; list:\n    \"\"\"Compute coordinates for register.\"\"\"\n\n    def evaluate_mapping(new_coords, *args):\n        \"\"\"Cost function to minimize. Ideally, the pairwise\n        distances are conserved\"\"\"\n        Q, shape = args\n        new_coords = np.reshape(new_coords, shape)\n        interaction_coeff = device.rydberg_level\n        new_Q = squareform(interaction_coeff / pdist(new_coords) ** 6)\n        return np.linalg.norm(new_Q - Q)\n\n    shape = (len(Q), 2)\n    np.random.seed(0)\n    x0 = np.random.random(shape).flatten()\n    res = minimize(\n        evaluate_mapping,\n        x0,\n        args=(Q, shape),\n        method=\"Nelder-Mead\",\n        tol=1e-6,\n        options={\"maxiter\": 200000, \"maxfev\": None},\n    )\n    return [(x, y) for (x, y) in np.reshape(res.x, (len(Q), 2))]\n</code></pre> <p>With the embedding routine under our belt, let's start by adding the required imports and ensure the reproducibility of this tutorial.</p> <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit, Register\nfrom qadence import RydbergDevice, AnalogRX, AnalogRZ, chain\nfrom qadence.ml_tools import Trainer, TrainConfig, num_parameters\nimport nevergrad as ng\nimport matplotlib.pyplot as plt\n\nseed = 0\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n</code></pre> <p>The QUBO problem is initially defined by a graph of weighted edges and a cost function to be optimized. The weighted edges are represented by a real-valued symmetric matrix <code>Q</code> which is used throughout the tutorial.</p> <pre><code># QUBO problem weights (real-value symmetric matrix)\nQ = np.array(\n    [\n        [-10.0, 19.7365809, 19.7365809, 5.42015853, 5.42015853],\n        [19.7365809, -10.0, 20.67626392, 0.17675796, 0.85604541],\n        [19.7365809, 20.67626392, -10.0, 0.85604541, 0.17675796],\n        [5.42015853, 0.17675796, 0.85604541, -10.0, 0.32306662],\n        [5.42015853, 0.85604541, 0.17675796, 0.32306662, -10.0],\n    ]\n)\n\ndef loss(model: QuantumModel, *args) -&gt; tuple[torch.Tensor, dict]:\n    to_arr_fn = lambda bitstring: np.array(list(bitstring), dtype=int)\n    cost_fn = lambda arr: arr.T @ Q @ arr\n    samples = model.sample({}, n_shots=1000)[0]  # extract samples\n    cost_fn = sum(samples[key] * cost_fn(to_arr_fn(key)) for key in samples)\n    return torch.tensor(cost_fn / sum(samples.values())), {}  # We return an optional metrics dict\n</code></pre> <p>The QAOA algorithm needs a variational quantum circuit with optimizable parameters. For that purpose, we use a fully analog circuit composed of two global rotations per layer on different axes of the Bloch sphere. The first rotation corresponds to the mixing Hamiltonian and the second one to the embedding Hamiltonian <sup>1</sup>. In this setting, the embedding is realized by the appropriate register coordinates and the resulting qubit interaction.</p> Rydberg level <p>The Rydberg level is set to 70. We initialize the weighted register graph from the QUBO definition similarly to what is done in the original tutorial, and set the device specifications with the updated Rydberg level.</p> <pre><code># Device specification and atomic register\ndevice = RydbergDevice(rydberg_level=70)\n\nreg = Register.from_coordinates(\n    qubo_register_coords(Q, device), device_specs=device\n)\n\n# Analog variational quantum circuit\nlayers = 2\nblock = chain(*[AnalogRX(f\"t{i}\") * AnalogRZ(f\"s{i}\") for i in range(layers)])\ncircuit = QuantumCircuit(reg, block)\n</code></pre> <pre><code>\n</code></pre> <p>By feeding the circuit to a <code>QuantumModel</code> we can check the initial counts where no clear solution can be found:</p> <pre><code>model = QuantumModel(circuit)\ninitial_counts = model.sample({}, n_shots=1000)[0]\n</code></pre> <pre><code>initial_counts = OrderedCounter({'01000': 197, '00001': 193, '00010': 189, '00100': 174, '10000': 169, '00000': 78})\n</code></pre> <p>Finally, we can proceed with the variational optimization. The cost function defined above is derived from bitstring computations and therefore non differentiable. We use Qadence ML facilities to run gradient-free optimizations using the <code>nevergrad</code> library.</p> <pre><code>Trainer.set_use_grad(False)\n\nconfig = TrainConfig(max_iter=100)\noptimizer = ng.optimizers.NGOpt(\n    budget=config.max_iter, parametrization=num_parameters(model)\n)\ntrainer = Trainer(model, optimizer, config, loss)\ntrainer.fit()\n\noptimal_counts = model.sample({}, n_shots=1000)[0]\n</code></pre>   optimal_count = OrderedCounter({'00000': 286, '10000': 153, '00010': 147, '00100': 142, '00001': 140, '01000': 132})    <p>Finally, let's plot the solution. The expected bitstrings are marked in red.</p> <pre><code># Known solutions to the QUBO problem.\nsolution_bitstrings = [\"01011\", \"00111\"]\n\ndef plot_distribution(C, ax, title):\n    C = dict(sorted(C.items(), key=lambda item: item[1], reverse=True))\n    indexes = solution_bitstrings # QUBO solutions\n    color_dict = {key: \"r\" if key in indexes else \"g\" for key in C}\n    ax.set_xlabel(\"bitstrings\")\n    ax.set_ylabel(\"counts\")\n    ax.set_xticks([i for i in range(len(C.keys()))], C.keys(), rotation=90)\n    ax.bar(list(C.keys())[:20], list(C.values())[:20])\n    ax.set_title(title)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nplot_distribution(initial_counts, axs[0], \"Initial counts\")\nplot_distribution(optimal_counts, axs[1], \"Optimal counts\")\n</code></pre> 2024-12-05T14:31:37.273438 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/analog-qubo/#references","title":"References","text":"<ol> <li> <p>Edward Farhi, Jeffrey Goldstone, Sam Gutmann, A Quantum Approximate Optimization Algorithm, arXiv:1411.4028 (2014) \u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/","title":"<code>CNOT</code> with interacting qubits","text":"<p>Digital-analog quantum computing focuses on using single qubit digital gates combined with more complex and device-dependent analog interactions to represent quantum programs. This paradigm has been shown to be universal for quantum computation<sup>1</sup>. However, while this approach may have advantages when adapting quantum programs to real devices, known quantum algorithms are very often expressed in a fully digital paradigm. As such, it is also important to have concrete ways to transform from one paradigm to another.</p> <p>This tutorial will exemplify the DAQC transformation starting with the representation of a simple digital <code>CNOT</code> using the universality of the Ising Hamiltonian<sup>2</sup>.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#cnot-with-cphase","title":"<code>CNOT</code> with <code>CPHASE</code>","text":"<p>Let's look at a single example of how the digital-analog transformation can be used to perform a <code>CNOT</code> on two qubits inside a register of globally interacting qubits.</p> <p>First, note that the <code>CNOT</code> can be decomposed with two Hadamard and a <code>CPHASE</code> gate with \\(\\phi=\\pi\\):</p> <pre><code>import torch\nfrom qadence import chain, sample, product_state\n\nfrom qadence.draw import display\nfrom qadence import X, I, Z, H, N, CPHASE, CNOT, HamEvo, PI\n\nn_qubits = 2\n\n# CNOT gate\ncnot_gate = CNOT(0, 1)\n\n# CNOT decomposed\nphi = PI\ncnot_decomp = chain(H(1), CPHASE(0, 1, phi), H(1))\n\ninit_state = product_state(\"10\")\n</code></pre> <pre><code>sample from CNOT gate and 100 shots = [OrderedCounter({'11': 100})]\nsample from decomposed CNOT gate and 100 shots = [OrderedCounter({'11': 100})]\n</code></pre> <p>The <code>CPHASE</code> matrix is diagonal, and can be implemented by exponentiating an Ising-like Hamiltonian, or generator,</p> \\[\\text{CPHASE}(i,j,\\phi)=\\text{exp}\\left(-i\\phi \\mathcal{H}_\\text{CP}(i, j)\\right)\\] \\[\\begin{aligned} \\mathcal{H}_\\text{CP}&amp;=-\\frac{1}{4}(I_i-Z_i)(I_j-Z_j)\\\\ &amp;=-N_iN_j \\end{aligned}\\] <p>where the number operator \\(N_i = \\frac{1}{2}(I_i-Z_i)=\\hat{n}_i\\) is used, leading to an Ising-like interaction \\(\\hat{n}_i\\hat{n}_j\\) realisable in neutral-atom systems. Let's rebuild the <code>CNOT</code> using this evolution.</p> <pre><code>from qadence import kron, block_to_tensor\n\n# Hamiltonian for the CPHASE gate\nh_cphase = (-1.0) * kron(N(0), N(1))\n\n# Exponentiating and time-evolving the Hamiltonian until t=phi.\ncphase_evo = HamEvo(h_cphase, phi)\n\n# Check that we have the CPHASE gate:\ncphase_matrix = block_to_tensor(CPHASE(0, 1, phi))\ncphase_evo_matrix = block_to_tensor(cphase_evo)\n</code></pre> <pre><code>cphase_matrix == cphase_evo_matrix: True\n</code></pre> <p>Now that the <code>CPHASE</code> generator is checked, it can be applied to the <code>CNOT</code>:</p> <pre><code># CNOT with Hamiltonian Evolution\ncnot_evo = chain(\n    H(1),\n    cphase_evo,\n    H(1)\n)\n\n# Initialize state to check CNOTs sample outcomes.\ninit_state = product_state(\"10\")\n</code></pre> <pre><code>sample cnot_gate = [OrderedCounter({'11': 100})]\nsample cnot_evo = [OrderedCounter({'11': 100})]\n</code></pre> <p>Thus, a <code>CNOT</code> gate can be created by combining a few single-qubit gates together with a two-qubit Ising interaction between the control and the target qubit which is the essence of the Ising transform proposed in the seminal DAQC paper<sup>2</sup> for \\(ZZ\\) interactions. In Qadence, both \\(ZZ\\) and \\(NN\\) interactions are supported.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#cnot-in-an-interacting-system-of-three-qubits","title":"<code>CNOT</code> in an interacting system of three qubits","text":"<p>Consider a simple experimental setup with \\(n=3\\) interacting qubits laid out in a triangular grid. For the sake of simplicity, all qubits interact with each other with an \\(NN\\)-Ising interaction of constant strength \\(g_\\text{int}\\). The Hamiltonian for the system can be written by summing interaction terms over all pairs:</p> \\[\\mathcal{H}_\\text{sys}=\\sum_{i=0}^{n}\\sum_{j=0}^{i-1}g_\\text{int}N_iN_j,\\] <p>which in this case leads to only three interaction terms,</p> \\[\\mathcal{H}_\\text{sys}=g_\\text{int}(N_0N_1+N_1N_2+N_0N_2)\\] <p>This generator can be easily built in Qadence:</p> <pre><code>from qadence import add, kron\nn_qubits = 3\n\n# Interaction strength.\ng_int = 1.0\n\n# Build a list of interactions.\ninteraction_list = []\nfor i in range(n_qubits):\n    for j in range(i):\n        interaction_list.append(g_int * kron(N(i), N(j)))\n\nh_sys = add(*interaction_list)\n</code></pre> <pre><code>h_sys = AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(0)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(2)\n\u2502       \u2514\u2500\u2500 N(0)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(1)\n</code></pre> <p>Now let's consider that the experimental system is fixed, and qubits can not be isolated one from another. The options are:</p> <ul> <li>Turn on or off the global system Hamiltonian.</li> <li>Perform local single-qubit rotations.</li> </ul> <p>To perform a fully digital <code>CNOT(0,1)</code>, the interacting control on qubit 0 and target on qubit 1 must be isolated from the third one to implement the gate directly. While this can be achieved for a three-qubit system, it becomes experimentally untractable when scaling the qubit count.</p> <p>However, this is not the case within the digital-analog paradigm. In fact, the two qubit Ising interaction required for the <code>CNOT</code> can be represented with a combination of the global system Hamiltonian and a specific set of single-qubit rotations. Full details about this transformation are to be found in the DAQC paper<sup>2</sup> but a more succint yet in-depth description takes place in the next section. It is conveniently available in Qadence by calling the <code>daqc_transform</code> function.</p> <p>In the most general sense, the <code>daqc_transform</code> function will return a circuit that represents the evolution of a target Hamiltonian \\(\\mathcal{H}_\\text{target}\\) (here the unitary of the gate) until a specified time \\(t_f\\) by using only the evolution of a build Hamiltonian \\(\\mathcal{H}_\\text{build}\\) (here \\(\\mathcal{H}_\\text{sys}\\)) together with local \\(X\\)-gates. In Qadence, <code>daqc_transform</code> is applicable for \\(\\mathcal{H}_\\text{target}\\) and \\(\\mathcal{H}_\\text{build}\\) composed only of \\(ZZ\\)- or \\(NN\\)-interactions. These generators are parsed by the <code>daqc_transform</code> function and the appropriate type is automatically determined together with the appropriate single-qubit detunings and global phases.</p> <p>Let's apply it for the <code>CNOT</code> implementation:</p> <pre><code>from qadence import daqc_transform, Strategy\n\n# Settings for the target CNOT operation\ni = 0  # Control qubit\nj = 1  # Target qubit\nk = 2  # The extra qubit\n\n# Define the target CNOT operation\n# by composing with identity on the extra qubit.\ncnot_target = kron(CNOT(i, j), I(k))\n\n# The two-qubit NN-Ising interaction term for the CPHASE\nh_int = (-1.0) * kron(N(i), N(j))\n\n# Transforming the two-qubit Ising interaction using only our system Hamiltonian\ntransformed_ising = daqc_transform(\n    n_qubits=3,        # Total number of qubits in the transformation\n    gen_target=h_int,  # The target Ising generator\n    t_f=PI,            # The target evolution time\n    gen_build=h_sys,   # The building block Ising generator to be used\n    strategy=Strategy.SDAQC,   # Currently only sDAQC is implemented\n    ignore_global_phases=False  # Global phases from mapping between Z and N\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_a3af8085f9764a9a9d571c21974369c1 cluster_7267aa45bc604209a5a45d8517bf2f7d cluster_d61aa52174a345c4bfe049be99bdbf7a cluster_3a31ded1fa4c4a7eb403dc899cc5f721 cluster_d7cbcf38770545fda454776a7ce33ada cluster_dca2649d0a1049c28028c8461131fdfd cluster_dffcfcd6ef524d45bcc170bc41b6117a bab84c63d69647f294d1fb2f55c43ca0 0 2261490fddc746d9a95c7b9362411171 HamEvo bab84c63d69647f294d1fb2f55c43ca0--2261490fddc746d9a95c7b9362411171 f90ce7da8e13416a97c44c099bcb89ad 1 b0aadea744a647169bcdc662744ffcb2 HamEvo 2261490fddc746d9a95c7b9362411171--b0aadea744a647169bcdc662744ffcb2 cd026f7075d940c48ac6bc4ff0b77a03 HamEvo b0aadea744a647169bcdc662744ffcb2--cd026f7075d940c48ac6bc4ff0b77a03 5cc417619d4a4c4c9dd83da744c8de27 X cd026f7075d940c48ac6bc4ff0b77a03--5cc417619d4a4c4c9dd83da744c8de27 1064ea9aa72c40e7ad859df0bcd428d1 HamEvo 5cc417619d4a4c4c9dd83da744c8de27--1064ea9aa72c40e7ad859df0bcd428d1 cfa8d5967b86412cb2d86bddc78f2682 HamEvo 1064ea9aa72c40e7ad859df0bcd428d1--cfa8d5967b86412cb2d86bddc78f2682 9442417a2cab4a3ab9f88f29b1717e01 X cfa8d5967b86412cb2d86bddc78f2682--9442417a2cab4a3ab9f88f29b1717e01 abe52d9d96b04dea8307601041c9351c 9442417a2cab4a3ab9f88f29b1717e01--abe52d9d96b04dea8307601041c9351c 57beab9496c54a0e817697cbb523d8a5 HamEvo abe52d9d96b04dea8307601041c9351c--57beab9496c54a0e817697cbb523d8a5 7e0ddc91ec7f4235a3a29d7c4976a111 HamEvo 57beab9496c54a0e817697cbb523d8a5--7e0ddc91ec7f4235a3a29d7c4976a111 8b7b510c0c5d42129b268df17debddb9 7e0ddc91ec7f4235a3a29d7c4976a111--8b7b510c0c5d42129b268df17debddb9 316df89f37df4715943dbb7ba0d3a6e5 8b7b510c0c5d42129b268df17debddb9--316df89f37df4715943dbb7ba0d3a6e5 4a617905b652433a85e965f7378db0d2 8b6c4c59b4d1412080dc7a6287dd6a81 t = -3.14 f90ce7da8e13416a97c44c099bcb89ad--8b6c4c59b4d1412080dc7a6287dd6a81 657a57a7a6fa43788df5f2a4d1b08f97 2 44fda9e1272443dea3910b14b2871174 t = 3.142 8b6c4c59b4d1412080dc7a6287dd6a81--44fda9e1272443dea3910b14b2871174 364d547db49442c3bd82877a9248f085 t = -3.14 44fda9e1272443dea3910b14b2871174--364d547db49442c3bd82877a9248f085 c6957539d8254aadab2f621fe1649417 364d547db49442c3bd82877a9248f085--c6957539d8254aadab2f621fe1649417 d9643115e51b4e7995d42e543ec0acf3 t = 1.571 c6957539d8254aadab2f621fe1649417--d9643115e51b4e7995d42e543ec0acf3 cd4347f22bbf4a15a84882b04fbdda23 t = 1.571 d9643115e51b4e7995d42e543ec0acf3--cd4347f22bbf4a15a84882b04fbdda23 2136b876a3c7404abfb3500c25b76c2c cd4347f22bbf4a15a84882b04fbdda23--2136b876a3c7404abfb3500c25b76c2c 36d2ef2c61fd4ae98112b9d254eab0ec X 2136b876a3c7404abfb3500c25b76c2c--36d2ef2c61fd4ae98112b9d254eab0ec dc21dbbb897d437094deb2652db623fe t = 1.571 36d2ef2c61fd4ae98112b9d254eab0ec--dc21dbbb897d437094deb2652db623fe 692ad6e08cba4cc0bb37c0682f98391f t = 1.571 dc21dbbb897d437094deb2652db623fe--692ad6e08cba4cc0bb37c0682f98391f e4a8da85498043e48ef8401fd253a94e X 692ad6e08cba4cc0bb37c0682f98391f--e4a8da85498043e48ef8401fd253a94e e4a8da85498043e48ef8401fd253a94e--4a617905b652433a85e965f7378db0d2 cb7b7b1d2dce4acfa56e964d97614c9a b1579b5c8e05421b80e65a9dece71a90 657a57a7a6fa43788df5f2a4d1b08f97--b1579b5c8e05421b80e65a9dece71a90 399e1e13c7674db0b8e727100b549fa6 b1579b5c8e05421b80e65a9dece71a90--399e1e13c7674db0b8e727100b549fa6 34c305f45970442382880fdf8e9e2102 399e1e13c7674db0b8e727100b549fa6--34c305f45970442382880fdf8e9e2102 624acb5bb2a94780961e032e56a46b07 X 34c305f45970442382880fdf8e9e2102--624acb5bb2a94780961e032e56a46b07 2df6539611fd479181ea4981426cc8eb 624acb5bb2a94780961e032e56a46b07--2df6539611fd479181ea4981426cc8eb bb03dd2a7ca24d0bb67a97899bacfabe 2df6539611fd479181ea4981426cc8eb--bb03dd2a7ca24d0bb67a97899bacfabe 508122fa7aad41cba0482508c99e072f X bb03dd2a7ca24d0bb67a97899bacfabe--508122fa7aad41cba0482508c99e072f 6b81f7b9511c4c5ba79af38ec3fdc51c X 508122fa7aad41cba0482508c99e072f--6b81f7b9511c4c5ba79af38ec3fdc51c 6b8c803571c34780b7f3d3bddf8f326c 6b81f7b9511c4c5ba79af38ec3fdc51c--6b8c803571c34780b7f3d3bddf8f326c d42c15a7cbfd4137be634a26614e6975 6b8c803571c34780b7f3d3bddf8f326c--d42c15a7cbfd4137be634a26614e6975 1dec9b7801654eafb9d47d035cea569d X d42c15a7cbfd4137be634a26614e6975--1dec9b7801654eafb9d47d035cea569d 1dec9b7801654eafb9d47d035cea569d--cb7b7b1d2dce4acfa56e964d97614c9a <p>The output circuit displays three groups of system Hamiltonian evolutions which account for global-phases and single-qubit detunings related to the mapping between the \\(Z\\) and \\(N\\) operators. Optionally, global phases can be ignored.</p> <p>In general, the mapping of a \\(n\\)-qubit Ising Hamiltonian to another will require at most \\(n(n-1)\\) evolutions. The transformed circuit performs these evolutions for specific times that are computed from the solution of a linear system of equations involving the set of interactions in the target and build Hamiltonians.</p> <p>In this case, the mapping is exact when using the step-wise DAQC strategy (<code>Strategy.SDAQC</code>) available in Qadence. In banged DAQC (<code>Strategy.BDAQC</code>) the mapping is approximate, but easier to implement on a physical device with always-on interactions such as neutral-atom systems.</p> <p>Just as before, the transformed Ising circuit can be checked to exactly recover the <code>CPHASE</code> gate:</p> <pre><code># CPHASE on (i, j), Identity on third qubit:\ncphase_matrix = block_to_tensor(kron(CPHASE(i, j, phi), I(k)))\n\n# CPHASE using the transformed circuit:\ncphase_evo_matrix = block_to_tensor(transformed_ising)\n\n# Check that it implements the CPHASE.\n# Will fail if global phases are ignored.\n</code></pre> <pre><code>cphase_matrix == cphase_evo_matrix : True\n</code></pre> <p>The <code>CNOT</code> gate can now finally be built:</p> <pre><code>from qadence import equivalent_state, run, sample\n\ncnot_daqc = chain(\n    H(j),\n    transformed_ising,\n    H(j)\n)\n\n# And finally apply the CNOT on a specific 3-qubit initial state:\ninit_state = product_state(\"101\")\n\n# Check we get an equivalent wavefunction\nwf_cnot = run(n_qubits, block=cnot_target, state=init_state)\nwf_daqc = run(n_qubits, block=cnot_daqc, state=init_state)\n\n# Visualize the CNOT bit-flip in samples.\n</code></pre> <pre><code>wf_cnot == wf_dacq : True\nsample cnot_target = [OrderedCounter({'111': 100})]\nsample cnot_dacq = [OrderedCounter({'111': 100})]\n</code></pre> <p>As one can see, a <code>CNOT</code> operation has been succesfully implemented on the desired target qubits by using only the global system as the building block Hamiltonian and single-qubit rotations. Decomposing a single digital gate into an Ising Hamiltonian serves as a proof of principle for the potential of this technique to represent universal quantum computation.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#technical-details-on-the-daqc-transformation","title":"Technical details on the DAQC transformation","text":"<ul> <li>The mapping between target generator and final circuit is performed by solving a linear system of size \\(n(n-1)\\) where \\(n\\) is the number of qubits, so it can be computed efficiently (i.e., with a polynomial cost in the number of qubits).</li> <li>The linear system to be solved is actually not invertible for \\(n=4\\) qubits. This is very specific edge case requiring a workaround, that is currently not yet implemented.</li> <li>As mentioned, the final circuit has at most \\(n(n-1)\\) slices, so there is at most a quadratic overhead in circuit depth.</li> </ul> <p>Finally, and most important to its usage:</p> <ul> <li>The target Hamiltonian should be sufficiently represented in the building block Hamiltonian.</li> </ul> <p>To illustrate this point, consider the following target and build Hamiltonians:</p> <pre><code># Interaction between qubits 0 and 1\ngen_target = 1.0 * (Z(0) @ Z(1))\n\n# Fixed interaction between qubits 1 and 2, and customizable between 0 and 1\ndef gen_build(g_int):\n    return g_int * (Z(0) @ Z(1)) + 1.0 * (Z(1) @ Z(2))\n</code></pre> <p>And now we perform the DAQC transform by setting <code>g_int=1.0</code>, exactly matching the target Hamiltonian:</p> <pre><code>transformed_ising = daqc_transform(\n    n_qubits=3,\n    gen_target=gen_target,\n    t_f=1.0,\n    gen_build=gen_build(g_int=1.0),\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_7b178e162bd04973b1cdd85fd65be1dc cluster_980b62b824c3459a943645943c520068 3b4e2fbb9d0547238d5ef81725a568cf 0 62794c9d3de74c948f3832771d1437a9 X 3b4e2fbb9d0547238d5ef81725a568cf--62794c9d3de74c948f3832771d1437a9 6a0ec9377ce74dba871b192093862307 1 e14e3a6a7a81473191d575cb238ec1c0 HamEvo 62794c9d3de74c948f3832771d1437a9--e14e3a6a7a81473191d575cb238ec1c0 d9ff99fb622d4c14ae2fb0cfa53d88ea X e14e3a6a7a81473191d575cb238ec1c0--d9ff99fb622d4c14ae2fb0cfa53d88ea 3a1762490e7247ea8bb730fa1f4aae62 d9ff99fb622d4c14ae2fb0cfa53d88ea--3a1762490e7247ea8bb730fa1f4aae62 1eb7c95940bc4d48b0782e2b5e37cc92 HamEvo 3a1762490e7247ea8bb730fa1f4aae62--1eb7c95940bc4d48b0782e2b5e37cc92 27f747b6e65c4c2fa46a763fd852d682 1eb7c95940bc4d48b0782e2b5e37cc92--27f747b6e65c4c2fa46a763fd852d682 5ae7e1bdd4784facae005a313c8d68ea 27f747b6e65c4c2fa46a763fd852d682--5ae7e1bdd4784facae005a313c8d68ea f481bebe23a44aff97ed844d92aef72f 557f0da88f434a2182f5f8d98f9f380f 6a0ec9377ce74dba871b192093862307--557f0da88f434a2182f5f8d98f9f380f 48cea49ff2bd401e93367dc554059fa5 2 75dda38c31d8472285d74603fc26e23c t = -0.50 557f0da88f434a2182f5f8d98f9f380f--75dda38c31d8472285d74603fc26e23c b4a043dcfde44589a9a89bdd3b1d7b86 75dda38c31d8472285d74603fc26e23c--b4a043dcfde44589a9a89bdd3b1d7b86 d16aa39d89a047f08199faa6579da807 X b4a043dcfde44589a9a89bdd3b1d7b86--d16aa39d89a047f08199faa6579da807 c6b73a8f101840fc96bb4c4dc953b29c t = -0.50 d16aa39d89a047f08199faa6579da807--c6b73a8f101840fc96bb4c4dc953b29c 761f535cc9e64765b47769fe0ea43551 X c6b73a8f101840fc96bb4c4dc953b29c--761f535cc9e64765b47769fe0ea43551 761f535cc9e64765b47769fe0ea43551--f481bebe23a44aff97ed844d92aef72f 1dd0e397e3a6408cb5521f3e475317b0 2864ff598f63482b9da32744659753f9 X 48cea49ff2bd401e93367dc554059fa5--2864ff598f63482b9da32744659753f9 d0a2aba949d846f88d32353fa3e4418d 2864ff598f63482b9da32744659753f9--d0a2aba949d846f88d32353fa3e4418d 9caf45f95d0f40a4afece8580aa7695b X d0a2aba949d846f88d32353fa3e4418d--9caf45f95d0f40a4afece8580aa7695b 63724a4a929e42de940af218bd1b3542 X 9caf45f95d0f40a4afece8580aa7695b--63724a4a929e42de940af218bd1b3542 6abb368c234845cc8f6a88d0a6121cfd 63724a4a929e42de940af218bd1b3542--6abb368c234845cc8f6a88d0a6121cfd 83c82fb976ec4d6f90244ab73b6ff41f X 6abb368c234845cc8f6a88d0a6121cfd--83c82fb976ec4d6f90244ab73b6ff41f 83c82fb976ec4d6f90244ab73b6ff41f--1dd0e397e3a6408cb5521f3e475317b0 <p>Now, if the interaction between qubits 0 and 1 is weakened in the build Hamiltonian:</p> <pre><code>transformed_ising = daqc_transform(\n    n_qubits=3,\n    gen_target=gen_target,\n    t_f=1.0,\n    gen_build=gen_build(g_int=0.001),\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_1201076366a04c52834007eafc49600c cluster_6a4b454f087b41f4a0843b8efe4d843f a430556fe7f84a18938e2397a77170bc 0 b4b9855cbd004b868d31c781801e0f2d X a430556fe7f84a18938e2397a77170bc--b4b9855cbd004b868d31c781801e0f2d 06a120c9cd76400e952ea597834169c1 1 23398e620dda41919152c8729aaeb158 HamEvo b4b9855cbd004b868d31c781801e0f2d--23398e620dda41919152c8729aaeb158 ac854dc6133649f4a8859ae9a5cb8d34 X 23398e620dda41919152c8729aaeb158--ac854dc6133649f4a8859ae9a5cb8d34 86bd02825ca84799bf59b755d0dc5dc0 ac854dc6133649f4a8859ae9a5cb8d34--86bd02825ca84799bf59b755d0dc5dc0 0c6d160e4ef348458ad079865e3d5edb HamEvo 86bd02825ca84799bf59b755d0dc5dc0--0c6d160e4ef348458ad079865e3d5edb 7b56aa9ae1f04cb0aae18dab82f18fb2 0c6d160e4ef348458ad079865e3d5edb--7b56aa9ae1f04cb0aae18dab82f18fb2 8995c95a453c4acbae9b1f3a053371a4 7b56aa9ae1f04cb0aae18dab82f18fb2--8995c95a453c4acbae9b1f3a053371a4 cbe3fe13829a4ab48bebc071586a208c dbe8395108c9402d8009e383d05d4ef9 06a120c9cd76400e952ea597834169c1--dbe8395108c9402d8009e383d05d4ef9 efd625ea153b48e9b84e99487b47cfe9 2 d9200d6788d14e7faf6fb157848aa0b5 t = -500. dbe8395108c9402d8009e383d05d4ef9--d9200d6788d14e7faf6fb157848aa0b5 f1ff5ca01128446db196148057045090 d9200d6788d14e7faf6fb157848aa0b5--f1ff5ca01128446db196148057045090 afde874167424f98a335733f8180268f X f1ff5ca01128446db196148057045090--afde874167424f98a335733f8180268f e879f91182164fc48aa28570605b9468 t = -500. afde874167424f98a335733f8180268f--e879f91182164fc48aa28570605b9468 81076a377a0a499e9aa91e9b69fd5ef5 X e879f91182164fc48aa28570605b9468--81076a377a0a499e9aa91e9b69fd5ef5 81076a377a0a499e9aa91e9b69fd5ef5--cbe3fe13829a4ab48bebc071586a208c a66db4ce41604ed98feb3d84477c456a aa7816a64f4d4490bde9c34706d06862 X efd625ea153b48e9b84e99487b47cfe9--aa7816a64f4d4490bde9c34706d06862 dc3e88ca907c46649501d641dab69d6c aa7816a64f4d4490bde9c34706d06862--dc3e88ca907c46649501d641dab69d6c de9cb31193b141f88886e8a4437520f6 X dc3e88ca907c46649501d641dab69d6c--de9cb31193b141f88886e8a4437520f6 1976b3032405435ea8baf5cdcae93753 X de9cb31193b141f88886e8a4437520f6--1976b3032405435ea8baf5cdcae93753 dc8bcdb33618428a93b3b448a1c4c516 1976b3032405435ea8baf5cdcae93753--dc8bcdb33618428a93b3b448a1c4c516 0685c61f4d024f35bd3aa9a4d9409041 X dc8bcdb33618428a93b3b448a1c4c516--0685c61f4d024f35bd3aa9a4d9409041 0685c61f4d024f35bd3aa9a4d9409041--a66db4ce41604ed98feb3d84477c456a <p>The times slices using the build Hamiltonian need now to evolve for much longer to represent the same interaction since it is not sufficiently represented in the building block Hamiltonian.</p> <p>In the limit where that interaction is not present, the transform will not work:</p> <pre><code>try:\n    transformed_ising = daqc_transform(\n        n_qubits=3,\n        gen_target=gen_target,\n        t_f=1.0,\n        gen_build=gen_build(g_int = 0.0),\n    )\nexcept ValueError as error:\n    print(\"Error:\", error)\n</code></pre> <pre><code>Error: Incompatible interactions between target and build Hamiltonians.\n</code></pre>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#references","title":"References","text":"<ol> <li> <p>Dodd et al., Universal quantum computation and simulation using any entangling Hamiltonian and local unitaries, PRA 65, 040301 (2002). \u21a9</p> </li> <li> <p>Parra-Rodriguez et al., Digital-Analog Quantum Computation, PRA 101, 022305 (2020). \u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/","title":"Fitting a function with a Hamiltonian ansatz","text":"<p>In the analog QCL tutorial we used analog blocks to learn a function of interest. The analog blocks are a direct abstraction of device execution with global addressing. However, we may want to directly program an Hamiltonian-level ansatz to have a finer control on our model. In Qadence this can easily be done through digital-analog programs. In this tutorial we will solve a simple QCL problem with this approach.</p>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#setting-up-the-problem","title":"Setting up the problem","text":"<p>The example problem considered is to fit a function of interest in a specified range. Below we define and plot the function \\(f(x)=x^5\\).</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Function to fit:\ndef f(x):\n    return x**5\n\nxmin = -1.0\nxmax = 1.0\nn_test = 100\n\nx_test = torch.linspace(xmin, xmax, steps = n_test)\ny_test = f(x_test)\n\nplt.plot(x_test, y_test)\nplt.xlim((-1.1, 1.1))\nplt.ylim((-1.1, 1.1))\n</code></pre> 2024-12-05T14:31:37.643619 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#digital-analog-ansatz","title":"Digital-Analog Ansatz","text":"<p>We start by defining the register of qubits. The topology we use now will define the interactions in the entangling Hamiltonian. As an example, we can define a rectangular lattice with 6 qubits.</p> <pre><code>from qadence import Register\n\nreg = Register.rectangular_lattice(\n    qubits_row = 3,\n    qubits_col = 2,\n)\n</code></pre> <p>Inspired by the Ising interaction mode of Rydberg atoms, we can now define an interaction Hamiltonian as \\(\\mathcal{H}_{ij}=\\frac{1}{r_{ij}^6}N_iN_j\\), where \\(N_i=(1/2)(I_i-Z_i)\\) is the number operator and and \\(r_{ij}\\) is the distance between qubits \\(i\\) and \\(j\\). We can easily instatiate this interaction Hamiltonian from the register information:</p> <pre><code>from qadence import N, add\n\ndef h_ij(i: int, j: int):\n    return N(i)@N(j)\n\nh_int = add(h_ij(*edge)/r**6 for edge, r in reg.edge_distances.items())\n</code></pre> <p>To build the digital-analog ansatz we can make use of the standard <code>hea</code> function by specifying we want to use the <code>Strategy.SDAQC</code> and passing the Hamiltonian we created as the entangler, as see in the QML constructors tutorial. The entangling operation will be replaced by the evolution of this Hamiltonian <code>HamEvo(h_int, t)</code>, where the time parameter <code>t</code> is considered to be a variational parameter at each layer.</p> <pre><code>from qadence import hea, Strategy, RX, RY\n\ndepth = 2\n\nda_ansatz = hea(\n    n_qubits = reg.n_qubits,\n    depth = depth,\n    operations = [RX, RY, RX],\n    entangler = h_int,\n    strategy = Strategy.SDAQC,\n)\n\nprint(html_string(da_ansatz))\n</code></pre> %3 cluster_6bc5ace3405842af82265d2234336f4b cluster_5a3e52b8d0e74d47801ab576b47b6e14 1452e2a92dc04b88ab63a487a8815845 0 3ebf7c22973e4cabba8646c67f09e563 RX(theta\u2080) 1452e2a92dc04b88ab63a487a8815845--3ebf7c22973e4cabba8646c67f09e563 74c19fff0499466ca0e2814d5b9ba11f 1 03e707414e204c7192b411afe7f4f397 RY(theta\u2086) 3ebf7c22973e4cabba8646c67f09e563--03e707414e204c7192b411afe7f4f397 017c00cda80442f4a72357245e5f8445 RX(theta\u2081\u2082) 03e707414e204c7192b411afe7f4f397--017c00cda80442f4a72357245e5f8445 97af864e9eb949139b7ab00d7589ab54 017c00cda80442f4a72357245e5f8445--97af864e9eb949139b7ab00d7589ab54 9a51bf6f3e2c449193e539456312b548 RX(theta\u2081\u2088) 97af864e9eb949139b7ab00d7589ab54--9a51bf6f3e2c449193e539456312b548 2a6b7e70ed23422183aa81ecfe145c4b RY(theta\u2082\u2084) 9a51bf6f3e2c449193e539456312b548--2a6b7e70ed23422183aa81ecfe145c4b 18d927b747da4f95af0d06f7836a6e78 RX(theta\u2083\u2080) 2a6b7e70ed23422183aa81ecfe145c4b--18d927b747da4f95af0d06f7836a6e78 4907da74892e4addaa3c89772ed5526b 18d927b747da4f95af0d06f7836a6e78--4907da74892e4addaa3c89772ed5526b d844801830574f1fabe6b45db0331d69 4907da74892e4addaa3c89772ed5526b--d844801830574f1fabe6b45db0331d69 d5b03a5722cf42b5830e9b7680ed5bb0 713dd6040def484c8ce1a68793fd15d8 RX(theta\u2081) 74c19fff0499466ca0e2814d5b9ba11f--713dd6040def484c8ce1a68793fd15d8 17e92f6c9dd44a15b428ba3924fdbcf0 2 8c081c2018ed431e92cdf7efd700b320 RY(theta\u2087) 713dd6040def484c8ce1a68793fd15d8--8c081c2018ed431e92cdf7efd700b320 36c33d3fef65421389e79afbc1f40a80 RX(theta\u2081\u2083) 8c081c2018ed431e92cdf7efd700b320--36c33d3fef65421389e79afbc1f40a80 2b7981f940734c388b0e70e3be77e2cf 36c33d3fef65421389e79afbc1f40a80--2b7981f940734c388b0e70e3be77e2cf ddbe10d6299247048b10beac7ecc400e RX(theta\u2081\u2089) 2b7981f940734c388b0e70e3be77e2cf--ddbe10d6299247048b10beac7ecc400e 53c205b2a2764155b51b9dc27f7f942d RY(theta\u2082\u2085) ddbe10d6299247048b10beac7ecc400e--53c205b2a2764155b51b9dc27f7f942d eec30c17f04a45aeb22a7329aaa38fde RX(theta\u2083\u2081) 53c205b2a2764155b51b9dc27f7f942d--eec30c17f04a45aeb22a7329aaa38fde af46551587c547049f6ce593a60a6ebf eec30c17f04a45aeb22a7329aaa38fde--af46551587c547049f6ce593a60a6ebf af46551587c547049f6ce593a60a6ebf--d5b03a5722cf42b5830e9b7680ed5bb0 8330afde03424e179829da5e59a7266a 42df8f471b8546abad748af7d4597d11 RX(theta\u2082) 17e92f6c9dd44a15b428ba3924fdbcf0--42df8f471b8546abad748af7d4597d11 38da0bf6fdc44e5b91ae30812c059ac1 3 a5b665a49ff64b5cbf0132510f95dee5 RY(theta\u2088) 42df8f471b8546abad748af7d4597d11--a5b665a49ff64b5cbf0132510f95dee5 022d50b2a3774b3ab4d78ceda0908e0b RX(theta\u2081\u2084) a5b665a49ff64b5cbf0132510f95dee5--022d50b2a3774b3ab4d78ceda0908e0b 4faed377f7ee46c1af70885c7a58a111 HamEvo 022d50b2a3774b3ab4d78ceda0908e0b--4faed377f7ee46c1af70885c7a58a111 501f64e750084899bb50ac425e8c979b RX(theta\u2082\u2080) 4faed377f7ee46c1af70885c7a58a111--501f64e750084899bb50ac425e8c979b 21dc77392a994f9cae8bc98889a86aca RY(theta\u2082\u2086) 501f64e750084899bb50ac425e8c979b--21dc77392a994f9cae8bc98889a86aca 6039b1f09cbd4105b4049e99f9cb7ce5 RX(theta\u2083\u2082) 21dc77392a994f9cae8bc98889a86aca--6039b1f09cbd4105b4049e99f9cb7ce5 1c5d1b9ef900462f95dbb107c9a26fe7 HamEvo 6039b1f09cbd4105b4049e99f9cb7ce5--1c5d1b9ef900462f95dbb107c9a26fe7 1c5d1b9ef900462f95dbb107c9a26fe7--8330afde03424e179829da5e59a7266a 4f2eb050b2c6451484e4d98e731d48c3 1a5803c791ae499a870bdbefdb30b902 RX(theta\u2083) 38da0bf6fdc44e5b91ae30812c059ac1--1a5803c791ae499a870bdbefdb30b902 b69d27300cd94549a98aba76671d3898 4 3eff87d2c931483cbdb3b552b04c6726 RY(theta\u2089) 1a5803c791ae499a870bdbefdb30b902--3eff87d2c931483cbdb3b552b04c6726 f25f2afb907547da88185fd9294e011a RX(theta\u2081\u2085) 3eff87d2c931483cbdb3b552b04c6726--f25f2afb907547da88185fd9294e011a e2d0a67b11dc475682164880aa02b888 t = theta_t\u2080 f25f2afb907547da88185fd9294e011a--e2d0a67b11dc475682164880aa02b888 a2cf7e1b74a24f969b57d92b87222e35 RX(theta\u2082\u2081) e2d0a67b11dc475682164880aa02b888--a2cf7e1b74a24f969b57d92b87222e35 2b96d4de5d014ea194a83e483d6cd3c9 RY(theta\u2082\u2087) a2cf7e1b74a24f969b57d92b87222e35--2b96d4de5d014ea194a83e483d6cd3c9 786773854a754ab08439966c8b80880c RX(theta\u2083\u2083) 2b96d4de5d014ea194a83e483d6cd3c9--786773854a754ab08439966c8b80880c 5c7d072dad694277ba8936111b36c0b1 t = theta_t\u2081 786773854a754ab08439966c8b80880c--5c7d072dad694277ba8936111b36c0b1 5c7d072dad694277ba8936111b36c0b1--4f2eb050b2c6451484e4d98e731d48c3 d32df0b8855a4efeacb83e58ee21bed9 3cc395ba94934e7480b43ad9da9b1b3c RX(theta\u2084) b69d27300cd94549a98aba76671d3898--3cc395ba94934e7480b43ad9da9b1b3c 549c231f6d91444aa5a4c06aadc86d07 5 bc15a6f808b54280b27003341201606d RY(theta\u2081\u2080) 3cc395ba94934e7480b43ad9da9b1b3c--bc15a6f808b54280b27003341201606d 09319c0d085042bda9a7c556b045f847 RX(theta\u2081\u2086) bc15a6f808b54280b27003341201606d--09319c0d085042bda9a7c556b045f847 ae87c469810047caba1fb8fef3a5a4bc 09319c0d085042bda9a7c556b045f847--ae87c469810047caba1fb8fef3a5a4bc ae5d9328b80e460d875997ae5b42340a RX(theta\u2082\u2082) ae87c469810047caba1fb8fef3a5a4bc--ae5d9328b80e460d875997ae5b42340a 8d646af52dc74ff7bc7b14fc7c198e91 RY(theta\u2082\u2088) ae5d9328b80e460d875997ae5b42340a--8d646af52dc74ff7bc7b14fc7c198e91 5bc30411e40f44af9edbb1f6b5572f43 RX(theta\u2083\u2084) 8d646af52dc74ff7bc7b14fc7c198e91--5bc30411e40f44af9edbb1f6b5572f43 522ed71df86f458fac5fcbfadea3adcd 5bc30411e40f44af9edbb1f6b5572f43--522ed71df86f458fac5fcbfadea3adcd 522ed71df86f458fac5fcbfadea3adcd--d32df0b8855a4efeacb83e58ee21bed9 69d1af226cf5414cb0d01ef283d251b7 d40a3b93971948748bf91fc9fcd2e55f RX(theta\u2085) 549c231f6d91444aa5a4c06aadc86d07--d40a3b93971948748bf91fc9fcd2e55f d053f2ee08734ad5918c6370e48b6cfd RY(theta\u2081\u2081) d40a3b93971948748bf91fc9fcd2e55f--d053f2ee08734ad5918c6370e48b6cfd 98050d7856e54fcf8b02e6b319770247 RX(theta\u2081\u2087) d053f2ee08734ad5918c6370e48b6cfd--98050d7856e54fcf8b02e6b319770247 c8138d321bfd41e49b915e8f4802262f 98050d7856e54fcf8b02e6b319770247--c8138d321bfd41e49b915e8f4802262f 603b4fc60f4941b493b48c305b57a6d4 RX(theta\u2082\u2083) c8138d321bfd41e49b915e8f4802262f--603b4fc60f4941b493b48c305b57a6d4 d338d66f15934ad09857387d69e087dd RY(theta\u2082\u2089) 603b4fc60f4941b493b48c305b57a6d4--d338d66f15934ad09857387d69e087dd 1b06c17ce97c4752b437d5d878218815 RX(theta\u2083\u2085) d338d66f15934ad09857387d69e087dd--1b06c17ce97c4752b437d5d878218815 fcc7405d011a432fa2de7839d78b0ec1 1b06c17ce97c4752b437d5d878218815--fcc7405d011a432fa2de7839d78b0ec1 fcc7405d011a432fa2de7839d78b0ec1--69d1af226cf5414cb0d01ef283d251b7"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#creating-the-quantummodel","title":"Creating the QuantumModel","text":"<p>The rest of the procedure is the same as any other Qadence workflow. We start by defining a feature map for input encoding and an observable for output decoding.</p> <pre><code>from qadence import feature_map, BasisSet, ReuploadScaling\nfrom qadence import Z, I\n\nfm = feature_map(\n    n_qubits = reg.n_qubits,\n    param = \"x\",\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.TOWER,\n)\n\n# Total magnetization\nobservable = add(Z(i) for i in range(reg.n_qubits))\n</code></pre> <p>And we have all the ingredients to initialize the <code>QuantumModel</code>:</p> <pre><code>from qadence import QuantumCircuit, QuantumModel\n\ncircuit = QuantumCircuit(reg, fm, da_ansatz)\n\nmodel = QuantumModel(circuit, observable = observable)\n</code></pre>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#training-the-model","title":"Training the model","text":"<p>We can now train the model. We use a set of 20 equally spaced training points.</p> <pre><code># Chebyshev FM does not accept x = -1, 1\nxmin = -0.99\nxmax = 0.99\nn_train = 20\n\nx_train = torch.linspace(xmin, xmax, steps = n_train)\ny_train = f(x_train)\n\n# Initial model prediction\ny_pred_initial = model.expectation({\"x\": x_test}).detach()\n</code></pre> <p>And we use a simple custom training loop.</p> <pre><code>criterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nn_epochs = 200\n\ndef loss_fn(x_train, y_train):\n    out = model.expectation({\"x\": x_train})\n    loss = criterion(out.squeeze(), y_train)\n    return loss\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_fn(x_train, y_train)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#results","title":"Results","text":"<p>Finally we can plot the resulting trained model.</p> <pre><code>y_pred_final = model.expectation({\"x\": x_test}).detach()\n\nplt.plot(x_test, y_pred_initial, label = \"Initial prediction\")\nplt.plot(x_test, y_pred_final, label = \"Final prediction\")\nplt.scatter(x_train, y_train, label = \"Training points\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.xlim((-1.1, 1.1))\nplt.ylim((-1.1, 1.1))\n</code></pre> 2024-12-05T14:31:46.670564 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/","title":"Pulse-level programming with Pulser","text":"<p>Qadence offers a direct interface with Pulser<sup>1</sup>, an open-source pulse-level interface written in Python and specifically designed for programming neutral atom quantum computers.</p> <p>Using directly Pulser requires advanced knowledge on pulse-level programming and on how neutral atom devices work. Qadence abstracts this complexity out by using the familiar block-based interface for building pulse sequences in Pulser while leaving the possibility to directly manipulate them if required by, for instance, optimal pulse shaping.</p> <p>Note</p> <p>The Pulser backend is still experimental and the interface might change in the future. Please note that it does not support <code>DiffMode.AD</code>.</p> <p>Note</p> <p>With the Pulser backend, <code>qadence</code> simulations can be executed on the cloud emulators available on the PASQAL cloud platform. In order to do so, make to have valid credentials for the PASQAL cloud platform and use the following configuration for the Pulser backend:</p> <pre><code>config = {\n    \"cloud_configuration\": {\n        \"username\": \"&lt;changeme&gt;\",\n        \"password\": \"&lt;changeme&gt;\",\n        \"project_id\": \"&lt;changeme&gt;\",  # the project should have access to emulators\n        \"platform\": \"EMU_FREE\"  # choose between `EMU_TN` and `EMU_FREE`\n    }\n}\n</code></pre> <p>For inquiries and more details on the cloud credentials, please contact info@pasqal.com.</p>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#default-qubit-interaction","title":"Default qubit interaction","text":"<p>When simulating pulse sequences written using Pulser, the underlying constructed Hamiltonian is equivalent to a digital-analog quantum computing program (see digital-analog emulation for more details) with the following interaction term:</p> \\[ \\mathcal{H}_{\\textrm{int}} = \\sum_{i&lt;j} \\frac{C_6}{|R_i - R_j|^6} \\hat{n}_i \\hat{n}_j \\] <p>where \\(C_6\\) is an interaction strength coefficient dependent on the principal quantum number of chosen the neutral atom system, \\(R_i\\) are atomic positions in Cartesian coordinates and \\(\\hat{n} = \\frac{1-\\sigma^z_i}{2}\\) the number operator.</p> <p>Note</p> <p>The Ising interaction is always-on for all computations performed with the Pulser backend. It cannot be switched off.</p>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#available-quantum-operations","title":"Available quantum operations","text":"<p>Currently, the Pulser backend supports the following operations:</p> gate description trainable parameter <code>RX</code>, <code>RY</code> Single qubit rotations. Notice that the interaction is on and this affects the resulting gate fidelity. rotation angle <code>AnalogRX</code>, <code>AnalogRY</code>, <code>AnalogRZ</code> Span a single qubit rotation among the entire register. rotation angle <code>entangle</code> Fully entangle the register. interaction time <code>AnalogInteraction</code> An idle block to to free-evolve for a duration according to the interaction. free evolution time"},{"location":"tutorials/digital_analog_qc/pulser-basic/#sequence-the-bell-state-on-a-two-qubit-register","title":"Sequence the Bell state on a two qubit register","text":"<p>The next example illustrates how to create a pulse sequence to prepare a Bell state. This is a sequence of an entanglement operation, represented as an <code>entangle</code> gate (using <code>CZ</code> interactions) in the \\(X\\)-basis and a \\(Y\\) rotation for readout in the \\(Z\\)-basis:</p> <pre><code>from qadence import chain, entangle, RY\n\nbell_state = chain(\n   entangle(\"t\", qubit_support=(0,1)),\n   RY(0, \"y\"),\n)\n</code></pre> <pre><code>bell_state = ChainBlock(0,1)\n\u251c\u2500\u2500 AnalogEntanglement(t=0.25022735767019744, support=(0, 1))\n\u2514\u2500\u2500 RY(0) [params: ['y']]\n</code></pre> <p>Next, a <code>Register</code> with two qubits is combined with the resulting <code>ChainBlock</code> to form a circuit. Then, the <code>QuantumModel</code> converts the circuit into a proper parametrized pulse sequence with the Pulser backend. Supplying the parameter values allows to sample the pulse sequence outcome:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom qadence import Register, QuantumCircuit, QuantumModel, PI\n\nregister = Register.line(2, spacing = 8.0)  # Two qubits with a distance of 8\u00b5m\ncircuit = QuantumCircuit(register, bell_state)\nmodel = QuantumModel(circuit, backend=\"pulser\", diff_mode=\"gpsr\")\n\nparams = {\n    \"t\": torch.tensor([1000]),  # ns\n    \"y\": torch.tensor([3*PI/2]),\n}\n\n# Return the final state vector\nfinal_vector = model.run(params)\n\n# Sample from the result state vector\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> <pre><code>final_vector = tensor([[-0.7114-0.0169j, -0.0339+0.0156j,  0.0109-0.0457j,  0.6630-0.2244j]])\nsample = Counter({'00': 30, '11': 20})\n</code></pre> <p>Plot the distribution:</p> <p><pre><code>\n</code></pre> 2024-12-05T14:31:46.985093 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/  One can visualise the pulse sequence with different parameters using the <code>assign_paramters</code> method.</p> <pre><code>model.assign_parameters(params).draw(show=False)\n</code></pre> 2024-12-05T14:31:47.086262 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#change-device-specifications","title":"Change device specifications","text":"<p>At variance with other backends, Pulser provides the concept of <code>Device</code>. A <code>Device</code> instance encapsulates all the properties for the definition of a real neutral atoms processor, including but not limited to the maximum laser amplitude for pulses, the maximum distance between two qubits and the maximum duration of the pulse. For more information, please check this tutorial.</p> <p>Qadence offers a simplified interface with only two devices which are detailed here:</p> <ul> <li><code>IDEALIZED</code> (default): ideal device which should be used only for testing purposes. It does not restrict the simulation of pulse sequences.</li> <li><code>REALISTIC</code>: device specification close to real neutral atom quantum processors.</li> </ul> <p>Note</p> <p>If you want to perform simulations closer to the specifications of real neutral atom machines, always select the <code>REALISTIC</code> device.</p> <p>One can use the <code>Configuration</code> of the Pulser backend to select the appropriate device:</p> <pre><code>from qadence import BackendName, DiffMode\nfrom qadence import RealisticDevice\n\n# Choose a realistic device\nregister = Register.line(2, spacing = 8.0, device_specs = RealisticDevice())\n\ncircuit = QuantumCircuit(register, bell_state)\n\nmodel = QuantumModel(\n    circuit,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR,\n)\n\nparams = {\n    \"t\": torch.tensor([1000]),  # ns\n    \"y\": torch.tensor([3*PI/2]),\n}\n\n# Sample from the result state vector\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> <pre><code>sample = Counter({'00': 27, '11': 21, '10': 2})\n</code></pre>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#create-a-custom-gate","title":"Create a custom gate","text":"<p>A major advantage of the block-based interface in Qadence is the ease to compose complex operations from a restricted set of primitive ones. In the following, a custom entanglement operation is used as an example.</p> <p>The operation consists of moving all the qubits to the \\(X\\)-basis. This is realized when the atomic interaction performs a controlled-\\(Z\\) operation during the free evolution. As seen before, this is implemented with the <code>AnalogInteraction</code> and <code>AnalogRY</code> blocks together with appropriate parameters.</p> <pre><code>from qadence import AnalogRY, chain, AnalogInteraction\n\n# Custom entanglement operation.\ndef my_entanglement(duration):\n    return chain(\n        AnalogRY(-PI / 2),\n        AnalogInteraction(duration)\n    )\n\nprotocol = chain(\n   my_entanglement(\"t\"),\n   RY(0, \"y\"),\n)\n\nregister = Register.line(2, spacing = 8.0)\ncircuit = QuantumCircuit(register, protocol)\nmodel = QuantumModel(circuit, backend=BackendName.PULSER, diff_mode=DiffMode.GPSR)\n\nparams = {\n    \"t\": torch.tensor([500]),  # ns\n    \"y\": torch.tensor([PI / 2]),\n}\n\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> 2024-12-05T14:31:47.543772 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#digital-analog-qnn-circuit","title":"Digital-analog QNN circuit","text":"<p>Finally, let's put all together by constructing a digital-analog version of a quantum neural network circuit with feature map and variational ansatz.</p> <pre><code>from qadence import kron, feature_map, BasisSet\nfrom qadence.operations import RX, RY, AnalogRX\n\nhea_one_layer = chain(\n    kron(RY(0, \"th00\"), RY(1, \"th01\")),\n    kron(RX(0, \"th10\"), RX(1, \"th11\")),\n    kron(RY(0, \"th20\"), RY(1, \"th21\")),\n    entangle(\"t\", qubit_support=(0,1)),\n)\n\nprotocol = chain(\n    feature_map(1, param=\"x\", fm_type=BasisSet.FOURIER),\n    hea_one_layer,\n    AnalogRX(PI/4)\n)\n\nregister = Register.line(2, spacing=8.0)\ncircuit = QuantumCircuit(register, protocol)\nmodel = QuantumModel(circuit, backend=BackendName.PULSER, diff_mode=DiffMode.GPSR)\n\nparams = {\n    \"x\": torch.tensor([0.8]), # rad\n    \"t\": torch.tensor([900]), # ns\n    \"th00\":  torch.rand(1), # rad\n    \"th01\":  torch.rand(1), # rad\n    \"th10\":  torch.rand(1), # rad\n    \"th11\":  torch.rand(1), # rad\n    \"th20\":  torch.rand(1), # rad\n    \"th21\":  torch.rand(1), # rad\n}\n\nmodel.assign_parameters(params).draw(draw_phase_area=True, show=False)\n</code></pre> 2024-12-05T14:31:47.680264 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#references","title":"References","text":"<ol> <li> <p>Pulser: An open-source package for the design of pulse sequences in programmable neutral-atom arrays \u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/","title":"Restricted local addressability","text":""},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#physics-behind-semi-local-addressing-patterns","title":"Physics behind semi-local addressing patterns","text":"<p>Recall that in Qadence the general neutral-atom Hamiltonian for a set of \\(n\\) interacting qubits is given by expression</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm drive} + \\mathcal{H}_{\\rm int} = \\sum_{i=0}^{n-1}\\left(\\mathcal{H}^\\text{d}_{i}(t) + \\sum_{j&lt;i}\\mathcal{H}^\\text{int}_{ij}\\right) \\] <p>as is described in detail in the analog interface basics documentation.</p> <p>The driving Hamiltonian term in priciple can model any local single-qubit rotation by addressing each qubit individually. However, some neutral-atom devices offer restricted local addressability using devices called spatial light modulators (SLMs).</p> <p>We refer to this regime as semi-local addressability. In this regime, the individual qubit addressing is restricted to a pattern of targeted qubits which is kept fixed during the execution of the quantum circuit. More formally, the addressing pattern appears as an additional term in the neutral-atom Hamiltonian:</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm drive} + \\mathcal{H}_{\\rm int} + \\mathcal{H}_{\\rm local} \\] <p>where \\(\\mathcal{H}_{\\rm pattern}\\) is given by</p> \\[ \\mathcal{H}_{\\rm local} = \\sum_{i=0}^{n-1}\\left(-\\Delta w_i^{\\rm det} \\hat{n}_i + \\Gamma w_i^{\\rm drive} \\hat{\\sigma}^x_i\\right). \\] <p>Here \\(\\Delta\\) specifies the maximal negative detuning that each qubit in the register can be exposed to. The weight \\(w_i^{\\rm det}\\in [0, 1]\\) determines the actual value of detuning that \\(i\\)-th qubit feels and this way the detuning pattern is emulated. Similarly, for the amplitude pattern \\(\\Gamma\\) determines the maximal additional positive drive that acts on qubits. In this case the corresponding weights \\(w_i^{\\rm drive}\\) can vary in the interval \\([0, 1]\\).</p> <p>Using the detuning and amplitude patterns described above one can modify the behavior of a selected set of qubits, thus achieving semi-local addressing.</p> <p>Qadence implements semi-local addressing in two different flavors of increasing complexity: either as a circuit constructor or directly as a pattern added to the general evolution Hamiltonian described by the circuit.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#using-circuit-constructors","title":"Using circuit constructors","text":"<p>The <code>rydberg_hea</code> constructor routine allows to build a circuit instance implementing a basic version of the Hamiltonian evolution described above where both \\(\\Delta\\) and \\(\\tilde{\\Omega}\\) coefficients are considered constants. Furthemore, no global drive and detuning are explicitly added to the Hamiltonian. Therefore, the final Hamiltonian generator of the circuit reads as follows:</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm local}(w^{\\rm drive}, w^{\\rm det}) + \\mathcal{H}_{\\textrm{int}} \\] <p>This implementation does not perform any checks on the weights normalization, thus making it not realistic. This implies that global drive and detuning can be retrieved by appropriately choosing the weights.</p> <p>You can easily create a Rydberg hardware efficient ansatz implementing multiple layers of the evolution generated by the local addressing Hamiltonian:</p> \\[ \\mathcal{H}_{\\rm evo} = \\sum_j \\mathcal{H}_{\\textrm{local}}(w_{j}^{\\rm drive}, w_{j}^{\\rm det}) \\] <p>Notice that in real-device implementation, one layer only is usually achievable.</p> <pre><code>import qadence as qd\nfrom qadence import rydberg_hea, rydberg_hea_layer\n\nn_qubits = 4\nn_layers = 2\nregister = qd.Register.line(n_qubits)\n\n# ansatz constructor\n# the evolution time is parametrized for each layer of the evolution\nansatz = rydberg_hea(\n    register,\n    n_layers=n_layers,  # number of subsequent layers of Hamiltonian evolution\n    addressable_detuning=True,  # make the local detuning weights w_i^{det} as variational parameters\n    addressable_drive=True, # make the local drive weights w_i^{drv} as variational parameters\n    tunable_phase=True, # make the phase \\phi as a variational parameter\n)\n\n# alternatively, a single ansatz layer can also be created for\n# better flexibility\n\n# these can be variational parameters\ntevo_drive = 1.0  # evolution time for the locally addressed drive term\ntevo_det = 1.0 # evolution time for the locally addressed detuning term\ntevo_int = 1.0  # evolution time for the interaction term\n\n# these can be list of variational parameters\nweights_drive = [0.0, 0.25, 0.5, 0.25]\nweights_det = [0.0, 0.0, 0.5, 0.5]\n\nansatz_layer = rydberg_hea_layer(\n    register,\n    tevo_det,\n    tevo_drive,\n    tevo_int,\n    detunings=weights_det,\n    drives=weights_drive,\n)\n</code></pre> <pre><code>\n</code></pre> <p>This circuit constructor is meant to be used with fully differentiable backends such as <code>pyqtorch</code> and mainly for quick experimentation with neutral atom compatible ansatze.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#using-addressing-patterns","title":"Using addressing patterns","text":"<p>In Qadence semi-local addressing patterns can be created by either specifying fixed values for the weights of the qubits being addressed or defining them as trainable parameters that can be optimized later in some training loop. Semi-local addressing patterns can be defined with the <code>AddressingPattern</code> dataclass.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#fixed-weights","title":"Fixed weights","text":"<p>With fixed weights, detuning/amplitude addressing patterns can be defined in the following way:</p> <pre><code>import torch\nfrom qadence.analog import AddressingPattern\n\nn_qubits = 3\n\nw_det = {0: 0.9, 1: 0.5, 2: 1.0}\nw_amp = {0: 0.1, 1: 0.4, 2: 0.8}\ndet = 9.0\namp = 6.5\npattern = AddressingPattern(\n    n_qubits=n_qubits,\n    det=det,\n    amp=amp,\n    weights_det=w_det,\n    weights_amp=w_amp,\n)\n</code></pre> <p>If only detuning or amplitude pattern is needed - the corresponding weights for all qubits can be set to 0.</p> <p>The created addressing pattern can now be passed as an argument to any Qadence device class, or to the <code>IdealDevice</code> or <code>RealisticDevice</code> to make use of the pre-defined options in those devices,</p> <pre><code>import torch\nfrom qadence import (\n    AnalogRX,\n    AnalogRY,\n    BackendName,\n    DiffMode,\n    Parameter,\n    QuantumCircuit,\n    QuantumModel,\n    Register,\n    chain,\n    total_magnetization,\n    IdealDevice,\n    PI\n)\n\n# define register and circuit\nspacing = 8.0\nx = Parameter(\"x\")\nblock = chain(AnalogRX(3 * x), AnalogRY(0.5 * x))\n\ndevice_specs = IdealDevice(pattern = pattern)\n\nreg = Register.line(\n    n_qubits,\n    spacing=spacing,\n    device_specs=device_specs,\n)\n\ncirc = QuantumCircuit(reg, block)\n\nobs = total_magnetization(n_qubits)\n\nmodel_pyq = QuantumModel(\n    circuit=circ, observable=obs, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD\n)\n\n# calculate expectation value of the circuit for random input value\nvalue = {\"x\": 1.0 + torch.rand(1)}\nexpval_pyq = model_pyq.expectation(values = value)\n</code></pre>   Expectation value on PyQ:  tensor([2.1555])     <p>The same configuration can also be seamlessly used to create a model with the Pulser backend.</p> <pre><code>model_pulser = QuantumModel(\n    circuit=circ,\n    observable=obs,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR\n)\n\n# calculate expectation value of the circuit for same random input value\nexpval_pulser = model_pulser.expectation(values = value)\n</code></pre>   Expectation value on Pulser:  tensor([2.1535])     <p>Note that by default the addressing pattern terms are added to every analog operation in the circuit. However, it is possible to turn the addressing pattern off for specific operations by passing <code>add_pattern=False</code> in the operation. For example <code>AnalogRX(pi)</code> will get the extra addressing pattern term, but <code>AnalogRX(pi, add_pattern=False)</code> will not. This is currently only implemented for the PyQTorch backend. If an addressing pattern is specified for the Pulser backend, it will be added to all the blocks.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#trainable-weights","title":"Trainable weights","text":"<p>Note</p> <p>Trainable parameters currently are supported only by <code>pyqtorch</code> backend.</p> <p>Since both the maximum detuning/amplitude value of the addressing pattern and the corresponding weights can be user specified, they can be variationally used in some QML setting. This can be achieved by defining pattern weights as trainable <code>Parameter</code> instances or strings specifying weight names.</p> <pre><code>n_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\n# some random target function value\nf_value = torch.rand(1)\n\n# define trainable addressing pattern\nw_amp = {i: f\"w_amp{i}\" for i in range(n_qubits)}\nw_det = {i: f\"w_det{i}\" for i in range(n_qubits)}\namp = \"max_amp\"\ndet = \"max_det\"\n\npattern = AddressingPattern(\n    n_qubits=n_qubits,\n    det=det,\n    amp=amp,\n    weights_det=w_det,\n    weights_amp=w_amp,\n)\n\n# some fixed analog operation\nblock = AnalogRX(PI)\n\ndevice_specs = IdealDevice(pattern = pattern)\n\nreg = Register.line(\n    n_qubits,\n    spacing=spacing,\n    device_specs=device_specs,\n)\n\ncirc = QuantumCircuit(reg, block)\n\n# define quantum model\nobs = total_magnetization(n_qubits)\nmodel = QuantumModel(circuit=circ, observable=obs, backend=BackendName.PYQTORCH)\n\n# prepare for training\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_criterion = torch.nn.MSELoss()\nn_epochs = 200\nloss_save = []\n\n# train model\nfor _ in range(n_epochs):\n    optimizer.zero_grad()\n    out = model.expectation()\n    loss = loss_criterion(f_value, out)\n    loss.backward()\n    optimizer.step()\n    loss_save.append(loss.item())\n\n# get final results\nf_value_model = model.expectation().detach()\n\nassert torch.isclose(f_value, f_value_model, atol=0.01)\n</code></pre>   The target function value:  tensor([0.5492]) The trained function value:  tensor([[0.5492]])    <p>Here, the expectation value of the circuit is fitted by varying the parameters of the addressing pattern.</p>"},{"location":"tutorials/qml/","title":"Variational quantum algorithms","text":"<p>Variational algorithms on noisy devices and quantum machine learning (QML)[^1] in particular are one of the main target applications for Qadence. For this purpose, the library offers both flexible symbolic expressions for the quantum circuit parameters via <code>sympy</code> (see here for more details) and native automatic differentiation via integration with PyTorch deep learning framework.</p> <p>Furthermore, Qadence offers a wide range of utilities for helping building and researching quantum machine learning algorithms, including:</p> <ul> <li>a set of constructors for circuits commonly used in quantum machine learning such as feature maps and ansatze</li> <li>a set of tools for training and optimizing quantum neural networks and loading classical data into a QML algorithm</li> </ul>"},{"location":"tutorials/qml/#some-simple-examples","title":"Some simple examples","text":"<p>Qadence symbolic parameter interface allows to create arbitrary feature maps to encode classical data into quantum circuits with an arbitrary non-linear function embedding for the input values:</p> <pre><code>import qadence as qd\nfrom qadence.operations import *\nimport torch\nfrom sympy import acos\n\nn_qubits = 4\n\n# Example feature map, also directly available with the `feature_map` function\nfp = qd.FeatureParameter(\"phi\")\nfm = qd.kron(RX(i, acos(fp)) for i in range(n_qubits))\n\n# the key in the dictionary must correspond to\n# the name of the assigned to the feature parameter\ninputs = {\"phi\": torch.rand(3)}\nsamples = qd.sample(fm, values=inputs)\n</code></pre> <pre><code>samples = OrderedCounter({'0000': 99, '0010': 1})\n</code></pre> <p>The <code>constructors.feature_map</code> module provides convenience functions to build commonly used feature maps where the input parameter is encoded in the single-qubit gates rotation angle. This function will be further demonstrated in the QML constructors tutorial.</p> <p>Furthermore, Qadence is natively integrated with PyTorch automatic differentiation engine thus Qadence quantum models can be used seamlessly in a PyTorch workflow.</p> <p>Let's create a quantum neural network model using the feature map just defined, a digital-analog variational ansatz (also explained here) and a simple observable \\(X(0) \\otimes X(1)\\). We use the convenience <code>QNN</code> quantum model abstraction.</p> <pre><code>ansatz = qd.hea(n_qubits, strategy=\"sDAQC\")\ncircuit = qd.QuantumCircuit(n_qubits, fm, ansatz)\nobservable = qd.kron(X(0), X(1))\n\nmodel = qd.QNN(circuit, observable)\n\n# NOTE: the `QNN` is a torch.nn.Module\nassert isinstance(model, torch.nn.Module)\n</code></pre> <pre><code>True\n</code></pre> <p>Differentiation works the same way as any other PyTorch module:</p> <pre><code>values = {\"phi\": torch.rand(10, requires_grad=True)}\n\n# the forward pass of the quantum model returns the expectation\n# value of the input observable\nout = model(values)\n\n# you can compute the gradient with respect to inputs using\n# PyTorch autograd differentiation engine\ndout = torch.autograd.grad(out, values[\"phi\"], torch.ones_like(out), create_graph=True)[0]\nprint(f\"First-order derivative w.r.t. the feature parameter: \\n{dout}\")\n\n# you can also call directly a backward pass to compute derivatives with respect\n# to the variational parameters and use it for implementing variational\n# optimization\nout.sum().backward()\n</code></pre> <pre><code>Quantum model output: \ntensor([[ 0.0231],\n        [-0.0133],\n        [ 0.3699],\n        [ 0.4348],\n        [ 0.3164],\n        [ 0.2681],\n        [-0.0471],\n        [ 0.1624],\n        [ 0.2670],\n        [ 0.3393]], grad_fn=&lt;CatBackward0&gt;)\n\nFirst-order derivative w.r.t. the feature parameter: \ntensor([ 0.6922,  0.6359,  0.5828, -0.0646, -2.0352,  0.7593,  0.5677,  0.7926,\n         0.7603,  0.6604], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>To run QML on real devices, Qadence offers generalized parameter shift rules (GPSR) <sup>1</sup> for arbitrary quantum operations which can be selected when constructing the <code>QNN</code> model:</p> <pre><code>model = qd.QNN(circuit, observable, diff_mode=\"gpsr\")\nout = model(values)\n\ndout = torch.autograd.grad(out, values[\"phi\"], torch.ones_like(out), create_graph=True)[0]\nprint(f\"First-order derivative w.r.t. the feature parameter: \\n{dout}\")\n</code></pre> <pre><code>First-order derivative w.r.t. the feature parameter: \ntensor([ 0.6922,  0.6359,  0.5828, -0.0646, -2.0352,  0.7593,  0.5677,  0.7926,\n         0.7603,  0.6604], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>See here for more details on how the parameter shift rules implementation works in Qadence.</p>"},{"location":"tutorials/qml/#references","title":"References","text":"<p>[^1] Schuld, Petruccione, Machine learning on Quantum Computers, Springer Nature (2021)</p> <ol> <li> <p>Kyriienko et al., General quantum circuit differentiation rules \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/config_qnn/","title":"Configuring a QNN","text":"<p>In <code>qadence</code>, the <code>QNN</code> is a variational quantum model that can potentially take multi-dimensional input.</p> <p>The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>The circuit has two parts, the feature map and the ansatz. The feature map is responsible for encoding the input data into the quantum state, while the ansatz is responsible for the variational part of the model. In addition, a third part of the QNN is the observables, which is (a list of) operators that are measured at the end of the circuit.</p> <p>In QML Constructors we have seen how to construct the feature map and the ansatz. In this tutorial, we will see how to do the same using configs.</p> <p>One convenient way to construct these three parts of the model is to use the config classes, namely, <code>ObservableConfig</code>, <code>FeatureMapConfig</code>, <code>AnsatzConfig</code>. These classes allow you to specify the type of circuit and the parameters of the circuit in a structured way.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-observable","title":"Defining the Observable","text":"<p>The model output is the expectation value of the defined observable(s). We use the <code>ObservableConfig</code> class to specify the observable.</p> <p>We can specify any Hamiltonian that we want to measure at the end of the circuit. Let us say we want to measure the \\(Z\\) operator.</p> <pre><code>from qadence import observable_from_config, ObservableConfig, Z\n\nobservable_config = ObservableConfig(\n    detuning=Z,\n    scale=3.0,\n    shift=-1.0,\n)\n\nobservable = observable_from_config(register=4, config=observable_config)\n</code></pre> %3 cluster_ba5eda7c8e784e38bc2f160b5ab5b51f ce88cc4338b44596bcc7b9179cd07d43 0 2f87f4e960f0498ca7044165871367bf ce88cc4338b44596bcc7b9179cd07d43--2f87f4e960f0498ca7044165871367bf b017d58fe48145aa8eda1878858ec0a5 1 254835940f9a4c2cbd54d03c06b62633 2f87f4e960f0498ca7044165871367bf--254835940f9a4c2cbd54d03c06b62633 3b8a78d542a049c6be00567a7ca9b0c3 9a1d875823ed443299e1446043d85686 AddBlock b017d58fe48145aa8eda1878858ec0a5--9a1d875823ed443299e1446043d85686 c3f43a4830494110b906fde17d4111e2 2 9a1d875823ed443299e1446043d85686--3b8a78d542a049c6be00567a7ca9b0c3 0c60e68b6b0f4bdd9495b03a9b905fe6 324be8ae6d8e406189e03dc1f7c8a032 c3f43a4830494110b906fde17d4111e2--324be8ae6d8e406189e03dc1f7c8a032 e80d66ea40ea42a885e808a60fe1fa9e 3 324be8ae6d8e406189e03dc1f7c8a032--0c60e68b6b0f4bdd9495b03a9b905fe6 33af86fe4ac6472b96f7ab2f8c1cb0e1 8a55524d5bc64f5196743ded5a1f9155 e80d66ea40ea42a885e808a60fe1fa9e--8a55524d5bc64f5196743ded5a1f9155 8a55524d5bc64f5196743ded5a1f9155--33af86fe4ac6472b96f7ab2f8c1cb0e1 <p>We have specified the observable Hamiltonian to be one with \\(Z\\)-detuning. The result is linearly scaled by 3.0 and shifted by -1.0. These parameters can optionally also be FeatureParameter or VariationalParameter</p> <p>One can also specify the observable as a list of observables, in which case the QNN will output a list of values.</p> <p>For full details on the <code>ObservableConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-feature-map","title":"Defining the Feature Map","text":"<p>Let us say we want to build a 4-qubit QNN that takes two inputs, namely, the \\(x\\) and the \\(y\\) coordinates of a point in the plane. We can use the <code>FeatureMapConfig</code> class to specify the feature map.</p> <pre><code>from qadence import BasisSet, chain, create_fm_blocks, FeatureMapConfig, ReuploadScaling\n\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs = [\"x\", \"y\"],\n    basis_set=BasisSet.CHEBYSHEV,\n    reupload_scaling=ReuploadScaling.TOWER,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\n\nfm_blocks = create_fm_blocks(register=4, config=fm_config)\nfeature_map = chain(*fm_blocks)\n</code></pre> %3 cluster_da50be8ccdcc4f4f9962ec31dc74d31f Tower Chebyshev FM cluster_4c21da3fe19b45439df73ea6e66dd805 Tower Chebyshev FM a82a44858d964038b076d2a03b2118d9 0 d616aa878c5e40b68215e0473ca2554e RX(1.0*acos(x)) a82a44858d964038b076d2a03b2118d9--d616aa878c5e40b68215e0473ca2554e c13fc5e3d31d475494f1272964c99c54 1 7d159fdec9594840a66f840738bafed0 d616aa878c5e40b68215e0473ca2554e--7d159fdec9594840a66f840738bafed0 8bcd7e7cab984287b0b6307f7c91112e c2c45170028b4ccb9b41edd4599d0586 RX(2.0*acos(x)) c13fc5e3d31d475494f1272964c99c54--c2c45170028b4ccb9b41edd4599d0586 56d201d879bc4f0eb9011fbaa399147e 2 c2c45170028b4ccb9b41edd4599d0586--8bcd7e7cab984287b0b6307f7c91112e 27d857cbed1647c6ae5a0d506e1ce49e 4e5ebce79aa34a868ce42359928bbcb2 RX(1.0*acos(2.0*y - 1.0)) 56d201d879bc4f0eb9011fbaa399147e--4e5ebce79aa34a868ce42359928bbcb2 44e0120dc5184574b39b1348144c99c1 3 4e5ebce79aa34a868ce42359928bbcb2--27d857cbed1647c6ae5a0d506e1ce49e c2b4a1b4761044b0b200f6ab59d10e76 a544a3cd6cf446d6914ddf71d92a6abd RX(2.0*acos(2.0*y - 1.0)) 44e0120dc5184574b39b1348144c99c1--a544a3cd6cf446d6914ddf71d92a6abd a544a3cd6cf446d6914ddf71d92a6abd--c2b4a1b4761044b0b200f6ab59d10e76 <p>We have specified that the feature map should take two features, and have named the <code>FeatureParameter</code> \"x\" and \"y\" respectively. Both these parameters are encoded using the Chebyshev basis set, and the reupload scaling is set to <code>ReuploadScaling.TOWER</code>. One can optionally add the basis and the reupload scaling for each parameter separately.</p> <p>The <code>feature_range</code> parameter is a dictionary that specifies the range of values that each feature comes from. This is useful for scaling the input data to the range that the encoding function can handle. In default case, this range is mapped to the target range of the Chebyshev basis set which is \\([-1, 1]\\). One can also specify the target range for each feature separately.</p> <p>For full details on the <code>FeatureMapConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-ansatz","title":"Defining the Ansatz","text":"<p>The next part of the QNN is the ansatz. We use <code>AnsatzConfig</code> class to specify the type of ansatz.</p> <p>Let us say, we want to follow this feature map with 2 layers of hardware efficient ansatz.</p> <pre><code>from qadence import AnsatzConfig, AnsatzType, create_ansatz, Strategy\n\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nansatz = create_ansatz(register=4, config=ansatz_config)\n</code></pre> %3 947fc89d7253403b98b9e93090846bdd 0 9a0e4e1f15d94fc19ee2e8aa15b8870d RX(theta\u2080) 947fc89d7253403b98b9e93090846bdd--9a0e4e1f15d94fc19ee2e8aa15b8870d 83b9b5d37fd14f32adf28b61e4d1904c 1 773e764611a144df8a051a821d77952a RY(theta\u2084) 9a0e4e1f15d94fc19ee2e8aa15b8870d--773e764611a144df8a051a821d77952a 949601ebbc5c4129a2465b8d1bca4f19 RX(theta\u2088) 773e764611a144df8a051a821d77952a--949601ebbc5c4129a2465b8d1bca4f19 7a3ae9297b1d47e1bb5bb0502ea03115 949601ebbc5c4129a2465b8d1bca4f19--7a3ae9297b1d47e1bb5bb0502ea03115 96b02fe349504f32a63688c1ee36d372 7a3ae9297b1d47e1bb5bb0502ea03115--96b02fe349504f32a63688c1ee36d372 bd581278df1943ed9e903fd4b4e8d21d RX(theta\u2081\u2082) 96b02fe349504f32a63688c1ee36d372--bd581278df1943ed9e903fd4b4e8d21d cc19b0735668439ca5414489d1fe9041 RY(theta\u2081\u2086) bd581278df1943ed9e903fd4b4e8d21d--cc19b0735668439ca5414489d1fe9041 1dfdc9bdd96a458e838ffa8f08954863 RX(theta\u2082\u2080) cc19b0735668439ca5414489d1fe9041--1dfdc9bdd96a458e838ffa8f08954863 f2d253e9f796430fb56b20107560db04 1dfdc9bdd96a458e838ffa8f08954863--f2d253e9f796430fb56b20107560db04 a6b983bf13f64d3ca9530f6b0e6972a3 f2d253e9f796430fb56b20107560db04--a6b983bf13f64d3ca9530f6b0e6972a3 c01507d8ba4449ee8636736dfb9a948e a6b983bf13f64d3ca9530f6b0e6972a3--c01507d8ba4449ee8636736dfb9a948e 95790e1eea864cbcb24c2b5de643eddf 761d70277ee342cb8949a8ea4d859207 RX(theta\u2081) 83b9b5d37fd14f32adf28b61e4d1904c--761d70277ee342cb8949a8ea4d859207 7407defb770947d78b211bc0f3fcbc79 2 d62be7a001d042e3bb742c0cf830cbed RY(theta\u2085) 761d70277ee342cb8949a8ea4d859207--d62be7a001d042e3bb742c0cf830cbed 35099afa8911436ba42bc3971238020b RX(theta\u2089) d62be7a001d042e3bb742c0cf830cbed--35099afa8911436ba42bc3971238020b 2043bee5e89c4069a3a510c3ef3a2a0a X 35099afa8911436ba42bc3971238020b--2043bee5e89c4069a3a510c3ef3a2a0a 2043bee5e89c4069a3a510c3ef3a2a0a--7a3ae9297b1d47e1bb5bb0502ea03115 33cb3af0408642618e833ac1cf5180a1 2043bee5e89c4069a3a510c3ef3a2a0a--33cb3af0408642618e833ac1cf5180a1 f3525d8d8987427f9a666363b24064de RX(theta\u2081\u2083) 33cb3af0408642618e833ac1cf5180a1--f3525d8d8987427f9a666363b24064de 6375ef0016854f26aa3b95a59a5b9509 RY(theta\u2081\u2087) f3525d8d8987427f9a666363b24064de--6375ef0016854f26aa3b95a59a5b9509 4b1e9275f749403bb9a9a9edcd70bcca RX(theta\u2082\u2081) 6375ef0016854f26aa3b95a59a5b9509--4b1e9275f749403bb9a9a9edcd70bcca 3efea8c45f1441899300dfb39d08b4fc X 4b1e9275f749403bb9a9a9edcd70bcca--3efea8c45f1441899300dfb39d08b4fc 3efea8c45f1441899300dfb39d08b4fc--f2d253e9f796430fb56b20107560db04 7b163a9d24e64254b93a7cfe396d52fd 3efea8c45f1441899300dfb39d08b4fc--7b163a9d24e64254b93a7cfe396d52fd 7b163a9d24e64254b93a7cfe396d52fd--95790e1eea864cbcb24c2b5de643eddf 78dcaf6d5d4a4a8e81cf775562d48c3f 83317269ecc14379a211d75a332cd9d8 RX(theta\u2082) 7407defb770947d78b211bc0f3fcbc79--83317269ecc14379a211d75a332cd9d8 f7989053301944148596bb6076b49e5a 3 322bd36d558842cc86d823633dc26bd6 RY(theta\u2086) 83317269ecc14379a211d75a332cd9d8--322bd36d558842cc86d823633dc26bd6 008e7c65245f47448e8c50fdc350c6e2 RX(theta\u2081\u2080) 322bd36d558842cc86d823633dc26bd6--008e7c65245f47448e8c50fdc350c6e2 d0753fae355d4880983b3a594a651a05 008e7c65245f47448e8c50fdc350c6e2--d0753fae355d4880983b3a594a651a05 c921cf9a35b749868db233ac27196097 X d0753fae355d4880983b3a594a651a05--c921cf9a35b749868db233ac27196097 c921cf9a35b749868db233ac27196097--33cb3af0408642618e833ac1cf5180a1 da392b217e2b454ab49849dd3dae4e91 RX(theta\u2081\u2084) c921cf9a35b749868db233ac27196097--da392b217e2b454ab49849dd3dae4e91 9e7005c647894f0db5460b0e50c87e61 RY(theta\u2081\u2088) da392b217e2b454ab49849dd3dae4e91--9e7005c647894f0db5460b0e50c87e61 ddbe78345fa440f88069c34885fa30dd RX(theta\u2082\u2082) 9e7005c647894f0db5460b0e50c87e61--ddbe78345fa440f88069c34885fa30dd aca6c72e49104b708b7f1bd0955bf1b1 ddbe78345fa440f88069c34885fa30dd--aca6c72e49104b708b7f1bd0955bf1b1 1ba615cc3d7f409aafd26cc43daf6abe X aca6c72e49104b708b7f1bd0955bf1b1--1ba615cc3d7f409aafd26cc43daf6abe 1ba615cc3d7f409aafd26cc43daf6abe--7b163a9d24e64254b93a7cfe396d52fd 1ba615cc3d7f409aafd26cc43daf6abe--78dcaf6d5d4a4a8e81cf775562d48c3f eff3119c9b5c4bbb9a7a623b0ea7ab06 e6bcd90bd7f74cb088b1729cfd85f0e6 RX(theta\u2083) f7989053301944148596bb6076b49e5a--e6bcd90bd7f74cb088b1729cfd85f0e6 413c58d14a7a42be9ca2243202fb6fb3 RY(theta\u2087) e6bcd90bd7f74cb088b1729cfd85f0e6--413c58d14a7a42be9ca2243202fb6fb3 dd19542be1a14bee9b2cc34587946bcf RX(theta\u2081\u2081) 413c58d14a7a42be9ca2243202fb6fb3--dd19542be1a14bee9b2cc34587946bcf 79e9735424f5453f9e2585457a6b1cb9 X dd19542be1a14bee9b2cc34587946bcf--79e9735424f5453f9e2585457a6b1cb9 79e9735424f5453f9e2585457a6b1cb9--d0753fae355d4880983b3a594a651a05 9f3f0b5b138c4f97a4bc9ffd85e1e750 79e9735424f5453f9e2585457a6b1cb9--9f3f0b5b138c4f97a4bc9ffd85e1e750 33178a4e3def477499ed08da326fa3d0 RX(theta\u2081\u2085) 9f3f0b5b138c4f97a4bc9ffd85e1e750--33178a4e3def477499ed08da326fa3d0 8dd7d390ba494b6aa311ab28f5c7a346 RY(theta\u2081\u2089) 33178a4e3def477499ed08da326fa3d0--8dd7d390ba494b6aa311ab28f5c7a346 a6586ba210c24fda9fc3b3eca2daa362 RX(theta\u2082\u2083) 8dd7d390ba494b6aa311ab28f5c7a346--a6586ba210c24fda9fc3b3eca2daa362 812729297a7c4f4b86ee43ed38363d44 X a6586ba210c24fda9fc3b3eca2daa362--812729297a7c4f4b86ee43ed38363d44 812729297a7c4f4b86ee43ed38363d44--aca6c72e49104b708b7f1bd0955bf1b1 d224c9d858cd4a2fb30fab3c82726520 812729297a7c4f4b86ee43ed38363d44--d224c9d858cd4a2fb30fab3c82726520 d224c9d858cd4a2fb30fab3c82726520--eff3119c9b5c4bbb9a7a623b0ea7ab06 <p>We have specified that the ansatz should have a depth of 2, and the ansatz type is \"hea\" (Hardware Efficient Ansatz). The ansatz strategy is set to \"digital\", which means digital gates are being used. One could alternatively use \"analog\" or \"rydberg\" as the ansatz strategy.</p> <p>For full details on the <code>AnsatzConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-qnn-from-the-configs","title":"Defining the QNN from the Configs","text":"<p>To build the QNN, we can now use the <code>QNN</code> class as a <code>QuantumModel</code> subtype. In addition to the feature map, ansatz and the observable configs, we can also specify options such as the <code>backend</code>, <code>diff_mode</code>, etc. For full details on the <code>QNN</code> class, see the API documentation or the documentation on the config constructor here.</p> <pre><code>from qadence import BackendName, DiffMode, QNN\n\nqnn = QNN.from_configs(\n    register=4,\n    obs_config=observable_config,\n    fm_config=fm_config,\n    ansatz_config=ansatz_config,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.AD,\n)\n</code></pre> %3 cluster_9835ae4eadbd454c95e8d29a4525d7ae Obs. cluster_79c4ee3491504703a49542e5bb7668b4 cluster_00e0922e9f2a4375b1ccd7d5a4bfa54e Tower Chebyshev FM cluster_36a8affd9bf14dc4927a6fa650722ec6 Tower Chebyshev FM cluster_78207a20bf884741abfd99ba52e3c537 HEA e5856b77d8e14d4eb60eb6260cfcd229 0 85c2f6aa5eaa458297be05a607200148 RX(1.0*acos(x)) e5856b77d8e14d4eb60eb6260cfcd229--85c2f6aa5eaa458297be05a607200148 d503e054ff614492a27115ad4b8cfac2 1 d98e9271d71e4fa98498534092522305 RX(theta\u2080) 85c2f6aa5eaa458297be05a607200148--d98e9271d71e4fa98498534092522305 18bc137172d6451d90bb30330b77f132 RY(theta\u2084) d98e9271d71e4fa98498534092522305--18bc137172d6451d90bb30330b77f132 4a549aa88e92474ab5a8601c9414b62d RX(theta\u2088) 18bc137172d6451d90bb30330b77f132--4a549aa88e92474ab5a8601c9414b62d 27cbe8f29687462db084ebf975309fab 4a549aa88e92474ab5a8601c9414b62d--27cbe8f29687462db084ebf975309fab ccbfc35884514f29a7c88f6c18ea12f5 27cbe8f29687462db084ebf975309fab--ccbfc35884514f29a7c88f6c18ea12f5 e251e83896414eefb90cf52f3450f773 RX(theta\u2081\u2082) ccbfc35884514f29a7c88f6c18ea12f5--e251e83896414eefb90cf52f3450f773 510afcee9f4a47fbb27223a08d669ca8 RY(theta\u2081\u2086) e251e83896414eefb90cf52f3450f773--510afcee9f4a47fbb27223a08d669ca8 d5f3720c14b04c5aac82b70e3adad55b RX(theta\u2082\u2080) 510afcee9f4a47fbb27223a08d669ca8--d5f3720c14b04c5aac82b70e3adad55b a278a14525c54a8ebd9730a22b75b7fd d5f3720c14b04c5aac82b70e3adad55b--a278a14525c54a8ebd9730a22b75b7fd f59a9493e0a14193bbee1ab66a6f398c a278a14525c54a8ebd9730a22b75b7fd--f59a9493e0a14193bbee1ab66a6f398c 6a7ee671193c454e96fc26f491631ffa f59a9493e0a14193bbee1ab66a6f398c--6a7ee671193c454e96fc26f491631ffa 4d9baa9ece764cc5b7712aa4205d7b9d 6a7ee671193c454e96fc26f491631ffa--4d9baa9ece764cc5b7712aa4205d7b9d c314c6297d0f45a69c125e6c89fc9c8a bcba55aeaba64f6babafcce0e6c2f126 RX(2.0*acos(x)) d503e054ff614492a27115ad4b8cfac2--bcba55aeaba64f6babafcce0e6c2f126 bf7dfc6da091477e8d224d8875350804 2 5d574c77104b418da9413796090b085e RX(theta\u2081) bcba55aeaba64f6babafcce0e6c2f126--5d574c77104b418da9413796090b085e c4e34e19ac8f421cb4d80748da21de8e RY(theta\u2085) 5d574c77104b418da9413796090b085e--c4e34e19ac8f421cb4d80748da21de8e 4337d6ab44c645ecbd5e08de09212b36 RX(theta\u2089) c4e34e19ac8f421cb4d80748da21de8e--4337d6ab44c645ecbd5e08de09212b36 121f68822f174b63836389d63e288079 X 4337d6ab44c645ecbd5e08de09212b36--121f68822f174b63836389d63e288079 121f68822f174b63836389d63e288079--27cbe8f29687462db084ebf975309fab b05de6dd92f54c3b9dbdcd6d7da594db 121f68822f174b63836389d63e288079--b05de6dd92f54c3b9dbdcd6d7da594db d1b53c3e449443c29fea0ad4c21820cc RX(theta\u2081\u2083) b05de6dd92f54c3b9dbdcd6d7da594db--d1b53c3e449443c29fea0ad4c21820cc 4e01b264fb37421da490098324fecf35 RY(theta\u2081\u2087) d1b53c3e449443c29fea0ad4c21820cc--4e01b264fb37421da490098324fecf35 70e3516597564615942da8c38e20d0ae RX(theta\u2082\u2081) 4e01b264fb37421da490098324fecf35--70e3516597564615942da8c38e20d0ae 68772cc7741d4709882001573a9b4119 X 70e3516597564615942da8c38e20d0ae--68772cc7741d4709882001573a9b4119 68772cc7741d4709882001573a9b4119--a278a14525c54a8ebd9730a22b75b7fd 0a759c9466e14ae49770837583344d93 68772cc7741d4709882001573a9b4119--0a759c9466e14ae49770837583344d93 3e1477c1cbe64f599cc912c5137a547a AddBlock 0a759c9466e14ae49770837583344d93--3e1477c1cbe64f599cc912c5137a547a 3e1477c1cbe64f599cc912c5137a547a--c314c6297d0f45a69c125e6c89fc9c8a fce2b54fed6b4620bed308474c1ba970 c68c38afc5fd4de7bfea74bd5318e8ac RX(1.0*acos(2.0*y - 1.0)) bf7dfc6da091477e8d224d8875350804--c68c38afc5fd4de7bfea74bd5318e8ac aedce3c00b874eb795a51ae46c739ff5 3 0bef3be3113243378d974a54791d8118 RX(theta\u2082) c68c38afc5fd4de7bfea74bd5318e8ac--0bef3be3113243378d974a54791d8118 61da16c907274ee293b342ac98df7b48 RY(theta\u2086) 0bef3be3113243378d974a54791d8118--61da16c907274ee293b342ac98df7b48 4d7e5e0b1fce490e90a66914f6c31841 RX(theta\u2081\u2080) 61da16c907274ee293b342ac98df7b48--4d7e5e0b1fce490e90a66914f6c31841 947bc7b7a1cf454ba778fcec9b28a82f 4d7e5e0b1fce490e90a66914f6c31841--947bc7b7a1cf454ba778fcec9b28a82f 78114657ac374e20b03455b8acefc1c9 X 947bc7b7a1cf454ba778fcec9b28a82f--78114657ac374e20b03455b8acefc1c9 78114657ac374e20b03455b8acefc1c9--b05de6dd92f54c3b9dbdcd6d7da594db 554ffe4bf70247498ce8e9a5720605b0 RX(theta\u2081\u2084) 78114657ac374e20b03455b8acefc1c9--554ffe4bf70247498ce8e9a5720605b0 8884a30bb79944aa8d12cb591cda4482 RY(theta\u2081\u2088) 554ffe4bf70247498ce8e9a5720605b0--8884a30bb79944aa8d12cb591cda4482 ca47d2db97804ba6831880956f243355 RX(theta\u2082\u2082) 8884a30bb79944aa8d12cb591cda4482--ca47d2db97804ba6831880956f243355 a4bfaa4e187a4895996074cc8edd8dac ca47d2db97804ba6831880956f243355--a4bfaa4e187a4895996074cc8edd8dac b7aefc5408a44d078e9b89b587a20d0c X a4bfaa4e187a4895996074cc8edd8dac--b7aefc5408a44d078e9b89b587a20d0c b7aefc5408a44d078e9b89b587a20d0c--0a759c9466e14ae49770837583344d93 b594b4343ba940fca4d75d5f72a1df14 b7aefc5408a44d078e9b89b587a20d0c--b594b4343ba940fca4d75d5f72a1df14 b594b4343ba940fca4d75d5f72a1df14--fce2b54fed6b4620bed308474c1ba970 16b5db84e7184803bba397a005976d06 c89026f8497949f58fd63f8ada1dd652 RX(2.0*acos(2.0*y - 1.0)) aedce3c00b874eb795a51ae46c739ff5--c89026f8497949f58fd63f8ada1dd652 3048f5fc1f11496fba650545a0226639 RX(theta\u2083) c89026f8497949f58fd63f8ada1dd652--3048f5fc1f11496fba650545a0226639 bbbb1f7bf6d34302a2be13da035349f0 RY(theta\u2087) 3048f5fc1f11496fba650545a0226639--bbbb1f7bf6d34302a2be13da035349f0 a5199a9cedce4a9bba003658ec9d98f6 RX(theta\u2081\u2081) bbbb1f7bf6d34302a2be13da035349f0--a5199a9cedce4a9bba003658ec9d98f6 83f0f763f9ef4d54b6d9e7bb40afea17 X a5199a9cedce4a9bba003658ec9d98f6--83f0f763f9ef4d54b6d9e7bb40afea17 83f0f763f9ef4d54b6d9e7bb40afea17--947bc7b7a1cf454ba778fcec9b28a82f adf6082540e941bf8dbba32068850bac 83f0f763f9ef4d54b6d9e7bb40afea17--adf6082540e941bf8dbba32068850bac 0d27e86af2604274bf5ee04b3e2bda28 RX(theta\u2081\u2085) adf6082540e941bf8dbba32068850bac--0d27e86af2604274bf5ee04b3e2bda28 d9aa6b2b40c340dc88196bd7b8f16031 RY(theta\u2081\u2089) 0d27e86af2604274bf5ee04b3e2bda28--d9aa6b2b40c340dc88196bd7b8f16031 6bf40fcbfc414c5e80dbab2b9f4a4ce1 RX(theta\u2082\u2083) d9aa6b2b40c340dc88196bd7b8f16031--6bf40fcbfc414c5e80dbab2b9f4a4ce1 084da2087c8f412dbe7f10f309f3880b X 6bf40fcbfc414c5e80dbab2b9f4a4ce1--084da2087c8f412dbe7f10f309f3880b 084da2087c8f412dbe7f10f309f3880b--a4bfaa4e187a4895996074cc8edd8dac 845ee614070149ada7ac1cec28ab9c2e 084da2087c8f412dbe7f10f309f3880b--845ee614070149ada7ac1cec28ab9c2e 137628e116484b5b81ff762cf0aae9df 845ee614070149ada7ac1cec28ab9c2e--137628e116484b5b81ff762cf0aae9df 137628e116484b5b81ff762cf0aae9df--16b5db84e7184803bba397a005976d06"},{"location":"tutorials/qml/dqc_1d/","title":"Solving a 1D ODE","text":"<p>In this tutorial we will show how to use Qadence to solve a basic 1D Ordinary Differential Equation (ODE) with a QNN using Differentiable Quantum Circuits (DQC) <sup>1</sup>.</p> <p>Consider the following non-linear ODE and boundary condition:</p> \\[ \\frac{df}{dx}= 5\\times(4x^3+x^2-2x-\\frac12), \\qquad f(0)=0 \\] <p>It admits an exact solution:</p> \\[ f(x)=5\\times(x^4+\\frac13x^3-x^2-\\frac12x) \\] <p>Our goal will be to find this solution for \\(x\\in[-1, 1]\\).</p> <pre><code>import torch\n\ndef dfdx_equation(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Derivative as per the equation.\"\"\"\n    return 5*(4*x**3 + x**2 - 2*x - 0.5)\n</code></pre> <p>For the purpose of this tutorial, we will compute the derivative of the circuit using <code>torch.autograd</code>. The point of the DQC algorithm is to use differentiable circuits with parameter shift rules. In Qadence, PSR is implemented directly as custom overrides of the derivative function in the autograd engine, and thus we can later change the derivative method for the model itself if we wish.</p> <pre><code>def calc_deriv(outputs: torch.Tensor, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute a derivative of model that learns f(x), computes df/dx using torch.autograd.\"\"\"\n    grad = torch.autograd.grad(\n        outputs=outputs,\n        inputs=inputs,\n        grad_outputs = torch.ones_like(inputs),\n        create_graph = True,\n        retain_graph = True,\n    )[0]\n    return grad\n</code></pre>"},{"location":"tutorials/qml/dqc_1d/#defining-the-loss-function","title":"Defining the loss function","text":"<p>The essential part of solving this problem is to define the right loss function to represent our goal. In this case, we want to define a model that has the capacity to learn the target solution, and we want to minimize: - The derivative of this model in comparison with the exact derivative in the equation; - The output of the model at the boundary in comparison with the value for the boundary condition;</p> <p>We can write it like so:</p> <pre><code># Mean-squared error as the comparison criterion\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Loss function encoding the problem to solve.\"\"\"\n    # Equation loss\n    model_output = model(inputs)\n    deriv_model = calc_deriv(model_output, inputs)\n    deriv_exact = dfdx_equation(inputs)\n    ode_loss = criterion(deriv_model, deriv_exact)\n\n    # Boundary loss, f(0) = 0\n    boundary_model = model(torch.tensor([[0.0]]))\n    boundary_exact = torch.tensor([[0.0]])\n    boundary_loss = criterion(boundary_model, boundary_exact)\n\n    return ode_loss + boundary_loss\n</code></pre> <p>Different loss criterions could be considered, and we could also play with the balance between the sum of the two loss terms. For now, let's proceed with the definition above.</p> <p>Note that so far we have not used any quantum specific assumption, and we could in principle use the same loss function with a classical neural network.</p>"},{"location":"tutorials/qml/dqc_1d/#defining-a-qnn-with-qadence","title":"Defining a QNN with Qadence","text":"<p>Now, we can finally use Qadence to write a QNN. We will use a feature map to encode the input values, a trainable ansatz circuit, and an observable to measure as the output.</p> <pre><code>from qadence import feature_map, hea, chain\nfrom qadence import QNN, QuantumCircuit, Z\nfrom qadence.types import BasisSet, ReuploadScaling\n\nn_qubits = 3\ndepth = 3\n\n# Feature map\nfm = feature_map(\n    n_qubits = n_qubits,\n    param = \"x\",\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.TOWER,\n)\n\n# Ansatz\nansatz = hea(n_qubits = n_qubits, depth = depth)\n\n# Observable\nobservable = Z(0)\n\ncircuit = QuantumCircuit(n_qubits, chain(fm, ansatz))\nmodel = QNN(circuit = circuit, observable = observable, inputs = [\"x\"])\n</code></pre> <p>We used a Chebyshev feature map with a tower-like scaling of the input reupload, and a standard hardware-efficient ansatz. You can check the qml constructors tutorial to see how you can customize these components. In the observable, for now we consider the simple case of measuring the magnetization of the first qubit.</p> <pre><code>from qadence.draw import display\n\n# display(circuit)\n</code></pre> %3 cluster_474b8253458444a7b0d80851878efce5 HEA cluster_aff673ef9cc0485a84d8ec3950f41e77 Tower Chebyshev FM 364706e01bef41a0b9a8c69aef0cb3f0 0 f969d1185e79409e942d6cdf3a5590bb RX(1.0*acos(x)) 364706e01bef41a0b9a8c69aef0cb3f0--f969d1185e79409e942d6cdf3a5590bb 6eb2953d7b4041acad7cbce65ee33faf 1 4d2688a458414e3b813d3bcfe921bde8 RX(theta\u2080) f969d1185e79409e942d6cdf3a5590bb--4d2688a458414e3b813d3bcfe921bde8 36a6334054614b909faa21b2f6c2dd9a RY(theta\u2083) 4d2688a458414e3b813d3bcfe921bde8--36a6334054614b909faa21b2f6c2dd9a 5a7a3bae2909468dab78494fa7a13567 RX(theta\u2086) 36a6334054614b909faa21b2f6c2dd9a--5a7a3bae2909468dab78494fa7a13567 c44c55e1632543c4a8bff344896dd9dc 5a7a3bae2909468dab78494fa7a13567--c44c55e1632543c4a8bff344896dd9dc 6773726c46e54509821305e1b6329a6e c44c55e1632543c4a8bff344896dd9dc--6773726c46e54509821305e1b6329a6e 6512fd60c1144fee8da6ca56cc6d69d3 RX(theta\u2089) 6773726c46e54509821305e1b6329a6e--6512fd60c1144fee8da6ca56cc6d69d3 2809cd7ac29c4552a6e66ddaf196de6f RY(theta\u2081\u2082) 6512fd60c1144fee8da6ca56cc6d69d3--2809cd7ac29c4552a6e66ddaf196de6f 8042a03d272343bc88a91b6627ea6f23 RX(theta\u2081\u2085) 2809cd7ac29c4552a6e66ddaf196de6f--8042a03d272343bc88a91b6627ea6f23 c4dd72a8e23841d8a5104dc500ce8bd1 8042a03d272343bc88a91b6627ea6f23--c4dd72a8e23841d8a5104dc500ce8bd1 61041b7ed7574116bd92447544f0e5dc c4dd72a8e23841d8a5104dc500ce8bd1--61041b7ed7574116bd92447544f0e5dc 55525c34784f400cbafdc9a715f2dec5 RX(theta\u2081\u2088) 61041b7ed7574116bd92447544f0e5dc--55525c34784f400cbafdc9a715f2dec5 4110ec7ecae0440a837506394d458b07 RY(theta\u2082\u2081) 55525c34784f400cbafdc9a715f2dec5--4110ec7ecae0440a837506394d458b07 0d10e30701bb429c84aa9975387c093e RX(theta\u2082\u2084) 4110ec7ecae0440a837506394d458b07--0d10e30701bb429c84aa9975387c093e 21a08bb235e84e39bdb0ff306078fd88 0d10e30701bb429c84aa9975387c093e--21a08bb235e84e39bdb0ff306078fd88 33a82d3f854f4aff8993975701e4d7e2 21a08bb235e84e39bdb0ff306078fd88--33a82d3f854f4aff8993975701e4d7e2 2480f3794283456d823694143b438185 33a82d3f854f4aff8993975701e4d7e2--2480f3794283456d823694143b438185 df6a5243b81c4aa89ec5fe66c84bdfd2 1deb565540fc40219cfa47cf04d3003b RX(2.0*acos(x)) 6eb2953d7b4041acad7cbce65ee33faf--1deb565540fc40219cfa47cf04d3003b f309a989b5ab45f69be79b1330e30ba9 2 d30c731cf4d94d9e908f31d34345d23e RX(theta\u2081) 1deb565540fc40219cfa47cf04d3003b--d30c731cf4d94d9e908f31d34345d23e 2dd2daef53a74633a6693a421d3d1a48 RY(theta\u2084) d30c731cf4d94d9e908f31d34345d23e--2dd2daef53a74633a6693a421d3d1a48 809bf9196b724b6e86c2417e08977348 RX(theta\u2087) 2dd2daef53a74633a6693a421d3d1a48--809bf9196b724b6e86c2417e08977348 4051b9b901c24c12a477033faea7275c X 809bf9196b724b6e86c2417e08977348--4051b9b901c24c12a477033faea7275c 4051b9b901c24c12a477033faea7275c--c44c55e1632543c4a8bff344896dd9dc 061d322397e84ee0917f23ffa4741914 4051b9b901c24c12a477033faea7275c--061d322397e84ee0917f23ffa4741914 77f15ccd40c9498da9d17612441db847 RX(theta\u2081\u2080) 061d322397e84ee0917f23ffa4741914--77f15ccd40c9498da9d17612441db847 f7bc2afa2cbb414f9686f9e80a3c32cf RY(theta\u2081\u2083) 77f15ccd40c9498da9d17612441db847--f7bc2afa2cbb414f9686f9e80a3c32cf 75ad156abc2643a9a9cc9ce3ccbac203 RX(theta\u2081\u2086) f7bc2afa2cbb414f9686f9e80a3c32cf--75ad156abc2643a9a9cc9ce3ccbac203 f298f63b4b204b6b9f4520168f04d426 X 75ad156abc2643a9a9cc9ce3ccbac203--f298f63b4b204b6b9f4520168f04d426 f298f63b4b204b6b9f4520168f04d426--c4dd72a8e23841d8a5104dc500ce8bd1 c434015c72e940958a33cb7272609f95 f298f63b4b204b6b9f4520168f04d426--c434015c72e940958a33cb7272609f95 45758cccbc934c229ae971c056bc1653 RX(theta\u2081\u2089) c434015c72e940958a33cb7272609f95--45758cccbc934c229ae971c056bc1653 8003dd69322b4ab2abb6966cca282320 RY(theta\u2082\u2082) 45758cccbc934c229ae971c056bc1653--8003dd69322b4ab2abb6966cca282320 f4980036b42c465cbf24b470bdb6c6b1 RX(theta\u2082\u2085) 8003dd69322b4ab2abb6966cca282320--f4980036b42c465cbf24b470bdb6c6b1 b02449dbdf934d57aa70213f455a71f5 X f4980036b42c465cbf24b470bdb6c6b1--b02449dbdf934d57aa70213f455a71f5 b02449dbdf934d57aa70213f455a71f5--21a08bb235e84e39bdb0ff306078fd88 590a5dfaf01b4e5e9ed877d3c5e48fcc b02449dbdf934d57aa70213f455a71f5--590a5dfaf01b4e5e9ed877d3c5e48fcc 590a5dfaf01b4e5e9ed877d3c5e48fcc--df6a5243b81c4aa89ec5fe66c84bdfd2 38f9fe1fb5c847fe9c90361b5bdc25bc f7991fadd28a4fc69aa6e0b820a1b087 RX(3.0*acos(x)) f309a989b5ab45f69be79b1330e30ba9--f7991fadd28a4fc69aa6e0b820a1b087 c1514c6dd3544950a6fb90aadcda0604 RX(theta\u2082) f7991fadd28a4fc69aa6e0b820a1b087--c1514c6dd3544950a6fb90aadcda0604 018523135d1f40df922fd4427a181430 RY(theta\u2085) c1514c6dd3544950a6fb90aadcda0604--018523135d1f40df922fd4427a181430 04586be679454365837251f4111055b3 RX(theta\u2088) 018523135d1f40df922fd4427a181430--04586be679454365837251f4111055b3 e6b4e2692ffa45ec84a8fed0ed95a679 04586be679454365837251f4111055b3--e6b4e2692ffa45ec84a8fed0ed95a679 2312be5e631b40f1a024bb7d9acf4527 X e6b4e2692ffa45ec84a8fed0ed95a679--2312be5e631b40f1a024bb7d9acf4527 2312be5e631b40f1a024bb7d9acf4527--061d322397e84ee0917f23ffa4741914 6766a37a697a4552b56130b00330b545 RX(theta\u2081\u2081) 2312be5e631b40f1a024bb7d9acf4527--6766a37a697a4552b56130b00330b545 5454864b8eac4d9c861e58e48b81244c RY(theta\u2081\u2084) 6766a37a697a4552b56130b00330b545--5454864b8eac4d9c861e58e48b81244c 925b575acfad456fbaf7b75f07c40942 RX(theta\u2081\u2087) 5454864b8eac4d9c861e58e48b81244c--925b575acfad456fbaf7b75f07c40942 c10f3b5da3f548d58cf8e65c60c77598 925b575acfad456fbaf7b75f07c40942--c10f3b5da3f548d58cf8e65c60c77598 7c45ca79ca9b40d2a6b6d2a005415844 X c10f3b5da3f548d58cf8e65c60c77598--7c45ca79ca9b40d2a6b6d2a005415844 7c45ca79ca9b40d2a6b6d2a005415844--c434015c72e940958a33cb7272609f95 4fffd8b6b0ec4ef282daaf7604f4549f RX(theta\u2082\u2080) 7c45ca79ca9b40d2a6b6d2a005415844--4fffd8b6b0ec4ef282daaf7604f4549f 24f11584f15c40e78dea83664e875dcb RY(theta\u2082\u2083) 4fffd8b6b0ec4ef282daaf7604f4549f--24f11584f15c40e78dea83664e875dcb bb5d347dee1943b9ab32bf2e9ff4c9e1 RX(theta\u2082\u2086) 24f11584f15c40e78dea83664e875dcb--bb5d347dee1943b9ab32bf2e9ff4c9e1 fdfd34fb10f441b3a7c3ec4cd7067d8c bb5d347dee1943b9ab32bf2e9ff4c9e1--fdfd34fb10f441b3a7c3ec4cd7067d8c 2c82c5c067f8403486e8ad165dde14a0 X fdfd34fb10f441b3a7c3ec4cd7067d8c--2c82c5c067f8403486e8ad165dde14a0 2c82c5c067f8403486e8ad165dde14a0--590a5dfaf01b4e5e9ed877d3c5e48fcc 2c82c5c067f8403486e8ad165dde14a0--38f9fe1fb5c847fe9c90361b5bdc25bc"},{"location":"tutorials/qml/dqc_1d/#training-the-model","title":"Training the model","text":"<p>Now that the model is defined we can proceed with the training. the <code>QNN</code> class can be used like any other <code>torch.nn.Module</code>. Here we write a simple training loop, but you can also look at the ml tools tutorial to use the convenience training functions that Qadence provides.</p> <p>To train the model, we will select a random set of collocation points uniformly distributed within \\(-1.0&lt; x &lt;1.0\\) and compute the loss function for those points.</p> <pre><code>n_epochs = 200\nn_points = 10\n\nxmin = -0.99\nxmax = 0.99\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Training data. We unsqueeze essentially making each batch have a single x value.\n    x_train = (xmin + (xmax-xmin)*torch.rand(n_points, requires_grad = True)).unsqueeze(1)\n\n    loss = loss_fn(inputs = x_train, model = model)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Note the values of \\(x\\) are only picked from \\(x\\in[-0.99, 0.99]\\) since we are using a Chebyshev feature map, and derivative of \\(\\text{acos}(x)\\) diverges at \\(-1\\) and \\(1\\).</p>"},{"location":"tutorials/qml/dqc_1d/#plotting-the-results","title":"Plotting the results","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef f_exact(x: torch.Tensor) -&gt; torch.Tensor:\n    return 5*(x**4 + (1/3)*x**3 - x**2 - 0.5*x)\n\nx_test = torch.arange(xmin, xmax, step = 0.01).unsqueeze(1)\n\nresult_exact = f_exact(x_test).flatten()\n\nresult_model = model(x_test).flatten().detach()\n\nplt.plot(x_test, result_exact, label = \"Exact solution\")\nplt.plot(x_test, result_model, label = \" Trained model\")\n</code></pre> 2024-12-05T14:32:13.961082 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/ <p>Clearly, the result is not optimal.</p>"},{"location":"tutorials/qml/dqc_1d/#improving-the-solution","title":"Improving the solution","text":"<p>One point to consider when defining the QNN is the possible output range, which is bounded by the spectrum of the chosen observable. For the magnetization of a single qubit, this means that the output is bounded between -1 and 1, which we can clearly see in the plot.</p> <p>One option would be to define the observable as the total magnetization over all qubits, which would allow a range of -3 to 3.</p> <pre><code>from qadence import add\n\nobservable = add(Z(i) for i in range(n_qubits))\n\nmodel = QNN(circuit = circuit, observable = observable, inputs = [\"x\"])\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Training data\n    x_train = (xmin + (xmax-xmin)*torch.rand(n_points, requires_grad = True)).unsqueeze(1)\n\n    loss = loss_fn(inputs = x_train, model = model)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>And we again plot the result:</p> <pre><code>x_test = torch.arange(xmin, xmax, step = 0.01).unsqueeze(1)\n\nresult_exact = f_exact(x_test).flatten()\n\nresult_model = model(x_test).flatten().detach()\n\nplt.plot(x_test, result_exact, label = \"Exact solution\")\nplt.plot(x_test, result_model, label = \"Trained model\")\n</code></pre> 2024-12-05T14:32:22.004647 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/qml/dqc_1d/#references","title":"References","text":"<ol> <li> <p>Kyriienko et al., Solving nonlinear differential equations with differentiable quantum circuits. \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/qaoa/","title":"Solving MaxCut with QAOA","text":"<p>This tutorial shows how to solve the maximum cut (MaxCut) combinatorial optimization problem on a graph using the Quantum Approximate Optimization Algorithm (QAOA), first introduced by Farhi et al. in 2014 <sup>1</sup>.</p> <p>Given an arbitrary graph, the MaxCut problem consists in finding a graph cut which partitions the nodes into two disjoint sets, such that the number of edges in the cut is maximized. This is a very common combinatorial optimization problem known to be computationally hard (NP-hard).</p> <p>The graph used for this tutorial is an unweighted graph randomly generated using the <code>networkx</code> library with a certain probability \\(p\\) of having an edge between two arbitrary nodes (known as Erd\u0151s\u2013R\u00e9nyi graph).</p> <pre><code>import numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport random\n\n# ensure reproducibility\nseed = 42\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Create random graph\nn_nodes = 4\nedge_prob = 0.8\ngraph = nx.gnp_random_graph(n_nodes, edge_prob)\n\nnx.draw(graph)\n</code></pre> 2024-12-05T14:32:22.104469 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/ <p>The goal of the MaxCut algorithm is to maximize the following cost function:</p> \\[\\mathcal{C}(p) = \\sum_{\\alpha}^m \\mathcal{C}_{\\alpha}(p)\\] <p>where \\(p\\) is a given cut of the graph, \\(\\alpha\\) is an index over the edges and \\(\\mathcal{C}_{\\alpha}(p)\\) is written such that if the nodes connected by the \\(\\alpha\\) edge are in the same set, it returns \\(0\\), otherwise it returns \\(1\\). We will represent a cut \\(p\\) as a bitstring of length \\(N\\), where \\(N\\) is the number of nodes, and where the bit in position \\(i\\) shows to which partition node \\(i\\) belongs. We assign value 0 to one of the partitions defined by the cut and 1 to the other. Since this choice is arbitrary, every cut is represented by two bitstrings, e.g. \"0011\" and \"1100\" are equivalent.</p> <p>Since in this tutorial we are only dealing with small graphs, we can find the maximum cut by brute force to make sure QAOA works as intended. <pre><code># Function to calculate the cost associated with a cut\ndef calculate_cost(cut: str, graph: nx.graph) -&gt; float:\n    \"\"\"Returns the cost of a given cut (represented by a bitstring)\"\"\"\n    cost = 0\n    for edge in graph.edges():\n        (i, j) = edge\n        if cut[i] != cut[j]:\n            cost += 1\n    return cost\n\n\n# Function to get a binary representation of an int\nget_binary = lambda x, n: format(x, \"b\").zfill(n)\n\n# List of all possible cuts\nall_possible_cuts = [bin(k)[2:].rjust(n_nodes, \"0\") for k in range(2**n_nodes)]\n\n# List with the costs associated to each cut\nall_costs = [calculate_cost(cut, graph) for cut in all_possible_cuts]\n\n# Get the maximum cost\nmaxcost = max(all_costs)\n\n# Get all cuts that correspond to the maximum cost\nmaxcuts = [get_binary(i, n_nodes) for i, j in enumerate(all_costs) if j == maxcost]\nprint(f\"The maximum cut is represented by the bitstrings {maxcuts}, with a cost of {maxcost}\")\n</code></pre> <pre><code>The maximum cut is represented by the bitstrings ['0011', '0101', '0110', '1001', '1010', '1100'], with a cost of 4\n</code></pre> </p>"},{"location":"tutorials/qml/qaoa/#the-qaoa-quantum-circuit","title":"The QAOA quantum circuit","text":"<p>The Max-Cut problem can be solved by using the QAOA algorithm. QAOA belongs to the class of Variational Quantum Algorithms (VQAs), which means that its quantum circuit contains a certain number of parametrized quantum gates that need to be optimized with a classical optimizer. The QAOA circuit is composed of two operators:</p> <ul> <li>The cost operator \\(U_c\\): a circuit generated by the cost Hamiltonian which encodes the cost function described above into a quantum circuit. The solution to the optimization problem is encoded in the ground state of the cost Hamiltonian \\(H_c\\). The cost operator  is simply the evolution of the cost Hamiltonian parametrized by a variational parameter \\(\\gamma\\) so that \\(U_c = e^{i\\gamma H_c}.\\)</li> <li>The mixing operator \\(U_b\\): a simple set of single-qubit rotations with adjustable   angles which are tuned during the classical optimization loop to minimize the cost</li> </ul> <p>The cost Hamiltonian of the MaxCut problem can be written as:</p> \\[H_c = \\frac12 \\sum_{\\langle i,j\\rangle} (\\mathbb{1} - Z_iZ_j)\\] <p>where \\(\\langle i,j\\rangle\\) represents the edge between nodes \\(i\\) and \\(j\\). The solution of the MaxCut problem is encoded in the ground state of the above Hamiltonian.</p> <p>The QAOA quantum circuit consists of a number of layers, each layer containing a cost and a mixing operator. Below, the QAOA quantum circuit is defined using <code>qadence</code> operations. First, a layer of Hadamard gates is applied to all qubits to prepare the initial state \\(|+\\rangle ^{\\otimes n}\\). The cost operator of each layer can be built \"manually\", implementing the \\(e^{iZZ\\gamma}\\) terms with CNOTs and a \\(\\rm{RZ}(2\\gamma)\\) rotation, or it can also be automatically decomposed into digital single and two-qubits operations via the <code>.digital_decomposition()</code> method. The decomposition is exact since the Hamiltonian generator is diagonal.</p> <pre><code>from qadence import tag, kron, chain, RX, RZ, Z, H, CNOT, I, add\nfrom qadence import HamEvo, QuantumCircuit, Parameter\n\nn_qubits = graph.number_of_nodes()\nn_edges = graph.number_of_edges()\nn_layers = 6\n\n# Generate the cost Hamiltonian\nzz_ops = add(Z(edge[0]) @ Z(edge[1]) for edge in graph.edges)\ncost_ham = 0.5 * (n_edges * kron(I(i) for i in range(n_qubits)) - zz_ops)\n\n\n# QAOA circuit\ndef build_qaoa_circuit(n_qubits, n_layers, graph):\n    layers = []\n    # Layer of Hadamards\n    initial_layer = kron(H(i) for i in range(n_qubits))\n    layers.append(initial_layer)\n    for layer in range(n_layers):\n\n        # cost layer with digital decomposition\n        # cost_layer = HamEvo(cost_ham, f\"g{layer}\").digital_decomposition(approximation=\"basic\")\n        cost_layer = []\n        for edge in graph.edges():\n            (q0, q1) = edge\n            zz_term = chain(\n                CNOT(q0, q1),\n                RZ(q1, Parameter(f\"g{layer}\")),\n                CNOT(q0, q1),\n            )\n            cost_layer.append(zz_term)\n        cost_layer = chain(*cost_layer)\n        cost_layer = tag(cost_layer, \"cost\")\n\n        # mixing layer with single qubit rotations\n        mixing_layer = kron(RX(i, f\"b{layer}\") for i in range(n_qubits))\n        mixing_layer = tag(mixing_layer, \"mixing\")\n\n        # putting all together in a single ChainBlock\n        layers.append(chain(cost_layer, mixing_layer))\n\n    final_b = chain(*layers)\n    return QuantumCircuit(n_qubits, final_b)\n\n\ncircuit = build_qaoa_circuit(n_qubits, n_layers, graph)\n\n# Print a single layer of the circuit\n</code></pre> %3 cluster_4477ef2ab94249c6b1a41950a3524ffc mixing cluster_59e0f1cc203c4c76834d561cfbc76e85 cost 71c507543e3d475da10d1a303ab72c2f 0 98ba19ed52654755beb0310eeb3cc213 H 71c507543e3d475da10d1a303ab72c2f--98ba19ed52654755beb0310eeb3cc213 a94eeaa82fb94e2d87bbfd0ec8bceb14 1 ad636b67b3194e30800c72d9764e0a31 98ba19ed52654755beb0310eeb3cc213--ad636b67b3194e30800c72d9764e0a31 200bedd13ec34a43816a6fe064884a5f ad636b67b3194e30800c72d9764e0a31--200bedd13ec34a43816a6fe064884a5f 4b69111b68ad4e30b1f2b89827b4b2b7 200bedd13ec34a43816a6fe064884a5f--4b69111b68ad4e30b1f2b89827b4b2b7 662868f1553a495699ac1b14d7a0d528 4b69111b68ad4e30b1f2b89827b4b2b7--662868f1553a495699ac1b14d7a0d528 7081620bd25f40b5969cb45374a9e68a 662868f1553a495699ac1b14d7a0d528--7081620bd25f40b5969cb45374a9e68a 6a8eaad4528f47a28884de0b800adf61 7081620bd25f40b5969cb45374a9e68a--6a8eaad4528f47a28884de0b800adf61 2bf538458c58456abb6b25d907674d53 6a8eaad4528f47a28884de0b800adf61--2bf538458c58456abb6b25d907674d53 a86b9bba1f5646e7abe36847f8a46bc6 2bf538458c58456abb6b25d907674d53--a86b9bba1f5646e7abe36847f8a46bc6 e6a68babfc354d3e8d9319fa89c2b8ff a86b9bba1f5646e7abe36847f8a46bc6--e6a68babfc354d3e8d9319fa89c2b8ff c6607f13945b4e97b1a461f4dd19ca46 e6a68babfc354d3e8d9319fa89c2b8ff--c6607f13945b4e97b1a461f4dd19ca46 c7e11ad9987b4f84b0399f8221d0e50a c6607f13945b4e97b1a461f4dd19ca46--c7e11ad9987b4f84b0399f8221d0e50a 8335afa86e124db1ad30bc169a31c127 c7e11ad9987b4f84b0399f8221d0e50a--8335afa86e124db1ad30bc169a31c127 8d827089e9464472a79c8c3c96e12c40 8335afa86e124db1ad30bc169a31c127--8d827089e9464472a79c8c3c96e12c40 ab26b19ed95642faa13c1d63f1c266c4 8d827089e9464472a79c8c3c96e12c40--ab26b19ed95642faa13c1d63f1c266c4 7855580b115f412ea29a91542ba64f7c ab26b19ed95642faa13c1d63f1c266c4--7855580b115f412ea29a91542ba64f7c 41b66ef471c04337a52977d25ad948e7 7855580b115f412ea29a91542ba64f7c--41b66ef471c04337a52977d25ad948e7 904b528679ed47339d9c662c7d923140 41b66ef471c04337a52977d25ad948e7--904b528679ed47339d9c662c7d923140 097d6bbfd57f46e28fae31debff9e7e3 904b528679ed47339d9c662c7d923140--097d6bbfd57f46e28fae31debff9e7e3 42c7b44320d6446d874360ccc8f0539a RX(b0) 097d6bbfd57f46e28fae31debff9e7e3--42c7b44320d6446d874360ccc8f0539a 0c1d600025354f6aa28eadf7e1f8e89a 42c7b44320d6446d874360ccc8f0539a--0c1d600025354f6aa28eadf7e1f8e89a 2ee93cb3432f40c693c2ce45aa678968 ee1e1761fdca472f8e1256799147b33d H a94eeaa82fb94e2d87bbfd0ec8bceb14--ee1e1761fdca472f8e1256799147b33d 4ca4abde60be4972b2ccf186c24973cb 2 3ea38e4ac21d48629638eb7adc08739d X ee1e1761fdca472f8e1256799147b33d--3ea38e4ac21d48629638eb7adc08739d 3ea38e4ac21d48629638eb7adc08739d--ad636b67b3194e30800c72d9764e0a31 923cfe7ddde945769f83f9e9e589a56c RZ(g0) 3ea38e4ac21d48629638eb7adc08739d--923cfe7ddde945769f83f9e9e589a56c 0071e9ed0a08407ba1250ed076fac682 X 923cfe7ddde945769f83f9e9e589a56c--0071e9ed0a08407ba1250ed076fac682 0071e9ed0a08407ba1250ed076fac682--4b69111b68ad4e30b1f2b89827b4b2b7 88ae4791353543faa92a02475d0ea873 0071e9ed0a08407ba1250ed076fac682--88ae4791353543faa92a02475d0ea873 a390d5339cb1497284493f79082fccb5 88ae4791353543faa92a02475d0ea873--a390d5339cb1497284493f79082fccb5 bd6d6f7695ca48df87ceca0d27aab128 a390d5339cb1497284493f79082fccb5--bd6d6f7695ca48df87ceca0d27aab128 182b861dbcc640f8bccbefc8ab610134 bd6d6f7695ca48df87ceca0d27aab128--182b861dbcc640f8bccbefc8ab610134 2aaf3d84134449fbbccd29f74df5b820 182b861dbcc640f8bccbefc8ab610134--2aaf3d84134449fbbccd29f74df5b820 64f2bf6cf3f347fd8072e1f083eb870f 2aaf3d84134449fbbccd29f74df5b820--64f2bf6cf3f347fd8072e1f083eb870f 12c03cc6a03345f0a06b97d4a60523ba 64f2bf6cf3f347fd8072e1f083eb870f--12c03cc6a03345f0a06b97d4a60523ba 78c24287ac3e4b6bbe8517bba5fa315a 12c03cc6a03345f0a06b97d4a60523ba--78c24287ac3e4b6bbe8517bba5fa315a 61f5151599754edd893752e3e4e72983 78c24287ac3e4b6bbe8517bba5fa315a--61f5151599754edd893752e3e4e72983 cc2fd8b0c8504ee5b5731917fbf7e2ae 61f5151599754edd893752e3e4e72983--cc2fd8b0c8504ee5b5731917fbf7e2ae 01a6b058ee04425fa887ef7469bc6b1d cc2fd8b0c8504ee5b5731917fbf7e2ae--01a6b058ee04425fa887ef7469bc6b1d 5c1d432059f14e138620357a252ea4d6 01a6b058ee04425fa887ef7469bc6b1d--5c1d432059f14e138620357a252ea4d6 5a81fbdc088645a5ab516996330173e1 5c1d432059f14e138620357a252ea4d6--5a81fbdc088645a5ab516996330173e1 31a6ac2d244245ca9aaa05e14132f529 5a81fbdc088645a5ab516996330173e1--31a6ac2d244245ca9aaa05e14132f529 d6012046fb9343008a50e33404c92880 31a6ac2d244245ca9aaa05e14132f529--d6012046fb9343008a50e33404c92880 0cc477225d3e4954a655ba69788d8251 RX(b0) d6012046fb9343008a50e33404c92880--0cc477225d3e4954a655ba69788d8251 0cc477225d3e4954a655ba69788d8251--2ee93cb3432f40c693c2ce45aa678968 cc1b9b1777654edea42125aa4b21e855 06a25eecef6f4b65bda485fdd7d323d2 H 4ca4abde60be4972b2ccf186c24973cb--06a25eecef6f4b65bda485fdd7d323d2 8acf3ad068134b298adc5448b4c0261b 3 26458160725f4fa6b1c355dc8ce99ac5 06a25eecef6f4b65bda485fdd7d323d2--26458160725f4fa6b1c355dc8ce99ac5 cc2c81a994e6487d94482eea17d4ae6c 26458160725f4fa6b1c355dc8ce99ac5--cc2c81a994e6487d94482eea17d4ae6c c91ab6aadece4a7498ea87769e6ca65b cc2c81a994e6487d94482eea17d4ae6c--c91ab6aadece4a7498ea87769e6ca65b 6f43e59aaaf945f89c644438c7373ec3 X c91ab6aadece4a7498ea87769e6ca65b--6f43e59aaaf945f89c644438c7373ec3 6f43e59aaaf945f89c644438c7373ec3--662868f1553a495699ac1b14d7a0d528 973d2fdcd6954a69b2b79770208b5b06 RZ(g0) 6f43e59aaaf945f89c644438c7373ec3--973d2fdcd6954a69b2b79770208b5b06 1b4a4fe00ba248e98223299817a65a9c X 973d2fdcd6954a69b2b79770208b5b06--1b4a4fe00ba248e98223299817a65a9c 1b4a4fe00ba248e98223299817a65a9c--6a8eaad4528f47a28884de0b800adf61 585d88c0e37848c184cc73308ce5bbd2 1b4a4fe00ba248e98223299817a65a9c--585d88c0e37848c184cc73308ce5bbd2 8694f59ac3e1441984cfbab25d6a43f3 585d88c0e37848c184cc73308ce5bbd2--8694f59ac3e1441984cfbab25d6a43f3 4867690b94bd41bdb5c55025644663e5 8694f59ac3e1441984cfbab25d6a43f3--4867690b94bd41bdb5c55025644663e5 21310d3f08974c32acabee58f690a80e X 4867690b94bd41bdb5c55025644663e5--21310d3f08974c32acabee58f690a80e 21310d3f08974c32acabee58f690a80e--12c03cc6a03345f0a06b97d4a60523ba 4c5de938705648d1b7549ae5f52c3dd0 RZ(g0) 21310d3f08974c32acabee58f690a80e--4c5de938705648d1b7549ae5f52c3dd0 6937f07eaa764b3083dfa9d9d015555b X 4c5de938705648d1b7549ae5f52c3dd0--6937f07eaa764b3083dfa9d9d015555b 6937f07eaa764b3083dfa9d9d015555b--61f5151599754edd893752e3e4e72983 686b1e0c00ee4de08d862e286f58ed27 6937f07eaa764b3083dfa9d9d015555b--686b1e0c00ee4de08d862e286f58ed27 e5d202a7767045ef888653ee70469299 686b1e0c00ee4de08d862e286f58ed27--e5d202a7767045ef888653ee70469299 28c21281caf543da8272860871bbd84d e5d202a7767045ef888653ee70469299--28c21281caf543da8272860871bbd84d 575b3c24ea4247c3b44854d4abd239cd 28c21281caf543da8272860871bbd84d--575b3c24ea4247c3b44854d4abd239cd c920e1fbecdf4471893c63397188c2e0 575b3c24ea4247c3b44854d4abd239cd--c920e1fbecdf4471893c63397188c2e0 d75fed1998ef419fa04b4b472fde925b c920e1fbecdf4471893c63397188c2e0--d75fed1998ef419fa04b4b472fde925b 31f7f88a6197452280eac98d1af337a7 RX(b0) d75fed1998ef419fa04b4b472fde925b--31f7f88a6197452280eac98d1af337a7 31f7f88a6197452280eac98d1af337a7--cc1b9b1777654edea42125aa4b21e855 fb98055e34f1423c9f64ec3dea172553 4d3a8eaf3a5844d0b6fc59ca37d0e05c H 8acf3ad068134b298adc5448b4c0261b--4d3a8eaf3a5844d0b6fc59ca37d0e05c c967328994ed42f8aebd8fe06d4e47b9 4d3a8eaf3a5844d0b6fc59ca37d0e05c--c967328994ed42f8aebd8fe06d4e47b9 32b99b575dca4828bb7cc0971fa9d108 c967328994ed42f8aebd8fe06d4e47b9--32b99b575dca4828bb7cc0971fa9d108 aa61fac26853483c9a3f2db13a8ca527 32b99b575dca4828bb7cc0971fa9d108--aa61fac26853483c9a3f2db13a8ca527 6329647f7ded4ccdadf47027713dea08 aa61fac26853483c9a3f2db13a8ca527--6329647f7ded4ccdadf47027713dea08 e553ca96e9fe43f8a537bd89567edc60 6329647f7ded4ccdadf47027713dea08--e553ca96e9fe43f8a537bd89567edc60 b25149e045244f708450c82be2a8bd29 e553ca96e9fe43f8a537bd89567edc60--b25149e045244f708450c82be2a8bd29 c578830d53554c0c80d504348cea7004 X b25149e045244f708450c82be2a8bd29--c578830d53554c0c80d504348cea7004 c578830d53554c0c80d504348cea7004--2bf538458c58456abb6b25d907674d53 76a6cd53d0764d668db2681ccd49bc23 RZ(g0) c578830d53554c0c80d504348cea7004--76a6cd53d0764d668db2681ccd49bc23 5ee3fea7e7324d6fbfc70e8061dea9bc X 76a6cd53d0764d668db2681ccd49bc23--5ee3fea7e7324d6fbfc70e8061dea9bc 5ee3fea7e7324d6fbfc70e8061dea9bc--e6a68babfc354d3e8d9319fa89c2b8ff 64371e4a17eb48c8995f9acdbd1daebf 5ee3fea7e7324d6fbfc70e8061dea9bc--64371e4a17eb48c8995f9acdbd1daebf 39d4cdff3b77476e9e5e9a1bfe93fce3 64371e4a17eb48c8995f9acdbd1daebf--39d4cdff3b77476e9e5e9a1bfe93fce3 b85abfcecec04591a4411fc13ef99171 39d4cdff3b77476e9e5e9a1bfe93fce3--b85abfcecec04591a4411fc13ef99171 9e2510df0e53491d88e6555abf2bb0f8 X b85abfcecec04591a4411fc13ef99171--9e2510df0e53491d88e6555abf2bb0f8 9e2510df0e53491d88e6555abf2bb0f8--cc2fd8b0c8504ee5b5731917fbf7e2ae 2f9fe9caeebf474999d3420e80df38fe RZ(g0) 9e2510df0e53491d88e6555abf2bb0f8--2f9fe9caeebf474999d3420e80df38fe 56291c6e3f934194aa68e802ca86688c X 2f9fe9caeebf474999d3420e80df38fe--56291c6e3f934194aa68e802ca86688c 56291c6e3f934194aa68e802ca86688c--5c1d432059f14e138620357a252ea4d6 e52829ff928144cb9a5c9ba034dcf42d X 56291c6e3f934194aa68e802ca86688c--e52829ff928144cb9a5c9ba034dcf42d e52829ff928144cb9a5c9ba034dcf42d--575b3c24ea4247c3b44854d4abd239cd 05de1dc935784db487af07373bdbc24e RZ(g0) e52829ff928144cb9a5c9ba034dcf42d--05de1dc935784db487af07373bdbc24e 4398553943e24d99ba5c6a5c956f08bd X 05de1dc935784db487af07373bdbc24e--4398553943e24d99ba5c6a5c956f08bd 4398553943e24d99ba5c6a5c956f08bd--d75fed1998ef419fa04b4b472fde925b acfec628a03846639de3df2d35f2b672 RX(b0) 4398553943e24d99ba5c6a5c956f08bd--acfec628a03846639de3df2d35f2b672 acfec628a03846639de3df2d35f2b672--fb98055e34f1423c9f64ec3dea172553"},{"location":"tutorials/qml/qaoa/#train-the-qaoa-circuit-to-solve-maxcut","title":"Train the QAOA circuit to solve MaxCut","text":"<p>Given the QAOA circuit above, one can construct the associated Qadence <code>QuantumModel</code> and train it using standard gradient based optimization.</p> <p>The loss function to be minimized reads:</p> \\[\\mathcal{L} =-\\langle \\psi | H_c| \\psi \\rangle= -\\frac12 \\sum_{\\langle i,j\\rangle}  \\left(1 - \\langle \\psi | Z_i Z_j | \\psi \\rangle \\right)\\] <p>where \\(|\\psi\\rangle(\\beta, \\gamma)\\) is the wavefunction obtained by running the QAQA quantum circuit and the sum runs over the edges of the graph \\(\\langle i,j\\rangle\\).</p> <pre><code>import torch\nfrom qadence import QuantumModel\n\ntorch.manual_seed(seed)\n\n\ndef loss_function(model: QuantumModel):\n    # The loss corresponds to the expectation\n    # value of the cost Hamiltonian\n    return -1.0 * model.expectation().squeeze()\n\n\n# initialize the parameters to random values\nmodel = QuantumModel(circuit, observable=cost_ham)\nmodel.reset_vparams(torch.rand(model.num_vparams))\ninitial_loss = loss_function(model)\nprint(f\"Initial loss: {initial_loss}\")\n\n# train the model\nn_epochs = 100\nlr = 0.1\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_function(model)\n    loss.backward()\n    optimizer.step()\n    if (i + 1) % (n_epochs // 10) == 0:\n        print(f\"MaxCut cost at iteration {i+1}: {-loss.item()}\")\n</code></pre> <pre><code>Initial loss: -2.1782381363858794\nMaxCut cost at iteration 10: 3.7470706807026417\nMaxCut cost at iteration 20: 3.8378810288930216\nMaxCut cost at iteration 30: 3.9424197899236133\nMaxCut cost at iteration 40: 3.9981256255766002\nMaxCut cost at iteration 50: 3.996470528508214\nMaxCut cost at iteration 60: 3.9991374608876606\nMaxCut cost at iteration 70: 3.9994678542919555\nMaxCut cost at iteration 80: 3.999872558672829\nMaxCut cost at iteration 90: 3.9999475834121063\nMaxCut cost at iteration 100: 3.9999793311641003\n</code></pre> <p>Qadence offers some convenience functions to implement this training loop with advanced logging and metrics track features. You can refer to this tutorial for more details.</p>"},{"location":"tutorials/qml/qaoa/#results","title":"Results","text":"<p>Given the trained quantum model, one needs to sample the resulting quantum state to recover the bitstring with the highest probability which corresponds to the maximum cut of the graph.</p> <pre><code>samples = model.sample(n_shots=100)[0]\nmost_frequent = max(samples, key=samples.get)\n\nprint(f\"Most frequently sampled bitstring corresponding to the maximum cut: {most_frequent}\")\n\n# let's now draw the cut obtained with the QAOA procedure\ncolors = []\nlabels = {}\nfor node, b in zip(graph.nodes(), most_frequent):\n    colors.append(\"green\") if int(b) == 0 else colors.append(\"red\")\n    labels[node] = \"A\" if int(b) == 0 else \"B\"\n\nnx.draw_networkx(graph, node_color=colors, with_labels=True, labels=labels)\n</code></pre>   Most frequently sampled bitstring corresponding to the maximum cut: 1001  2024-12-05T14:32:26.679793 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/qml/qaoa/#references","title":"References","text":"<ol> <li> <p>Farhi et al. - A Quantum Approximate Optimization Algorithm\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/qml/qcl/","title":"Quantum circuit learning","text":"<p>This tutorial shows how to apply <code>qadence</code> for solving a basic quantum machine learning application: fitting a simple function with the quantum circuit learning<sup>1</sup> (QCL) algorithm.</p> <p>QCL is a supervised quantum machine learning algorithm that uses a parametrized quantum neural network to learn the behavior of an arbitrary mathematical function using a set of function values as training data. This tutorial shows how to fit the \\(\\sin(x)\\) function in the \\([-1, 1]\\) domain.</p> <p>In the following, train and test data are defined.</p> <pre><code>import torch\nfrom torch.utils.data import random_split\n\n# make sure all tensors are kept on the same device\n# only available from PyTorch 2.0\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.set_default_device(device)\n\ndef qcl_training_data(\n    domain: tuple = (0, 2*torch.pi), n_points: int = 200\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n\n    start, end = domain\n\n    x_rand, _ = torch.sort(torch.DoubleTensor(n_points).uniform_(start, end))\n    y_rand = torch.sin(x_rand)\n\n    return x_rand, y_rand\n\nx, y = qcl_training_data()\n\n# random train/test split of the dataset\ntrain_subset, test_subset = random_split(x, [0.75, 0.25])\ntrain_ind = sorted(train_subset.indices)\ntest_ind = sorted(test_subset.indices)\n\nx_train, y_train = x[train_ind], y[train_ind]\nx_test, y_test = x[test_ind], y[test_ind]\n</code></pre>"},{"location":"tutorials/qml/qcl/#train-the-qcl-model","title":"Train the QCL model","text":"<p>Qadence provides the <code>QNN</code> convenience constructor to build a quantum neural network. The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>Total qubit magnetization is used as observable:</p> \\[ \\hat{O} = \\sum_i^N \\hat{\\sigma}_i^z \\] <p>In the following the observable, quantum circuit and corresponding QNN model are constructed.</p> <pre><code>import qadence as qd\n\nn_qubits = 4\n\n# create a simple feature map to encode the input data\nfeature_param = qd.FeatureParameter(\"phi\")\nfeature_map = qd.kron(qd.RX(i, feature_param) for i in range(n_qubits))\nfeature_map = qd.tag(feature_map, \"feature_map\")\n\n# create a digital-analog variational ansatz using Qadence convenience constructors\nansatz = qd.hea(n_qubits, depth=n_qubits)\nansatz = qd.tag(ansatz, \"ansatz\")\n\n# total qubit magnetization observable\nobservable = qd.hamiltonian_factory(n_qubits, detuning=qd.Z)\n\ncircuit = qd.QuantumCircuit(n_qubits, feature_map, ansatz)\nmodel = qd.QNN(circuit, [observable])\nexpval = model(values=torch.rand(10))\n</code></pre> <pre><code>tensor([[ 0.0092],\n        [-0.0207],\n        [-0.4616],\n        [-0.1845],\n        [-0.3761],\n        [-0.1463],\n        [-0.4563],\n        [-0.0016],\n        [-0.3436],\n        [-0.4252]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> <p>The QCL algorithm uses the output of the quantum neural network as a tunable universal function approximator. Standard PyTorch code is used for training the QNN using a mean-square error loss, Adam optimizer. Training is performend on the GPU if available:</p> <pre><code>n_epochs = 100\nlr = 0.25\n\ninput_values = {\"phi\": x_train}\nmse_loss = torch.nn.MSELoss()  # standard PyTorch loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)  # standard PyTorch Adam optimizer\n\nprint(f\"Initial loss: {mse_loss(model(values=x_train), y_train)}\")\ny_pred_initial = model(values=x_test)\n\nfor i in range(n_epochs):\n\n    optimizer.zero_grad()\n\n    # given a `n_batch` number of input points and a `n_observables`\n    # number of input observables to measure, the QNN returns\n    # an output of the following shape: [n_batch x n_observables]\n    # given that there is only one observable, a squeeze is applied to get\n    # a 1-dimensional tensor\n    loss = mse_loss(model(values=x_train).squeeze(), y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (i+1) % 20 == 0:\n        print(f\"Epoch {i+1} - Loss: {loss.item()}\")\n\nassert loss.item() &lt; 1e-3\n</code></pre> <pre><code>Initial loss: 0.6272721767455237\nEpoch 20 - Loss: 0.008173087377230498\nEpoch 40 - Loss: 0.0011247726222838813\nEpoch 60 - Loss: 0.0001415308609619855\nEpoch 80 - Loss: 2.3606578815826947e-05\nEpoch 100 - Loss: 2.503287372853267e-06\n</code></pre> <p>Qadence offers some convenience functions to implement this training loop with advanced logging and metrics track features. You can refer to this tutorial for more details.</p> <p>The quantum model is now trained on the training data points. To determine the quality of the results, one can check to see how well it fits the function on the test set.</p> <pre><code>import matplotlib.pyplot as plt\n\ny_pred = model({\"phi\": x_test})\n\n# convert all the results to numpy arrays for plotting\nx_train_np = x_train.cpu().detach().numpy().flatten()\ny_train_np = y_train.cpu().detach().numpy().flatten()\nx_test_np = x_test.cpu().detach().numpy().flatten()\ny_test_np = y_test.cpu().detach().numpy().flatten()\ny_pred_initial_np = y_pred_initial.cpu().detach().numpy().flatten()\ny_pred_np = y_pred.cpu().detach().numpy().flatten()\n\nfig, _ = plt.subplots()\nplt.scatter(x_test_np, y_test_np, label=\"Test points\", marker=\"o\", color=\"orange\")\nplt.plot(x_test_np, y_pred_initial_np, label=\"Initial prediction\", color=\"green\", alpha=0.5)\nplt.plot(x_test_np, y_pred_np, label=\"Final prediction\")\nplt.legend()\n</code></pre> 2024-12-05T14:32:31.219078 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/qml/qcl/#references","title":"References","text":"<ol> <li> <p>Mitarai et al., Quantum Circuit Learning \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/ml_tools/callbacks/","title":"Callbacks for Trainer","text":"<p>Qadence <code>ml_tools</code> provides a powerful callback system for customizing various stages of the training process. With callbacks, you can monitor, log, save, and alter your training workflow efficiently. A <code>CallbackManager</code> is used with <code>Trainer</code> to execute the training process with defined callbacks. Following default callbacks are already provided in the <code>Trainer</code>.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#default-callbacks","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks already implemented in the <code>CallbackManager</code> used with <code>Trainer</code>:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>This guide covers how to define and use callbacks in <code>TrainConfig</code>, integrate them with the <code>Trainer</code> class, and create custom callbacks using hooks.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#1-built-in-callbacks","title":"1. Built-in Callbacks","text":"<p>Qadence ml_tools offers several built-in callbacks for common tasks like saving checkpoints, logging metrics, and tracking models. Below is an overview of each.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#11-printmetrics","title":"1.1. <code>PrintMetrics</code>","text":"<p>Prints metrics at specified intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\nprint_metrics_callback = PrintMetrics(on=\"val_batch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[print_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#12-writemetrics","title":"1.2. <code>WriteMetrics</code>","text":"<p>Writes metrics to a specified logging destination.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import WriteMetrics\n\nwrite_metrics_callback = WriteMetrics(on=\"train_epoch_end\", called_every=50)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[write_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#13-plotmetrics","title":"1.3. <code>PlotMetrics</code>","text":"<p>Plots metrics based on user-defined plotting functions.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PlotMetrics\n\nplot_metrics_callback = PlotMetrics(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[plot_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#14-loghyperparameters","title":"1.4. <code>LogHyperparameters</code>","text":"<p>Logs hyperparameters to keep track of training settings.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogHyperparameters\n\nlog_hyper_callback = LogHyperparameters(on=\"train_start\", called_every=1)\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_hyper_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#15-savecheckpoint","title":"1.5. <code>SaveCheckpoint</code>","text":"<p>Saves model checkpoints at specified intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint\n\nsave_checkpoint_callback = SaveCheckpoint(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#16-savebestcheckpoint","title":"1.6. <code>SaveBestCheckpoint</code>","text":"<p>Saves the best model checkpoint based on a validation criterion.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveBestCheckpoint\n\nsave_best_checkpoint_callback = SaveBestCheckpoint(on=\"val_epoch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_best_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#17-loadcheckpoint","title":"1.7. <code>LoadCheckpoint</code>","text":"<p>Loads a saved model checkpoint at the start of training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LoadCheckpoint\n\nload_checkpoint_callback = LoadCheckpoint(on=\"train_start\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[load_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#18-logmodeltracker","title":"1.8. <code>LogModelTracker</code>","text":"<p>Logs the model structure and parameters.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogModelTracker\n\nlog_model_callback = LogModelTracker(on=\"train_end\")\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_model_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#2-custom-callbacks","title":"2. Custom Callbacks","text":"<p>The base <code>Callback</code> class in Qadence allows defining custom behavior that can be triggered at specified events (e.g., start of training, end of epoch). You can set parameters such as when the callback runs (<code>on</code>), frequency of execution (<code>called_every</code>), and optionally define a <code>callback_condition</code>.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#defining-callbacks","title":"Defining Callbacks","text":"<p>There are two main ways to define a callback: 1. Directly providing a function in the <code>Callback</code> instance. 2. Subclassing the <code>Callback</code> class and implementing custom logic.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-1-providing-a-callback-function-directly","title":"Example 1: Providing a Callback Function Directly","text":"<pre><code>from qadence.ml_tools.callbacks import Callback\n\n# Define a custom callback function\ndef custom_callback_function(trainer, config, writer):\n    print(\"Executing custom callback.\")\n\n# Create the callback instance\ncustom_callback = Callback(\n    on=\"train_end\",\n    callback=custom_callback_function\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-2-subclassing-the-callback","title":"Example 2: Subclassing the Callback","text":"<pre><code>from qadence.ml_tools.callbacks import Callback\n\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in run_callback method.\")\n\n# Create the subclassed callback instance\ncustom_callback = CustomCallback(on=\"train_batch_end\", called_every=10)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#3-adding-callbacks-to-trainconfig","title":"3. Adding Callbacks to <code>TrainConfig</code>","text":"<p>To use callbacks in <code>TrainConfig</code>, add them to the <code>callbacks</code> list when configuring the training process.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint, PrintMetrics\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[\n        SaveCheckpoint(on=\"val_epoch_end\", called_every=50),\n        PrintMetrics(on=\"train_epoch_end\", called_every=100),\n    ]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#4-using-callbacks-with-trainer","title":"4. Using Callbacks with <code>Trainer</code>","text":"<p>The <code>Trainer</code> class in <code>qadence.ml_tools</code> provides built-in support for executing callbacks at various stages in the training process, managed through a callback manager. By default, several callbacks are added to specific hooks to automate common tasks, such as check-pointing, metric logging, and model tracking.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#default-callbacks_1","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks and their assigned hooks:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>These defaults handle common needs, but you can also add custom callbacks to any hook.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-adding-a-custom-callback","title":"Example: Adding a Custom Callback","text":"<p>To create a custom <code>Trainer</code> that includes a <code>PrintMetrics</code> callback executed specifically at the end of each epoch, follow the steps below.</p> <pre><code>from qadence.ml_tools.trainer import Trainer\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.print_metrics_callback = PrintMetrics(on=\"train_epoch_end\", called_every = 10)\n\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        self.print_metrics_callback.run_callback(self)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/data_and_config/","title":"Data and Configurations","text":""},{"location":"tutorials/qml/ml_tools/data_and_config/#1-dataloaders","title":"1. Dataloaders","text":"<p>When using Qadence, you can supply classical data to a quantum machine learning algorithm by using a standard PyTorch <code>DataLoader</code> instance. Qadence also provides the <code>DictDataLoader</code> convenience class which allows to build dictionaries of <code>DataLoader</code>s instances and easily iterate over them.</p> <pre><code>import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom qadence.ml_tools import DictDataLoader, to_dataloader\n\n\ndef dataloader(data_size: int = 25, batch_size: int = 5, infinite: bool = False) -&gt; DataLoader:\n    x = torch.linspace(0, 1, data_size).reshape(-1, 1)\n    y = torch.sin(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=infinite)\n\n\ndef dictdataloader(data_size: int = 25, batch_size: int = 5) -&gt; DictDataLoader:\n    dls = {}\n    for k in [\"y1\", \"y2\"]:\n        x = torch.rand(data_size, 1)\n        y = torch.sin(x)\n        dls[k] = to_dataloader(x, y, batch_size=batch_size, infinite=True)\n    return DictDataLoader(dls)\n\n\n# iterate over standard DataLoader\nfor (x,y) in dataloader(data_size=6, batch_size=2):\n    print(f\"Standard {x = }\")\n\n# construct an infinite dataset which will keep sampling indefinitely\nn_epochs = 5\ndl = iter(dataloader(data_size=6, batch_size=2, infinite=True))\nfor _ in range(n_epochs):\n    (x, y) = next(dl)\n    print(f\"Infinite {x = }\")\n\n# iterate over DictDataLoader\nddl = dictdataloader()\ndata = next(iter(ddl))\nprint(f\"{data = }\")\n</code></pre> <pre><code>Standard x = tensor([[0.0000],\n        [0.2000]])\nStandard x = tensor([[0.4000],\n        [0.6000]])\nStandard x = tensor([[0.8000],\n        [1.0000]])\nInfinite x = tensor([[0.2000],\n        [0.4000]])\nInfinite x = tensor([[1.0000],\n        [0.6000]])\nInfinite x = tensor([[0.0000],\n        [0.8000]])\nInfinite x = tensor([[0.2000],\n        [0.4000]])\nInfinite x = tensor([[1.0000],\n        [0.6000]])\ndata = {'y1': [tensor([[0.4409],\n        [0.0250],\n        [0.3490],\n        [0.3025],\n        [0.4626]]), tensor([[0.4267],\n        [0.0250],\n        [0.3420],\n        [0.2979],\n        [0.4463]])], 'y2': [tensor([[0.4027],\n        [0.4736],\n        [0.4973],\n        [0.2373],\n        [0.9213]]), tensor([[0.3919],\n        [0.4561],\n        [0.4770],\n        [0.2350],\n        [0.7964]])]}\n</code></pre> <p>Note:     In case of <code>infinite</code>=True, the dataloader iterator will provide a random sample from the dataset.</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#2-training-configuration","title":"2. Training Configuration","text":"<p>The <code>TrainConfig</code> class provides a comprehensive configuration setup for training quantam machine learning models in Qadence. This configuration includes settings for batch size, logging, check-pointing, validation, and additional custom callbacks that control the training process's granularity and flexibility.</p> <p>The <code>TrainConfig</code> tells <code>Trainer</code>  what batch_size should be used, how many epochs to train, in which intervals to print/log metrics and how often to store intermediate checkpoints. It is also possible to provide custom callback functions by instantiating a <code>Callback</code> with a function <code>callback</code>.</p> <p>For example of how to use the TrainConfig with <code>Trainer</code>, please see Examples in Trainer</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#21-explanation-of-trainconfig-attributes","title":"2.1 Explanation of <code>TrainConfig</code> Attributes","text":"Attribute Type Default Description <code>max_iter</code> <code>int</code> <code>10000</code> Total number of training epochs. <code>batch_size</code> <code>int</code> <code>1</code> Batch size for training. <code>print_every</code> <code>int</code> <code>0</code> Frequency of console output. Set to <code>0</code> to disable. <code>write_every</code> <code>int</code> <code>0</code> Frequency of logging metrics. Set to <code>0</code> to disable. <code>plot_every</code> <code>int</code> <code>0</code> Frequency of plotting metrics. Set to <code>0</code> to disable. <code>checkpoint_every</code> <code>int</code> <code>0</code> Frequency of saving checkpoints. Set to <code>0</code> to disable. <code>val_every</code> <code>int</code> <code>0</code> Frequency of validation checks. Set to <code>0</code> to disable. <code>val_epsilon</code> <code>float</code> <code>1e-5</code> Threshold for validation improvement. <code>validation_criterion</code> <code>Callable</code> <code>None</code> Function for validating metric improvement. <code>trainstop_criterion</code> <code>Callable</code> <code>None</code> Function to stop training early. <code>callbacks</code> <code>list[Callback]</code> <code>[]</code> List of custom callbacks. <code>root_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Root directory for saving logs and checkpoints. <code>log_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Logging directory for saving logs and checkpoints. <code>log_model</code> <code>bool</code> <code>False</code> Enables model logging. <code>verbose</code> <code>bool</code> <code>True</code> Enables detailed logging. <code>tracking_tool</code> <code>ExperimentTrackingTool</code> <code>TENSORBOARD</code> Tool for tracking training metrics. <code>plotting_functions</code> <code>tuple</code> <code>()</code> Functions for plotting metrics. <pre><code>from qadence.ml_tools import OptimizeResult, TrainConfig\nfrom qadence.ml_tools.callbacks import Callback\n\nbatch_size = 5\nn_epochs = 100\n\nprint_parameters = lambda opt_res: print(opt_res.model.parameters())\ncondition_print = lambda opt_res: opt_res.loss &lt; 1.0e-03\nmodify_extra_opt_res = {\"n_epochs\": n_epochs}\ncustom_callback = Callback(on=\"train_end\", callback = print_parameters, callback_condition=condition_print, modify_optimize_result=modify_extra_opt_res, called_every=10,)\n\nconfig = TrainConfig(\n    root_folder=\"some_path/\",\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n    callbacks = [custom_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/data_and_config/#22-key-configuration-options-in-trainconfig","title":"2.2 Key Configuration Options in <code>TrainConfig</code>","text":""},{"location":"tutorials/qml/ml_tools/data_and_config/#iterations-and-batch-size","title":"Iterations and Batch Size","text":"<ul> <li><code>max_iter</code> (int): Specifies the total number of training iterations (epochs). For an <code>InfiniteTensorDataset</code>, each epoch contains one batch; for a <code>TensorDataset</code>, it contains <code>len(dataloader)</code> batches.</li> <li><code>batch_size</code> (int): Defines the number of samples processed in each training iteration.</li> </ul> <p>Example: <pre><code>config = TrainConfig(max_iter=2000, batch_size=32)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#training-parameters","title":"Training Parameters","text":"<ul> <li><code>print_every</code> (int): Controls how often loss and metrics are printed to the console.</li> <li><code>write_every</code> (int): Determines how frequently metrics are written to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>checkpoint_every</code> (int): Sets the frequency for saving model checkpoints.</li> </ul> <p>Note: Set 0 to diable.</p> <p>Example: <pre><code>config = TrainConfig(print_every=100, write_every=50, checkpoint_every=50)\n</code></pre></p> <p>The user can provide either the <code>root_folder</code> or the <code>log_folder</code> for saving checkpoints and logging. When neither are provided, the default <code>root_folder</code> \"./qml_logs\" is used.</p> <ul> <li><code>root_folder</code> (Path): The root directory for saving checkpoints and logs. All training logs will be saved inside a subfolder in this root directory. (The path to these subfolders can be accessed using config._subfolders, and the current logging folder is config.log_folder)</li> <li><code>create_subfolder_per_run</code> (bool): Creates a unique subfolder for each training run within the specified folder.</li> <li><code>tracking_tool</code> (ExperimentTrackingTool): Specifies the tracking tool to log metrics, e.g., TensorBoard or MLflow.</li> <li><code>log_model</code> (bool): Enables logging of a serialized version of the model, which is useful for model versioning. Thi happens at the end of training.</li> </ul> <p>Note     - The user can also provide <code>log_folder</code> argument - which will only be used when <code>create_subfolder_per_run</code> = False.     -  <code>log_folder</code> (Path): The log folder used for saving checkpoints and logs.</p> <p>Example: <pre><code>config = TrainConfig(root_folder=\"path/to/checkpoints\", tracking_tool=ExperimentTrackingTool.MLFLOW, checkpoint_best_only=True)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#validation-parameters","title":"Validation Parameters","text":"<ul> <li><code>checkpoint_best_only</code> (bool): If set to <code>True</code>, saves checkpoints only when there is an improvement in the validation metric.</li> <li><code>val_every</code> (int): Frequency of validation checks. Setting this to <code>0</code> disables validation.</li> <li><code>val_epsilon</code> (float): A small threshold used to compare the current validation loss with previous best losses.</li> <li><code>validation_criterion</code> (Callable): A custom function to assess if the validation metric meets a specified condition.</li> </ul> <p>Example: <pre><code>config = TrainConfig(val_every=200, checkpoint_best_only = True, validation_criterion=lambda current, best: current &lt; best - 0.001)\n</code></pre></p> <p>If it is desired to only the save the \"best\" checkpoint, the following must be ensured:</p> <pre><code>(a) `checkpoint_best_only = True` is used while creating the configuration through `TrainConfig`,\n(b) `val_every` is set to a valid integer value (for example, `val_every = 10`) which controls the no. of iterations after which the validation data should be used to evaluate the model during training, which can also be set through `TrainConfig`,\n(c) a validation criterion is provided through the `validation_criterion`, set through `TrainConfig` to quantify the definition of \"best\", and\n(d) the validation dataloader passed to `Trainer` is of type `DataLoader`. In this case, it is expected that a validation dataloader is also provided along with the train dataloader since the validation data will be used to decide the \"best\" checkpoint.\n</code></pre> <p>The criterion used to decide the \"best\" checkpoint can be customized by <code>validation_criterion</code>, which should be a function that can take val_loss, best_loss, and val_epsilon arguments and return a boolean value (True or False) indicating whether some validation metric is satisfied or not. An example of a simple <code>validation_criterion</code> is: <pre><code>def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:\n    return val_loss &lt; (best_val_loss - val_epsilon)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#custom-callbacks","title":"Custom Callbacks","text":"<p><code>TrainConfig</code> supports custom callbacks that can be triggered at specific stages of training. The <code>callbacks</code> attribute accepts a list of callback instances, which allow for custom behaviors like early stopping or additional logging. See Callbacks for more details.</p> <ul> <li><code>callbacks</code> (list[Callback]): List of custom callbacks to execute during training.</li> </ul> <p>Example: <pre><code>from qadence.ml_tools.callbacks import Callback\n\ndef callback_fn(trainer, config, writer):\n    if trainer.opt_res.loss &lt; 0.001:\n        print(\"Custom Callback: Loss threshold reached!\")\n\ncustom_callback = Callback(on = \"train_epoch_end\", called_every = 10, callback_function = callback_fn )\n\nconfig = TrainConfig(callbacks=[custom_callback])\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#hyperparameters-and-plotting","title":"Hyperparameters and Plotting","text":"<ul> <li><code>hyperparams</code> (dict): A dictionary of hyperparameters (e.g., learning rate, regularization) to be tracked by the tracking tool.</li> <li><code>plot_every</code> (int): Determines how frequently plots are saved to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>plotting_functions</code> (tuple[LoggablePlotFunction, ...]): Functions for in-training plotting of metrics or model state.</li> </ul> <p>Note: Please ensure that plotting_functions are provided when plot_every &gt; 0</p> <p>Example: <pre><code>config = TrainConfig(\n    plot_every=10,\n    hyperparams={\"learning_rate\": 0.001, \"batch_size\": 32},\n    plotting_functions=(plot_loss_function,)\n)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#3-experiment-tracking-with-mlflow","title":"3. Experiment tracking with mlflow","text":"<p>Qadence allows to track runs and log hyperparameters, models and plots with tensorboard and mlflow. In the following, we demonstrate the integration with mlflow.</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#mlflow-configuration","title":"mlflow configuration","text":"<p>We have control over our tracking configuration by setting environment variables. First, let's look at the tracking URI. For the purpose of this demo we will be working with a local database, in a similar fashion as described here, <pre><code>export MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n</code></pre></p> <p>Qadence can also read the following two environment variables to define the mlflow experiment name and run name <pre><code>export MLFLOW_EXPERIMENT=test_experiment\nexport MLFLOW_RUN_NAME=run_0\n</code></pre></p> <p>If no tracking URI is provided, mlflow stores run information and artifacts in the local <code>./mlflow</code> directory and if no names are defined, the experiment and run will be named with random UUIDs.</p>"},{"location":"tutorials/qml/ml_tools/trainer/","title":"Qadence Trainer Guide","text":"<p>The <code>Trainer</code> class in <code>qadence.ml_tools</code> is a versatile tool designed to streamline the training of quantum machine learning models. It offers flexibility for both gradient-based and gradient-free optimization methods, supports custom loss functions, and integrates seamlessly with tracking tools like TensorBoard and MLflow. Additionally, it provides hooks for implementing custom behaviors during the training process.</p> <p>For training QML models, Qadence offers this out-of-the-box <code>Trainer</code> for optimizing differentiable models, e.g. <code>QNN</code>s and <code>QuantumModel</code>, containing either trainable and/or non-trainable parameters (see the parameters tutorial for detailed information about parameter types):</p>"},{"location":"tutorials/qml/ml_tools/trainer/#1-overview","title":"1. Overview","text":"<p>The <code>Trainer</code> class simplifies the training workflow by managing the training loop, handling data loading, and facilitating model evaluation. It is compatible with various optimization strategies and allows for extensive customization to meet specific training requirements.</p> <p>Example of initializing the <code>Trainer</code>:</p> <pre><code>from qadence.ml_tools import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\n# Initialize Trainer with model, optimizer, and configuration\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\n</code></pre> <p>Notes: <code>qadence</code> versions prior to 1.9.0 provided <code>train_with_grad</code> and <code>train_no_grad</code> functions, which are being replaced with <code>Trainer</code>. The user can transition as following. <pre><code>from qadence.ml_tools import train_with_grad\ntrain_with_grad(model=model, optimizer=optimizer, config=config, data = data)\n</code></pre> to <pre><code>from qadence.ml_tools import Trainer\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\ntrainer.fit(train_dataloader = data)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/trainer/#2-gradient-based-and-gradient-free-optimization","title":"2. Gradient-Based and Gradient-Free Optimization","text":"<p>The <code>Trainer</code> supports both gradient-based and gradient-free optimization methods. Default is gradient-based optimization.</p> <ul> <li>Gradient-Based Optimization: Utilizes optimizers from PyTorch's <code>torch.optim</code> module. This is the default behaviour of the <code>Trainer</code>, thus setting this is not necessary. However, it can be explicity mentioned as follows. Example of using gradient-based optimization:</li> </ul> <pre><code>from qadence.ml_tools import Trainer\n\n# set_use_grad(True) to enable gradient based training. This is the default behaviour of Trainer.\nTrainer.set_use_grad(True)\n</code></pre> <ul> <li>Gradient-Free Optimization: Employs optimization algorithms from the Nevergrad library.</li> </ul> <p>Example of using gradient-free optimization with Nevergrad:</p> <pre><code>from qadence.ml_tools import Trainer\n\n# set_use_grad(False) to disable gradient based training.\nTrainer.set_use_grad(False)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#using-context-managers-for-mixed-optimization","title":"Using Context Managers for Mixed Optimization","text":"<p>For cases requiring both optimization methods in a single training session, the <code>Trainer</code> class provides context managers to enable or disable gradients.</p> <pre><code># Temporarily switch to gradient-based optimization\nwith trainer.enable_grad_opt(optimizer):\n    print(\"Gradient Based Optimization\")\n    # trainer.fit(train_loader)\n\n# Switch to gradient-free optimization for specific steps\nwith trainer.disable_grad_opt(ng_optimizer):\n    print(\"Gradient Free Optimization\")\n    # trainer.fit(train_loader)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#3-custom-loss-functions","title":"3. Custom Loss Functions","text":"<p>Users can define custom loss functions tailored to their specific tasks. The <code>Trainer</code> accepts a <code>loss_fn</code> parameter, which should be a callable that takes the model and data as inputs and returns a tuple containing the loss tensor and a dictionary of metrics.</p> <p>Example of using a custom loss function:</p> <pre><code>import torch\nfrom itertools import count\ncnt = count()\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n</code></pre> <p>This custom loss function can be used in the trainer <pre><code>from qadence.ml_tools import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\ntrainer = Trainer(model=model, optimizer=optimizer, config=config, loss_fn=loss_fn)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/trainer/#4-hooks-for-custom-behavior","title":"4. Hooks for Custom Behavior","text":"<p>The <code>Trainer</code> class provides several hooks that enable users to inject custom behavior at different stages of the training process. These hooks are methods that can be overridden in a subclass to execute custom code. The available hooks include:</p> <ul> <li><code>on_train_start</code>: Called at the beginning of the training process.</li> <li><code>on_train_end</code>: Called at the end of the training process.</li> <li><code>on_train_epoch_start</code>: Called at the start of each training epoch.</li> <li><code>on_train_epoch_end</code>: Called at the end of each training epoch.</li> <li><code>on_train_batch_start</code>: Called at the start of each training batch.</li> <li><code>on_train_batch_end</code>: Called at the end of each training batch.</li> </ul> <p>Each \"start\" and \"end\" hook receives data and loss metrics as arguments. The specific values provided for these arguments depend on the training stage associated with the hook. The context of the training stage (e.g., training, validation, or testing) determines which metrics are relevant and how they are populated. For details of inputs on each hook, please review the documentation of <code>BaseTrainer</code>.</p> <pre><code>- Example of what inputs are provided to training hooks.\n\n    ```\n    def on_train_batch_start(self, batch: Tuple[torch.Tensor, ...] | None) -&gt; None:\n        \"\"\"\n        Called at the start of each training batch.\n\n        Args:\n            batch: A batch of data from the DataLoader. Typically a tuple containing\n                input tensors and corresponding target tensors.\n        \"\"\"\n        pass\n    ```\n    ```\n    def on_train_batch_end(self, train_batch_loss_metrics: Tuple[torch.Tensor, Any]) -&gt; None:\n        \"\"\"\n        Called at the end of each training batch.\n\n        Args:\n            train_batch_loss_metrics: Metrics for the training batch loss.\n                Tuple of (loss, metrics)\n        \"\"\"\n        pass\n    ```\n</code></pre> <p>Example of using a hook to log a message at the end of each epoch:</p> <pre><code>from qadence.ml_tools import Trainer\n\nclass CustomTrainer(Trainer):\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        print(f\"End of epoch - Loss and Metrics: {train_epoch_loss_metrics}\")\n</code></pre> <p>Notes: Trainer offers inbuilt callbacks as well. Callbacks are mainly for logging/tracking purposes, but the above mentioned hooks are generic. The workflow for every train batch looks like: 1. perform on_train_batch_start callbacks, 2. call the on_train_batch_start hook, 3. do the batch training, 4. call the on_train_batch_end hook, and 5. perform on_train_batch_end callbacks.</p> <p>The use of <code>on_</code>{phase}<code>_start</code> and <code>on_</code>{phase}<code>_end</code> hooks is not specifically to add extra callbacks, but for any other generic pre/post processing. For example, reshaping input batch in case of RNNs/LSTMs, post processing loss and adding an extra metric. They could also be used to add more callbacks (which is not recommended - as we provide methods to add extra callbacks in the TrainCofig)</p>"},{"location":"tutorials/qml/ml_tools/trainer/#5-experiment-tracking-with-tensorboard-and-mlflow","title":"5. Experiment Tracking with TensorBoard and MLflow","text":"<p>The <code>Trainer</code> integrates with TensorBoard and MLflow for experiment tracking:</p> <ul> <li> <p>TensorBoard: Logs metrics and visualizations during training, allowing users to monitor the training process.</p> </li> <li> <p>MLflow: Tracks experiments, logs parameters, metrics, and artifacts, and provides a user-friendly interface for comparing different runs.</p> </li> </ul> <p>To utilize these tracking tools, the <code>Trainer</code> can be configured with appropriate writers that handle the logging of metrics and other relevant information during training.</p> <p>Example of using TensorBoard tracking:</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.types import ExperimentTrackingTool\n\n# Set up tracking with TensorBoard\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.TENSORBOARD)\n</code></pre> <p>Example of using MLflow tracking:</p> <pre><code>from qadence.types import ExperimentTrackingTool\n\n# Set up tracking with MLflow\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.MLFLOW)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#6-examples","title":"6. Examples","text":""},{"location":"tutorials/qml/ml_tools/trainer/#61-training-with-trainer-and-trainconfig","title":"6.1. Training with <code>Trainer</code> and <code>TrainConfig</code>","text":""},{"location":"tutorials/qml/ml_tools/trainer/#setup","title":"Setup","text":"<p>Let's do the necessary imports and declare a <code>DataLoader</code>. We can already define some hyperparameters here, including the seed for random number generators. mlflow can log hyperparameters with arbitrary types, for example the observable that we want to monitor (<code>Z</code> in this case, which has a <code>qadence.Operation</code> type).</p> <pre><code>import random\nfrom itertools import count\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom matplotlib.figure import Figure\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\n\nfrom qadence import hea, QuantumCircuit, Z\nfrom qadence.constructors import feature_map, hamiltonian_factory\nfrom qadence.ml_tools import Trainer, TrainConfig\nfrom qadence.ml_tools.data import to_dataloader\nfrom qadence.ml_tools.utils import rand_featureparameters\nfrom qadence.ml_tools.models import QNN, QuantumModel\nfrom qadence.types import ExperimentTrackingTool\n\nhyperparams = {\n    \"seed\": 42,\n    \"batch_size\": 10,\n    \"n_qubits\": 2,\n    \"ansatz_depth\": 1,\n    \"observable\": Z,\n}\n\nnp.random.seed(hyperparams[\"seed\"])\ntorch.manual_seed(hyperparams[\"seed\"])\nrandom.seed(hyperparams[\"seed\"])\n\n\ndef dataloader(batch_size: int = 25) -&gt; DataLoader:\n    x = torch.linspace(0, 1, batch_size).reshape(-1, 1)\n    y = torch.cos(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=True)\n</code></pre> <p>We continue with the regular QNN definition, together with the loss function and optimizer.</p> <pre><code>obs = hamiltonian_factory(register=hyperparams[\"n_qubits\"], detuning=hyperparams[\"observable\"])\n\ndata = dataloader(hyperparams[\"batch_size\"])\nfm = feature_map(hyperparams[\"n_qubits\"], param=\"x\")\n\nmodel = QNN(\n    QuantumCircuit(\n        hyperparams[\"n_qubits\"], fm, hea(hyperparams[\"n_qubits\"], hyperparams[\"ansatz_depth\"])\n    ),\n    observable=obs,\n    inputs=[\"x\"],\n)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ninputs = rand_featureparameters(model, 1)\n\ndef loss_fn(model: QuantumModel, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    out = model.expectation(inputs)\n    loss = criterion(out, torch.rand(1))\n    return loss, {}\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#trainconfig-specifications","title":"<code>TrainConfig</code> specifications","text":"<p>Qadence offers different tracking options via <code>TrainConfig</code>. Here we use the <code>ExperimentTrackingTool</code> type to specify that we want to track the experiment with mlflow. Tracking with tensorboard is also possible. We can then indicate what and how often we want to track or log.</p> <p>For Training <code>write_every</code> controls the number of epochs after which the loss values is logged. Thanks to the <code>plotting_functions</code> and <code>plot_every</code>arguments, we are also able to plot model-related quantities throughout training. Notice that arbitrary plotting functions can be passed, as long as the signature is the same as <code>plot_fn</code> below. Finally, the trained model can be logged by setting <code>log_model=True</code>. Here is an example of plotting function and training configuration</p> <pre><code>def plot_fn(model: Module, iteration: int) -&gt; tuple[str, Figure]:\n    descr = f\"ufa_prediction_epoch_{iteration}.png\"\n    fig, ax = plt.subplots()\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    out = model.expectation(x)\n    ax.plot(x.detach().numpy(), out.detach().numpy())\n    return descr, fig\n\n\nconfig = TrainConfig(\n    root_folder=\"mlflow_demonstration\",\n    max_iter=10,\n    checkpoint_every=1,\n    plot_every=2,\n    write_every=1,\n    log_model=True,\n    tracking_tool=ExperimentTrackingTool.MLFLOW,\n    hyperparams=hyperparams,\n    plotting_functions=(plot_fn,),\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#training-and-inspecting","title":"Training and inspecting","text":"<p>Model training happens as usual <pre><code>trainer = Trainer(model, optimizer, config, loss_fn)\ntrainer.fit(train_dataloader=data)\n</code></pre></p> <p>After training , we can inspect our experiment via the mlflow UI <pre><code>mlflow ui --port 8080 --backend-store-uri sqlite:///mlruns.db\n</code></pre> In this case, since we're running on a local server, we can access the mlflow UI by navigating to http://localhost:8080/.</p>"},{"location":"tutorials/qml/ml_tools/trainer/#62-fitting-a-function-with-a-qnn-using-ml_tools","title":"6.2. Fitting a function with a QNN using <code>ml_tools</code>","text":"<p>In Quantum Machine Learning, the general consensus is to use <code>complex128</code> precision for states and operators and <code>float64</code> precision for parameters. This is also the convention which is used in <code>qadence</code>. However, for specific usecases, lower precision can greatly speed up training and reduce memory consumption. When using the <code>pyqtorch</code> backend, <code>qadence</code> offers the option to move a <code>QuantumModel</code> instance to a specific precision using the torch <code>to</code> syntax.</p> <p>Let's look at a complete example of how to use <code>Trainer</code> now. Here we perform a validation check during training and use a validation criterion that checks whether the validation loss in the current iteration has decreased compared to the lowest validation loss from all previous iterations. For demonstration, the train and the validation data are kept the same here. However, it is beneficial and encouraged to keep them distinct in practice to understand model's generalization capabilities.</p> <pre><code>from pathlib import Path\nimport torch\nfrom functools import reduce\nfrom operator import add\nfrom itertools import count\nimport matplotlib.pyplot as plt\n\nfrom qadence import Parameter, QuantumCircuit, Z\nfrom qadence import hamiltonian_factory, hea, feature_map, chain\nfrom qadence import QNN\nfrom qadence.ml_tools import  TrainConfig, Trainer, to_dataloader\n\nTrainer.set_use_grad(True)\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDTYPE = torch.complex64\nn_qubits = 4\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 100\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data[0], data[1]\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n\ndef validation_criterion(\n    current_validation_loss: float, current_best_validation_loss: float, val_epsilon: float\n) -&gt; bool:\n    return current_validation_loss &lt;= current_best_validation_loss - val_epsilon\n\nn_epochs = 300\n\nconfig = TrainConfig(\n    max_iter=n_epochs,\n    batch_size=batch_size,\n    checkpoint_best_only=True,\n    val_every=10,  # The model will be run on the validation data after every `val_every` epochs.\n    validation_criterion=validation_criterion\n)\n\nfn = lambda x, degree: .05 * reduce(add, (torch.cos(i*x) + torch.sin(i*x) for i in range(degree)), 0.)\nx = torch.linspace(0, 10, batch_size, dtype=torch.float32).reshape(-1, 1)\ny = fn(x, 5)\n\ntrain_dataloader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_dataloader =  to_dataloader(x, y, batch_size=batch_size, infinite=True)\n\ntrainer = Trainer(model, optimizer, config, loss_fn=loss_fn,\n                    train_dataloader = train_dataloader, val_dataloader = val_dataloader,\n                    device=DEVICE, dtype=DTYPE)\ntrainer.fit()\n\nplt.clf()\nplt.plot(x.numpy(), y.numpy(), label='truth')\nplt.plot(x.numpy(), model(x).detach().numpy(), \"--\", label=\"final\", linewidth=3)\nplt.legend()\n</code></pre> 2024-12-05T14:32:54.841476 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/qml/ml_tools/trainer/#63-fitting-a-function-low-level-api","title":"6.3. Fitting a function - Low-level API","text":"<p>For users who want to use the low-level API of <code>qadence</code>, here an example written without <code>Trainer</code>.</p> <pre><code>from pathlib import Path\nimport torch\nfrom itertools import count\nfrom qadence.constructors import hamiltonian_factory, hea, feature_map\nfrom qadence import chain, Parameter, QuantumCircuit, Z\nfrom qadence import QNN\nfrom qadence.ml_tools import TrainConfig\n\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 1\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nn_epochs=50\ncnt = count()\n\ntmp_path = Path(\"/tmp\")\n\nconfig = TrainConfig(\n    root_folder=tmp_path,\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n)\n\nx = torch.linspace(0, 1, batch_size).reshape(-1, 1)\ny = torch.sin(x)\n\nfor i in range(n_epochs):\n    out = model(x)\n    loss = criterion(out, y)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#64-custom-train-loop","title":"6.4. Custom <code>train</code> loop","text":"<p>If you need custom training functionality that goes beyond what is available in <code>qadence.ml_tools.Trainer</code> you can write your own training loop based on the building blocks that are available in Qadence.</p> <p>A simplified version of Qadence's train loop is defined below. Feel free to copy it and modify at will.</p> <p>For logging we can use the <code>get_writer</code> from the <code>Writer Registry</code>. This will set up the default writer based on the experiment tracking tool. All writers from the <code>Writer Registry</code> offer <code>open</code>, <code>close</code>, <code>print_metrics</code>, <code>write_metrics</code>, <code>plot_metrics</code>, etc methods.</p> <pre><code>from typing import Callable, Union\n\nfrom torch.nn import Module\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom qadence.ml_tools.config import TrainConfig\nfrom qadence.ml_tools.data import DictDataLoader, data_to_device\nfrom qadence.ml_tools.optimize_step import optimize_step\nfrom qadence.ml_tools.callbacks import get_writer\nfrom qadence.ml_tools.callbacks.saveload import load_checkpoint, write_checkpoint\n\n\ndef train(\n    model: Module,\n    data: DataLoader,\n    optimizer: Optimizer,\n    config: TrainConfig,\n    loss_fn: Callable,\n    device: str = \"cpu\",\n    optimize_step: Callable = optimize_step,\n    write_tensorboard: Callable = write_tensorboard,\n) -&gt; tuple[Module, Optimizer]:\n\n    # Move model to device before optimizer is loaded\n    model = model.to(device)\n\n    # load available checkpoint\n    init_iter = 0\n    if config.log_folder:\n        model, optimizer, init_iter = load_checkpoint(config.log_folder, model, optimizer)\n\n    # Initialize writer based on the tracking tool specified in the configuration\n    writer = get_writer(config.tracking_tool)  # Uses ExperimentTrackingTool to select writer\n    writer.open(config, iteration=init_iter)\n\n    dl_iter = iter(dataloader)\n\n    # outer epoch loop\n    for iteration in range(init_iter, init_iter + config.max_iter):\n        data = data_to_device(next(dl_iter), device)\n        loss, metrics = optimize_step(model, optimizer, loss_fn, data)\n\n        if iteration % config.print_every == 0 and config.verbose:\n            writer.print_metrics(OptimizeResult(iteration, model, optimizer, loss, metrics))\n\n        if iteration % config.write_every == 0:\n            writer.write(OptimizeResult(iteration, model, optimizer, loss, metrics))\n\n        if config.log_folder:\n            if iteration % config.checkpoint_every == 0:\n                write_checkpoint(config.log_folder, model, optimizer, iteration)\n\n    # Final writing and checkpointing\n    if config.log_folder:\n        write_checkpoint(config.log_folder, model, optimizer, iteration)\n    writer.write(OptimizeResult(iteration, model, optimizer, loss, metrics))\n    writer.close()\n\n    return model, optimizer\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#65-gradient-free-optimization-using-trainer","title":"6.5. Gradient-free optimization using <code>Trainer</code>","text":"<p>Solving a QUBO using gradient free optimization based on <code>Nevergrad</code> optimizers and <code>Trainer</code>. This problem is further defined in QUBO Tutorial</p> <p>We can achieve gradient free optimization by. <pre><code>Trainer.set_use_grad(False)\n\n# or\n\ntrainer.disable_grad_opt(ng_optimizer):\n    print(\"Gradient free opt\")\n</code></pre></p> <pre><code>import numpy as np\nimport numpy.typing as npt\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist, squareform\nfrom qadence import RydbergDevice\n\nimport torch\nfrom qadence import QuantumModel, QuantumCircuit, Register\nfrom qadence import RydbergDevice, AnalogRX, AnalogRZ, chain\nfrom qadence.ml_tools import Trainer, TrainConfig, num_parameters\nimport nevergrad as ng\nimport matplotlib.pyplot as plt\n\nTrainer.set_use_grad(False)\n\nseed = 0\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\ndef qubo_register_coords(Q: np.ndarray, device: RydbergDevice) -&gt; list:\n    \"\"\"Compute coordinates for register.\"\"\"\n\n    def evaluate_mapping(new_coords, *args):\n        \"\"\"Cost function to minimize. Ideally, the pairwise\n        distances are conserved\"\"\"\n        Q, shape = args\n        new_coords = np.reshape(new_coords, shape)\n        interaction_coeff = device.rydberg_level\n        new_Q = squareform(interaction_coeff / pdist(new_coords) ** 6)\n        return np.linalg.norm(new_Q - Q)\n\n    shape = (len(Q), 2)\n    np.random.seed(0)\n    x0 = np.random.random(shape).flatten()\n    res = minimize(\n        evaluate_mapping,\n        x0,\n        args=(Q, shape),\n        method=\"Nelder-Mead\",\n        tol=1e-6,\n        options={\"maxiter\": 200000, \"maxfev\": None},\n    )\n    return [(x, y) for (x, y) in np.reshape(res.x, (len(Q), 2))]\n\n\n# QUBO problem weights (real-value symmetric matrix)\nQ = np.array([\n    [-10.0, 19.7365809, 19.7365809, 5.42015853, 5.42015853],\n    [19.7365809, -10.0, 20.67626392, 0.17675796, 0.85604541],\n    [19.7365809, 20.67626392, -10.0, 0.85604541, 0.17675796],\n    [5.42015853, 0.17675796, 0.85604541, -10.0, 0.32306662],\n    [5.42015853, 0.85604541, 0.17675796, 0.32306662, -10.0],\n    ])\n\n# Device specification and atomic register\ndevice = RydbergDevice(rydberg_level=70)\n\nreg = Register.from_coordinates(\nqubo_register_coords(Q, device), device_specs=device)\n\n# Analog variational quantum circuit\nlayers = 2\nblock = chain(*[AnalogRX(f\"t{i}\") * AnalogRZ(f\"s{i}\") for i in range(layers)])\ncircuit = QuantumCircuit(reg, block)\n\nmodel = QuantumModel(circuit)\ninitial_counts = model.sample({}, n_shots=1000)[0]\n\n\ndef loss(model: QuantumModel, *args) -&gt; tuple[torch.Tensor, dict]:\n    to_arr_fn = lambda bitstring: np.array(list(bitstring), dtype=int)\n    cost_fn = lambda arr: arr.T @ Q @ arr\n    samples = model.sample({}, n_shots=1000)[0]  # extract samples\n    cost_fn = sum(samples[key] * cost_fn(to_arr_fn(key)) for key in samples)\n    return torch.tensor(cost_fn / sum(samples.values())), {}  # We return an optional metrics dict\n\n\n\n# Training\nconfig = TrainConfig(max_iter=100)\noptimizer = ng.optimizers.NGOpt(\n    budget=config.max_iter, parametrization=num_parameters(model)\n    )\ntrainer = Trainer(model, optimizer, config, loss)\ntrainer.fit()\n\noptimal_counts = model.sample({}, n_shots=1000)[0]\n\n\n# Known solutions to the QUBO problem.\nsolution_bitstrings = [\"01011\", \"00111\"]\n\ndef plot_distribution(C, ax, title):\n    C = dict(sorted(C.items(), key=lambda item: item[1], reverse=True))\n    indexes = solution_bitstrings # QUBO solutions\n    color_dict = {key: \"r\" if key in indexes else \"g\" for key in C}\n    ax.set_xlabel(\"bitstrings\")\n    ax.set_ylabel(\"counts\")\n    ax.set_xticks([i for i in range(len(C.keys()))], C.keys(), rotation=90)\n    ax.bar(list(C.keys())[:20], list(C.values())[:20])\n    ax.set_title(title)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nplot_distribution(initial_counts, axs[0], \"Initial counts\")\nplot_distribution(optimal_counts, axs[1], \"Optimal counts\")\n</code></pre>   initial_counts = OrderedCounter({'01000': 197, '00001': 193, '00010': 189, '00100': 174, '10000': 169, '00000': 78}) optimal_count = OrderedCounter({'00000': 286, '10000': 153, '00010': 147, '00100': 142, '00001': 140, '01000': 132})  2024-12-05T14:33:02.071035 image/svg+xml Matplotlib v3.9.3, https://matplotlib.org/"},{"location":"tutorials/realistic_sims/","title":"Realistic simulations","text":"<p>This section describes how to perform realistic simulations in Qadence.</p>"},{"location":"tutorials/realistic_sims/measurements/","title":"Measurement protocols","text":"<p>Sample-based measurement protocols are fundamental tools for the prediction and estimation of a quantum state as the result of NISQ programs executions. Their resource efficient implementation is a current and active research field. Qadence offers two main measurement protocols: quantum state tomography and classical shadows.</p>"},{"location":"tutorials/realistic_sims/measurements/#quantum-state-tomography","title":"Quantum state tomography","text":"<p>The fundamental task of quantum state tomography is to learn an approximate classical description of an output quantum state described by a density matrix \\(\\rho\\), from repeated measurements of copies on a chosen basis. To do so, \\(\\rho\\) is expanded in a basis of observables (the tomography step) and for a given observable \\(\\hat{\\mathcal{O}}\\), the expectation value is calculated with \\(\\langle \\hat{\\mathcal{O}} \\rangle=\\textrm{Tr}(\\hat{\\mathcal{O}}\\rho)\\). A number of measurement repetitions in a suitable basis is then required to estimate \\(\\langle \\hat{\\mathcal{O}} \\rangle\\).</p> <p>The main drawback is the scaling in measurements for the retrieval of the classical expression for a \\(n\\)-qubit quantum state as \\(2^n \\times 2^n\\), together with a large amount of classical post-processing.</p> <p>For an observable expressed as a Pauli string \\(\\hat{\\mathcal{P}}\\), the expectation value for a state \\(|\\psi \\rangle\\) can be derived as:</p> \\[ \\langle \\hat{\\mathcal{P}} \\rangle=\\langle \\psi | \\hat{\\mathcal{P}} |\\psi \\rangle=\\langle \\psi | \\hat{\\mathcal{R}}^\\dagger \\hat{\\mathcal{D}} \\hat{\\mathcal{R}} |\\psi \\rangle \\] <p>The operator \\(\\hat{\\mathcal{R}}\\) diagonalizes \\(\\hat{\\mathcal{P}}\\) and rotates the state into an eigenstate in the computational basis. Therefore, \\(\\hat{\\mathcal{R}}|\\psi \\rangle=\\sum\\limits_{z}a_z|z\\rangle\\) and the expectation value can finally be expressed as:</p> \\[ \\langle \\hat{\\mathcal{P}} \\rangle=\\sum_{z,z'}\\langle z |\\bar{a}_z\\hat{\\mathcal{D}}a_{z'}|z'\\rangle = \\sum_{z}|a_z|^2(-1)^{\\phi_z(\\hat{\\mathcal{P}})} \\] <p>In Qadence, running a tomographical experiment is made simple by defining a <code>Measurements</code> object that captures all options for execution:</p> <pre><code>from torch import tensor\nfrom qadence import hamiltonian_factory, BackendName, DiffMode\nfrom qadence import Parameter, chain, kron, RX, RY, Z, QuantumCircuit, QuantumModel\nfrom qadence.measurements import Measurements\n\n# Define parameters for a circuit.\ntheta1 = Parameter(\"theta1\", trainable=False)\ntheta2 = Parameter(\"theta2\", trainable=False)\ntheta3 = Parameter(\"theta3\", trainable=False)\ntheta4 = Parameter(\"theta4\", trainable=False)\n\nblocks = chain(\n    kron(RX(0, theta1), RY(1, theta2)),\n    kron(RX(0, theta3), RY(1, theta4)),\n)\n\nvalues = {\n    \"theta1\": tensor([0.5]),\n    \"theta2\": tensor([1.5]),\n    \"theta3\": tensor([2.0]),\n    \"theta4\": tensor([2.5]),\n}\n\n# Create a circuit and an observable.\ncircuit = QuantumCircuit(2, blocks)\nobservable = hamiltonian_factory(2, detuning=Z)\n\n# Create a model.\nmodel = QuantumModel(\n    circuit=circuit,\n    observable=observable,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.GPSR,\n)\n\n# Define a measurement protocol by passing the shot budget as an option.\ntomo_options = {\"n_shots\": 100000}\ntomo_measurement = Measurements(protocol=Measurements.TOMOGRAPHY, options=tomo_options)\n\n# Get the exact expectation value.\nexact_values = model.expectation(\n    values=values,\n)\n\n# Run the tomography experiment.\nestimated_values_tomo = model.expectation(\n    values=values,\n    measurement=tomo_measurement,\n)\n</code></pre> <pre><code>Exact expectation value = tensor([[-1.4548]])\nEstimated expectation value tomo = tensor([[-1.4524]])\n</code></pre>"},{"location":"tutorials/realistic_sims/measurements/#classical-shadows","title":"Classical shadows","text":"<p>Recently, a much less resource demanding protocol based on classical shadows has been proposed<sup>1</sup>. It combines ideas from shadow tomography<sup>2</sup> and randomized measurement protocols capable of learning a classical shadow of an unknown quantum state \\(\\rho\\). It relies on deliberately discarding the full classical characterization of the quantum state, and instead focuses on accurately predicting a restricted set of properties that provide efficient protocols for the study of the system.</p> <p>A random measurement consists of applying random unitary rotations before a fixed measurement on each copy of a state. Appropriately averaging over these measurements produces an efficient estimator for the expectation value of an observable. This protocol therefore creates a robust classical representation of the quantum state or classical shadow. The captured measurement information is then reuseable for multiple purposes, i.e. any observable expected value and available for noise mitigation postprocessing.</p> <p>A classical shadow is therefore an unbiased estimator of a quantum state \\(\\rho\\). Such an estimator is obtained with the following procedure<sup>1</sup>: first, apply a random unitary gate \\(U\\) to rotate the state: \\(\\rho \\rightarrow U \\rho U^\\dagger\\) and then perform a basis measurement to obtain a \\(n\\)-bit measurement \\(|\\hat{b}\\rangle \\in \\{0, 1\\}^n\\). Both unitary gates \\(U\\) and the measurement outcomes \\(|\\hat{b}\\rangle\\) are stored on a classical computer for postprocessing v \\(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U\\), a classical snapshot of the state \\(\\rho\\). The whole procedure can be seen as a quantum channel \\(\\mathcal{M}\\) that maps the initial unknown quantum state \\(\\rho\\) to the average result of the measurement protocol:</p> \\[ \\mathbb{E}[U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U] = \\mathcal{M}(\\rho) \\Rightarrow \\rho = \\mathbb{E}[\\mathcal{M}^{-1}(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U)] \\] <p>It is worth noting that the single classical snapshot \\(\\hat{\\rho}=\\mathcal{M}^{-1}(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U)\\) equals \\(\\rho\\) in expectation: \\(\\mathbb{E}[\\hat{\\rho}]=\\rho\\) despite \\(\\mathcal{M}^{-1}\\) not being a completely positive map. Repeating this procedure \\(N\\) times results in an array of \\(N\\) independent, classical snapshots of \\(\\rho\\) called the classical shadow:</p> \\[ S(\\rho, N) = \\{ \\hat{\\rho}_1=\\mathcal{M}^{-1}(U_1^\\dagger |\\hat{b}_1\\rangle\\langle \\hat{b}_1|U_1),\\cdots,\\hat{\\rho}_N=\\mathcal{M}^{-1}(U_N^\\dagger |\\hat{b}_N\\rangle\\langle \\hat{b}_N|U_N)\\} \\] <p>Along the same lines as the example before, estimating the expectation value using classical shadows in Qadence only requires to pass the right set of parameters to the <code>Measurements</code> object:</p> <pre><code># Classical shadows are defined up to some accuracy and confidence.\nshadow_options = {\"accuracy\": 0.1, \"confidence\": 0.1}  # Shadow size N=54400.\nshadow_measurement = Measurements(protocol=Measurements.SHADOW, options=shadow_options)\n\n# Run the experiment with classical shadows.\nestimated_values_shadow = model.expectation(\n    values=values,\n    measurement=shadow_measurement,\n)\n</code></pre> <pre><code>Estimated expectation value shadow = tensor([[-1.4499]])\n</code></pre>"},{"location":"tutorials/realistic_sims/measurements/#references","title":"References","text":"<ol> <li> <p>Hsin-Yuan Huang, Richard Kueng and John Preskill, Predicting Many Properties of a Quantum System from Very Few Measurements (2020) \u21a9\u21a9</p> </li> <li> <p>S. Aaronson. Shadow tomography of quantum states. In Proceedings of the 50th Annual A ACM SIGACT Symposium on Theory of Computing, STOC 2018, pages 325\u2013338, New York, NY, USA, 2018. ACM\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/realistic_sims/mitigation/","title":"Error mitigation","text":"<p>Beyond running noisy simulations, Qadence offers a number of noise mitigation techniques to achieve better accuracy of simulation outputs. Currently, mitigation addresses readout errors and depolarizing and dephasing noise for analog blocks.</p>"},{"location":"tutorials/realistic_sims/mitigation/#readout-error-mitigation","title":"Readout error mitigation","text":"<p>The complete implementation of the mitigation technique is to measure \\(T\\) and classically apply \\(T^{\u22121}\\) to measured probability distributions. However there are several limitations of this approach:</p> <ul> <li>The complete implementation requires \\(2^n\\) characterization experiments (probability measurements), which is not scalable. The classical processing of the calibration data is also inefficient.</li> <li>The matrix \\(T\\) may become singular for large \\(n\\), preventing direct inversion.</li> <li>The inverse \\(T^{\u22121}\\) might not be a stochastic matrix, meaning that it can produce negative corrected probabilities.</li> <li>The correction is not rigorously justified, so we cannot be sure that we are only removing SPAM errors and not otherwise corrupting an estimated probability distribution.</li> </ul> <p>Qadence relies on the assumption of uncorrelated readout errors:</p> \\[ T=T_1\\otimes T_2\\otimes \\dots \\otimes T_n \\] <p>for which the inversion is straightforward:</p> \\[ T^{-1}=T_1^{-1}\\otimes T_2^{-1}\\otimes \\dots \\otimes T_n^{-1} \\] <p>However, even for a reduced \\(n\\) the third limitation holds. This can be avoided by reformulating into a minimization problem<sup>1</sup>:</p> \\[ \\lVert Tp_{\\textrm{corr}}-p_{\\textrm{raw}}\\rVert_{2}^{2} \\] <p>subjected to physicality constraints \\(0 \\leq p_{corr}(x) \\leq 1\\) and \\(\\lVert p_{corr} \\rVert = 1\\). At this point, two methods are implemented to solve this problem. The first one relies on solving using standard optimization tools, the second on Maximum-Likelihood Estimation<sup>2</sup>. In Qadence, this can be user defined using the mitigation protocol:</p> <pre><code>from qadence import QuantumModel, QuantumCircuit, kron, H, Z\nfrom qadence import hamiltonian_factory\nfrom qadence.noise import NoiseHandler\nfrom qadence.mitigations import Mitigations\nfrom qadence.types import ReadOutOptimization, NoiseProtocol\n\n# Simple circuit and observable construction.\nblock = kron(H(0), Z(1))\ncircuit = QuantumCircuit(2, block)\nobservable = hamiltonian_factory(circuit.n_qubits, detuning=Z)\n\n# Construct a quantum model.\nmodel = QuantumModel(circuit=circuit, observable=observable)\n\n# Define a noise model to use:\nnoise = NoiseHandler(NoiseProtocol.READOUT.INDEPENDENT)\n# Define the mitigation method solving the minimization problem:\noptions={\"optimization_type\": ReadOutOptimization.CONSTRAINED}  # ReadOutOptimization.MLE for the alternative method.\nmitigation = Mitigations(protocol=Mitigations.READOUT, options=options)\n\n# Run noiseless, noisy and mitigated simulations.\nn_shots = 100\nnoiseless_samples = model.sample(n_shots=n_shots)\nnoisy_samples = model.sample(noise=noise, n_shots=n_shots)\nmitigated_samples = model.sample(\n    noise=noise, mitigation=mitigation, n_shots=n_shots\n)\n\nprint(f\"noiseless {noiseless_samples}\")\nprint(f\"noisy {noisy_samples}\")\nprint(f\"mitigated {mitigated_samples}\")\n</code></pre> <pre><code>noiseless [OrderedCounter({'00': 61, '10': 39})]\nnoisy [Counter({'00': 48, '10': 39, '11': 8, '01': 5})]\nmitigated [Counter({'00': 36, '10': 34, '11': 29, '01': 1})]\n</code></pre>"},{"location":"tutorials/realistic_sims/mitigation/#wip-zero-noise-extrapolation-for-analog-blocks","title":"[WIP] Zero-noise extrapolation for analog blocks","text":"<p>Zero-noise extrapolation (ZNE) is an error mitigation technique in which an expectation value is computed at different noise levels and, as a second step, the ideal expectation value is inferred by extrapolating the measured results to the zero-noise limit. In digital computing, this is typically implemented by \"folding\" the circuit and its dagger to artificially increase the noise through sequences of identities<sup>3</sup>. In the analog ZNE variation, analog blocks are time stretched to again artificially increase noise<sup>3</sup>.</p>"},{"location":"tutorials/realistic_sims/mitigation/#references","title":"References","text":"<ol> <li> <p>Michael R. Geller and Mingyu Sun, Efficient correction of multiqubit measurement errors, (2020) \u21a9</p> </li> <li> <p>Smolin et al., Maximum Likelihood, Minimum Effort, (2011) \u21a9</p> </li> <li> <p>Mitiq: What's the theory behind ZNE? \u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/realistic_sims/noise/","title":"Simulated errors","text":"<p>Running programs on NISQ devices often leads to partially useful results due to the presence of noise. In order to perform realistic simulations, a number of noise models (for digital operations, analog operations and simulated readout errors) are supported in Qadence through their implementation in backends and corresponding error mitigation techniques whenever possible.</p>"},{"location":"tutorials/realistic_sims/noise/#noisehandler","title":"NoiseHandler","text":"<p>Noise models can be defined via the <code>NoiseHandler</code>. It is a container of several noise instances which require to specify a <code>protocols</code> and a dictionary of <code>options</code> (or lists). The <code>protocol</code> field is to be instantiated from <code>NoiseProtocol</code>.</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\nanalog_noise = NoiseHandler(protocol=NoiseProtocol.ANALOG.DEPOLARIZING, options={\"noise_probs\": 0.1})\ndigital_noise = NoiseHandler(protocol=NoiseProtocol.DIGITAL.DEPOLARIZING, options={\"error_probability\": 0.1})\nreadout_noise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options={\"error_probability\": 0.1, \"seed\": 0})\n</code></pre> <pre><code>\n</code></pre> <p>One can also define a <code>NoiseHandler</code> passing a list of protocols and a list of options (careful with the order):</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\nprotocols = [NoiseProtocol.DIGITAL.DEPOLARIZING, NoiseProtocol.READOUT]\noptions = [{\"error_probability\": 0.1}, {\"error_probability\": 0.1, \"seed\": 0}]\n\nnoise_combination = NoiseHandler(protocols, options)\nprint(noise_combination)\n</code></pre> <pre><code>Noise(Depolarizing, {'error_probability': 0.1})\nNoise(&lt;enum 'ReadoutNoise'&gt;, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>One can also append to a <code>NoiseHandler</code> other <code>NoiseHandler</code> instances:</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\ndepo_noise = NoiseHandler(protocol=NoiseProtocol.DIGITAL.DEPOLARIZING, options={\"error_probability\": 0.1})\nreadout_noise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options={\"error_probability\": 0.1, \"seed\": 0})\n\nnoise_combination = NoiseHandler(protocol=NoiseProtocol.DIGITAL.BITFLIP, options={\"error_probability\": 0.1})\nnoise_combination.append([depo_noise, readout_noise])\nprint(noise_combination)\n</code></pre> <pre><code>Noise(BitFlip, {'error_probability': 0.1})\nNoise(Depolarizing, {'error_probability': 0.1})\nNoise(Independent Readout, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>Finally, one can add directly a few pre-defined types using several <code>NoiseHandler</code> methods:</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\nnoise_combination = NoiseHandler(protocol=NoiseProtocol.DIGITAL.BITFLIP, options={\"error_probability\": 0.1})\nnoise_combination.digital_depolarizing({\"error_probability\": 0.1}).readout_independent({\"error_probability\": 0.1, \"seed\": 0})\nprint(noise_combination)\n</code></pre> <pre><code>Noise(BitFlip, {'error_probability': 0.1})\nNoise(Depolarizing, {'error_probability': 0.1})\nNoise(Independent Readout, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>NoiseHandler scope</p> <p>Note it is not possible to define a <code>NoiseHandler</code> instances with both digital and analog noises, both readout and analog noises, several analog noises, several readout noises, or a readout noise that is not the last defined protocol within <code>NoiseHandler</code>.</p>"},{"location":"tutorials/realistic_sims/noise/#readout-errors","title":"Readout errors","text":"<p>State Preparation and Measurement (SPAM) in the hardware is a major source of noise in the execution of quantum programs. They are typically described using confusion matrices of the form:</p> \\[ T(x|x')=\\delta_{xx'} \\] <p>Two types of readout protocols are available: - <code>NoiseProtocol.READOUT.INDEPENDENT</code> where each bit can be corrupted independently of each other. - <code>NoiseProtocol.READOUT.CORRELATED</code> where we can define of confusion matrix of corruption between each possible bitstrings.</p> <p>Qadence offers to simulate readout errors with the <code>NoiseHandler</code> to corrupt the output samples of a simulation, through execution via a <code>QuantumModel</code>:</p> <pre><code>from qadence import QuantumModel, QuantumCircuit, kron, H, Z\nfrom qadence import hamiltonian_factory\n\n# Simple circuit and observable construction.\nblock = kron(H(0), Z(1))\ncircuit = QuantumCircuit(2, block)\nobservable = hamiltonian_factory(circuit.n_qubits, detuning=Z)\n\n# Construct a quantum model.\nmodel = QuantumModel(circuit=circuit, observable=observable)\n\n# Define a noise model to use.\nnoise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT)\n\n# Run noiseless and noisy simulations.\nnoiseless_samples = model.sample(n_shots=100)\nnoisy_samples = model.sample(noise=noise, n_shots=100)\n</code></pre> <pre><code>noiseless = [OrderedCounter({'10': 51, '00': 49})]\nnoisy = [Counter({'00': 46, '10': 43, '01': 8, '11': 3})]\n</code></pre> <p>It is possible to pass options to the noise model. In the previous example, a noise matrix is implicitly computed from a uniform distribution.</p> <p>For <code>NoiseProtocol.READOUT.INDEPENDENT</code>, the <code>option</code> dictionary argument accepts the following options:</p> <ul> <li><code>seed</code>: defaulted to <code>None</code>, for reproducibility purposes</li> <li><code>error_probability</code>: If float, the same probability is applied to every bit. By default, this is 0.1.     If a 1D tensor with the number of elements equal to the number of qubits, a different probability can be set for each qubit. If a tensor of shape (n_qubits, 2, 2) is passed, that is a confusion matrix obtained from experiments, we extract the error_probability.     and do not compute internally the confusion matrix as in the other cases.</li> <li><code>noise_distribution</code>: defaulted to <code>WhiteNoise.UNIFORM</code>, for non-uniform noise distributions</li> </ul> <p>For <code>NoiseProtocol.READOUT.CORRELATED</code>, the <code>option</code> dictionary argument accepts the following options: - <code>confusion_matrix</code>: The square matrix representing \\(T(x|x')\\) for each possible bitstring of length <code>n</code> qubits. Should be of size (2n, 2n). - <code>seed</code>: defaulted to <code>None</code>, for reproducibility purposes</p> <p>Noisy simulations go hand-in-hand with measurement protocols discussed in the previous section, to assess the impact of noise on expectation values. In this case, both measurement and noise protocols have to be defined appropriately. Please note that a noise protocol without a measurement protocol will be ignored for expectation values computations.</p> <pre><code>from qadence.measurements import Measurements\n\n# Define a noise model with options.\noptions = {\"error_probability\": 0.01}\nnoise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options=options)\n\n# Define a tomographical measurement protocol with options.\noptions = {\"n_shots\": 10000}\nmeasurement = Measurements(protocol=Measurements.TOMOGRAPHY, options=options)\n\n# Run noiseless and noisy simulations.\nnoiseless_exp = model.expectation(measurement=measurement)\nnoisy_exp = model.expectation(measurement=measurement, noise=noise)\n</code></pre> <pre><code>noiseless = tensor([[1.0098]], grad_fn=&lt;TransposeBackward0&gt;)\nnoisy = tensor([[0.9860]], grad_fn=&lt;TransposeBackward0&gt;)\n</code></pre>"},{"location":"tutorials/realistic_sims/noise/#analog-noisy-simulation","title":"Analog noisy simulation","text":"<p>At the moment, analog noisy simulations are only compatable with the Pulser backend. <pre><code>from qadence import DiffMode, NoiseHandler, QuantumModel\nfrom qadence.blocks import chain, kron\nfrom qadence.circuit import QuantumCircuit\nfrom qadence.operations import AnalogRX, AnalogRZ, Z\nfrom qadence.types import PI, BackendName, NoiseProtocol\n\n\nanalog_block = chain(AnalogRX(PI / 2.0), AnalogRZ(PI))\nobservable = Z(0) + Z(1)\ncircuit = QuantumCircuit(2, analog_block)\n\noptions = {\"noise_probs\": 0.1}\nnoise = NoiseHandler(protocol=NoiseProtocol.ANALOG.DEPOLARIZING, options=options)\nmodel_noisy = QuantumModel(\n    circuit=circuit,\n    observable=observable,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR,\n    noise=noise,\n)\nnoisy_expectation = model_noisy.expectation()\n</code></pre> <pre><code>noisy = tensor([[0.3597]])\n</code></pre> </p>"},{"location":"tutorials/realistic_sims/noise/#digital-noisy-simulation","title":"Digital noisy simulation","text":"<p>When dealing with programs involving only digital operations, several options are made available from PyQTorch via the <code>NoiseProtocol.DIGITAL</code>. One can define noisy digital operations as follows:</p> <pre><code>from qadence import NoiseProtocol, RX, run\nimport torch\n\nnoise = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, {\"error_probability\": 0.2})\nop = RX(0, torch.pi, noise = noise)\n\nprint(run(op))\n</code></pre> <pre><code>DensityMatrix([[[0.2000+0.0000e+00j, 0.0000+3.6739e-17j],\n                [0.0000-3.6739e-17j, 0.8000+0.0000e+00j]]])\n</code></pre> <p>It is also possible to set a noise configuration to gates within a composite block or circuit as follows:</p> <pre><code>from qadence import set_noise, chain\n\nn_qubits = 2\n\nblock = chain(RX(i, f\"theta_{i}\") for i in range(n_qubits))\n\nnoise = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, {\"error_probability\": 0.1})\n\n# The function changes the block in place:\nset_noise(block, noise)\nprint(run(block))\n</code></pre> <pre><code>DensityMatrix([[[ 0.5881+0.0000j,  0.0000+0.2672j,  0.0000+0.1836j,\n                 -0.0834+0.0000j],\n                [ 0.0000-0.2672j,  0.2218+0.0000j,  0.0834+0.0000j,\n                  0.0000+0.0693j],\n                [ 0.0000-0.1836j,  0.0834+0.0000j,  0.1380+0.0000j,\n                  0.0000+0.0627j],\n                [-0.0834+0.0000j,  0.0000-0.0693j,  0.0000-0.0627j,\n                  0.0521+0.0000j]]])\n</code></pre> <p>There is an extra optional argument to specify the type of block we want to apply noise to. E.g., let's say we want to apply noise only to <code>X</code> gates, a <code>target_class</code> argument can be passed with the corresponding block:</p> <pre><code>from qadence import X\nblock = chain(RX(0, \"theta\"), X(0))\nset_noise(block, noise, target_class = X)\n\nfor block in block.blocks:\n    print(block.noise)\n</code></pre> <pre><code>None\nNoise(BitFlip, {'error_probability': 0.1})\n</code></pre>"}]}