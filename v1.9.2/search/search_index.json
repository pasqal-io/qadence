{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api/blocks/","title":"Block system","text":"<p><code>qadence</code> offers a block-based system to construct quantum circuits in a flexible manner.</p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock","title":"<code>AbstractBlock(tag=None, __array_priority__=1000)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for both primitive and composite blocks.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>A human-readable name attached to the block type. Notice, this is the same for all the class instances so it cannot be used for identifying different blocks</p> <p> TYPE: <code>str</code> </p> <code>qubit_support</code> <p>The qubit support of the block expressed as a tuple of integers</p> <p> TYPE: <code>tuple[int, ...]</code> </p> <code>tag</code> <p>A tag identifying a particular instance of the block which can be used for identification and pretty printing</p> <p> TYPE: <code>str | None</code> </p> <code>eigenvalues</code> <p>The eigenvalues of the matrix representing the block. This is used mainly for primitive blocks and it's needed for generalized parameter shift rule computations. Currently unused.</p> <p> TYPE: <code>list[float] | None</code> </p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.is_identity","title":"<code>is_identity: bool</code>  <code>property</code>","text":"<p>Identity predicate for blocks.</p>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.n_qubits","title":"<code>n_qubits()</code>","text":"<p>The number of qubits in the whole system.</p> <p>A block acting on qubit N would has at least n_qubits &gt;= N + 1.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef n_qubits(self) -&gt; int:\n    \"\"\"The number of qubits in the whole system.\n\n    A block acting on qubit N would has at least n_qubits &gt;= N + 1.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.n_supports","title":"<code>n_supports()</code>","text":"<p>The number of qubits the block is acting on.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef n_supports(self) -&gt; int:\n    \"\"\"The number of qubits the block is acting on.\"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.abstract.AbstractBlock.qubit_support","title":"<code>qubit_support()</code>","text":"<p>The indices of the qubit(s) the block is acting on.</p> <p>Qadence uses the ordering [0..,N-1] for qubits.</p> Source code in <code>qadence/blocks/abstract.py</code> <pre><code>@abstractproperty\ndef qubit_support(self) -&gt; Tuple[int, ...]:\n    \"\"\"The indices of the qubit(s) the block is acting on.\n\n    Qadence uses the ordering [0..,N-1] for qubits.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#primitive-blocks","title":"Primitive blocks","text":""},{"location":"api/blocks/#qadence.blocks.primitive.ControlBlock","title":"<code>ControlBlock(control, target_block, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The abstract ControlBlock.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: tuple[int, ...],\n    target_block: PrimitiveBlock,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.control = control\n    self.blocks = (target_block,)\n    self.target = target_block.qubit_support\n\n    # using tuple expansion because some control operations could\n    # have multiple targets, e.g. CSWAP\n    super().__init__((*control, *self.target), noise=noise)  # target_block.qubit_support[0]))\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricBlock","title":"<code>ParametricBlock(qubit_support, noise=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>Parameterized primitive blocks.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricBlock.num_parameters","title":"<code>num_parameters()</code>  <code>abstractmethod</code>","text":"<p>The number of parameters required by the block.</p> <p>This is a class property since the number of parameters is defined automatically before instantiating the operation. Also, this could correspond to a larger number of actual user-facing parameters since any parameter expression is allowed</p> <p>Examples: - RX operation has 1 parameter - U operation has 3 parameters - HamEvo has 2 parameters (generator and time evolution)</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>@abstractmethod\ndef num_parameters(cls) -&gt; int:\n    \"\"\"The number of parameters required by the block.\n\n    This is a class property since the number of parameters is defined\n    automatically before instantiating the operation. Also, this could\n    correspond to a larger number of actual user-facing parameters\n    since any parameter expression is allowed\n\n    Examples:\n    - RX operation has 1 parameter\n    - U operation has 3 parameters\n    - HamEvo has 2 parameters (generator and time evolution)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ParametricControlBlock","title":"<code>ParametricControlBlock(control, target_block, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The abstract parametrized ControlBlock.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: tuple[int, ...],\n    target_block: ParametricBlock,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.blocks = (target_block,)\n    self.control = control\n    self.parameters = target_block.parameters\n    super().__init__((*control, *target_block.qubit_support), noise=noise)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.PrimitiveBlock","title":"<code>PrimitiveBlock(qubit_support, noise=None)</code>","text":"<p>               Bases: <code>AbstractBlock</code></p> <p>Primitive blocks represent elementary unitary operations.</p> <p>Examples are single/multi-qubit gates or Hamiltonian evolution. See <code>qadence.operations</code> for a full list of primitive blocks.</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.PrimitiveBlock.digital_decomposition","title":"<code>digital_decomposition()</code>","text":"<p>Decomposition into purely digital gates.</p> <p>This method returns a decomposition of the Block in a combination of purely digital single-qubit and two-qubit 'gates', by manual/custom knowledge of how this can be done efficiently. :return:</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def digital_decomposition(self) -&gt; AbstractBlock:\n    \"\"\"Decomposition into purely digital gates.\n\n    This method returns a decomposition of the Block in a\n    combination of purely digital single-qubit and two-qubit\n    'gates', by manual/custom knowledge of how this can be done efficiently.\n    :return:\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ProjectorBlock","title":"<code>ProjectorBlock(ket, bra, qubit_support, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The abstract ProjectorBlock.</p> <p>Arguments:</p> <pre><code>ket (str): The ket given as a bitstring.\nbra (str): The bra given as a bitstring.\nqubit_support (int | tuple[int]): The qubit_support of the block.\n</code></pre> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    ket: str,\n    bra: str,\n    qubit_support: int | tuple[int, ...],\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    \"\"\"\n    Arguments:\n\n        ket (str): The ket given as a bitstring.\n        bra (str): The bra given as a bitstring.\n        qubit_support (int | tuple[int]): The qubit_support of the block.\n    \"\"\"\n    if isinstance(qubit_support, int):\n        qubit_support = (qubit_support,)\n    if len(bra) != len(ket):\n        raise ValueError(\n            \"Bra and ket must be bitstrings of same length in the 'Projector' definition.\"\n        )\n    elif len(bra) != len(qubit_support):\n        raise ValueError(\"Bra or ket must be of same length as the 'qubit_support'\")\n    for wf in [bra, ket]:\n        if not all(int(item) == 0 or int(item) == 1 for item in wf):\n            raise ValueError(\n                \"All qubits must be either in the '0' or '1' state\"\n                \" in the 'ProjectorBlock' definition.\"\n            )\n\n    self.ket = ket\n    self.bra = bra\n    super().__init__(qubit_support, noise=noise)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.ScaleBlock","title":"<code>ScaleBlock(block, parameter)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>Scale blocks are created when multiplying a block by a number or parameter.</p> <p>Example: <pre><code>from qadence import X\n\nprint(X(0) * 2)\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 X(0)\n</code></pre> </p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(self, block: AbstractBlock, parameter: Any):\n    self.block = block\n    # TODO: more meaningful name like `scale`?\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    super().__init__(block.qubit_support)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.primitive.TimeEvolutionBlock","title":"<code>TimeEvolutionBlock(qubit_support, noise=None)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>Simple time evolution block with time-independent Hamiltonian.</p> <p>This class is just a convenience class which is used to label blocks which contains simple time evolution with time-independent Hamiltonian operators</p> Source code in <code>qadence/blocks/primitive.py</code> <pre><code>def __init__(\n    self,\n    qubit_support: tuple[int, ...],\n    noise: NoiseHandler | None = None,\n):\n    self._qubit_support = qubit_support\n    self._noise = noise\n</code></pre>"},{"location":"api/blocks/#analog-blocks","title":"Analog blocks","text":"<p>To learn how to use analog blocks and how to mix digital &amp; analog blocks, check out the digital-analog section of the documentation.</p> <p>Examples on how to use digital-analog blocks can be found in the *examples folder of the qadence repo:</p> <ul> <li>Fit a simple sinus: <code>examples/digital-analog/fit-sin.py</code></li> <li>Solve a QUBO: <code>examples/digital-analog/qubo.py</code></li> </ul>"},{"location":"api/blocks/#qadence.blocks.analog.AnalogChain","title":"<code>AnalogChain(blocks)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogComposite</code></p> <p>A chain of analog blocks.</p> <p>Needed because analog blocks require stricter validation than the general <code>ChainBlock</code>.</p> <p><code>AnalogChain</code>s can only be constructed from <code>AnalogKron</code> blocks or globally supported, primitive, analog blocks (like <code>InteractionBlock</code>s and <code>ConstantAnalogRotation</code>s).</p> <p>Automatically constructed by the <code>chain</code> function if only analog blocks are given.</p> <p>Example: <pre><code>from qadence import X, chain, AnalogInteraction\n\nb = chain(AnalogInteraction(200), AnalogInteraction(200))\nprint(type(b))  # this is an `AnalogChain`\n\nb = chain(X(0), AnalogInteraction(200))\nprint(type(b))  # this is a general `ChainBlock`\n</code></pre> <pre><code>&lt;class 'qadence.blocks.analog.AnalogChain'&gt;\n&lt;class 'qadence.blocks.composite.ChainBlock'&gt;\n</code></pre> </p> Source code in <code>qadence/blocks/analog.py</code> <pre><code>def __init__(self, blocks: Tuple[AnalogBlock, ...]):\n    \"\"\"A chain of analog blocks.\n\n    Needed because analog blocks require\n    stricter validation than the general `ChainBlock`.\n\n    `AnalogChain`s can only be constructed from `AnalogKron` blocks or\n    _**globally supported**_, primitive, analog blocks (like `InteractionBlock`s and\n    `ConstantAnalogRotation`s).\n\n    Automatically constructed by the [`chain`][qadence.blocks.utils.chain]\n    function if only analog blocks are given.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, chain, AnalogInteraction\n\n    b = chain(AnalogInteraction(200), AnalogInteraction(200))\n    print(type(b))  # this is an `AnalogChain`\n\n    b = chain(X(0), AnalogInteraction(200))\n    print(type(b))  # this is a general `ChainBlock`\n    ```\n    \"\"\"\n    for b in blocks:\n        if not (isinstance(b, AnalogKron) or b.qubit_support.is_global):\n            raise ValueError(\"Only KronBlocks or global blocks can be chain'ed.\")\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.analog.AnalogKron","title":"<code>AnalogKron(blocks, interaction=Interaction.NN)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogComposite</code></p> <p>Stack analog blocks vertically (i.e. in time).</p> <p>Needed because analog require stricter validation than the general <code>KronBlock</code>.</p> <p><code>AnalogKron</code>s can only be constructed from non-global, analog blocks with the same duration.</p> Source code in <code>qadence/blocks/analog.py</code> <pre><code>def __init__(self, blocks: Tuple[AnalogBlock, ...], interaction: Interaction = Interaction.NN):\n    \"\"\"Stack analog blocks vertically (i.e. in time).\n\n    Needed because analog require\n    stricter validation than the general `KronBlock`.\n\n    `AnalogKron`s can only be constructed from _**non-global**_, analog blocks\n    with the _**same duration**_.\n    \"\"\"\n    if len(blocks) == 0:\n        raise NotImplementedError(\"Empty KronBlocks not supported\")\n\n    self.blocks = blocks\n    self.interaction = interaction\n\n    qubit_support = QubitSupport()\n    duration = blocks[0].duration\n    for b in blocks:\n        if not isinstance(b, AnalogBlock):\n            raise ValueError(\"Can only kron `AnalgoBlock`s with other `AnalgoBlock`s.\")\n\n        if b.qubit_support == QubitSupport(\"global\"):\n            raise ValueError(\"Blocks with global support cannot be kron'ed.\")\n\n        if not qubit_support.is_disjoint(b.qubit_support):\n            raise ValueError(\"Make sure blocks act on distinct qubits!\")\n\n        if not np.isclose(evaluate(duration), evaluate(b.duration)):\n            raise ValueError(\"Kron'ed blocks have to have same duration.\")\n\n        qubit_support += b.qubit_support\n\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.analog.ConstantAnalogRotation","title":"<code>ConstantAnalogRotation(tag=None, __array_priority__=1000, _eigenvalues_generator=None, parameters=ParamMap(alpha=0.0, duration=1000.0, omega=0.0, delta=0.0, phase=0.0), qubit_support=QubitSupport('global'), add_pattern=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogBlock</code></p> <p>Implements a constant analog rotation with interaction dictated by the chosen Hamiltonian.</p> <pre><code>H/h = \u2211\u1d62(\u03a9/2 cos(\u03c6)*X\u1d62 - sin(\u03c6)*Y\u1d62 - \u03b4n\u1d62) + H\u1d62\u2099\u209c.\n</code></pre> <p>To construct this block you can use of the following convenience wrappers: - The general rotation operation <code>AnalogRot</code> - Shorthands for rotatins around an axis:   <code>AnalogRX</code>,   <code>AnalogRY</code>,   <code>AnalogRZ</code></p> <p>WARNING: do not use <code>ConstantAnalogRotation</code> with <code>alpha</code> as differentiable parameter - use the convenience wrappers mentioned above.</p>"},{"location":"api/blocks/#qadence.blocks.analog.InteractionBlock","title":"<code>InteractionBlock(tag=None, __array_priority__=1000, _eigenvalues_generator=None, parameters=ParamMap(duration=1000.0), qubit_support=QubitSupport('global'), add_pattern=True)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AnalogBlock</code></p> <p>Free-evolution for the Hamiltonian interaction term of a register of qubits.</p> <p>In real interacting quantum devices, it means letting the system evolve freely according to the time-dependent Schrodinger equation. With emulators, this block is translated to an appropriate interaction Hamiltonian, for example, an Ising interaction</p> <pre><code>H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c\n</code></pre> <p>or an XY-interaction</p> <pre><code>H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2083/r\u2c7c\u2c7c\u00b3 (X\u1d62X\u2c7c + Z\u1d62Z\u2c7c)\n</code></pre> <p>with <code>n\u1d62 = (1-Z\u1d62)/2</code>.</p> <p>To construct, use the <code>AnalogInteraction</code> function.</p>"},{"location":"api/blocks/#composite-blocks","title":"Composite blocks","text":""},{"location":"api/blocks/#qadence.blocks.utils.chain","title":"<code>chain(*args)</code>","text":"<p>Chain blocks sequentially.</p> <p>On digital backends this can be interpreted loosely as a matrix mutliplication of blocks. In the analog case it chains blocks in time.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to chain. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator, List[AbstractBlock]]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>ChainBlock</p> <p>Example: <pre><code>from qadence import X, Y, chain\n\nb = chain(X(0), Y(0))\n\n# or use a generator\nb = chain(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>ChainBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def chain(*args: Union[AbstractBlock, Generator, List[AbstractBlock]]) -&gt; ChainBlock:\n    \"\"\"Chain blocks sequentially.\n\n    On digital backends this can be interpreted\n    loosely as a matrix mutliplication of blocks. In the analog case it chains\n    blocks in time.\n\n    Arguments:\n        *args: Blocks to chain. Can also be a generator.\n\n    Returns:\n        ChainBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, chain\n\n    b = chain(X(0), Y(0))\n\n    # or use a generator\n    b = chain(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    # ugly hack to use `AnalogChain` if we are dealing only with analog blocks\n    if len(args) and all(\n        isinstance(a, AnalogBlock) or isinstance(a, AnalogComposite) for a in args\n    ):\n        return analog_chain(*args)  # type: ignore[return-value,arg-type]\n    return _construct(ChainBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.utils.kron","title":"<code>kron(*args)</code>","text":"<p>Stack blocks vertically.</p> <p>On digital backends this can be intepreted loosely as a kronecker product of blocks. In the analog case it executes blocks parallel in time.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to kron. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>KronBlock</p> <p>Example: <pre><code>from qadence import X, Y, kron\n\nb = kron(X(0), Y(1))\n\n# or use a generator\nb = kron(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def kron(*args: Union[AbstractBlock, Generator]) -&gt; KronBlock:\n    \"\"\"Stack blocks vertically.\n\n    On digital backends this can be intepreted\n    loosely as a kronecker product of blocks. In the analog case it executes\n    blocks parallel in time.\n\n    Arguments:\n        *args: Blocks to kron. Can also be a generator.\n\n    Returns:\n        KronBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, kron\n\n    b = kron(X(0), Y(1))\n\n    # or use a generator\n    b = kron(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    # ugly hack to use `AnalogKron` if we are dealing only with analog blocks\n    if len(args) and all(\n        isinstance(a, AnalogBlock) or isinstance(a, AnalogComposite) for a in args\n    ):\n        return analog_kron(*args)  # type: ignore[return-value,arg-type]\n    return _construct(KronBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.utils.add","title":"<code>add(*args)</code>","text":"<p>Sums blocks.</p> PARAMETER DESCRIPTION <code>*args</code> <p>Blocks to add. Can also be a generator.</p> <p> TYPE: <code>Union[AbstractBlock, Generator]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>AddBlock</code> <p>AddBlock</p> <p>Example: <pre><code>from qadence import X, Y, add\n\nb = add(X(0), Y(0))\n\n# or use a generator\nb = add(X(i) for i in range(3))\nprint(b)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> </p> Source code in <code>qadence/blocks/utils.py</code> <pre><code>def add(*args: Union[AbstractBlock, Generator]) -&gt; AddBlock:\n    \"\"\"Sums blocks.\n\n    Arguments:\n        *args: Blocks to add. Can also be a generator.\n\n    Returns:\n        AddBlock\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import X, Y, add\n\n    b = add(X(0), Y(0))\n\n    # or use a generator\n    b = add(X(i) for i in range(3))\n    print(b)\n    ```\n    \"\"\"\n    return _construct(AddBlock, args)\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.AddBlock","title":"<code>AddBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Adds blocks.</p> <p>Constructed via <code>add</code>.</p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.ChainBlock","title":"<code>ChainBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Chains blocks sequentially.</p> <p>Constructed via <code>chain</code></p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#qadence.blocks.composite.CompositeBlock","title":"<code>CompositeBlock(tag=None, __array_priority__=1000)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractBlock</code></p> <p>Block which composes multiple blocks into one larger block (which can again be composed).</p> <p>Composite blocks are constructed via <code>chain</code>, <code>kron</code>, and <code>add</code>.</p>"},{"location":"api/blocks/#qadence.blocks.composite.KronBlock","title":"<code>KronBlock(blocks)</code>","text":"<p>               Bases: <code>CompositeBlock</code></p> <p>Stacks blocks horizontally.</p> <p>Constructed via <code>kron</code>.</p> Source code in <code>qadence/blocks/composite.py</code> <pre><code>def __init__(self, blocks: Tuple[AbstractBlock, ...]):\n    if len(blocks) == 0:\n        raise NotImplementedError(\"Empty KronBlocks not supported\")\n\n    qubit_support = QubitSupport()\n    for b in blocks:\n        assert (\n            QubitSupportType.GLOBAL,\n        ) != b.qubit_support, \"Blocks with global support cannot be kron'ed.\"\n        assert qubit_support.is_disjoint(\n            b.qubit_support\n        ), \"Make sure blocks act on distinct qubits!\"\n        qubit_support += b.qubit_support\n\n    self.blocks = blocks\n</code></pre>"},{"location":"api/blocks/#converting-blocks-to-matrices","title":"Converting blocks to matrices","text":""},{"location":"api/blocks/#qadence.blocks.block_to_tensor.block_to_tensor","title":"<code>block_to_tensor(block, values={}, qubit_support=None, use_full_support=True, tensor_type=TensorType.DENSE, endianness=Endianness.BIG, device=None)</code>","text":"<p>Convert a block into a torch tensor.</p> PARAMETER DESCRIPTION <code>block</code> <p>The block to convert.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>values</code> <p>A optional dict with values for parameters.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>qubit_support</code> <p>The qubit_support of the block.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>use_full_support</code> <p>True infers the total number of qubits.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>tensor_type</code> <p>the target tensor type.</p> <p> TYPE: <code>TensorType</code> DEFAULT: <code>DENSE</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence import hea, hamiltonian_factory, Z, block_to_tensor\n\nblock = hea(2,2)\nprint(block_to_tensor(block))\n\n# In case you have a diagonal observable, you can use\nobs = hamiltonian_factory(2, detuning = Z)\nprint(block_to_tensor(obs, tensor_type=\"SparseDiagonal\"))\n</code></pre> <pre><code>tensor([[[ 0.3063+0.4951j, -0.5945-0.4631j, -0.2244-0.0349j, -0.1914-0.0698j],\n         [-0.0719-0.0777j,  0.1524+0.2532j, -0.6283-0.4511j, -0.5335-0.1362j],\n         [-0.3898-0.6789j, -0.3944-0.3648j, -0.0536+0.1782j, -0.1754-0.1821j],\n         [-0.1231-0.1478j, -0.2174-0.0940j, -0.2653-0.4951j,  0.5172+0.5691j]]],\n       grad_fn=&lt;UnsafeViewBackward0&gt;)\ntensor(indices=tensor([[0, 3],\n                       [0, 3]]),\n       values=tensor([ 2.+0.j, -2.+0.j]),\n       size=(4, 4), nnz=2, layout=torch.sparse_coo)\n</code></pre> </p> Source code in <code>qadence/blocks/block_to_tensor.py</code> <pre><code>def block_to_tensor(\n    block: AbstractBlock,\n    values: dict[str, TNumber | torch.Tensor] = {},\n    qubit_support: tuple | None = None,\n    use_full_support: bool = True,\n    tensor_type: TensorType = TensorType.DENSE,\n    endianness: Endianness = Endianness.BIG,\n    device: torch.device = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Convert a block into a torch tensor.\n\n    Arguments:\n        block (AbstractBlock): The block to convert.\n        values (dict): A optional dict with values for parameters.\n        qubit_support (tuple): The qubit_support of the block.\n        use_full_support (bool): True infers the total number of qubits.\n        tensor_type (TensorType): the target tensor type.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import hea, hamiltonian_factory, Z, block_to_tensor\n\n    block = hea(2,2)\n    print(block_to_tensor(block))\n\n    # In case you have a diagonal observable, you can use\n    obs = hamiltonian_factory(2, detuning = Z)\n    print(block_to_tensor(obs, tensor_type=\"SparseDiagonal\"))\n    ```\n    \"\"\"\n\n    # FIXME: default use_full_support to False. In general, it would\n    # be more efficient to do that, and make sure that computations such\n    # as observables only do the matmul of the size of the qubit support.\n\n    if tensor_type == TensorType.DENSE:\n        from qadence.blocks import embedding\n\n        (ps, embed) = embedding(block)\n        return _block_to_tensor_embedded(\n            block,\n            embed(ps, values),\n            qubit_support,\n            use_full_support,\n            endianness=endianness,\n            device=device,\n        )\n\n    elif tensor_type == TensorType.SPARSEDIAGONAL:\n        t = block_to_diagonal(block, endianness=endianness)\n        indices, values, size = torch.nonzero(t), t[t != 0], len(t)\n        indices = torch.stack((indices.flatten(), indices.flatten()))\n        return torch.sparse_coo_tensor(indices, values, (size, size))\n</code></pre>"},{"location":"api/constructors/","title":"Constructors for common quantum circuits","text":""},{"location":"api/constructors/#qadence.constructors.feature_maps.exp_fourier_feature_map","title":"<code>exp_fourier_feature_map(n_qubits, support=None, param='x', feature_range=None)</code>","text":"<p>Exponential fourier feature map.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the feature</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>qubit support</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>param</code> <p>name of feature <code>Parameter</code></p> <p> TYPE: <code>str</code> DEFAULT: <code>'x'</code> </p> <code>feature_range</code> <p>min and max value of the feature, as floats in a Tuple</p> <p> TYPE: <code>tuple[float, float]</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/constructors/feature_maps.py</code> <pre><code>def exp_fourier_feature_map(\n    n_qubits: int,\n    support: tuple[int, ...] = None,\n    param: str = \"x\",\n    feature_range: tuple[float, float] = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Exponential fourier feature map.\n\n    Args:\n        n_qubits: number of qubits in the feature\n        support: qubit support\n        param: name of feature `Parameter`\n        feature_range: min and max value of the feature, as floats in a Tuple\n    \"\"\"\n\n    if feature_range is None:\n        feature_range = (0.0, 2.0**n_qubits)\n\n    support = tuple(range(n_qubits)) if support is None else support\n    hlayer = kron(H(qubit) for qubit in support)\n    rlayer = feature_map(\n        n_qubits,\n        support=support,\n        param=param,\n        op=RZ,\n        fm_type=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.EXP,\n        feature_range=feature_range,\n        target_range=(0.0, 2 * PI),\n    )\n    rlayer.tag = None\n    return tag(chain(hlayer, rlayer), f\"ExpFourierFM({param})\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.feature_maps.feature_map","title":"<code>feature_map(n_qubits, support=None, param='phi', op=RX, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT, feature_range=None, target_range=None, multiplier=None, param_prefix=None)</code>","text":"<p>Construct a feature map of a given type.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Number of qubits the feature map covers. Results in <code>support=range(n_qubits)</code>.</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>Puts one feature-encoding rotation gate on every qubit in <code>support</code>. n_qubits in this case specifies the total overall qubits of the circuit, which may be wider than the support itself, but not narrower.</p> <p> TYPE: <code>tuple[int, ...] | None</code> DEFAULT: <code>None</code> </p> <code>param</code> <p>Parameter of the feature map; you can pass a string or Parameter; it will be set as non-trainable (FeatureParameter) regardless.</p> <p> TYPE: <code>Parameter | str</code> DEFAULT: <code>'phi'</code> </p> <code>op</code> <p>Rotation operation of the feature map; choose from RX, RY, RZ or PHASE.</p> <p> TYPE: <code>RotationTypes</code> DEFAULT: <code>RX</code> </p> <code>fm_type</code> <p>Basis set for data encoding; choose from <code>BasisSet.FOURIER</code> for Fourier encoding, or <code>BasisSet.CHEBYSHEV</code> for Chebyshev polynomials of the first kind.</p> <p> TYPE: <code>BasisSet | Callable | str</code> DEFAULT: <code>FOURIER</code> </p> <code>reupload_scaling</code> <p>how the feature map scales the data that is re-uploaded for each qubit. choose from <code>ReuploadScaling</code> enumeration or provide your own function with a single int as input and int or float as output.</p> <p> TYPE: <code>ReuploadScaling | Callable | str</code> DEFAULT: <code>CONSTANT</code> </p> <code>feature_range</code> <p>range of data that the input data provided comes from. Used to map input data to the correct domain of the feature-encoding function.</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> <code>target_range</code> <p>range of data the data encoder assumes as the natural range. For example, in Chebyshev polynomials it is (-1, 1), while for Fourier it may be chosen as (0, 2*PI). Used to map data to the correct domain of the feature-encoding function.</p> <p> TYPE: <code>tuple[float, float] | None</code> DEFAULT: <code>None</code> </p> <code>multiplier</code> <p>overall multiplier; this is useful for reuploading the feature map serially with different scalings; can be a number or parameter/expression.</p> <p> TYPE: <code>Parameter | TParameter | None</code> DEFAULT: <code>None</code> </p> <code>param_prefix</code> <p>string prefix to create trainable parameters multiplying the feature parameter inside the feature-encoding function. Note that currently this does not take into account the domain of the feature-encoding function.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <p>Example: <pre><code>from qadence import feature_map, BasisSet, ReuploadScaling\n\nfm = feature_map(3, fm_type=BasisSet.FOURIER)\nprint(f\"{fm = }\")\n\nfm = feature_map(3, fm_type=BasisSet.CHEBYSHEV)\nprint(f\"{fm = }\")\n\nfm = feature_map(3, fm_type=BasisSet.FOURIER, reupload_scaling = ReuploadScaling.TOWER)\nprint(f\"{fm = }\")\n</code></pre> <pre><code>fm = KronBlock(0,1,2) [tag: Constant Fourier FM]\n\u251c\u2500\u2500 RX(0) [params: ['phi']]\n\u251c\u2500\u2500 RX(1) [params: ['phi']]\n\u2514\u2500\u2500 RX(2) [params: ['phi']]\nfm = KronBlock(0,1,2) [tag: Constant Chebyshev FM]\n\u251c\u2500\u2500 RX(0) [params: ['acos(phi)']]\n\u251c\u2500\u2500 RX(1) [params: ['acos(phi)']]\n\u2514\u2500\u2500 RX(2) [params: ['acos(phi)']]\nfm = KronBlock(0,1,2) [tag: Tower Fourier FM]\n\u251c\u2500\u2500 RX(0) [params: ['1_0*phi']]\n\u251c\u2500\u2500 RX(1) [params: ['2_0*phi']]\n\u2514\u2500\u2500 RX(2) [params: ['3_0*phi']]\n</code></pre> </p> Source code in <code>qadence/constructors/feature_maps.py</code> <pre><code>def feature_map(\n    n_qubits: int,\n    support: tuple[int, ...] | None = None,\n    param: Parameter | str = \"phi\",\n    op: RotationTypes = RX,\n    fm_type: BasisSet | Callable | str = BasisSet.FOURIER,\n    reupload_scaling: ReuploadScaling | Callable | str = ReuploadScaling.CONSTANT,\n    feature_range: tuple[float, float] | None = None,\n    target_range: tuple[float, float] | None = None,\n    multiplier: Parameter | TParameter | None = None,\n    param_prefix: str | None = None,\n) -&gt; KronBlock:\n    \"\"\"Construct a feature map of a given type.\n\n    Arguments:\n        n_qubits: Number of qubits the feature map covers. Results in `support=range(n_qubits)`.\n        support: Puts one feature-encoding rotation gate on every qubit in `support`. n_qubits in\n            this case specifies the total overall qubits of the circuit, which may be wider than the\n            support itself, but not narrower.\n        param: Parameter of the feature map; you can pass a string or Parameter;\n            it will be set as non-trainable (FeatureParameter) regardless.\n        op: Rotation operation of the feature map; choose from RX, RY, RZ or PHASE.\n        fm_type: Basis set for data encoding; choose from `BasisSet.FOURIER` for Fourier\n            encoding, or `BasisSet.CHEBYSHEV` for Chebyshev polynomials of the first kind.\n        reupload_scaling: how the feature map scales the data that is re-uploaded for each qubit.\n            choose from `ReuploadScaling` enumeration or provide your own function with a single\n            int as input and int or float as output.\n        feature_range: range of data that the input data provided comes from. Used to map input data\n            to the correct domain of the feature-encoding function.\n        target_range: range of data the data encoder assumes as the natural range. For example,\n            in Chebyshev polynomials it is (-1, 1), while for Fourier it may be chosen as (0, 2*PI).\n            Used to map data to the correct domain of the feature-encoding function.\n        multiplier: overall multiplier; this is useful for reuploading the feature map serially with\n            different scalings; can be a number or parameter/expression.\n        param_prefix: string prefix to create trainable parameters multiplying the feature parameter\n            inside the feature-encoding function. Note that currently this does not take into\n            account the domain of the feature-encoding function.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import feature_map, BasisSet, ReuploadScaling\n\n    fm = feature_map(3, fm_type=BasisSet.FOURIER)\n    print(f\"{fm = }\")\n\n    fm = feature_map(3, fm_type=BasisSet.CHEBYSHEV)\n    print(f\"{fm = }\")\n\n    fm = feature_map(3, fm_type=BasisSet.FOURIER, reupload_scaling = ReuploadScaling.TOWER)\n    print(f\"{fm = }\")\n    ```\n    \"\"\"\n\n    # Process input\n    if support is None:\n        support = tuple(range(n_qubits))\n    elif len(support) != n_qubits:\n        raise ValueError(\"Wrong qubit support supplied\")\n\n    if op not in ROTATIONS:\n        raise ValueError(\n            f\"Operation {op} not supported. \"\n            f\"Please provide one from {[rot.__name__ for rot in ROTATIONS]}.\"\n        )\n\n    scaled_fparam = fm_parameter_scaling(\n        fm_type, param, feature_range=feature_range, target_range=target_range\n    )\n\n    transform_func = fm_parameter_func(fm_type)\n\n    basis_tag = fm_type.value if isinstance(fm_type, BasisSet) else str(fm_type)\n    rs_func, rs_tag = fm_reupload_scaling_fn(reupload_scaling)\n\n    # Set overall multiplier\n    multiplier = 1 if multiplier is None else Parameter(multiplier)\n\n    # Build feature map\n    op_list = []\n    fparam = scaled_fparam\n    for i, qubit in enumerate(support):\n        if param_prefix is not None:\n            train_param = VariationalParameter(param_prefix + f\"_{i}\")\n            fparam = train_param * scaled_fparam\n        op_list.append(op(qubit, multiplier * rs_func(i) * transform_func(fparam)))\n    fm = kron(*op_list)\n\n    fm.tag = rs_tag + \" \" + basis_tag + \" FM\"\n\n    return fm\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.alt","title":"<code>alt(n_qubits, m_block_qubits, depth=1, param_prefix='theta', support=None, strategy=Strategy.DIGITAL, **strategy_args)</code>","text":"<p>Factory function for the alternating layer ansatz (alt).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>number of qubits in the local entangling block</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the alt</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indexes where the alt is applied</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>strategy</code> <p>Strategy.Digital or Strategy.DigitalAnalog</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>**strategy_args</code> <p>see below</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> PARAMETER DESCRIPTION <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer. Valid for Digital .</p> <p> TYPE: <code>list</code> </p> <code>entangler</code> <ul> <li>Digital: 2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational parameters on the rotation angles.</li> </ul> <p> TYPE: <code>AbstractBlock</code> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def alt(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] = None,\n    strategy: Strategy = Strategy.DIGITAL,\n    **strategy_args: Any,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Factory function for the alternating layer ansatz (alt).\n\n    Args:\n        n_qubits: number of qubits in the block\n        m_block_qubits: number of qubits in the local entangling block\n        depth: number of layers of the alt\n        param_prefix: the base name of the variational parameters\n        support: qubit indexes where the alt is applied\n        strategy: Strategy.Digital or Strategy.DigitalAnalog\n        **strategy_args: see below\n\n    Keyword Arguments:\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer. Valid for\n            Digital .\n        entangler (AbstractBlock):\n            - Digital: 2-qubit entangling operation. Supports CNOT, CZ,\n            CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational\n            parameters on the rotation angles.\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    alt_func_dict = {\n        Strategy.DIGITAL: alt_digital,\n        Strategy.SDAQC: alt_sDAQC,\n        Strategy.BDAQC: alt_bDAQC,\n        Strategy.ANALOG: alt_analog,\n    }\n\n    try:\n        alt_func = alt_func_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    hea_block: AbstractBlock = alt_func(\n        n_qubits=n_qubits,\n        m_block_qubits=m_block_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        **strategy_args,\n    )  # type: ignore\n\n    return hea_block\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.alt_digital","title":"<code>alt_digital(n_qubits, m_block_qubits, depth=1, support=None, param_prefix='theta', operations=[RX, RY], entangler=CNOT)</code>","text":"<p>Construct the digital alternating layer ansatz (ALT).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the ansatz.</p> <p> TYPE: <code>int</code> </p> <code>m_block_qubits</code> <p>number of qubits in the local entangling block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the ALT.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY]</code> </p> <code>support</code> <p>qubit indexes where the ALT is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def alt_digital(\n    n_qubits: int,\n    m_block_qubits: int,\n    depth: int = 1,\n    support: tuple[int, ...] = None,\n    param_prefix: str = \"theta\",\n    operations: list[type[AbstractBlock]] = [RX, RY],\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the digital alternating layer ansatz (ALT).\n\n    Args:\n        n_qubits (int): number of qubits in the ansatz.\n        m_block_qubits (int): number of qubits in the local entangling block.\n        depth (int): number of layers of the ALT.\n        param_prefix (str): the base name of the variational parameters\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        support (tuple): qubit indexes where the ALT is applied.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n    \"\"\"\n\n    try:\n        if entangler not in [CNOT, CZ, CRX, CRY, CRZ, CPHASE]:\n            raise ValueError(\n                \"Please provide a valid two-qubit entangler operation for digital ALT.\"\n            )\n    except TypeError:\n        raise ValueError(\"Please provide a valid two-qubit entangler operation for digital ALT.\")\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        support=support,\n        param_prefix=param_prefix,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_block_digital(\n        n_qubits,\n        m_block_qubits,\n        param_prefix=param_prefix + \"_ent\",\n        depth=depth,\n        support=support,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n\n    return tag(chain(*layers), \"ALT\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.hea","title":"<code>hea(n_qubits, depth=1, param_prefix='theta', support=None, strategy=Strategy.DIGITAL, **strategy_args)</code>","text":"<p>Factory function for the Hardware Efficient Ansatz (HEA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>support</code> <p>qubit indexes where the HEA is applied</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>strategy</code> <p>Strategy.Digital or Strategy.DigitalAnalog</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>**strategy_args</code> <p>see below</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> PARAMETER DESCRIPTION <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer. Valid for Digital and DigitalAnalog HEA.</p> <p> TYPE: <code>list</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c. Valid for only for Digital HEA.</p> <p> TYPE: <code>bool</code> </p> <code>entangler</code> <ul> <li>Digital: 2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational parameters on the rotation angles.</li> <li>DigitaAnalog | Analog: Hamiltonian generator for the analog entangling layer. Defaults to global ZZ Hamiltonian. Time parameter is considered variational.</li> </ul> <p> TYPE: <code>AbstractBlock</code> </p> <p>Examples: <pre><code>from qadence import RZ, RX\nfrom qadence import hea\n\n# create the circuit\nn_qubits, depth = 2, 4\nansatz = hea(\n    n_qubits=n_qubits,\n    depth=depth,\n    strategy=\"sDAQC\",\n    operations=[RZ,RX,RZ]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def hea(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    support: tuple[int, ...] = None,\n    strategy: Strategy = Strategy.DIGITAL,\n    **strategy_args: Any,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Factory function for the Hardware Efficient Ansatz (HEA).\n\n    Args:\n        n_qubits: number of qubits in the block\n        depth: number of layers of the HEA\n        param_prefix: the base name of the variational parameters\n        support: qubit indexes where the HEA is applied\n        strategy: Strategy.Digital or Strategy.DigitalAnalog\n        **strategy_args: see below\n\n    Keyword Arguments:\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer. Valid for\n            Digital and DigitalAnalog HEA.\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c. Valid for only\n            for Digital HEA.\n        entangler (AbstractBlock):\n            - Digital: 2-qubit entangling operation. Supports CNOT, CZ,\n            CRX, CRY, CRZ, CPHASE. Controlled rotations will have variational\n            parameters on the rotation angles.\n            - DigitaAnalog | Analog: Hamiltonian generator for the\n            analog entangling layer. Defaults to global ZZ Hamiltonian.\n            Time parameter is considered variational.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import RZ, RX\n    from qadence import hea\n\n    # create the circuit\n    n_qubits, depth = 2, 4\n    ansatz = hea(\n        n_qubits=n_qubits,\n        depth=depth,\n        strategy=\"sDAQC\",\n        operations=[RZ,RX,RZ]\n    )\n    ```\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    hea_func_dict = {\n        Strategy.DIGITAL: hea_digital,\n        Strategy.SDAQC: hea_sDAQC,\n        Strategy.BDAQC: hea_bDAQC,\n        Strategy.ANALOG: hea_analog,\n    }\n\n    try:\n        hea_func = hea_func_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    hea_block: AbstractBlock = hea_func(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        **strategy_args,\n    )  # type: ignore\n\n    return hea_block\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.hea_digital","title":"<code>hea_digital(n_qubits, depth=1, param_prefix='theta', periodic=False, operations=[RX, RY, RX], support=None, entangler=CNOT)</code>","text":"<p>Construct the Digital Hardware Efficient Ansatz (HEA).</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>periodic</code> <p>if the qubits should be linked periodically. periodic=False is not supported in emu-c.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY, RX]</code> </p> <code>support</code> <p>qubit indexes where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>entangler</code> <p>2-qubit entangling operation. Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations will have variational parameters on the rotation angles.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>CNOT</code> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def hea_digital(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    periodic: bool = False,\n    operations: list[type[AbstractBlock]] = [RX, RY, RX],\n    support: tuple[int, ...] = None,\n    entangler: Type[DigitalEntanglers] = CNOT,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the Digital Hardware Efficient Ansatz (HEA).\n\n    Args:\n        n_qubits (int): number of qubits in the block.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        periodic (bool): if the qubits should be linked periodically.\n            periodic=False is not supported in emu-c.\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        support (tuple): qubit indexes where the HEA is applied.\n        entangler (AbstractBlock): 2-qubit entangling operation.\n            Supports CNOT, CZ, CRX, CRY, CRZ. Controlld rotations\n            will have variational parameters on the rotation angles.\n    \"\"\"\n    try:\n        if entangler not in [CNOT, CZ, CRX, CRY, CRZ, CPHASE]:\n            raise ValueError(\n                \"Please provide a valid two-qubit entangler operation for digital HEA.\"\n            )\n    except TypeError:\n        raise ValueError(\"Please provide a valid two-qubit entangler operation for digital HEA.\")\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        periodic=periodic,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n    return tag(chain(*layers), \"HEA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.ansatze.hea_sDAQC","title":"<code>hea_sDAQC(n_qubits, depth=1, param_prefix='theta', operations=[RX, RY, RX], support=None, entangler=None)</code>","text":"<p>Construct the Hardware Efficient Ansatz (HEA) with analog entangling layers.</p> <p>It uses step-wise digital-analog computation.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the block.</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>number of layers of the HEA.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>param_prefix</code> <p>the base name of the variational parameters</p> <p> TYPE: <code>str</code> DEFAULT: <code>'theta'</code> </p> <code>operations</code> <p>list of operations to cycle through in the digital single-qubit rotations of each layer.</p> <p> TYPE: <code>list</code> DEFAULT: <code>[RX, RY, RX]</code> </p> <code>support</code> <p>qubit indexes where the HEA is applied.</p> <p> TYPE: <code>tuple</code> DEFAULT: <code>None</code> </p> <code>entangler</code> <p>Hamiltonian generator for the analog entangling layer. Defaults to global ZZ Hamiltonian. Time parameter is considered variational.</p> <p> TYPE: <code>AbstractBlock</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/constructors/ansatze.py</code> <pre><code>def hea_sDAQC(\n    n_qubits: int,\n    depth: int = 1,\n    param_prefix: str = \"theta\",\n    operations: list[type[AbstractBlock]] = [RX, RY, RX],\n    support: tuple[int, ...] = None,\n    entangler: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Construct the Hardware Efficient Ansatz (HEA) with analog entangling layers.\n\n    It uses step-wise digital-analog computation.\n\n    Args:\n        n_qubits (int): number of qubits in the block.\n        depth (int): number of layers of the HEA.\n        param_prefix (str): the base name of the variational parameters\n        operations (list): list of operations to cycle through in the\n            digital single-qubit rotations of each layer.\n        support (tuple): qubit indexes where the HEA is applied.\n        entangler (AbstractBlock): Hamiltonian generator for the\n            analog entangling layer. Defaults to global ZZ Hamiltonian.\n            Time parameter is considered variational.\n    \"\"\"\n\n    # TODO: Add qubit support\n    if entangler is None:\n        entangler = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n    try:\n        if not block_is_qubit_hamiltonian(entangler):\n            raise ValueError(\n                \"Please provide a valid Pauli Hamiltonian generator for digital-analog HEA.\"\n            )\n    except NotImplementedError:\n        raise ValueError(\n            \"Please provide a valid Pauli Hamiltonian generator for digital-analog HEA.\"\n        )\n\n    rot_list = _rotations_digital(\n        n_qubits=n_qubits,\n        depth=depth,\n        param_prefix=param_prefix,\n        support=support,\n        operations=operations,\n    )\n\n    ent_list = _entanglers_analog(\n        depth=depth,\n        param_prefix=param_prefix,\n        entangler=entangler,\n    )\n\n    layers = []\n    for d in range(depth):\n        layers.append(rot_list[d])\n        layers.append(ent_list[d])\n    return tag(chain(*layers), \"HEA-sDA\")\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig","title":"<code>ObservableConfig(detuning, scale=1.0, shift=0.0, transformation_type=ObservableTransform.NONE, trainable_transform=None)</code>  <code>dataclass</code>","text":""},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.detuning","title":"<code>detuning: TDetuning</code>  <code>instance-attribute</code>","text":"<p>Single qubit detuning of the observable Hamiltonian.</p> <p>Accepts single-qubit operator N, X, Y, or Z.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.scale","title":"<code>scale: TParameter = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The scale by which to multiply the output of the observable.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.shift","title":"<code>shift: TParameter = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The shift to add to the output of the observable.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.trainable_transform","title":"<code>trainable_transform: bool | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to have a trainable transformation on the output of the observable.</p> <p>If None, the scale and shift are numbers. If True, the scale and shift are VariationalParameter. If False, the scale and shift are FeatureParameter.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.ObservableConfig.transformation_type","title":"<code>transformation_type: ObservableTransform = ObservableTransform.NONE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The type of transformation.</p>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.hamiltonian_factory","title":"<code>hamiltonian_factory(register, interaction=None, detuning=None, interaction_strength=None, detuning_strength=None, random_strength=False, use_all_node_pairs=False)</code>","text":"<p>General Hamiltonian creation function.</p> <p>Can be used to create Hamiltonians with 2-qubit interactions and single-qubit detunings, both with arbitrary strength or parameterized.</p> PARAMETER DESCRIPTION <code>register</code> <p>register of qubits with a specific graph topology, or number of qubits. When passing a number of qubits a register with all-to-all connectivity is created.</p> <p> TYPE: <code>Register | int</code> </p> <code>interaction</code> <p>Interaction.ZZ, Interaction.NN, Interaction.XY, or Interacton.XYZ.</p> <p> TYPE: <code>Interaction | Callable | None</code> DEFAULT: <code>None</code> </p> <code>detuning</code> <p>single-qubit operator N, X, Y, or Z.</p> <p> TYPE: <code>TDetuning | None</code> DEFAULT: <code>None</code> </p> <code>interaction_strength</code> <p>list of values to be used as the interaction strength for each pair of qubits. Should be ordered following the order of <code>Register(n_qubits).edges</code>. Alternatively, some string \"x\" can be passed, which will create a parameterized interactions for each pair of qubits, each labelled as <code>\"x_ij\"</code>.</p> <p> TYPE: <code>TArray | str | None</code> DEFAULT: <code>None</code> </p> <code>detuning_strength</code> <p>list of values to be used as the detuning strength for each qubit. Alternatively, some string \"x\" can be passed, which will create a parameterized detuning for each qubit, each labelled as <code>\"x_i\"</code>.</p> <p> TYPE: <code>TArray | str | None</code> DEFAULT: <code>None</code> </p> <code>random_strength</code> <p>set random interaction and detuning strengths between -1 and 1.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>use_all_node_pairs</code> <p>computes an interaction term for every pair of nodes in the graph, independent of the edge topology in the register. Useful for defining Hamiltonians where the interaction strength decays with the distance.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>from qadence import hamiltonian_factory, Interaction, Register, Z\n\nn_qubits = 3\n\n# Constant total magnetization observable:\nobservable = hamiltonian_factory(n_qubits, detuning = Z)\n\n# Parameterized total magnetization observable:\nobservable = hamiltonian_factory(n_qubits, detuning = Z, detuning_strength = \"z\")\n\n# Random all-to-all XY Hamiltonian generator:\ngenerator = hamiltonian_factory(\n    n_qubits,\n    interaction = Interaction.XY,\n    random_strength = True,\n    )\n\n# Parameterized NN Hamiltonian generator with a square grid interaction topology:\nregister = Register.square(qubits_side = n_qubits)\ngenerator = hamiltonian_factory(\n    register,\n    interaction = Interaction.NN,\n    interaction_strength = \"theta\"\n    )\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def hamiltonian_factory(\n    register: Register | int,\n    interaction: Interaction | Callable | None = None,\n    detuning: TDetuning | None = None,\n    interaction_strength: TArray | str | None = None,\n    detuning_strength: TArray | str | None = None,\n    random_strength: bool = False,\n    use_all_node_pairs: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    General Hamiltonian creation function.\n\n    Can be used to create Hamiltonians with 2-qubit\n    interactions and single-qubit detunings, both with arbitrary strength or parameterized.\n\n    Arguments:\n        register: register of qubits with a specific graph topology, or number of qubits.\n            When passing a number of qubits a register with all-to-all connectivity\n            is created.\n        interaction: Interaction.ZZ, Interaction.NN, Interaction.XY, or Interacton.XYZ.\n        detuning: single-qubit operator N, X, Y, or Z.\n        interaction_strength: list of values to be used as the interaction strength for each\n            pair of qubits. Should be ordered following the order of `Register(n_qubits).edges`.\n            Alternatively, some string \"x\" can be passed, which will create a parameterized\n            interactions for each pair of qubits, each labelled as `\"x_ij\"`.\n        detuning_strength: list of values to be used as the detuning strength for each qubit.\n            Alternatively, some string \"x\" can be passed, which will create a parameterized\n            detuning for each qubit, each labelled as `\"x_i\"`.\n        random_strength: set random interaction and detuning strengths between -1 and 1.\n        use_all_node_pairs: computes an interaction term for every pair of nodes in the graph,\n            independent of the edge topology in the register. Useful for defining Hamiltonians\n            where the interaction strength decays with the distance.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import hamiltonian_factory, Interaction, Register, Z\n\n        n_qubits = 3\n\n        # Constant total magnetization observable:\n        observable = hamiltonian_factory(n_qubits, detuning = Z)\n\n        # Parameterized total magnetization observable:\n        observable = hamiltonian_factory(n_qubits, detuning = Z, detuning_strength = \"z\")\n\n        # Random all-to-all XY Hamiltonian generator:\n        generator = hamiltonian_factory(\n            n_qubits,\n            interaction = Interaction.XY,\n            random_strength = True,\n            )\n\n        # Parameterized NN Hamiltonian generator with a square grid interaction topology:\n        register = Register.square(qubits_side = n_qubits)\n        generator = hamiltonian_factory(\n            register,\n            interaction = Interaction.NN,\n            interaction_strength = \"theta\"\n            )\n        ```\n    \"\"\"\n\n    if interaction is None and detuning is None:\n        raise ValueError(\"Please provide an interaction and/or detuning for the Hamiltonian.\")\n\n    # If number of qubits is given, creates all-to-all register\n    register = Register(register) if isinstance(register, int) else register\n\n    # Get interaction function\n    if interaction is not None:\n        if callable(interaction):\n            int_fn = interaction\n            try:\n                if not block_is_qubit_hamiltonian(interaction(0, 1)):\n                    raise ValueError(\"Custom interactions must be composed of Pauli operators.\")\n            except TypeError:\n                raise TypeError(\n                    \"Please use a custom interaction function signed with two integer parameters.\"\n                )\n        else:\n            int_fn = INTERACTION_DICT.get(interaction, None)  # type: ignore [arg-type]\n            if int_fn is None:\n                raise KeyError(f\"Interaction {interaction} not supported.\")\n\n    # Check single-qubit detuning\n    if (detuning is not None) and (detuning not in DETUNINGS):\n        raise TypeError(f\"Detuning of type {type(detuning)} not supported.\")\n\n    # Pre-process detuning and interaction strengths and update register\n    detuning_strength_array = _preprocess_strengths(\n        register, detuning_strength, \"nodes\", random_strength\n    )\n\n    edge_str = \"all_node_pairs\" if use_all_node_pairs else \"edges\"\n    interaction_strength_array = _preprocess_strengths(\n        register, interaction_strength, edge_str, random_strength\n    )\n\n    # Create single-qubit detunings:\n    single_qubit_terms: List[AbstractBlock] = []\n    if detuning is not None:\n        for strength, node in zip(detuning_strength_array, register.nodes):\n            single_qubit_terms.append(strength * detuning(node))\n\n    # Create two-qubit interactions:\n    two_qubit_terms: List[AbstractBlock] = []\n    edge_data = register.all_node_pairs if use_all_node_pairs else register.edges\n    if interaction is not None and int_fn is not None:\n        for strength, edge in zip(interaction_strength_array, edge_data):\n            two_qubit_terms.append(strength * int_fn(*edge))\n\n    return add(*single_qubit_terms, *two_qubit_terms)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_nn","title":"<code>interaction_nn(i, j)</code>","text":"<p>Ising NN interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_nn(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Ising NN interaction.\"\"\"\n    return N(i) @ N(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_xy","title":"<code>interaction_xy(i, j)</code>","text":"<p>XY interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_xy(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"XY interaction.\"\"\"\n    return X(i) @ X(j) + Y(i) @ Y(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_xyz","title":"<code>interaction_xyz(i, j)</code>","text":"<p>Heisenberg XYZ interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_xyz(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Heisenberg XYZ interaction.\"\"\"\n    return X(i) @ X(j) + Y(i) @ Y(j) + Z(i) @ Z(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.hamiltonians.interaction_zz","title":"<code>interaction_zz(i, j)</code>","text":"<p>Ising ZZ interaction.</p> Source code in <code>qadence/constructors/hamiltonians.py</code> <pre><code>def interaction_zz(i: int, j: int) -&gt; AbstractBlock:\n    \"\"\"Ising ZZ interaction.\"\"\"\n    return Z(i) @ Z(j)\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.qft.qft","title":"<code>qft(n_qubits, support=None, inverse=False, reverse_in=False, swaps_out=False, strategy=Strategy.DIGITAL, gen_build=None)</code>","text":"<p>The Quantum Fourier Transform.</p> <p>Depending on the application, user should be careful with qubit ordering in the input and output. This can be controlled with reverse_in and swaps_out arguments.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>number of qubits in the QFT</p> <p> TYPE: <code>int</code> </p> <code>support</code> <p>qubit support to use</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>inverse</code> <p>True performs the inverse QFT</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>reverse_in</code> <p>Reverses the input qubits to account for endianness</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>swaps_out</code> <p>Performs swaps on the output qubits to match the \"textbook\" QFT.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>strategy</code> <p>Strategy.Digital or Strategy.sDAQC</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>DIGITAL</code> </p> <code>gen_build</code> <p>building block Ising Hamiltonian for the DAQC transform. Defaults to constant all-to-all Ising.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>from qadence import qft\n\nn_qubits = 3\n\nqft_circuit = qft(n_qubits, strategy = \"sDAQC\")\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/qft.py</code> <pre><code>def qft(\n    n_qubits: int,\n    support: tuple[int, ...] = None,\n    inverse: bool = False,\n    reverse_in: bool = False,\n    swaps_out: bool = False,\n    strategy: Strategy = Strategy.DIGITAL,\n    gen_build: AbstractBlock | None = None,\n) -&gt; AbstractBlock:\n    \"\"\"\n    The Quantum Fourier Transform.\n\n    Depending on the application, user should be careful with qubit ordering\n    in the input and output. This can be controlled with reverse_in and swaps_out\n    arguments.\n\n    Args:\n        n_qubits: number of qubits in the QFT\n        support: qubit support to use\n        inverse: True performs the inverse QFT\n        reverse_in: Reverses the input qubits to account for endianness\n        swaps_out: Performs swaps on the output qubits to match the \"textbook\" QFT.\n        strategy: Strategy.Digital or Strategy.sDAQC\n        gen_build: building block Ising Hamiltonian for the DAQC transform.\n            Defaults to constant all-to-all Ising.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import qft\n\n        n_qubits = 3\n\n        qft_circuit = qft(n_qubits, strategy = \"sDAQC\")\n        ```\n    \"\"\"\n\n    if support is None:\n        support = tuple(range(n_qubits))\n\n    assert len(support) &lt;= n_qubits, \"Wrong qubit support supplied\"\n\n    if reverse_in:\n        support = support[::-1]\n\n    qft_layer_dict = {\n        Strategy.DIGITAL: _qft_layer_digital,\n        Strategy.SDAQC: _qft_layer_sDAQC,\n        Strategy.BDAQC: _qft_layer_bDAQC,\n        Strategy.ANALOG: _qft_layer_analog,\n    }\n\n    try:\n        layer_func = qft_layer_dict[strategy]\n    except KeyError:\n        raise KeyError(f\"Strategy {strategy} not recognized.\")\n\n    qft_layers = reversed(range(n_qubits)) if inverse else range(n_qubits)\n\n    qft_circ = chain(\n        layer_func(\n            n_qubits=n_qubits, support=support, layer=layer, inverse=inverse, gen_build=gen_build\n        )  # type: ignore\n        for layer in qft_layers\n    )\n\n    if swaps_out:\n        swap_ops = [SWAP(support[i], support[n_qubits - i - 1]) for i in range(n_qubits // 2)]\n        qft_circ = chain(*swap_ops, qft_circ) if inverse else chain(qft_circ, *swap_ops)\n\n    return tag(qft_circ, tag=\"iQFT\") if inverse else tag(qft_circ, tag=\"QFT\")\n</code></pre>"},{"location":"api/constructors/#hardware-efficient-ansatz-for-rydberg-atom-arrays","title":"Hardware efficient ansatz for Rydberg atom arrays","text":""},{"location":"api/constructors/#qadence.constructors.rydberg_hea.rydberg_hea","title":"<code>rydberg_hea(register, n_layers=1, addressable_detuning=True, addressable_drive=False, tunable_phase=False, additional_prefix=None)</code>","text":"<p>Hardware efficient ansatz for neutral atom (Rydberg) platforms.</p> <p>This constructor implements a variational ansatz which is very close to what is implementable on 2nd generation PASQAL quantum devices. In particular, it implements evolution over a specific Hamiltonian which can be realized on the device. This Hamiltonian contains:</p> <ul> <li> <p>an interaction term given by the standard NN interaction and determined starting     from the positions in the input register: H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c</p> </li> <li> <p>a detuning term which corresponding to a n_i = (1+sigma_i^z)/2 applied to     all the qubits. If the <code>addressable_detuning</code> flag is set to True, the routine     effectively a local n_i = (1+sigma_i^z)/2 term in the     evolved Hamiltonian with a different coefficient for each atom. These     coefficients determine a local addressing pattern for the detuning on a subset     of the qubits. In this routine, the coefficients are variational parameters     and they will therefore be optimized at each optimizer step</p> </li> <li> <p>a drive term which corresponding to a sigma^x evolution operation applied to     all the qubits. If the <code>addressable_drive</code> flag is set to True, the routine     effectively a local sigma_i^x term in the evolved Hamiltonian with a different     coefficient for each atom. These coefficients determine a local addressing pattern     for the drive on a subset of the qubits. In this routine, the coefficients are     variational parameters and they will therefore be optimized at each optimizer step</p> </li> <li> <p>if the <code>tunable_phase</code> flag is set to True, the drive term is modified in the following     way: drive = cos(phi) * sigma^x - sin(phi) * sigma^y     The addressable pattern above is maintained and the phase is considered just as an     additional variational parameter which is optimized with the rest</p> </li> </ul> <p>Notice that, on real devices, the coefficients assigned to each qubit in both the detuning and drive patterns should be non-negative and they should always sum to 1. This is not the case for the implementation in this routine since the coefficients (weights) do not have any constraint. Therefore, this HEA is not completely realizable on neutral atom devices.</p> PARAMETER DESCRIPTION <code>register</code> <p>the input atomic register with Cartesian coordinates.</p> <p> TYPE: <code>Register</code> </p> <code>n_layers</code> <p>number layers in the HEA, each layer includes a drive, detuning and pure interaction pulses whose is a variational parameter</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>addressable_detuning</code> <p>whether to turn on the trainable semi-local addressing pattern on the detuning (n_i terms in the Hamiltonian)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>addressable_drive</code> <p>whether to turn on the trainable semi-local addressing pattern on the drive (sigma_i^x terms in the Hamiltonian)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>tunable_phase</code> <p>whether to have a tunable phase to get both sigma^x and sigma^y rotations in the drive term. If False, only a sigma^x term will be included in the drive part of the Hamiltonian generator</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>additional_prefix</code> <p>an additional prefix to attach to the parameter names</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>The Rydberg HEA block</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def rydberg_hea(\n    register: qd.Register,\n    n_layers: int = 1,\n    addressable_detuning: bool = True,\n    addressable_drive: bool = False,\n    tunable_phase: bool = False,\n    additional_prefix: str = None,\n) -&gt; qd.blocks.ChainBlock:\n    \"\"\"Hardware efficient ansatz for neutral atom (Rydberg) platforms.\n\n    This constructor implements a variational ansatz which is very close to\n    what is implementable on 2nd generation PASQAL quantum devices. In particular,\n    it implements evolution over a specific Hamiltonian which can be realized on\n    the device. This Hamiltonian contains:\n\n    * an interaction term given by the standard NN interaction and determined starting\n        from the positions in the input register: H\u1d62\u2099\u209c = \u2211\u1d62\u2c7c C\u2086/r\u1d62\u2c7c\u2076 n\u1d62n\u2c7c\n\n    * a detuning term which corresponding to a n_i = (1+sigma_i^z)/2 applied to\n        all the qubits. If the `addressable_detuning` flag is set to True, the routine\n        effectively a local n_i = (1+sigma_i^z)/2 term in the\n        evolved Hamiltonian with a different coefficient for each atom. These\n        coefficients determine a local addressing pattern for the detuning on a subset\n        of the qubits. In this routine, the coefficients are variational parameters\n        and they will therefore be optimized at each optimizer step\n\n    * a drive term which corresponding to a sigma^x evolution operation applied to\n        all the qubits. If the `addressable_drive` flag is set to True, the routine\n        effectively a local sigma_i^x term in the evolved Hamiltonian with a different\n        coefficient for each atom. These coefficients determine a local addressing pattern\n        for the drive on a subset of the qubits. In this routine, the coefficients are\n        variational parameters and they will therefore be optimized at each optimizer step\n\n    * if the `tunable_phase` flag is set to True, the drive term is modified in the following\n        way: drive = cos(phi) * sigma^x - sin(phi) * sigma^y\n        The addressable pattern above is maintained and the phase is considered just as an\n        additional variational parameter which is optimized with the rest\n\n    Notice that, on real devices, the coefficients assigned to each qubit in both the detuning\n    and drive patterns should be non-negative and they should always sum to 1. This is not the\n    case for the implementation in this routine since the coefficients (weights) do not have any\n    constraint. Therefore, this HEA is not completely realizable on neutral atom devices.\n\n    Args:\n        register: the input atomic register with Cartesian coordinates.\n        n_layers: number layers in the HEA, each layer includes a drive, detuning and\n            pure interaction pulses whose is a variational parameter\n        addressable_detuning: whether to turn on the trainable semi-local addressing pattern\n            on the detuning (n_i terms in the Hamiltonian)\n        addressable_drive: whether to turn on the trainable semi-local addressing pattern\n            on the drive (sigma_i^x terms in the Hamiltonian)\n        tunable_phase: whether to have a tunable phase to get both sigma^x and sigma^y rotations\n            in the drive term. If False, only a sigma^x term will be included in the drive part\n            of the Hamiltonian generator\n        additional_prefix: an additional prefix to attach to the parameter names\n\n    Returns:\n        The Rydberg HEA block\n    \"\"\"\n    n_qubits = register.n_qubits\n    prefix = \"\" if additional_prefix is None else \"_\" + additional_prefix\n\n    detunings = None\n    # add a detuning pattern locally addressing the atoms\n    if addressable_detuning:\n        detunings = [qd.VariationalParameter(f\"detmap_{j}\") for j in range(n_qubits)]\n\n    drives = None\n    # add a drive pattern locally addressing the atoms\n    if addressable_drive:\n        drives = [qd.VariationalParameter(f\"drivemap_{j}\") for j in range(n_qubits)]\n\n    phase = None\n    if tunable_phase:\n        phase = qd.VariationalParameter(\"phase\")\n\n    return chain(\n        rydberg_hea_layer(\n            register,\n            VariationalParameter(f\"At{prefix}_{layer}\"),\n            VariationalParameter(f\"Omega{prefix}_{layer}\"),\n            VariationalParameter(f\"wait{prefix}_{layer}\"),\n            detunings=detunings,\n            drives=drives,\n            phase=phase,\n        )\n        for layer in range(n_layers)\n    )\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.rydberg_hea.rydberg_hea_layer","title":"<code>rydberg_hea_layer(register, tevo_drive, tevo_det, tevo_wait, phase=None, detunings=None, drives=None, drive_scaling=1.0)</code>","text":"<p>A single layer of the Rydberg hardware efficient ansatz.</p> PARAMETER DESCRIPTION <code>register</code> <p>the input register with atomic coordinates needed to build the interaction.</p> <p> TYPE: <code>Register</code> </p> <code>tevo_drive</code> <p>a variational parameter for the duration of the drive term of the Hamiltonian generator, including optional semi-local addressing</p> <p> TYPE: <code>Parameter | float</code> </p> <code>tevo_det</code> <p>a variational parameter for the duration of the detuning term of the Hamiltonian generator, including optional semi-local addressing</p> <p> TYPE: <code>Parameter | float</code> </p> <code>tevo_wait</code> <p>a variational parameter for the duration of the waiting time with interaction only</p> <p> TYPE: <code>Parameter | float</code> </p> <code>phase</code> <p>a variational parameter representing the global phase. If None, the global phase is set to 0 which results in a drive term in sigma^x only. Otherwise both sigma^x and sigma^y terms will be present</p> <p> TYPE: <code>Parameter | float | None</code> DEFAULT: <code>None</code> </p> <code>detunings</code> <p>a list of parameters with the weights of the locally addressed detuning terms. These are variational parameters which are tuned by the optimizer</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> <code>drives</code> <p>a list of parameters with the weights of the locally addressed drive terms. These are variational parameters which are tuned by the optimizer</p> <p> TYPE: <code>list[Parameter] | list[float] | None</code> DEFAULT: <code>None</code> </p> <code>drive_scaling</code> <p>a scaling term to be added to the drive Hamiltonian generator</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>A block with a single layer of Rydberg HEA</p> Source code in <code>qadence/constructors/rydberg_hea.py</code> <pre><code>def rydberg_hea_layer(\n    register: qd.Register,\n    tevo_drive: Parameter | float,\n    tevo_det: Parameter | float,\n    tevo_wait: Parameter | float,\n    phase: Parameter | float | None = None,\n    detunings: list[Parameter] | list[float] | None = None,\n    drives: list[Parameter] | list[float] | None = None,\n    drive_scaling: float = 1.0,\n) -&gt; ChainBlock:\n    \"\"\"A single layer of the Rydberg hardware efficient ansatz.\n\n    Args:\n        register: the input register with atomic coordinates needed to build the interaction.\n        tevo_drive: a variational parameter for the duration of the drive term of\n            the Hamiltonian generator, including optional semi-local addressing\n        tevo_det: a variational parameter for the duration of the detuning term of the\n            Hamiltonian generator, including optional semi-local addressing\n        tevo_wait: a variational parameter for the duration of the waiting\n            time with interaction only\n        phase: a variational parameter representing the global phase. If None, the\n            global phase is set to 0 which results in a drive term in sigma^x only. Otherwise\n            both sigma^x and sigma^y terms will be present\n        detunings: a list of parameters with the weights of the locally addressed\n            detuning terms. These are variational parameters which are tuned by the optimizer\n        drives: a list of parameters with the weights of the locally addressed\n            drive terms. These are variational parameters which are tuned by the optimizer\n        drive_scaling: a scaling term to be added to the drive Hamiltonian generator\n\n    Returns:\n        A block with a single layer of Rydberg HEA\n    \"\"\"\n    n_qubits = register.n_qubits\n\n    drive_x = _amplitude_map(n_qubits, qd.X, weights=drives)\n    drive_y = _amplitude_map(n_qubits, qd.Y, weights=drives)\n    detuning = _amplitude_map(n_qubits, qd.N, weights=detunings)\n    interaction = hamiltonian_factory(register, qd.Interaction.NN)\n\n    # drive and interaction are not commuting thus they need to be\n    # added directly into the final Hamiltonian generator\n    if phase is not None:\n        generator = (\n            drive_scaling * sympy.cos(phase) * drive_x\n            - drive_scaling * sympy.sin(phase) * drive_y\n            + interaction\n        )\n    else:\n        generator = drive_scaling * drive_x + interaction\n\n    return chain(\n        qd.HamEvo(generator, tevo_drive),\n        # detuning and interaction are commuting, so they\n        # can be ordered arbitrarily and treated separately\n        qd.HamEvo(interaction, tevo_wait),\n        qd.HamEvo(detuning, tevo_det),\n    )\n</code></pre>"},{"location":"api/constructors/#the-daqc-transform","title":"The DAQC Transform","text":""},{"location":"api/constructors/#qadence.constructors.daqc.daqc.daqc_transform","title":"<code>daqc_transform(n_qubits, gen_target, t_f, gen_build=None, zero_tol=1e-08, strategy=Strategy.SDAQC, ignore_global_phases=False)</code>","text":"<p>Implements the DAQC transform for representing an arbitrary 2-body Hamiltonian.</p> <p>The result is another fixed 2-body Hamiltonian.</p> <p>Reference for universality of 2-body Hamiltonians:</p> <p>-- https://arxiv.org/abs/quant-ph/0106064</p> <p>Based on the transformation for Ising (ZZ) interactions, as described in the paper</p> <p>-- https://arxiv.org/abs/1812.03637</p> <p>The transform translates a target weighted generator of the type:</p> <pre><code>`gen_target = add(g_jk * kron(op(j), op(k)) for j &lt; k)`\n</code></pre> <p>To a circuit using analog evolutions with a fixed building block generator:</p> <pre><code>`gen_build = add(f_jk * kron(op(j), op(k)) for j &lt; k)`\n</code></pre> <p>where <code>op = Z</code> or <code>op = N</code>.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>total number of qubits to use.</p> <p> TYPE: <code>int</code> </p> <code>gen_target</code> <p>target generator built with the structure above. The type of the generator will be automatically evaluated when parsing.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>t_f</code> <p>total time for the gen_target evolution.</p> <p> TYPE: <code>float</code> </p> <code>gen_build</code> <p>fixed generator to act as a building block. Defaults to constant NN: add(1.0 * kron(N(j), N(k)) for j &lt; k). The type of the generator will be automatically evaluated when parsing.</p> <p> TYPE: <code>AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <code>zero_tol</code> <p>default \"zero\" for a missing interaction. Included for numerical reasons, see notes below.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-08</code> </p> <code>strategy</code> <p>sDAQC or bDAQC, following definitions in the reference paper.</p> <p> TYPE: <code>Strategy</code> DEFAULT: <code>SDAQC</code> </p> <code>ignore_global_phases</code> <p>if <code>True</code> the transform does not correct the global phases coming from the mapping between ZZ and NN interactions.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Notes:</p> <pre><code>The paper follows an index convention of running from 1 to N. A few functions\nhere also use that convention to be consistent with the paper. However, for qadence\nrelated things the indices are converted to [0, N-1].\n\nThe case for `n_qubits = 4` is an edge case where the sign matrix is not invertible.\nThere is a workaround for this described in the paper, but it is currently not implemented.\n\nThe current implementation may result in evolution times that are both positive or\nnegative. In practice, both can be represented by simply changing the signs of the\ninteractions. However, for a real implementation where the interactions should remain\nfixed, the paper discusses a workaround that is not currently implemented.\n\nThe transformation works by representing each interaction in the target hamiltonian by\na set of evolutions using the build hamiltonian. As a consequence, some care must be\ntaken when choosing the build hamiltonian. Some cases:\n\n- The target hamiltonian can have any interaction, as long as it is sufficiently\nrepresented in the build hamiltonian. E.g., if the interaction `g_01 * kron(Z(0), Z(1))`\nis in the target hamiltonian, the corresponding interaction `f_01 * kron(Z(0), Z(1))`\nneeds to be in the build hamiltonian. This is checked when the generators are parsed.\n\n- The build hamiltonian can have any interaction, irrespectively of it being needed\nfor the target hamiltonian. This is especially useful for designing local operations\nthrough the repeated evolution of a \"global\" hamiltonian.\n\n- The parameter `zero_tol` controls what it means for an interaction to be \"missing\".\nAny interaction strength smaller than `zero_tol` in the build hamiltonian will not be\nconsidered, and thus that interaction is missing.\n\n- The various ratios `g_jk / f_jk` will influence the time parameter for the various\nevolution slices, meaning that if there is a big discrepancy in the interaction strength\nfor a given qubit pair (j, k), the output circuit may require the usage of hamiltonian\nevolutions with very large times.\n\n- A warning will be issued for evolution times larger than `1/sqrt(zero_tol)`. Evolution\ntimes smaller than `zero_tol` will not be represented.\n</code></pre> <p>Examples:</p> <pre><code>from qadence import Z, N, daqc_transform\n\nn_qubits = 3\n\ngen_build = 0.5 * (N(0)@N(1)) + 0.7 * (N(1)@N(2)) + 0.2 * (N(0)@N(2))\n\ngen_target = 0.1 * (Z(1)@Z(2))\n\nt_f = 2.0\n\ntransformed_circuit = daqc_transform(\n    n_qubits = n_qubits,\n    gen_target = gen_target,\n    t_f = t_f,\n    gen_build = gen_build,\n)\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/constructors/daqc/daqc.py</code> <pre><code>def daqc_transform(\n    n_qubits: int,\n    gen_target: AbstractBlock,\n    t_f: float,\n    gen_build: AbstractBlock | None = None,\n    zero_tol: float = 1e-08,\n    strategy: Strategy = Strategy.SDAQC,\n    ignore_global_phases: bool = False,\n) -&gt; AbstractBlock:\n    \"\"\"\n    Implements the DAQC transform for representing an arbitrary 2-body Hamiltonian.\n\n    The result is another fixed 2-body Hamiltonian.\n\n    Reference for universality of 2-body Hamiltonians:\n\n    -- https://arxiv.org/abs/quant-ph/0106064\n\n    Based on the transformation for Ising (ZZ) interactions, as described in the paper\n\n    -- https://arxiv.org/abs/1812.03637\n\n    The transform translates a target weighted generator of the type:\n\n        `gen_target = add(g_jk * kron(op(j), op(k)) for j &lt; k)`\n\n    To a circuit using analog evolutions with a fixed building block generator:\n\n        `gen_build = add(f_jk * kron(op(j), op(k)) for j &lt; k)`\n\n    where `op = Z` or `op = N`.\n\n    Args:\n        n_qubits: total number of qubits to use.\n        gen_target: target generator built with the structure above. The type\n            of the generator will be automatically evaluated when parsing.\n        t_f (float): total time for the gen_target evolution.\n        gen_build: fixed generator to act as a building block. Defaults to\n            constant NN: add(1.0 * kron(N(j), N(k)) for j &lt; k). The type\n            of the generator will be automatically evaluated when parsing.\n        zero_tol: default \"zero\" for a missing interaction. Included for\n            numerical reasons, see notes below.\n        strategy: sDAQC or bDAQC, following definitions in the reference paper.\n        ignore_global_phases: if `True` the transform does not correct the global\n            phases coming from the mapping between ZZ and NN interactions.\n\n    Notes:\n\n        The paper follows an index convention of running from 1 to N. A few functions\n        here also use that convention to be consistent with the paper. However, for qadence\n        related things the indices are converted to [0, N-1].\n\n        The case for `n_qubits = 4` is an edge case where the sign matrix is not invertible.\n        There is a workaround for this described in the paper, but it is currently not implemented.\n\n        The current implementation may result in evolution times that are both positive or\n        negative. In practice, both can be represented by simply changing the signs of the\n        interactions. However, for a real implementation where the interactions should remain\n        fixed, the paper discusses a workaround that is not currently implemented.\n\n        The transformation works by representing each interaction in the target hamiltonian by\n        a set of evolutions using the build hamiltonian. As a consequence, some care must be\n        taken when choosing the build hamiltonian. Some cases:\n\n        - The target hamiltonian can have any interaction, as long as it is sufficiently\n        represented in the build hamiltonian. E.g., if the interaction `g_01 * kron(Z(0), Z(1))`\n        is in the target hamiltonian, the corresponding interaction `f_01 * kron(Z(0), Z(1))`\n        needs to be in the build hamiltonian. This is checked when the generators are parsed.\n\n        - The build hamiltonian can have any interaction, irrespectively of it being needed\n        for the target hamiltonian. This is especially useful for designing local operations\n        through the repeated evolution of a \"global\" hamiltonian.\n\n        - The parameter `zero_tol` controls what it means for an interaction to be \"missing\".\n        Any interaction strength smaller than `zero_tol` in the build hamiltonian will not be\n        considered, and thus that interaction is missing.\n\n        - The various ratios `g_jk / f_jk` will influence the time parameter for the various\n        evolution slices, meaning that if there is a big discrepancy in the interaction strength\n        for a given qubit pair (j, k), the output circuit may require the usage of hamiltonian\n        evolutions with very large times.\n\n        - A warning will be issued for evolution times larger than `1/sqrt(zero_tol)`. Evolution\n        times smaller than `zero_tol` will not be represented.\n\n    Examples:\n        ```python exec=\"on\" source=\"material-block\" result=\"json\"\n        from qadence import Z, N, daqc_transform\n\n        n_qubits = 3\n\n        gen_build = 0.5 * (N(0)@N(1)) + 0.7 * (N(1)@N(2)) + 0.2 * (N(0)@N(2))\n\n        gen_target = 0.1 * (Z(1)@Z(2))\n\n        t_f = 2.0\n\n        transformed_circuit = daqc_transform(\n            n_qubits = n_qubits,\n            gen_target = gen_target,\n            t_f = t_f,\n            gen_build = gen_build,\n        )\n        ```\n    \"\"\"\n\n    ##################\n    # Input controls #\n    ##################\n\n    if strategy != Strategy.SDAQC:\n        raise NotImplementedError(\"Currently only the sDAQC transform is implemented.\")\n\n    if n_qubits == 4:\n        raise NotImplementedError(\"DAQC transform 4-qubit edge case not implemented.\")\n\n    if gen_build is None:\n        gen_build = hamiltonian_factory(n_qubits, interaction=Interaction.NN)\n\n    try:\n        if (not block_is_qubit_hamiltonian(gen_target)) or (\n            not block_is_qubit_hamiltonian(gen_build)\n        ):\n            raise ValueError(\n                \"Generator block is not a qubit Hamiltonian. Only ZZ or NN interactions allowed.\"\n            )\n    except NotImplementedError:\n        # Happens when block_is_qubit_hamiltonian is called on something that is not a block.\n        raise TypeError(\n            \"Generator block is not a qubit Hamiltonian. Only ZZ or NN interactions allowed.\"\n        )\n\n    #####################\n    # Generator parsing #\n    #####################\n\n    g_jk_target, mat_jk_target, target_type = _parse_generator(n_qubits, gen_target, 0.0)\n    g_jk_build, mat_jk_build, build_type = _parse_generator(n_qubits, gen_build, zero_tol)\n\n    # Get the global phase hamiltonian and single-qubit detuning hamiltonian\n    if build_type == GenDAQC.NN:\n        h_phase_build, h_sq_build = _nn_phase_and_detunings(n_qubits, mat_jk_build)\n\n    if target_type == GenDAQC.NN:\n        h_phase_target, h_sq_target = _nn_phase_and_detunings(n_qubits, mat_jk_target)\n\n    # Time re-scalings\n    if build_type == GenDAQC.ZZ and target_type == GenDAQC.NN:\n        t_star = t_f / 4.0\n    elif build_type == GenDAQC.NN and target_type == GenDAQC.ZZ:\n        t_star = 4.0 * t_f\n    else:\n        t_star = t_f\n\n    # Check if target Hamiltonian can be mapped with the build Hamiltonian\n    assert _check_compatibility(g_jk_target, g_jk_build, zero_tol)\n\n    ##################\n    # DAQC Transform #\n    ##################\n\n    # Section III A of https://arxiv.org/abs/1812.03637:\n\n    # Matrix M for the linear system, exemplified in Table I:\n    matrix_M = _build_matrix_M(n_qubits)\n\n    # Linear system mapping interaction ratios -&gt; evolution times.\n    t_slices = torch.linalg.solve(matrix_M, g_jk_target / g_jk_build) * t_star\n\n    # ZZ-DAQC with ZZ or NN build Hamiltonian\n    daqc_slices = []\n    for m in range(2, n_qubits + 1):\n        for n in range(1, m):\n            alpha = _ix_map(n_qubits, n, m)\n            t = t_slices[alpha - 1]\n            if abs(t) &gt; zero_tol:\n                if abs(t) &gt; (1 / (zero_tol**0.5)):\n                    logger.warning(\n                        \"\"\"\nTransformed circuit with very long evolution time.\nMake sure your target interactions are sufficiently\nrepresented in the build Hamiltonian.\"\"\"\n                    )\n                x_gates = kron(X(n - 1), X(m - 1))\n                analog_evo = HamEvo(gen_build, t)\n                # TODO: Fix repeated X-gates\n                if build_type == GenDAQC.NN:\n                    # Local detuning at each DAQC layer for NN build Hamiltonian\n                    sq_detuning_build = HamEvo(h_sq_build, t)\n                    daqc_slices.append(chain(x_gates, sq_detuning_build, analog_evo, x_gates))\n                elif build_type == GenDAQC.ZZ:\n                    daqc_slices.append(chain(x_gates, analog_evo, x_gates))\n\n    daqc_circuit = chain(*daqc_slices)\n\n    ########################\n    # Phases and Detunings #\n    ########################\n\n    if target_type == GenDAQC.NN:\n        # Local detuning given a NN target Hamiltonian\n        sq_detuning_target = HamEvo(h_sq_target, t_f).dagger()\n        daqc_circuit = chain(sq_detuning_target, daqc_circuit)\n\n    if not ignore_global_phases:\n        if build_type == GenDAQC.NN:\n            # Constant global phase given a NN build Hamiltonian\n            global_phase_build = HamEvo(h_phase_build, t_slices.sum())\n            daqc_circuit = chain(global_phase_build, daqc_circuit)\n\n        if target_type == GenDAQC.NN:\n            # Constant global phase and given a NN target Hamiltonian\n            global_phase_target = HamEvo(h_phase_target, t_f).dagger()\n            daqc_circuit = chain(global_phase_target, daqc_circuit)\n\n    return daqc_circuit\n</code></pre>"},{"location":"api/constructors/#some-utility-functions","title":"Some utility functions","text":""},{"location":"api/constructors/#qadence.constructors.utils.build_idx_fms","title":"<code>build_idx_fms(basis, fm_pauli, multivariate_strategy, n_features, n_qubits, reupload_scaling)</code>","text":"<p>Builds the index feature maps based on the given parameters.</p> PARAMETER DESCRIPTION <code>basis</code> <p>Type of basis chosen for the feature map.</p> <p> TYPE: <code>BasisSet</code> </p> <code>fm_pauli</code> <p>The chosen Pauli rotation type.</p> <p> TYPE: <code>PrimitiveBlock type</code> </p> <code>multivariate_strategy</code> <p>The strategy used for encoding the multivariate feature map.</p> <p> TYPE: <code>MultivariateStrategy</code> </p> <code>n_features</code> <p>The number of features.</p> <p> TYPE: <code>int</code> </p> <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>reupload_scaling</code> <p>The chosen scaling for the reupload.</p> <p> TYPE: <code>ReuploadScaling</code> </p> RETURNS DESCRIPTION <code>list[KronBlock]</code> <p>List[KronBlock]: The list of index feature maps.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def build_idx_fms(\n    basis: BasisSet,\n    fm_pauli: Type[RY],\n    multivariate_strategy: MultivariateStrategy,\n    n_features: int,\n    n_qubits: int,\n    reupload_scaling: ReuploadScaling,\n) -&gt; list[KronBlock]:\n    \"\"\"Builds the index feature maps based on the given parameters.\n\n    Args:\n        basis (BasisSet): Type of basis chosen for the feature map.\n        fm_pauli (PrimitiveBlock type): The chosen Pauli rotation type.\n        multivariate_strategy (MultivariateStrategy): The strategy used for encoding\n            the multivariate feature map.\n        n_features (int): The number of features.\n        n_qubits (int): The number of qubits.\n        reupload_scaling (ReuploadScaling): The chosen scaling for the reupload.\n\n    Returns:\n        List[KronBlock]: The list of index feature maps.\n    \"\"\"\n    idx_fms = []\n    for i in range(n_features):\n        target_qubits = get_fm_qubits(multivariate_strategy, i, n_qubits, n_features)\n        param = FeatureParameter(f\"x{i}\")\n        block = kron(\n            *[\n                fm_pauli(qubit, generator_prefactor(reupload_scaling, j) * basis_func(basis, param))\n                for j, qubit in enumerate(target_qubits)\n            ]\n        )\n        idx_fm = block\n        idx_fms.append(idx_fm)\n    return idx_fms\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.utils.generator_prefactor","title":"<code>generator_prefactor(reupload_scaling, qubit_index)</code>","text":"<p>Converts a spectrum string, e.g. tower or exponential.</p> <p>The result is the correct generator prefactor.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def generator_prefactor(reupload_scaling: ReuploadScaling, qubit_index: int) -&gt; float | int:\n    \"\"\"Converts a spectrum string, e.g. tower or exponential.\n\n    The result is the correct generator prefactor.\n    \"\"\"\n    conversion_dict: dict[str, float | int] = {\n        ReuploadScaling.CONSTANT: 1,\n        ReuploadScaling.TOWER: qubit_index + 1,\n        ReuploadScaling.EXP: 2 * PI / (2 ** (qubit_index + 1)),\n    }\n    return conversion_dict[reupload_scaling]\n</code></pre>"},{"location":"api/constructors/#qadence.constructors.utils.get_fm_qubits","title":"<code>get_fm_qubits(multivariate_strategy, i, n_qubits, n_features)</code>","text":"<p>Returns the list of target qubits for the given feature map strategy and feature index.</p> PARAMETER DESCRIPTION <code>multivariate_strategy</code> <p>The strategy used for encoding the multivariate feature map.</p> <p> TYPE: <code>MultivariateStrategy</code> </p> <code>i</code> <p>The feature index.</p> <p> TYPE: <code>int</code> </p> <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>n_features</code> <p>The number of features.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Iterable</code> <p>List[int]: The list of target qubits.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the feature map strategy is not implemented.</p> Source code in <code>qadence/constructors/utils.py</code> <pre><code>def get_fm_qubits(\n    multivariate_strategy: MultivariateStrategy, i: int, n_qubits: int, n_features: int\n) -&gt; Iterable:\n    \"\"\"Returns the list of target qubits for the given feature map strategy and feature index.\n\n    Args:\n        multivariate_strategy (MultivariateStrategy): The strategy used for encoding\n            the multivariate feature map.\n        i (int): The feature index.\n        n_qubits (int): The number of qubits.\n        n_features (int): The number of features.\n\n    Returns:\n        List[int]: The list of target qubits.\n\n    Raises:\n        ValueError: If the feature map strategy is not implemented.\n    \"\"\"\n    if multivariate_strategy == MultivariateStrategy.PARALLEL:\n        n_qubits_per_feature = int(n_qubits / n_features)\n        target_qubits = range(i * n_qubits_per_feature, (i + 1) * n_qubits_per_feature)\n    elif multivariate_strategy == MultivariateStrategy.SERIES:\n        target_qubits = range(0, n_qubits)\n    else:\n        raise ValueError(f\"Multivariate strategy {multivariate_strategy} not implemented.\")\n    return target_qubits\n</code></pre>"},{"location":"api/draw/","title":"Drawing","text":""},{"location":"api/draw/#drawing","title":"Drawing","text":""},{"location":"api/draw/#qadence.draw.display","title":"<code>display(x, qcd=None, layout='LR', theme='light', fill=True, **kwargs)</code>","text":"<p>Display a block, circuit, or quantum model.</p> <p>The <code>kwargs</code> are forwarded to the underlying <code>nx.Graph</code>, so you can e.g. specify the size of the resulting plot via <code>size=\"2,2\"</code> (see examples)</p> PARAMETER DESCRIPTION <code>x</code> <p><code>AbstractBlock</code>, <code>QuantumCircuit</code>, or <code>QuantumModel</code>.</p> <p> TYPE: <code>Any</code> </p> <code>qcd</code> <p>Circuit diagram to plot the block into.</p> <p> TYPE: <code>QuantumCircuitDiagram | Cluster | None</code> DEFAULT: <code>None</code> </p> <code>layout</code> <p>Can be either \"LR\" (left-right), or \"TB\" (top-bottom).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'LR'</code> </p> <code>theme</code> <p>Available themes are: [\"light\", \"dark\", \"black\", \"white\"].</p> <p> TYPE: <code>str</code> DEFAULT: <code>'light'</code> </p> <code>fill</code> <p>Whether to fill the passed <code>x</code> with identities.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>kwargs</code> <p>Passed on to <code>nx.Graph</code></p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Examples: <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\ndisplay(b, size=\"1,1\", theme=\"dark\")\n</code></pre> </p> Source code in <code>qadence/draw/__init__.py</code> <pre><code>def display(\n    x: Any,\n    qcd: QuantumCircuitDiagram | Cluster | None = None,\n    layout: str = \"LR\",\n    theme: str = \"light\",\n    fill: bool = True,\n    **kwargs: Any,\n) -&gt; Graph:\n    \"\"\"Display a block, circuit, or quantum model.\n\n    The `kwargs` are forwarded to\n    the underlying `nx.Graph`, so you can e.g. specify the size of the resulting plot via\n    `size=\"2,2\"` (see examples)\n\n    Arguments:\n        x: `AbstractBlock`, `QuantumCircuit`, or `QuantumModel`.\n        qcd: Circuit diagram to plot the block into.\n        layout: Can be either \"LR\" (left-right), or \"TB\" (top-bottom).\n        theme: Available themes are: [\"light\", \"dark\", \"black\", \"white\"].\n        fill: Whether to fill the passed `x` with identities.\n        kwargs: Passed on to `nx.Graph`\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" html=\"1\"\n    from qadence import X, Y, kron\n    from qadence.draw import display\n\n    b = kron(X(0), Y(1))\n    def display(*args, **kwargs): return args # markdown-exec: hide\n    display(b, size=\"1,1\", theme=\"dark\")\n    ```\n    \"\"\"\n    return make_diagram(x, **kwargs).show()\n</code></pre>"},{"location":"api/draw/#qadence.draw.savefig","title":"<code>savefig(x, filename, *args, **kwargs)</code>","text":"<p>Save a block, circuit, or quantum model to file. Accepts the same args/kwargs as <code>display</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p><code>AbstractBlock</code>, <code>QuantumCircuit</code>, or <code>QuantumModel</code>.</p> <p> TYPE: <code>Any</code> </p> <code>filename</code> <p>Should end in svg/png.</p> <p> TYPE: <code>str</code> </p> <code>args</code> <p>Same as in <code>display</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>()</code> </p> <code>kwargs</code> <p>Same as in <code>display</code>.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> <p>Examples: <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\nsavefig(b, \"test.svg\", size=\"1,1\", theme=\"dark\")\n</code></pre> </p> Source code in <code>qadence/draw/__init__.py</code> <pre><code>def savefig(x: Any, filename: str, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Save a block, circuit, or quantum model to file. Accepts the same args/kwargs as `display`.\n\n    Arguments:\n        x: `AbstractBlock`, `QuantumCircuit`, or `QuantumModel`.\n        filename: Should end in svg/png.\n        args: Same as in `display`.\n        kwargs: Same as in `display`.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" html=\"1\"\n    from qadence import X, Y, kron\n    from qadence.draw import display\n\n    b = kron(X(0), Y(1))\n    def savefig(*args, **kwargs): return args # markdown-exec: hide\n    savefig(b, \"test.svg\", size=\"1,1\", theme=\"dark\")\n    ```\n    \"\"\"\n    make_diagram(x, *args, **kwargs).savefig(filename)\n</code></pre>"},{"location":"api/execution/","title":"Execution","text":""},{"location":"api/execution/#qadence.execution.expectation","title":"<code>expectation(x, observable, values=None, state=None, backend=BackendName.PYQTORCH, diff_mode=None, noise=None, endianness=Endianness.BIG, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.expectation</code> method.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>observable</code> <p>Observable(s) w.r.t. which the expectation is computed.</p> <p> TYPE: <code>Union[list[AbstractBlock], AbstractBlock]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>Which differentiation mode to use.</p> <p> TYPE: <code>Union[DiffMode, str, None]</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A wavefunction</p> <pre><code>from qadence import RX, Z, Register, QuantumCircuit, expectation\n\nreg = Register(1)\nblock = RX(0, 0.5)\nobservable = Z(0)\ncirc = QuantumCircuit(reg, block)\n\n# You can compute the expectation for a\n# QuantumCircuit with a given observable.\nexpectation(circ, observable)\n\n# You can also use only a block.\n# In this case the register is constructed automatically to\n# Register.line(block.n_qubits)\nexpectation(block, observable)\n\n# Or a register and block\nexpectation(reg, block, observable)\n</code></pre> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef expectation(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    observable: Union[list[AbstractBlock], AbstractBlock],\n    values: Union[dict, None] = None,\n    state: Tensor = None,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: Union[DiffMode, str, None] = None,\n    noise: Union[NoiseHandler, None] = None,\n    endianness: Endianness = Endianness.BIG,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; Tensor:\n    \"\"\"Convenience wrapper for the `QuantumModel.expectation` method.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        observable: Observable(s) w.r.t. which the expectation is computed.\n        values: User-facing parameter dict.\n        state: Initial state.\n        backend: Name of the backend to run on.\n        diff_mode: Which differentiation mode to use.\n        endianness: The target device endianness.\n        configuration: The backend configuration.\n\n    Returns:\n        A wavefunction\n\n    ```python exec=\"on\" source=\"material-block\"\n    from qadence import RX, Z, Register, QuantumCircuit, expectation\n\n    reg = Register(1)\n    block = RX(0, 0.5)\n    observable = Z(0)\n    circ = QuantumCircuit(reg, block)\n\n    # You can compute the expectation for a\n    # QuantumCircuit with a given observable.\n    expectation(circ, observable)\n\n    # You can also use only a block.\n    # In this case the register is constructed automatically to\n    # Register.line(block.n_qubits)\n    expectation(block, observable)\n\n    # Or a register and block\n    expectation(reg, block, observable)\n    ```\n    \"\"\"\n\n    raise ValueError(f\"Cannot execute {type(x)}\")\n</code></pre>"},{"location":"api/execution/#qadence.execution.run","title":"<code>run(x, *args, values=None, state=None, backend=BackendName.PYQTORCH, endianness=Endianness.BIG, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.run</code> method.</p> <p>This is a <code>functools.singledispatch</code>ed function so it can be called with a number of different arguments. See the examples of the <code>expectation</code> function. This function works exactly the same.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A wavefunction</p> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef run(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    *args: Any,\n    values: Union[dict, None] = None,\n    state: Tensor = None,\n    backend: BackendName = BackendName.PYQTORCH,\n    endianness: Endianness = Endianness.BIG,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; Tensor:\n    \"\"\"Convenience wrapper for the `QuantumModel.run` method.\n\n     This is a\n    `functools.singledispatch`ed function so it can be called with a number of different arguments.\n    See the examples of the [`expectation`][qadence.execution.expectation] function. This function\n    works exactly the same.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        values: User-facing parameter dict.\n        state: Initial state.\n        backend: Name of the backend to run on.\n        endianness: The target device endianness.\n        configuration: The backend configuration.\n\n    Returns:\n        A wavefunction\n    \"\"\"\n    raise ValueError(f\"Cannot run {type(x)}\")\n</code></pre>"},{"location":"api/execution/#qadence.execution.sample","title":"<code>sample(x, *args, values=None, state=None, n_shots=100, backend=BackendName.PYQTORCH, endianness=Endianness.BIG, noise=None, configuration=None)</code>","text":"<p>Convenience wrapper for the <code>QuantumModel.sample</code> method.</p> PARAMETER DESCRIPTION <code>x</code> <p>Circuit, block, or (register+block) to run.</p> <p> TYPE: <code>Union[QuantumCircuit, AbstractBlock, Register, int]</code> </p> <code>values</code> <p>User-facing parameter dict.</p> <p> TYPE: <code>Union[dict, None]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Union[Tensor, None]</code> DEFAULT: <code>None</code> </p> <code>n_shots</code> <p>Number of shots per element in the batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>100</code> </p> <code>backend</code> <p>Name of the backend to run on.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>endianness</code> <p>The target device endianness.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> <code>noise</code> <p>The noise model to use if any.</p> <p> TYPE: <code>Union[NoiseHandler, None]</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>The backend configuration.</p> <p> TYPE: <code>Union[BackendConfiguration, dict, None]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[Counter]</code> <p>A list of Counter instances with the sample results</p> Source code in <code>qadence/execution.py</code> <pre><code>@singledispatch\ndef sample(\n    x: Union[QuantumCircuit, AbstractBlock, Register, int],\n    *args: Any,\n    values: Union[dict, None] = None,\n    state: Union[Tensor, None] = None,\n    n_shots: int = 100,\n    backend: BackendName = BackendName.PYQTORCH,\n    endianness: Endianness = Endianness.BIG,\n    noise: Union[NoiseHandler, None] = None,\n    configuration: Union[BackendConfiguration, dict, None] = None,\n) -&gt; list[Counter]:\n    \"\"\"Convenience wrapper for the `QuantumModel.sample` method.\n\n    Arguments:\n        x: Circuit, block, or (register+block) to run.\n        values: User-facing parameter dict.\n        state: Initial state.\n        n_shots: Number of shots per element in the batch.\n        backend: Name of the backend to run on.\n        endianness: The target device endianness.\n        noise: The noise model to use if any.\n        configuration: The backend configuration.\n\n    Returns:\n        A list of Counter instances with the sample results\n    \"\"\"\n    raise ValueError(f\"Cannot sample from {type(x)}\")\n</code></pre>"},{"location":"api/ml_tools/","title":"QML tools","text":""},{"location":"api/ml_tools/#ml-tools","title":"ML Tools","text":"<p>This module implements a <code>Trainer</code> class for torch <code>Modules</code> and <code>QuantumModel</code>. It also implements the <code>QNN</code> class and callbacks that can be used with the trainer module.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer","title":"<code>Trainer(model, optimizer, config, loss_fn='mse', train_dataloader=None, val_dataloader=None, test_dataloader=None, optimize_step=optimize_step, device=None, dtype=None, max_batches=None)</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer class to manage and execute training, validation, and testing loops for a model (eg.</p> <p>QNN).</p> <p>This class handles the overall training process, including: - Managing epochs and steps - Handling data loading and batching - Computing and updating gradients - Logging and monitoring training metrics</p> ATTRIBUTE DESCRIPTION <code>current_epoch</code> <p>The current epoch number.</p> <p> TYPE: <code>int</code> </p> <code>global_step</code> <p>The global step across all epochs.</p> <p> TYPE: <code>int</code> </p> <code>log_device</code> <p>Device for logging, default is \"cpu\".</p> <p> TYPE: <code>str</code> </p> <code>device</code> <p>Device used for computation.</p> <p> TYPE: <code>device</code> </p> <code>dtype</code> <p>Data type used for computation.</p> <p> TYPE: <code>dtype | None</code> </p> <code>data_dtype</code> <p>Data type for data. Depends on the model's data type.</p> <p> TYPE: <code>dtype | None</code> </p> Inherited Attributes <p>use_grad (bool): Indicates if gradients are used for optimization. Default is True.</p> <p>model (nn.Module): The neural network model. optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training. config (TrainConfig): The configuration settings for training. train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data. val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data. test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.</p> <p>optimize_step (Callable): Function for performing an optimization step. loss_fn (Callable): loss function to use.</p> <p>num_training_batches (int): Number of training batches. num_validation_batches (int): Number of validation batches. num_test_batches (int): Number of test batches.</p> <p>state (str): Current state in the training process</p> <p>Default training routine <pre><code>for epoch in max_iter + 1:\n    # Training\n    for batch in train_batches:\n        train model\n    # Validation\n    if val_every % epoch == 0:\n        for batch in val_batches:\n            train model\n</code></pre></p> Notes <ul> <li>In case of InfiniteTensorDataset, number of batches = 1.</li> <li>In case of TensorDataset, number of batches are default.</li> <li>Training is run for max_iter + 1 epochs. Epoch 0 logs untrained model.</li> <li>Please look at the CallbackManager initialize_callbacks method to review the default     logging behavior.</li> </ul> <p>Examples:</p> <pre><code>import torch\nfrom torch.optim import SGD\nfrom qadence import (\n    feature_map,\n    hamiltonian_factory,\n    hea,\n    QNN,\n    QuantumCircuit,\n    TrainConfig,\n    Z,\n)\nfrom qadence.ml_tools.trainer import Trainer\nfrom qadence.ml_tools.optimize_step import optimize_step\nfrom qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.data import to_dataloader\n\n# Initialize the model\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=2)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\n\n# Set up the optimizer\noptimizer = SGD(model.parameters(), lr=0.001)\n\n# Use TrainConfig for configuring the training process\nconfig = TrainConfig(\n    max_iter=100,\n    print_every=10,\n    write_every=10,\n    checkpoint_every=10,\n    val_every=10\n)\n\n# Create the Trainer instance with TrainConfig\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\",\n    optimize_step=optimize_step\n)\n\nbatch_size = 25\nx = torch.linspace(0, 1, 32).reshape(-1, 1)\ny = torch.sin(x)\ntrain_loader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_loader = to_dataloader(x, y, batch_size=batch_size, infinite=False)\n\n# Train the model\nmodel, optimizer = trainer.fit(train_loader, val_loader)\n</code></pre> <p>This also supports both gradient based and gradient free optimization. The default support is for gradient based optimization.</p> <p>Notes:</p> <ul> <li>set_use_grad() (class level):This method is used to set the global <code>use_grad</code> flag,     controlling whether the trainer uses gradient-based optimization. <pre><code># gradient based\nTrainer.set_use_grad(True)\n\n# gradient free\nTrainer.set_use_grad(False)\n</code></pre></li> <li>Context Managers (instance level):  <code>enable_grad_opt()</code> and <code>disable_grad_opt()</code> are     context managers that temporarily switch the optimization mode for specific code blocks.     This is useful when you want to mix gradient-based and gradient-free optimization     in the same training process. <pre><code># gradient based\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit()\n\n# gradient free\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit()\n</code></pre></li> </ul> <p>Examples</p> <p>Gradient based optimization example Usage: <pre><code>from torch import optim\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nTrainer.set_use_grad(True)\ntrainer = Trainer(\n    model=model,\n    optimizer=optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>trainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.enable_grad_opt(optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Gradient free optimization example Usage: <pre><code>import nevergrad as ng\nfrom qadence.ml_tools.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n                budget=config.max_iter, parametrization= num_parameters(model)\n                )\n\nTrainer.set_use_grad(False)\ntrainer = Trainer(\n    model=model,\n    optimizer=ng_optimizer,\n    config=config,\n    loss_fn=\"mse\"\n)\ntrainer.fit(train_loader, val_loader)\n</code></pre> or <pre><code>import nevergrad as ng\nfrom qadence.ml_tools.parameters import num_parameters\nng_optimizer = ng.optimizers.NGOpt(\n        budget=config.max_iter, parametrization= num_parameters(model)\n        )\n\ntrainer = Trainer(\n    model=model,\n    config=config,\n    loss_fn=\"mse\"\n)\nwith trainer.disable_grad_opt(ng_optimizer):\n    trainer.fit(train_loader, val_loader)\n</code></pre></p> <p>Initializes the Trainer class.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>Training configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>Loss function used for training. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>optimize_step</code> <p>Function to execute an optimization step.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>optimize_step</code> </p> <code>device</code> <p>Device to use for computation.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for computation.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    optimize_step: Callable = optimize_step,\n    device: torch_device | None = None,\n    dtype: torch_dtype | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the Trainer class.\n\n    Args:\n        model (nn.Module): The PyTorch model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer for training.\n        config (TrainConfig): Training configuration object.\n        loss_fn (str | Callable ): Loss function used for training.\n            If not specified, default mse loss will be used.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for test data.\n        optimize_step (Callable): Function to execute an optimization step.\n        device (torch_device): Device to use for computation.\n        dtype (torch_dtype): Data type for computation.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    super().__init__(\n        model=model,\n        optimizer=optimizer,\n        config=config,\n        loss_fn=loss_fn,\n        optimize_step=optimize_step,\n        train_dataloader=train_dataloader,\n        val_dataloader=val_dataloader,\n        test_dataloader=test_dataloader,\n        max_batches=max_batches,\n    )\n    self.current_epoch: int = 0\n    self.global_step: int = 0\n    self.log_device: str = \"cpu\" if device is None else device\n    self.device: torch_device | None = device\n    self.dtype: torch_dtype | None = dtype\n    self.data_dtype: torch_dtype | None = None\n    self.stop_training: bool = False\n    if self.dtype:\n        self.data_dtype = float64 if (self.dtype == complex128) else float32\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.build_optimize_result","title":"<code>build_optimize_result(result)</code>","text":"<p>Builds and stores the optimization result by calculating the average loss and metrics.</p> <p>Result (or loss_metrics) can have multiple formats: - <code>None</code> Indicates no loss or metrics data is provided. - <code>tuple[torch.Tensor, dict[str, Any]]</code> A single tuple containing the loss tensor     and metrics dictionary - at the end of batch. - <code>list[tuple[torch.Tensor, dict[str, Any]]]</code> A list of tuples for     multiple batches. - <code>list[list[tuple[torch.Tensor, dict[str, Any]]]]</code> A list of lists of tuples, where each inner list represents metrics across multiple batches within an epoch.</p> PARAMETER DESCRIPTION <code>result</code> <p>(None |     tuple[torch.Tensor, dict[Any, Any]] |     list[tuple[torch.Tensor, dict[Any, Any]]] |     list[list[tuple[torch.Tensor, dict[Any, Any]]]])         The loss and metrics data, which can have multiple formats</p> <p> TYPE: <code>None | tuple[Tensor, dict[Any, Any]] | list[tuple[Tensor, dict[Any, Any]]] | list[list[tuple[Tensor, dict[Any, Any]]]]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>This method does not return anything. It sets <code>self.opt_result</code> with</p> <p> TYPE: <code>None</code> </p> <code>None</code> <p>the computed average loss and metrics.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def build_optimize_result(\n    self,\n    result: None\n    | tuple[torch.Tensor, dict[Any, Any]]\n    | list[tuple[torch.Tensor, dict[Any, Any]]]\n    | list[list[tuple[torch.Tensor, dict[Any, Any]]]],\n) -&gt; None:\n    \"\"\"\n    Builds and stores the optimization result by calculating the average loss and metrics.\n\n    Result (or loss_metrics) can have multiple formats:\n    - `None` Indicates no loss or metrics data is provided.\n    - `tuple[torch.Tensor, dict[str, Any]]` A single tuple containing the loss tensor\n        and metrics dictionary - at the end of batch.\n    - `list[tuple[torch.Tensor, dict[str, Any]]]` A list of tuples for\n        multiple batches.\n    - `list[list[tuple[torch.Tensor, dict[str, Any]]]]` A list of lists of tuples,\n    where each inner list represents metrics across multiple batches within an epoch.\n\n    Args:\n        result: (None |\n                tuple[torch.Tensor, dict[Any, Any]] |\n                list[tuple[torch.Tensor, dict[Any, Any]]] |\n                list[list[tuple[torch.Tensor, dict[Any, Any]]]])\n                    The loss and metrics data, which can have multiple formats\n\n    Returns:\n        None: This method does not return anything. It sets `self.opt_result` with\n        the computed average loss and metrics.\n    \"\"\"\n    loss_metrics = result\n    if loss_metrics is None:\n        loss = None\n        metrics: dict[Any, Any] = {}\n    elif isinstance(loss_metrics, tuple):\n        # Single tuple case\n        loss, metrics = loss_metrics\n    else:\n        last_epoch: list[tuple[torch.Tensor, dict[Any, Any]]] = []\n        if isinstance(loss_metrics, list):\n            # Check if it's a list of tuples\n            if all(isinstance(item, tuple) for item in loss_metrics):\n                last_epoch = cast(list[tuple[torch.Tensor, dict[Any, Any]]], loss_metrics)\n            # Check if it's a list of lists of tuples\n            elif all(isinstance(item, list) for item in loss_metrics):\n                last_epoch = cast(\n                    list[tuple[torch.Tensor, dict[Any, Any]]],\n                    loss_metrics[-1] if loss_metrics else [],\n                )\n            else:\n                raise ValueError(\n                    \"Invalid format for result: Expected None, tuple, list of tuples,\"\n                    \" or list of lists of tuples.\"\n                )\n\n        if not last_epoch:\n            loss, metrics = None, {}\n        else:\n            # Compute the average loss over the batches\n            loss_tensor = torch.stack([loss_batch for loss_batch, _ in last_epoch])\n            avg_loss = loss_tensor.mean()\n\n            # Collect and average metrics for all batches\n            metric_keys = last_epoch[0][1].keys()\n            metrics_stacked: dict = {key: [] for key in metric_keys}\n\n            for _, metrics_batch in last_epoch:\n                for key in metric_keys:\n                    value = metrics_batch[key]\n                    metrics_stacked[key].append(value)\n\n            avg_metrics = {key: torch.stack(metrics_stacked[key]).mean() for key in metric_keys}\n\n            loss, metrics = avg_loss, avg_metrics\n\n    # Store the optimization result\n    self.opt_result = OptimizeResult(\n        self.current_epoch, self.model_old, self.optimizer_old, loss, metrics\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.fit","title":"<code>fit(train_dataloader=None, val_dataloader=None)</code>","text":"<p>Fits the model using the specified training configuration.</p> <p>The dataloaders can be provided to train on new datasets, or the default dataloaders provided in the trainer will be used.</p> PARAMETER DESCRIPTION <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Module, Optimizer]</code> <p>tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def fit(\n    self,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; tuple[nn.Module, optim.Optimizer]:\n    \"\"\"\n    Fits the model using the specified training configuration.\n\n    The dataloaders can be provided to train on new datasets, or the default dataloaders\n    provided in the trainer will be used.\n\n    Args:\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n\n    Returns:\n        tuple[nn.Module, optim.Optimizer]: The trained model and optimizer.\n    \"\"\"\n    if train_dataloader is not None:\n        self.train_dataloader = train_dataloader\n    if val_dataloader is not None:\n        self.val_dataloader = val_dataloader\n\n    self._fit_setup()\n    self._train()\n    self._fit_end()\n    self.training_stage = TrainingStage(\"idle\")\n    return self.model, self.optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_test_batch","title":"<code>run_test_batch(batch)</code>","text":"<p>Runs a single test batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"test_batch\")\ndef run_test_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single test batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_train_batch","title":"<code>run_train_batch(batch)</code>","text":"<p>Runs a single training batch, performing optimization.</p> <p>We use the step function to optimize the model based on use_grad.     use_grad = True entails gradient based optimization, for which we use     optimize_step function.     use_grad = False entails gradient free optimization, for which we use     update_ng_parameters function.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch. tuple of (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_batch\")\ndef run_train_batch(\n    self, batch: tuple[torch.Tensor, ...]\n) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single training batch, performing optimization.\n\n    We use the step function to optimize the model based on use_grad.\n        use_grad = True entails gradient based optimization, for which we use\n        optimize_step function.\n        use_grad = False entails gradient free optimization, for which we use\n        update_ng_parameters function.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n            tuple of (loss, metrics)\n    \"\"\"\n\n    if self.use_grad:\n        # Perform gradient-based optimization\n        loss_metrics = self.optimize_step(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            xs=batch,\n            device=self.device,\n            dtype=self.data_dtype,\n        )\n    else:\n        # Perform optimization using Nevergrad\n        loss, metrics, ng_params = update_ng_parameters(\n            model=self.model,\n            optimizer=self.optimizer,\n            loss_fn=self.loss_fn,\n            data=batch,\n            ng_params=self.ng_params,  # type: ignore[arg-type]\n        )\n        self.ng_params = ng_params\n        loss_metrics = loss, metrics\n\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_training","title":"<code>run_training(dataloader)</code>","text":"<p>Runs the training for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"train_epoch\")\ndef run_training(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the training for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for training data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.train()\n    train_epoch_loss_metrics = []\n    # Quick Fix for iteration 0\n    self._reset_model_and_opt()\n\n    for batch in self._batch_iter(dataloader, self.num_training_batches):\n        self.on_train_batch_start(batch)\n        train_batch_loss_metrics = self.run_train_batch(batch)\n        train_epoch_loss_metrics.append(train_batch_loss_metrics)\n        self.on_train_batch_end(train_batch_loss_metrics)\n\n    return train_epoch_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_val_batch","title":"<code>run_val_batch(batch)</code>","text":"<p>Runs a single validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>Batch of data from the DataLoader.</p> <p> TYPE: <code>tuple[Tensor, ...]</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor, dict[str, Any]]</code> <p>tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_batch\")\ndef run_val_batch(self, batch: tuple[torch.Tensor, ...]) -&gt; tuple[torch.Tensor, dict[str, Any]]:\n    \"\"\"\n    Runs a single validation batch.\n\n    Args:\n        batch (tuple[torch.Tensor, ...]): Batch of data from the DataLoader.\n\n    Returns:\n        tuple[torch.Tensor, dict[str, Any]]: Loss and metrics for the batch.\n    \"\"\"\n    with torch.no_grad():\n        loss_metrics = self.loss_fn(self.model, batch)\n    return self._modify_batch_end_loss_metrics(loss_metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.run_validation","title":"<code>run_validation(dataloader)</code>","text":"<p>Runs the validation loop for a single epoch, iterating over multiple batches.</p> PARAMETER DESCRIPTION <code>dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                  -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>@BaseTrainer.callback(\"val_epoch\")\ndef run_validation(self, dataloader: DataLoader) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the validation loop for a single epoch, iterating over multiple batches.\n\n    Args:\n        dataloader (DataLoader): DataLoader for validation data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                  -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    self.model.eval()\n    val_epoch_loss_metrics = []\n\n    for batch in self._batch_iter(dataloader, self.num_validation_batches):\n        self.on_val_batch_start(batch)\n        val_batch_loss_metrics = self.run_val_batch(batch)\n        val_epoch_loss_metrics.append(val_batch_loss_metrics)\n        self.on_val_batch_end(val_batch_loss_metrics)\n\n    return val_epoch_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.trainer.Trainer.test","title":"<code>test(test_dataloader=None)</code>","text":"<p>Runs the testing loop if a test DataLoader is provided.</p> <p>if the test_dataloader is not provided, default test_dataloader defined in the Trainer class is used.</p> PARAMETER DESCRIPTION <code>test_dataloader</code> <p>DataLoader for test data.</p> <p> TYPE: <code>DataLoader</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>list[tuple[Tensor, dict[str, Any]]]</code> <p>list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch. list                    -&gt; tuples Test Batches            -&gt; (loss, metrics)</p> Source code in <code>qadence/ml_tools/trainer.py</code> <pre><code>def test(self, test_dataloader: DataLoader = None) -&gt; list[tuple[torch.Tensor, dict[str, Any]]]:\n    \"\"\"\n    Runs the testing loop if a test DataLoader is provided.\n\n    if the test_dataloader is not provided, default test_dataloader defined\n    in the Trainer class is used.\n\n    Args:\n        test_dataloader (DataLoader): DataLoader for test data.\n\n    Returns:\n        list[tuple[torch.Tensor, dict[str, Any]]]: Loss and metrics for each batch.\n            list                    -&gt; tuples\n            Test Batches            -&gt; (loss, metrics)\n    \"\"\"\n    if test_dataloader is not None:\n        self.test_dataloader = test_dataloader\n\n    self.model.eval()\n    test_loss_metrics = []\n\n    for batch in self._batch_iter(test_dataloader, self.num_training_batches):\n        self.on_test_batch_start(batch)\n        loss_metrics = self.run_test_batch(batch)\n        test_loss_metrics.append(loss_metrics)\n        self.on_test_batch_end(loss_metrics)\n\n    return test_loss_metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig","title":"<code>AnsatzConfig(depth=1, ansatz_type=AnsatzType.HEA, ansatz_strategy=Strategy.DIGITAL, strategy_args=dict(), param_prefix='theta', tag=None)</code>  <code>dataclass</code>","text":""},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.ansatz_strategy","title":"<code>ansatz_strategy: Strategy = Strategy.DIGITAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Ansatz strategy.</p> <p><code>Strategy.DIGITAL</code> for fully digital ansatz. Required if <code>ansatz_type</code> is <code>AnsatzType.IIA</code>. <code>Strategy.SDAQC</code> for analog entangling block. <code>Strategy.RYDBERG</code> for fully rydberg hea ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.ansatz_type","title":"<code>ansatz_type: AnsatzType = AnsatzType.HEA</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>What type of ansatz.</p> <p><code>AnsatzType.HEA</code> for Hardware Efficient Ansatz. <code>AnsatzType.IIA</code> for Identity intialized Ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.depth","title":"<code>depth: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of layers of the ansatz.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.param_prefix","title":"<code>param_prefix: str = 'theta'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The base bame of the variational parameter.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.strategy_args","title":"<code>strategy_args: dict = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary containing keyword arguments to the function creating the ansatz.</p> <p>Details about each below.</p> <p>For <code>Strategy.DIGITAL</code> strategy, accepts the following:     periodic (bool): if the qubits should be linked periodically.         periodic=False is not supported in emu-c.     operations (list): list of operations to cycle through in the         digital single-qubit rotations of each layer.         Defaults to  [RX, RY, RX] for hea and [RX, RY] for iia.     entangler (AbstractBlock): 2-qubit entangling operation.         Supports CNOT, CZ, CRX, CRY, CRZ, CPHASE. Controlld rotations         will have variational parameters on the rotation angles.         Defaults to CNOT</p> <p>For <code>Strategy.SDAQC</code> strategy, accepts the following:     operations (list): list of operations to cycle through in the         digital single-qubit rotations of each layer.         Defaults to  [RX, RY, RX] for hea and [RX, RY] for iia.     entangler (AbstractBlock): Hamiltonian generator for the         analog entangling layer. Time parameter is considered variational.         Defaults to NN interaction.</p> <p>For <code>Strategy.RYDBERG</code> strategy, accepts the following:     addressable_detuning: whether to turn on the trainable semi-local addressing pattern         on the detuning (n_i terms in the Hamiltonian).         Defaults to True.     addressable_drive: whether to turn on the trainable semi-local addressing pattern         on the drive (sigma_i^x terms in the Hamiltonian).         Defaults to False.     tunable_phase: whether to have a tunable phase to get both sigma^x and sigma^y rotations         in the drive term. If False, only a sigma^x term will be included in the drive part         of the Hamiltonian generator.         Defaults to False.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.AnsatzConfig.tag","title":"<code>tag: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to indicate the name tag of the ansatz.</p> <p>Defaults to None, in which case no tag will be applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig","title":"<code>FeatureMapConfig(num_features=0, basis_set=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT, feature_range=None, target_range=None, multivariate_strategy=MultivariateStrategy.PARALLEL, feature_map_strategy=Strategy.DIGITAL, param_prefix=None, num_repeats=0, operation=None, inputs=None, tag=None)</code>  <code>dataclass</code>","text":""},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.basis_set","title":"<code>basis_set: BasisSet | dict[str, BasisSet] = BasisSet.FOURIER</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basis set for feature encoding.</p> <p>Takes qadence.BasisSet. Give a single BasisSet to use the same for all features. Give a dict of (str, BasisSet) where the key is the name of the variable and the value is the BasisSet to use for encoding that feature. BasisSet.FOURIER for Fourier encoding. BasisSet.CHEBYSHEV for Chebyshev encoding.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.feature_map_strategy","title":"<code>feature_map_strategy: Strategy = Strategy.DIGITAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Strategy for feature map.</p> <p>Accepts DIGITAL, ANALOG or RYDBERG. Defaults to DIGITAL. If the strategy is incompatible with the <code>operation</code> chosen, then <code>operation</code> gets preference and the given strategy is ignored.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.feature_range","title":"<code>feature_range: tuple[float, float] | dict[str, tuple[float, float]] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Range of data that the input data is assumed to come from.</p> <p>Give a single tuple to use the same range for all features. Give a dict of (str, tuple) where the key is the name of the variable and the value is the feature range to use for that feature.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.inputs","title":"<code>inputs: list[Basic | str] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List that indicates the order of variables of the tensors that are passed.</p> <p>Optional if a single feature is being encoded, required otherwise. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.multivariate_strategy","title":"<code>multivariate_strategy: MultivariateStrategy = MultivariateStrategy.PARALLEL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The encoding strategy in case of multi-variate function.</p> <p>Takes qadence.MultivariateStrategy. If PARALLEL, the features are encoded in one block of rotation gates with the register being split in sub-registers for each feature. If SERIES, the features are encoded sequentially using the full register for each feature, with an ansatz block between them. PARALLEL is allowed only for DIGITAL <code>feature_map_strategy</code>.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.num_features","title":"<code>num_features: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of feature parameters to be encoded.</p> <p>Defaults to 0. Thus, no feature parameters are encoded.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.num_repeats","title":"<code>num_repeats: int | dict[str, int] = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of feature map layers repeated in the data reuploading step.</p> <p>If all features are to be repeated the same number of times, then can give a single <code>int</code>. For different number of repetitions for each feature, provide a dict of (str, int) where the key is the name of the variable and the value is the number of repetitions for that feature. This amounts to the number of additional reuploads. So if <code>num_repeats</code> is N, the data gets uploaded N+1 times. Defaults to no repetition.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.operation","title":"<code>operation: Callable[[Parameter | Basic], AnalogBlock] | Type[RX] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of operation.</p> <p>Choose among the analog or digital rotations or a custom callable function returning an AnalogBlock instance. If the type of operation is incompatible with the <code>strategy</code> chosen, then <code>operation</code> gets preference and the given strategy is ignored.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.param_prefix","title":"<code>param_prefix: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String prefix to create trainable parameters in Feature Map.</p> <p>A string prefix to create trainable parameters multiplying the feature parameter inside the feature-encoding function. Note that currently this does not take into account the domain of the feature-encoding function. Defaults to <code>None</code> and thus, the feature map is not trainable. Note that this is separate from the name of the parameter. The user can provide a single prefix for all features, and it will be appended by appropriate feature name automatically.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.reupload_scaling","title":"<code>reupload_scaling: ReuploadScaling | dict[str, ReuploadScaling] = ReuploadScaling.CONSTANT</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Scaling for encoding the same feature on different qubits.</p> <p>Scaling used to encode the same feature on different qubits in the same layer of the feature maps. Takes qadence.ReuploadScaling. Give a single ReuploadScaling to use the same for all features. Give a dict of (str, ReuploadScaling) where the key is the name of the variable and the value is the ReuploadScaling to use for encoding that feature. ReuploadScaling.CONSTANT for constant scaling. ReuploadScaling.TOWER for linearly increasing scaling. ReuploadScaling.EXP for exponentially increasing scaling.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.tag","title":"<code>tag: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String to indicate the name tag of the feature map.</p> <p>Defaults to None, in which case no tag will be applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.FeatureMapConfig.target_range","title":"<code>target_range: tuple[float, float] | dict[str, tuple[float, float]] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Range of data the data encoder assumes as natural range.</p> <p>Give a single tuple to use the same range for all features. Give a dict of (str, tuple) where the key is the name of the variable and the value is the target range to use for that feature.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig","title":"<code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=lambda: list()(), log_model=False, root_folder=Path('./qml_logs'), create_subfolder_per_run=False, log_folder=Path('./'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=ExperimentTrackingTool.TENSORBOARD, hyperparams=dict(), plotting_functions=tuple(), _subfolders=list())</code>  <code>dataclass</code>","text":"<p>Default configuration for the training process.</p> <p>This class provides default settings for various aspects of the training loop, such as logging, checkpointing, and validation. The default values for these fields can be customized when an instance of <code>TrainConfig</code> is created.</p> <p>Example: <pre><code>from qadence.ml_tools import TrainConfig\nc = TrainConfig(root_folder=\"/tmp/train\")\n</code></pre> <pre><code>TrainConfig(max_iter=10000, print_every=0, write_every=0, checkpoint_every=0, plot_every=0, callbacks=[], log_model=False, root_folder='/tmp/train', create_subfolder_per_run=False, log_folder=PosixPath('.'), checkpoint_best_only=False, val_every=0, val_epsilon=1e-05, validation_criterion=None, trainstop_criterion=None, batch_size=1, verbose=True, tracking_tool=&lt;ExperimentTrackingTool.TENSORBOARD: 'tensorboard'&gt;, hyperparams={}, plotting_functions=(), _subfolders=[])\n</code></pre> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.batch_size","title":"<code>batch_size: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The batch size to use when processing a list or tuple of torch.Tensors.</p> <p>This specifies how many samples are processed in each training iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.callbacks","title":"<code>callbacks: list = field(default_factory=lambda: list())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of callbacks to execute during training.</p> <p>Callbacks can be used for custom behaviors, such as early stopping, custom logging, or other actions triggered at specific events.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.checkpoint_best_only","title":"<code>checkpoint_best_only: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If <code>True</code>, checkpoints are only saved if there is an improvement in the.</p> <p>validation metric. This conserves storage by only keeping the best models.</p> <p>validation_criterion is required when this is set to True.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.checkpoint_every","title":"<code>checkpoint_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for saving model and optimizer checkpoints during training.</p> <p>Set to 0 to disable checkpointing. This helps in resuming training or recovering models. Note that setting checkpoint_best_only = True will disable this and only best checkpoints will be saved.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.create_subfolder_per_run","title":"<code>create_subfolder_per_run: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to create a subfolder for each run, named <code>&lt;id&gt;_&lt;timestamp&gt;_&lt;PID&gt;</code>.</p> <p>This ensures logs and checkpoints from different runs do not overwrite each other, which is helpful for rapid prototyping. If <code>False</code>, training will resume from the latest checkpoint if one exists in the specified log folder.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.hyperparams","title":"<code>hyperparams: dict = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of hyperparameters to be tracked.</p> <p>This can include learning rates, regularization parameters, or any other training-related configurations.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.log_folder","title":"<code>log_folder: Path = Path('./')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The log folder for saving checkpoints and tensorboard logs.</p> <p>This stores the path where all logs and checkpoints are being saved for this training session. <code>log_folder</code> takes precedence over <code>root_folder</code> and <code>create_subfolder_per_run</code> arguments. If the user specifies a log_folder, all checkpoints will be saved in this folder and <code>root_folder</code> argument will not be used.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.log_model","title":"<code>log_model: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to log a serialized version of the model.</p> <p>When set to <code>True</code>, the model's state will be logged, useful for model versioning and reproducibility.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.max_iter","title":"<code>max_iter: int = 10000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of training iterations (epochs) to perform.</p> <p>This defines the total number of times the model will be updated.</p> <p>In case of InfiniteTensorDataset, each epoch will have 1 batch. In case of TensorDataset, each epoch will have len(dataloader) batches.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.plot_every","title":"<code>plot_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for generating and saving figures during training.</p> <p>Set to 0 to disable plotting.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.plotting_functions","title":"<code>plotting_functions: tuple[LoggablePlotFunction, ...] = field(default_factory=tuple)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Functions used for in-training plotting.</p> <p>These are called to generate plots that are logged or saved at specified intervals.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.print_every","title":"<code>print_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for printing loss and metrics to the console during training.</p> <p>Set to 0 to disable this output, meaning that metrics and loss will not be printed during training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.root_folder","title":"<code>root_folder: Path = Path('./qml_logs')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The root folder for saving checkpoints and tensorboard logs.</p> <p>The default path is \"./qml_logs\"</p> <p>This can be set to a specific directory where training artifacts are to be stored. Checkpoints will be saved inside a subfolder in this directory. Subfolders will be created based on <code>create_subfolder_per_run</code> argument.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.tracking_tool","title":"<code>tracking_tool: ExperimentTrackingTool = ExperimentTrackingTool.TENSORBOARD</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The tool used for tracking training progress and logging metrics.</p> <p>Options include tools like TensorBoard, which help visualize and monitor model training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.trainstop_criterion","title":"<code>trainstop_criterion: Callable | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to determine if the training process should stop based on a.</p> <p>specific stopping metric. If <code>None</code>, training continues until <code>max_iter</code> is reached.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.val_epsilon","title":"<code>val_epsilon: float = 1e-05</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A small safety margin used to compare the current validation loss with the.</p> <p>best previous validation loss. This is used to determine improvements in metrics.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.val_every","title":"<code>val_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for performing validation.</p> <p>If set to 0, validation is not performed. Note that metrics from validation are always written, regardless of the <code>write_every</code> setting. Note that initial validation happens at the start of training (when val_every &gt; 0)     For initial validation  - initial metrics are written.                             - checkpoint is saved (when checkpoint_best_only = False)</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.validation_criterion","title":"<code>validation_criterion: Callable | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A function to evaluate whether a given validation metric meets a desired condition.</p> <p>The validation_criterion has the following format: def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:     # process</p> <p>If <code>None</code>, no custom validation criterion is applied.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.verbose","title":"<code>verbose: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to print metrics and status messages during training.</p> <p>If <code>True</code>, detailed metrics and status updates will be displayed in the console.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.config.TrainConfig.write_every","title":"<code>write_every: int = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Frequency (in epochs) for writing loss and metrics using the tracking tool during training.</p> <p>Set to 0 to disable this logging, which prevents metrics from being logged to the tracking tool. Note that the metrics will always be written at the end of training regardless of this setting.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.get_parameters","title":"<code>get_parameters(model)</code>","text":"<p>Retrieve all trainable model parameters in a single vector.</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a 1-dimensional tensor with the parameters</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def get_parameters(model: Module) -&gt; Tensor:\n    \"\"\"Retrieve all trainable model parameters in a single vector.\n\n    Args:\n        model (Module): the input PyTorch model\n\n    Returns:\n        Tensor: a 1-dimensional tensor with the parameters\n    \"\"\"\n    ps = [p.reshape(-1) for p in model.parameters() if p.requires_grad]\n    return torch.concat(ps)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.num_parameters","title":"<code>num_parameters(model)</code>","text":"<p>Return the total number of parameters of the given model.</p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def num_parameters(model: Module) -&gt; int:\n    \"\"\"Return the total number of parameters of the given model.\"\"\"\n    return len(get_parameters(model))\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.parameters.set_parameters","title":"<code>set_parameters(model, theta)</code>","text":"<p>Set all trainable parameters of a model from a single vector.</p> <p>Notice that this function assumes prior knowledge of right number of parameters in the model</p> PARAMETER DESCRIPTION <code>model</code> <p>the input PyTorch model</p> <p> TYPE: <code>Module</code> </p> <code>theta</code> <p>the parameters to assign</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/parameters.py</code> <pre><code>def set_parameters(model: Module, theta: Tensor) -&gt; None:\n    \"\"\"Set all trainable parameters of a model from a single vector.\n\n    Notice that this function assumes prior knowledge of right number\n    of parameters in the model\n\n    Args:\n        model (Module): the input PyTorch model\n        theta (Tensor): the parameters to assign\n    \"\"\"\n\n    with torch.no_grad():\n        idx = 0\n        for ps in model.parameters():\n            if ps.requires_grad:\n                n = torch.numel(ps)\n                if ps.ndim == 0:\n                    ps[()] = theta[idx : idx + n]\n                else:\n                    ps[:] = theta[idx : idx + n].reshape(ps.size())\n                idx += n\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.optimize_step.optimize_step","title":"<code>optimize_step(model, optimizer, loss_fn, xs, device=None, dtype=None)</code>","text":"<p>Default Torch optimize step with closure.</p> <p>This is the default optimization step.</p> PARAMETER DESCRIPTION <code>model</code> <p>The input model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The chosen Torch optimizer.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable</code> </p> <code>xs</code> <p>The input data. If None, it means the given model does not require any input data.</p> <p> TYPE: <code>dict | list | Tensor | None</code> </p> <code>device</code> <p>A target device to run computations on.</p> <p> TYPE: <code>device</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>Data type for <code>xs</code> conversion.</p> <p> TYPE: <code>dtype</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>tuple[Tensor | float, dict | None]</code> <p>tuple[Tensor | float, dict | None]: A tuple containing the computed loss value and a dictionary with collected metrics.</p> Source code in <code>qadence/ml_tools/optimize_step.py</code> <pre><code>def optimize_step(\n    model: Module,\n    optimizer: Optimizer,\n    loss_fn: Callable,\n    xs: dict | list | torch.Tensor | None,\n    device: torch.device = None,\n    dtype: torch.dtype = None,\n) -&gt; tuple[torch.Tensor | float, dict | None]:\n    \"\"\"Default Torch optimize step with closure.\n\n    This is the default optimization step.\n\n    Args:\n        model (Module): The input model to be optimized.\n        optimizer (Optimizer): The chosen Torch optimizer.\n        loss_fn (Callable): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        xs (dict | list | Tensor | None): The input data. If None, it means\n            the given model does not require any input data.\n        device (torch.device): A target device to run computations on.\n        dtype (torch.dtype): Data type for `xs` conversion.\n\n    Returns:\n        tuple[Tensor | float, dict | None]: A tuple containing the computed loss value\n            and a dictionary with collected metrics.\n    \"\"\"\n\n    loss, metrics = None, {}\n    xs_to_device = data_to_device(xs, device=device, dtype=dtype)\n\n    def closure() -&gt; Any:\n        # NOTE: We need the nonlocal as we can't return a metric dict and\n        # because e.g. LBFGS calls this closure multiple times but for some\n        # reason the returned loss is always the first one...\n        nonlocal metrics, loss\n        optimizer.zero_grad()\n        loss, metrics = loss_fn(model, xs_to_device)\n        loss.backward(retain_graph=True)\n        return loss.item()\n\n    optimizer.step(closure)\n    # return the loss/metrics that are being mutated inside the closure...\n    return loss, metrics\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.optimize_step.update_ng_parameters","title":"<code>update_ng_parameters(model, optimizer, loss_fn, data, ng_params)</code>","text":"<p>Update the model parameters using Nevergrad.</p> <p>This function integrates Nevergrad for derivative-free optimization.</p> PARAMETER DESCRIPTION <code>model</code> <p>The PyTorch model to be optimized.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>A Nevergrad optimizer instance.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>A custom loss function that returns the loss value and a dictionary of metrics.</p> <p> TYPE: <code>Callable[[Module, Tensor | None], tuple[float, dict]]</code> </p> <code>data</code> <p>Input data for the model. If None, it means the model does not require input data.</p> <p> TYPE: <code>Tensor | None</code> </p> <code>ng_params</code> <p>The current set of parameters managed by Nevergrad.</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>tuple[float, dict, Array]</code> <p>tuple[float, dict, ng.p.Array]: A tuple containing the computed loss value, a dictionary of metrics, and the updated Nevergrad parameters.</p> Source code in <code>qadence/ml_tools/optimize_step.py</code> <pre><code>def update_ng_parameters(\n    model: Module,\n    optimizer: ng.optimizers.Optimizer,\n    loss_fn: Callable[[Module, torch.Tensor | None], tuple[float, dict]],\n    data: torch.Tensor | None,\n    ng_params: ng.p.Array,\n) -&gt; tuple[float, dict, ng.p.Array]:\n    \"\"\"Update the model parameters using Nevergrad.\n\n    This function integrates Nevergrad for derivative-free optimization.\n\n    Args:\n        model (Module): The PyTorch model to be optimized.\n        optimizer (ng.optimizers.Optimizer): A Nevergrad optimizer instance.\n        loss_fn (Callable[[Module, Tensor | None], tuple[float, dict]]): A custom loss function\n            that returns the loss value and a dictionary of metrics.\n        data (Tensor | None): Input data for the model. If None, it means the model does\n            not require input data.\n        ng_params (ng.p.Array): The current set of parameters managed by Nevergrad.\n\n    Returns:\n        tuple[float, dict, ng.p.Array]: A tuple containing the computed loss value,\n            a dictionary of metrics, and the updated Nevergrad parameters.\n    \"\"\"\n    loss, metrics = loss_fn(model, data)  # type: ignore[misc]\n    optimizer.tell(ng_params, float(loss))\n    ng_params = optimizer.ask()  # type: ignore[assignment]\n    params = promote_to_tensor(ng_params.value, requires_grad=False)\n    set_parameters(model, params)\n    return loss, metrics, ng_params\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.DictDataLoader","title":"<code>DictDataLoader(dataloaders)</code>  <code>dataclass</code>","text":"<p>This class only holds a dictionary of <code>DataLoader</code>s and samples from them.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.InfiniteTensorDataset","title":"<code>InfiniteTensorDataset(*tensors)</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Randomly sample points from the first dimension of the given tensors.</p> <p>Behaves like a normal torch <code>Dataset</code> just that we can sample from it as many times as we want.</p> <p>Examples: <pre><code>import torch\nfrom qadence.ml_tools.data import InfiniteTensorDataset\n\nx_data, y_data = torch.rand(5,2), torch.ones(5,1)\n# The dataset accepts any number of tensors with the same batch dimension\nds = InfiniteTensorDataset(x_data, y_data)\n\n# call `next` to get one sample from each tensor:\nxs = next(iter(ds))\n</code></pre> <pre><code>(tensor([0.4300, 0.5015]), tensor([1.]))\n</code></pre></p> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>def __init__(self, *tensors: Tensor):\n    \"\"\"Randomly sample points from the first dimension of the given tensors.\n\n    Behaves like a normal torch `Dataset` just that we can sample from it as\n    many times as we want.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from qadence.ml_tools.data import InfiniteTensorDataset\n\n    x_data, y_data = torch.rand(5,2), torch.ones(5,1)\n    # The dataset accepts any number of tensors with the same batch dimension\n    ds = InfiniteTensorDataset(x_data, y_data)\n\n    # call `next` to get one sample from each tensor:\n    xs = next(iter(ds))\n    print(str(xs)) # markdown-exec: hide\n    ```\n    \"\"\"\n    self.tensors = tensors\n    self.indices = list(range(self.tensors[0].size(0)))\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult","title":"<code>OptimizeResult(iteration, model, optimizer, loss=None, metrics=lambda: dict()(), extra=lambda: dict()())</code>  <code>dataclass</code>","text":"<p>OptimizeResult stores many optimization intermediate values.</p> <p>We store at a current iteration, the model, optimizer, loss values, metrics. An extra dict can be used for saving other information to be used for callbacks.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.extra","title":"<code>extra: dict = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Extra dict for saving anything else to be used in callbacks.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.iteration","title":"<code>iteration: int</code>  <code>instance-attribute</code>","text":"<p>Current iteration number.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.loss","title":"<code>loss: Tensor | float | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Loss value.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.metrics","title":"<code>metrics: dict = field(default_factory=lambda: dict())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Metrics that can be saved during training.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.model","title":"<code>model: Module</code>  <code>instance-attribute</code>","text":"<p>Model at iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.OptimizeResult.optimizer","title":"<code>optimizer: Optimizer | NGOptimizer</code>  <code>instance-attribute</code>","text":"<p>Optimizer at iteration.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.data.data_to_device","title":"<code>data_to_device(xs, *args, **kwargs)</code>","text":"<p>Utility method to move arbitrary data to 'device'.</p> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>@singledispatch\ndef data_to_device(xs: Any, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Utility method to move arbitrary data to 'device'.\"\"\"\n    raise ValueError(f\"Unable to move {type(xs)} with input args: {args} and kwargs: {kwargs}.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.data.to_dataloader","title":"<code>to_dataloader(*tensors, batch_size=1, infinite=False)</code>","text":"<p>Convert torch tensors an (infinite) Dataloader.</p> PARAMETER DESCRIPTION <code>*tensors</code> <p>Torch tensors to use in the dataloader.</p> <p> TYPE: <code>Tensor</code> DEFAULT: <code>()</code> </p> <code>batch_size</code> <p>batch size of sampled tensors</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>infinite</code> <p>if <code>True</code>, the dataloader will keep sampling indefinitely even after the whole dataset was sampled once</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Examples:</p> <pre><code>import torch\nfrom qadence.ml_tools import to_dataloader\n\n(x, y, z) = [torch.rand(10) for _ in range(3)]\nloader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\nprint(next(loader))\nprint(next(loader))\nprint(next(loader))\n</code></pre> <pre><code>[tensor([0.8244, 0.2922, 0.3246, 0.6345, 0.7915]), tensor([0.4082, 0.7652, 0.1494, 0.6814, 0.9754]), tensor([0.8941, 0.1731, 0.6468, 0.8306, 0.5045])]\n[tensor([0.8815, 0.2183, 0.1525, 0.8522, 0.3831]), tensor([0.2366, 0.4871, 0.4839, 0.8105, 0.1986]), tensor([0.1423, 0.6649, 0.1249, 0.9310, 0.9246])]\n[tensor([0.8244, 0.2922, 0.3246, 0.6345, 0.7915]), tensor([0.4082, 0.7652, 0.1494, 0.6814, 0.9754]), tensor([0.8941, 0.1731, 0.6468, 0.8306, 0.5045])]\n</code></pre> Source code in <code>qadence/ml_tools/data.py</code> <pre><code>def to_dataloader(*tensors: Tensor, batch_size: int = 1, infinite: bool = False) -&gt; DataLoader:\n    \"\"\"Convert torch tensors an (infinite) Dataloader.\n\n    Arguments:\n        *tensors: Torch tensors to use in the dataloader.\n        batch_size: batch size of sampled tensors\n        infinite: if `True`, the dataloader will keep sampling indefinitely even after the whole\n            dataset was sampled once\n\n    Examples:\n\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    import torch\n    from qadence.ml_tools import to_dataloader\n\n    (x, y, z) = [torch.rand(10) for _ in range(3)]\n    loader = iter(to_dataloader(x, y, z, batch_size=5, infinite=True))\n    print(next(loader))\n    print(next(loader))\n    print(next(loader))\n    ```\n    \"\"\"\n    ds = InfiniteTensorDataset(*tensors) if infinite else TensorDataset(*tensors)\n    return DataLoader(ds, batch_size=batch_size)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN","title":"<code>QNN(circuit, observable, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, inputs=None, input_diff_mode=InputDiffMode.AD)</code>","text":"<p>               Bases: <code>QuantumModel</code></p> <p>Quantum neural network model for n-dimensional inputs.</p> <p>Examples: <pre><code>import torch\nfrom qadence import QuantumCircuit, QNN, Z\nfrom qadence import hea, feature_map, hamiltonian_factory, kron\n\n# create the circuit\nn_qubits, depth = 2, 4\nfm = kron(\n    feature_map(1, support=(0,), param=\"x\"),\n    feature_map(1, support=(1,), param=\"y\")\n)\nansatz = hea(n_qubits=n_qubits, depth=depth)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nobs_base = hamiltonian_factory(n_qubits, detuning=Z)\n\n# the QNN will yield two outputs\nobs = [2.0 * obs_base, 4.0 * obs_base]\n\n# initialize and use the model\nqnn = QNN(circuit, obs, inputs=[\"x\", \"y\"])\ny = qnn(torch.rand(3, 2))\n</code></pre> <pre><code>tensor([[-0.4102, -0.8204],\n        [ 0.0515,  0.1030],\n        [ 0.0287,  0.0573]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> <p>Initialize the QNN.</p> <p>The number of inputs is determined by the feature parameters in the input quantum circuit while the number of outputs is determined by how many observables are provided as input</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The quantum circuit to use for the QNN.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>The observable.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>optional configuration for the backend</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>inputs</code> <p>List that indicates the order of variables of the tensors that are passed to the model. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p> <p> TYPE: <code>list[Basic | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode | str</code> DEFAULT: <code>AD</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    inputs: list[sympy.Basic | str] | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n):\n    \"\"\"Initialize the QNN.\n\n    The number of inputs is determined by the feature parameters in the input\n    quantum circuit while the number of outputs is determined by how many\n    observables are provided as input\n\n    Args:\n        circuit: The quantum circuit to use for the QNN.\n        observable: The observable.\n        backend: The chosen quantum backend.\n        diff_mode: The differentiation engine to use. Choices 'gpsr' or 'ad'.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        configuration: optional configuration for the backend\n        inputs: List that indicates the order of variables of the tensors that are passed\n            to the model. Given input tensors `xs = torch.rand(batch_size, input_size:=2)` a QNN\n            with `inputs=[\"t\", \"x\"]` will assign `t, x = xs[:,0], xs[:,1]`.\n        input_diff_mode: The differentiation mode for the input tensor.\n    \"\"\"\n    super().__init__(\n        circuit,\n        observable=observable,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        configuration=configuration,\n        noise=noise,\n    )\n    if self._observable is None:\n        raise ValueError(\"You need to provide at least one observable in the QNN constructor\")\n    if (inputs is not None) and (len(self.inputs) == len(inputs)):\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in inputs]  # type: ignore[union-attr]\n    elif (inputs is None) and len(self.inputs) &lt;= 1:\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in self.inputs]  # type: ignore[union-attr]\n    else:\n        raise ValueError(\n            \"\"\"\n            Your QNN has more than one input. Please provide a list of inputs in the order of\n            your tensor domain. For example, if you want to pass\n            `xs = torch.rand(batch_size, input_size:=3)` to you QNN, where\n            ```\n            t = x[:,0]\n            x = x[:,1]\n            y = x[:,2]\n            ```\n            you have to specify\n            ```\n            QNN(circuit, observable, inputs=[\"t\", \"x\", \"y\"])\n            ```\n            You can also pass a list of sympy symbols.\n        \"\"\"\n        )\n    self.format_to_dict = format_to_dict_fn(self.inputs)  # type: ignore[arg-type]\n    self.input_diff_mode = InputDiffMode(input_diff_mode)\n    if self.input_diff_mode == InputDiffMode.FD:\n        from qadence.backends.utils import finitediff\n\n        self.__derivative = finitediff\n    elif self.input_diff_mode == InputDiffMode.AD:\n        self.__derivative = _torch_derivative  # type: ignore[assignment]\n    else:\n        raise ValueError(f\"Unkown forward diff mode: {self.input_diff_mode}\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN.forward","title":"<code>forward(values=None, state=None, measurement=None, noise=None, endianness=Endianness.BIG)</code>","text":"<p>Forward pass of the model.</p> <p>This returns the (differentiable) expectation value of the given observable operator defined in the constructor. Differently from the base QuantumModel class, the QNN accepts also a tensor as input for the forward pass. The tensor is expected to have shape: <code>n_batches x in_features</code> where <code>n_batches</code> is the number of data points and <code>in_features</code> is the dimensionality of the problem</p> <p>The output of the forward pass is the expectation value of the input observable(s). If a single observable is given, the output shape is <code>n_batches</code> while if multiple observables are given the output shape is instead <code>n_batches x n_observables</code></p> PARAMETER DESCRIPTION <code>values</code> <p>the values of the feature parameters</p> <p> TYPE: <code>dict[str, Tensor] | Tensor</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a tensor with the expectation value of the observables passed in the constructor of the model</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def forward(\n    self,\n    values: dict[str, Tensor] | Tensor = None,\n    state: Tensor | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the model.\n\n    This returns the (differentiable) expectation value of the given observable\n    operator defined in the constructor. Differently from the base QuantumModel\n    class, the QNN accepts also a tensor as input for the forward pass. The\n    tensor is expected to have shape: `n_batches x in_features` where `n_batches`\n    is the number of data points and `in_features` is the dimensionality of the problem\n\n    The output of the forward pass is the expectation value of the input\n    observable(s). If a single observable is given, the output shape is\n    `n_batches` while if multiple observables are given the output shape\n    is instead `n_batches x n_observables`\n\n    Args:\n        values: the values of the feature parameters\n        state: Initial state.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n\n    Returns:\n        Tensor: a tensor with the expectation value of the observables passed\n            in the constructor of the model\n    \"\"\"\n    return self.expectation(\n        values, state=state, measurement=measurement, noise=noise, endianness=endianness\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.QNN.from_configs","title":"<code>from_configs(register, obs_config, fm_config=FeatureMapConfig(), ansatz_config=AnsatzConfig(), backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, input_diff_mode=InputDiffMode.AD)</code>  <code>classmethod</code>","text":"<p>Create a QNN from a set of configurations.</p> PARAMETER DESCRIPTION <code>register</code> <p>The number of qubits or a register object.</p> <p> TYPE: <code>int | Register</code> </p> <code>obs_config</code> <p>The configuration(s) for the observable(s).</p> <p> TYPE: <code>list[ObservableConfig] | ObservableConfig</code> </p> <code>fm_config</code> <p>The configuration for the feature map. Defaults to no feature encoding block.</p> <p> TYPE: <code>FeatureMapConfig</code> DEFAULT: <code>FeatureMapConfig()</code> </p> <code>ansatz_config</code> <p>The configuration for the ansatz. Defaults to a single layer of hardware efficient ansatz.</p> <p> TYPE: <code>AnsatzConfig</code> DEFAULT: <code>AnsatzConfig()</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices are 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>Noise</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Optional backend configuration.</p> <p> TYPE: <code>BackendConfiguration | dict</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode</code> DEFAULT: <code>AD</code> </p> RETURNS DESCRIPTION <code>QNN</code> <p>A QNN object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the observable configuration is not provided.</p> <p>Example: <pre><code>import torch\nfrom qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\nfrom qadence.ml_tools import QNN\nfrom qadence.constructors import ObservableConfig\nfrom qadence.operations import Z\nfrom qadence.types import (\n    AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n)\n\nregister = 4\nobs_config = ObservableConfig(\n    detuning=Z,\n    scale=5.0,\n    shift=0.0,\n    transformation_type=ObservableTransform.SCALE,\n    trainable_transform=None,\n)\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs=[\"x\", \"y\"],\n    basis_set=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.CONSTANT,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nqnn = QNN.from_configs(\n    register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n)\n\nx = torch.rand(2, 2)\ny = qnn(x)\n</code></pre> <pre><code>tensor([[2.6101],\n        [4.7983]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>@classmethod\ndef from_configs(\n    cls,\n    register: int | Register,\n    obs_config: Any,\n    fm_config: Any = FeatureMapConfig(),\n    ansatz_config: Any = AnsatzConfig(),\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n) -&gt; QNN:\n    \"\"\"Create a QNN from a set of configurations.\n\n    Args:\n        register (int | Register): The number of qubits or a register object.\n        obs_config (list[ObservableConfig] | ObservableConfig): The configuration(s)\n            for the observable(s).\n        fm_config (FeatureMapConfig): The configuration for the feature map.\n            Defaults to no feature encoding block.\n        ansatz_config (AnsatzConfig): The configuration for the ansatz.\n            Defaults to a single layer of hardware efficient ansatz.\n        backend (BackendName): The chosen quantum backend.\n        diff_mode (DiffMode): The differentiation engine to use. Choices are\n            'gpsr' or 'ad'.\n        measurement (Measurements): Optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator.\n        noise (Noise): A noise model to use.\n        configuration (BackendConfiguration | dict): Optional backend configuration.\n        input_diff_mode (InputDiffMode): The differentiation mode for the input tensor.\n\n    Returns:\n        A QNN object.\n\n    Raises:\n        ValueError: If the observable configuration is not provided.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\n    from qadence.ml_tools import QNN\n    from qadence.constructors import ObservableConfig\n    from qadence.operations import Z\n    from qadence.types import (\n        AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n    )\n\n    register = 4\n    obs_config = ObservableConfig(\n        detuning=Z,\n        scale=5.0,\n        shift=0.0,\n        transformation_type=ObservableTransform.SCALE,\n        trainable_transform=None,\n    )\n    fm_config = FeatureMapConfig(\n        num_features=2,\n        inputs=[\"x\", \"y\"],\n        basis_set=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.CONSTANT,\n        feature_range={\n            \"x\": (-1.0, 1.0),\n            \"y\": (0.0, 1.0),\n        },\n    )\n    ansatz_config = AnsatzConfig(\n        depth=2,\n        ansatz_type=AnsatzType.HEA,\n        ansatz_strategy=Strategy.DIGITAL,\n    )\n\n    qnn = QNN.from_configs(\n        register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n    )\n\n    x = torch.rand(2, 2)\n    y = qnn(x)\n    print(str(y)) # markdown-exec: hide\n    ```\n    \"\"\"\n    from .constructors import build_qnn_from_configs\n\n    return build_qnn_from_configs(\n        register=register,\n        observable_config=obs_config,\n        fm_config=fm_config,\n        ansatz_config=ansatz_config,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        noise=noise,\n        configuration=configuration,\n        input_diff_mode=input_diff_mode,\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.derivative","title":"<code>derivative(ufa, x, derivative_indices)</code>","text":"<p>Compute derivatives w.r.t.</p> <p>inputs of a UFA with a single output. The <code>derivative_indices</code> specify which derivative(s) are computed.  E.g. <code>derivative_indices=(1,2)</code> would compute the a second order derivative w.r.t to the indices <code>1</code> and <code>2</code> of the input tensor.</p> PARAMETER DESCRIPTION <code>ufa</code> <p>The model for which we want to compute the derivative.</p> <p> TYPE: <code>Module</code> </p> <code>x</code> <p>(batch_size, input_size) input tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>derivative_indices</code> <p>Define which derivatives to compute.</p> <p> TYPE: <code>tuple</code> </p> <p>Examples: If we create a UFA with three inputs and denote the first, second, and third input with <code>x</code>, <code>y</code>, and <code>z</code> we can compute the following derivatives w.r.t to those inputs: <pre><code>import torch\nfrom qadence.ml_tools.models import derivative, QNN\nfrom qadence.ml_tools.config import FeatureMapConfig, AnsatzConfig\nfrom qadence.constructors.hamiltonians import ObservableConfig\nfrom qadence.operations import Z\n\nfm_config = FeatureMapConfig(num_features=3, inputs=[\"x\", \"y\", \"z\"])\nansatz_config = AnsatzConfig()\nobs_config = ObservableConfig(detuning=Z)\n\nf = QNN.from_configs(\n    register=3, obs_config=obs_config, fm_config=fm_config, ansatz_config=ansatz_config,\n)\ninputs = torch.rand(5,3,requires_grad=True)\n\n# df_dx\nderivative(f, inputs, (0,))\n\n# d2f_dydz\nderivative(f, inputs, (1,2))\n\n# d3fdy2dx\nderivative(f, inputs, (1,1,0))\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def derivative(ufa: torch.nn.Module, x: Tensor, derivative_indices: tuple[int, ...]) -&gt; Tensor:\n    \"\"\"Compute derivatives w.r.t.\n\n    inputs of a UFA with a single output. The\n    `derivative_indices` specify which derivative(s) are computed.  E.g.\n    `derivative_indices=(1,2)` would compute the a second order derivative w.r.t\n    to the indices `1` and `2` of the input tensor.\n\n    Arguments:\n        ufa: The model for which we want to compute the derivative.\n        x (Tensor): (batch_size, input_size) input tensor.\n        derivative_indices (tuple): Define which derivatives to compute.\n\n    Examples:\n    If we create a UFA with three inputs and denote the first, second, and third\n    input with `x`, `y`, and `z` we can compute the following derivatives w.r.t\n    to those inputs:\n    ```py exec=\"on\" source=\"material-block\"\n    import torch\n    from qadence.ml_tools.models import derivative, QNN\n    from qadence.ml_tools.config import FeatureMapConfig, AnsatzConfig\n    from qadence.constructors.hamiltonians import ObservableConfig\n    from qadence.operations import Z\n\n    fm_config = FeatureMapConfig(num_features=3, inputs=[\"x\", \"y\", \"z\"])\n    ansatz_config = AnsatzConfig()\n    obs_config = ObservableConfig(detuning=Z)\n\n    f = QNN.from_configs(\n        register=3, obs_config=obs_config, fm_config=fm_config, ansatz_config=ansatz_config,\n    )\n    inputs = torch.rand(5,3,requires_grad=True)\n\n    # df_dx\n    derivative(f, inputs, (0,))\n\n    # d2f_dydz\n    derivative(f, inputs, (1,2))\n\n    # d3fdy2dx\n    derivative(f, inputs, (1,1,0))\n    ```\n    \"\"\"\n    assert ufa.out_features == 1, \"Can only call `derivative` on models with 1D output.\"\n    return ufa._derivative(x, derivative_indices)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.models.format_to_dict_fn","title":"<code>format_to_dict_fn(inputs=[])</code>","text":"<p>Format an input tensor into the format required by the forward pass.</p> <p>The tensor is assumed to have dimensions: n_batches x in_features where in_features corresponds to the number of input features of the QNN</p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def format_to_dict_fn(\n    inputs: list[sympy.Symbol | str] = [],\n) -&gt; Callable[[Tensor | ParamDictType], ParamDictType]:\n    \"\"\"Format an input tensor into the format required by the forward pass.\n\n    The tensor is assumed to have dimensions: n_batches x in_features where in_features\n    corresponds to the number of input features of the QNN\n    \"\"\"\n    in_features = len(inputs)\n\n    def tensor_to_dict(values: Tensor | ParamDictType) -&gt; ParamDictType:\n        if isinstance(values, Tensor):\n            values = values.reshape(-1, 1) if len(values.size()) == 1 else values\n            if not values.shape[1] == in_features:\n                raise ValueError(\n                    f\"Model expects in_features={in_features} but got {values.shape[1]}.\"\n                )\n            values = {fparam.name: values[:, inputs.index(fparam)] for fparam in inputs}  # type: ignore[union-attr]\n        return values\n\n    return tensor_to_dict\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback","title":"<code>Callback(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>Base class for defining various training callbacks.</p> ATTRIBUTE DESCRIPTION <code>on</code> <p>The event on which to trigger the callback. Must be a valid on value from: [\"train_start\", \"train_end\",     \"train_epoch_start\", \"train_epoch_end\", \"train_batch_start\",     \"train_batch_end\",\"val_epoch_start\", \"val_epoch_end\",     \"val_batch_start\", \"val_batch_end\", \"test_batch_start\",     \"test_batch_end\"]</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>callback</code> <p>The function to call if the condition is met.</p> <p> TYPE: <code>CallbackFunction | None</code> </p> <code>callback_condition</code> <p>Condition to check before calling.</p> <p> TYPE: <code>CallbackConditionFunction | None</code> </p> <code>modify_optimize_result</code> <p>Function to modify <code>OptimizeResult</code>.</p> <p> TYPE: <code>CallbackFunction | dict[str, Any] | None</code> </p> <p>A callback can be defined in two ways:</p> <ol> <li>By providing a callback function directly in the base class:    This is useful for simple callbacks that don't require subclassing.</li> </ol> <p>Example:    <pre><code>from qadence.ml_tools.callbacks import Callback\n\ndef custom_callback_function(trainer, config, writer):\n    print(\"Custom callback executed.\")\n\ncustom_callback = Callback(\n    on=\"train_end\",\n    called_every=5,\n    callback=custom_callback_function\n)\n</code></pre> <pre><code>\n</code></pre> </p> <ol> <li>By inheriting and implementing the <code>run_callback</code> method:    This is suitable for more complex callbacks that require customization.</li> </ol> <p>Example:    <pre><code>from qadence.ml_tools.callbacks import Callback\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in the inherited run_callback method.\")\n\ncustom_callback = CustomCallback(on=\"train_end\", called_every=10)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.on","title":"<code>on: TrainingStage | str</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the TrainingStage.</p> RETURNS DESCRIPTION <code>TrainingStage</code> <p>TrainingStage for the callback</p> <p> TYPE: <code>TrainingStage | str</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.__call__","title":"<code>__call__(when, trainer, config, writer)</code>","text":"<p>Executes the callback if conditions are met.</p> PARAMETER DESCRIPTION <code>when</code> <p>The event when the callback is triggered.</p> <p> TYPE: <code>str</code> </p> <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback function if executed.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __call__(\n    self, when: TrainingStage, trainer: Any, config: TrainConfig, writer: BaseWriter\n) -&gt; Any:\n    \"\"\"Executes the callback if conditions are met.\n\n    Args:\n        when (str): The event when the callback is triggered.\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback function if executed.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if self.on == when:\n        if opt_result:\n            opt_result = self.modify_optimize_result(opt_result)\n        if self._should_call(when, opt_result):\n            return self.run_callback(trainer, config, writer)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.Callback.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Executes the defined callback.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Result of the callback execution.</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>If not implemented in subclasses.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Executes the defined callback.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: Result of the callback execution.\n\n    Raises:\n        NotImplementedError: If not implemented in subclasses.\n    \"\"\"\n    if self.callback is not None:\n        return self.callback(trainer, config, writer)\n    raise NotImplementedError(\"Subclasses should override the run_callback method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.EarlyStopping","title":"<code>EarlyStopping(on, called_every, monitor, patience=5, mode='min')</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Stops training when a monitored metric has not improved for a specified number of epochs.</p> <p>This callback monitors a specified metric (e.g., validation loss or accuracy). If the metric does not improve for a given patience period, training is stopped.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>EarlyStopping</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import EarlyStopping\n\n# Create an instance of the EarlyStopping callback\nearly_stopping = EarlyStopping(on=\"val_epoch_end\",\n                               called_every=1,\n                               monitor=\"val_loss\",\n                               patience=5,\n                               mode=\"min\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    print_every=1000,\n    callbacks=[early_stopping]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the EarlyStopping callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback (e.g., \"val_epoch_end\").</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>monitor</code> <p>The metric to monitor (e.g., \"val_loss\" or \"train_loss\"). All metrics returned by optimize step are available to monitor. Please add \"val_\" and \"train_\" strings at the start of the metric name.</p> <p> TYPE: <code>str</code> </p> <code>patience</code> <p>Number of iterations to wait for improvement. Default is 5.</p> <p> TYPE: <code>int</code> DEFAULT: <code>5</code> </p> <code>mode</code> <p>Whether to minimize (\"min\") or maximize (\"max\") the metric. Default is \"min\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'min'</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self, on: str, called_every: int, monitor: str, patience: int = 5, mode: str = \"min\"\n):\n    \"\"\"Initializes the EarlyStopping callback.\n\n    Args:\n        on (str): The event to trigger the callback (e.g., \"val_epoch_end\").\n        called_every (int): Frequency of callback calls in terms of iterations.\n        monitor (str): The metric to monitor (e.g., \"val_loss\" or \"train_loss\").\n            All metrics returned by optimize step are available to monitor.\n            Please add \"val_\" and \"train_\" strings at the start of the metric name.\n        patience (int, optional): Number of iterations to wait for improvement. Default is 5.\n        mode (str, optional): Whether to minimize (\"min\") or maximize (\"max\") the metric.\n            Default is \"min\".\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.monitor = monitor\n    self.patience = patience\n    self.mode = mode\n    self.best_value = float(\"inf\") if mode == \"min\" else -float(\"inf\")\n    self.counter = 0\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.EarlyStopping.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Monitors the metric and stops training if no improvement is observed.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Monitors the metric and stops training if no improvement is observed.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    current_value = trainer.opt_result.metrics.get(self.monitor)\n    if current_value is None:\n        raise ValueError(f\"Metric '{self.monitor}' is not available in the trainer's metrics.\")\n\n    if (self.mode == \"min\" and current_value &lt; self.best_value) or (\n        self.mode == \"max\" and current_value &gt; self.best_value\n    ):\n        self.best_value = current_value\n        self.counter = 0\n    else:\n        self.counter += 1\n\n    if self.counter &gt;= self.patience:\n        logger.info(\n            f\"EarlyStopping: No improvement in '{self.monitor}' for {self.patience} epochs. \"\n            \"Stopping training.\"\n        )\n        trainer.stop_training = True\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.GradientMonitoring","title":"<code>GradientMonitoring(on, called_every=1)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Logs gradient statistics (e.g., mean, standard deviation, max) during training.</p> <p>This callback monitors and logs statistics about the gradients of the model parameters to help debug or optimize the training process.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>GradientMonitoring</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import GradientMonitoring\n\n# Create an instance of the GradientMonitoring callback\ngradient_monitoring = GradientMonitoring(on=\"train_batch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    print_every=1000,\n    callbacks=[gradient_monitoring]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the GradientMonitoring callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback (e.g., \"train_batch_end\").</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int = 1):\n    \"\"\"Initializes the GradientMonitoring callback.\n\n    Args:\n        on (str): The event to trigger the callback (e.g., \"train_batch_end\").\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.GradientMonitoring.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs gradient statistics.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Logs gradient statistics.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    gradient_stats = {}\n    for name, param in trainer.model.named_parameters():\n        if param.grad is not None:\n            grad = param.grad\n            gradient_stats.update(\n                {\n                    name + \"_mean\": grad.mean().item(),\n                    name + \"_std\": grad.std().item(),\n                    name + \"_max\": grad.max().item(),\n                    name + \"_min\": grad.min().item(),\n                }\n            )\n\n    writer.write(trainer.opt_result.iteration, gradient_stats)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCosineAnnealing","title":"<code>LRSchedulerCosineAnnealing(on, called_every, t_max, min_lr=0.0)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Applies cosine annealing to the learning rate during training.</p> <p>This callback decreases the learning rate following a cosine curve, starting from the initial learning rate and annealing to a minimum (min_lr).</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerCosineAnnealing</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCosineAnnealing\n\n# Create an instance of the LRSchedulerCosineAnnealing callback\nlr_cosine = LRSchedulerCosineAnnealing(on=\"train_batch_end\",\n                                       called_every=1,\n                                       t_max=5000,\n                                       min_lr=1e-6)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_cosine]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerCosineAnnealing callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>t_max</code> <p>The total number of iterations for one annealing cycle.</p> <p> TYPE: <code>int</code> </p> <code>min_lr</code> <p>The minimum learning rate. Default is 0.0.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, t_max: int, min_lr: float = 0.0):\n    \"\"\"Initializes the LRSchedulerCosineAnnealing callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        t_max (int): The total number of iterations for one annealing cycle.\n        min_lr (float, optional): The minimum learning rate. Default is 0.0.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.t_max = t_max\n    self.min_lr = min_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCosineAnnealing.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Adjusts the learning rate using cosine annealing.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Adjusts the learning rate using cosine annealing.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    for param_group in trainer.optimizer.param_groups:\n        max_lr = param_group[\"lr\"]\n        new_lr = (\n            self.min_lr\n            + (max_lr - self.min_lr)\n            * (1 + math.cos(math.pi * trainer.opt_result.iteration / self.t_max))\n            / 2\n        )\n        param_group[\"lr\"] = new_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCyclic","title":"<code>LRSchedulerCyclic(on, called_every, base_lr, max_lr, step_size)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Applies a cyclic learning rate schedule during training.</p> <p>This callback oscillates the learning rate between a minimum (base_lr) and a maximum (max_lr) over a defined cycle length (step_size). The learning rate follows a triangular wave pattern.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerCyclic</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCyclic\n\n# Create an instance of the LRSchedulerCyclic callback\nlr_cyclic = LRSchedulerCyclic(on=\"train_batch_end\",\n                              called_every=1,\n                              base_lr=0.001,\n                              max_lr=0.01,\n                              step_size=2000)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_cyclic]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerCyclic callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>base_lr</code> <p>The minimum learning rate.</p> <p> TYPE: <code>float</code> </p> <code>max_lr</code> <p>The maximum learning rate.</p> <p> TYPE: <code>float</code> </p> <code>step_size</code> <p>Number of iterations for half a cycle.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, base_lr: float, max_lr: float, step_size: int):\n    \"\"\"Initializes the LRSchedulerCyclic callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        base_lr (float): The minimum learning rate.\n        max_lr (float): The maximum learning rate.\n        step_size (int): Number of iterations for half a cycle.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.base_lr = base_lr\n    self.max_lr = max_lr\n    self.step_size = step_size\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerCyclic.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Adjusts the learning rate cyclically.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Adjusts the learning rate cyclically.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    cycle = trainer.opt_result.iteration // (2 * self.step_size)\n    x = abs(trainer.opt_result.iteration / self.step_size - 2 * cycle - 1)\n    scale = max(0, (1 - x))\n    new_lr = self.base_lr + (self.max_lr - self.base_lr) * scale\n    for param_group in trainer.optimizer.param_groups:\n        param_group[\"lr\"] = new_lr\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerStepDecay","title":"<code>LRSchedulerStepDecay(on, called_every, gamma=0.5)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Reduces the learning rate by a factor at regular intervals.</p> <p>This callback adjusts the learning rate by multiplying it with a decay factor after a specified number of iterations. The learning rate is updated as:     lr = lr * gamma</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LRSchedulerStepDecay</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerStepDecay\n\n# Create an instance of the LRSchedulerStepDecay callback\nlr_step_decay = LRSchedulerStepDecay(on=\"train_epoch_end\",\n                                     called_every=100,\n                                     gamma=0.5)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback\n    callbacks=[lr_step_decay]\n)\n</code></pre> <pre><code>\n</code></pre> </p> <p>Initializes the LRSchedulerStepDecay callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> <code>gamma</code> <p>The decay factor applied to the learning rate. A value &lt; 1 reduces the learning rate over time. Default is 0.5.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int, gamma: float = 0.5):\n    \"\"\"Initializes the LRSchedulerStepDecay callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n        gamma (float, optional): The decay factor applied to the learning rate.\n            A value &lt; 1 reduces the learning rate over time. Default is 0.5.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.gamma = gamma\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LRSchedulerStepDecay.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Runs the callback to apply step decay to the learning rate.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; None:\n    \"\"\"\n    Runs the callback to apply step decay to the learning rate.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter): The writer object for logging.\n    \"\"\"\n    for param_group in trainer.optimizer.param_groups:\n        param_group[\"lr\"] *= self.gamma\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LoadCheckpoint","title":"<code>LoadCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to load a model checkpoint.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LoadCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Loads a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>The result of loading the checkpoint.</p> <p> TYPE: <code>Any</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Loads a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n\n    Returns:\n        Any: The result of loading the checkpoint.\n    \"\"\"\n    folder = config.log_folder\n    model = trainer.model\n    optimizer = trainer.optimizer\n    device = trainer.log_device\n    return load_checkpoint(folder, model, optimizer, device=device)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogHyperparameters","title":"<code>LogHyperparameters(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log hyperparameters using the writer.</p> <p>The <code>LogHyperparameters</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>LogHyperparameters</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogHyperparameters\n\n# Create an instance of the LogHyperparameters callback\nlog_hyper_callback = LogHyperparameters(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[log_hyper_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogHyperparameters.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs hyperparameters using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs hyperparameters using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    hyperparams = config.hyperparams\n    writer.log_hyperparams(hyperparams)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogModelTracker","title":"<code>LogModelTracker(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to log the model using the writer.</p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.LogModelTracker.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Logs the model using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Logs the model using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    model = trainer.model\n    writer.log_model(\n        model, trainer.train_dataloader, trainer.val_dataloader, trainer.test_dataloader\n    )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PlotMetrics","title":"<code>PlotMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to plot metrics using the writer.</p> <p>The <code>PlotMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PlotMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PlotMetrics\n\n# Create an instance of the PlotMetrics callback\nplot_metrics_callback = PlotMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[plot_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PlotMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Plots metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Plots metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    plotting_functions = config.plotting_functions\n    writer.plot(trainer.model, opt_result.iteration, plotting_functions)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PrintMetrics","title":"<code>PrintMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to print metrics using the writer.</p> <p>The <code>PrintMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>PrintMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\n# Create an instance of the PrintMetrics callback\nprint_metrics_callback = PrintMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[print_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.PrintMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Prints metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Prints metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.print_metrics(opt_result)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveBestCheckpoint","title":"<code>SaveBestCheckpoint(on, called_every)</code>","text":"<p>               Bases: <code>SaveCheckpoint</code></p> <p>Callback to save the best model checkpoint based on a validation criterion.</p> <p>Initializes the SaveBestCheckpoint callback.</p> PARAMETER DESCRIPTION <code>on</code> <p>The event to trigger the callback.</p> <p> TYPE: <code>str</code> </p> <code>called_every</code> <p>Frequency of callback calls in terms of iterations.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(self, on: str, called_every: int):\n    \"\"\"Initializes the SaveBestCheckpoint callback.\n\n    Args:\n        on (str): The event to trigger the callback.\n        called_every (int): Frequency of callback calls in terms of iterations.\n    \"\"\"\n    super().__init__(on=on, called_every=called_every)\n    self.best_loss = float(\"inf\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveBestCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves the checkpoint if the current loss is better than the best loss.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves the checkpoint if the current loss is better than the best loss.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    if config.validation_criterion and config.validation_criterion(\n        opt_result.loss, self.best_loss, config.val_epsilon\n    ):\n        self.best_loss = opt_result.loss\n\n        folder = config.log_folder\n        model = trainer.model\n        optimizer = trainer.optimizer\n        opt_result = trainer.opt_result\n        write_checkpoint(folder, model, optimizer, \"best\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveCheckpoint","title":"<code>SaveCheckpoint(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to save a model checkpoint.</p> <p>The <code>SaveCheckpoint</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>SaveCheckpoint</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint\n\n# Create an instance of the SaveCheckpoint callback\nsave_checkpoint_callback = SaveCheckpoint(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.SaveCheckpoint.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Saves a model checkpoint.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Saves a model checkpoint.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    folder = config.log_folder\n    model = trainer.model\n    optimizer = trainer.optimizer\n    opt_result = trainer.opt_result\n    write_checkpoint(folder, model, optimizer, opt_result.iteration)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.WriteMetrics","title":"<code>WriteMetrics(on='idle', called_every=1, callback=None, callback_condition=None, modify_optimize_result=None)</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to write metrics using the writer.</p> <p>The <code>WriteMetrics</code> callback can be added to the <code>TrainConfig</code> callbacks as a custom user defined callback.</p> <p>Example Usage in <code>TrainConfig</code>: To use <code>WriteMetrics</code>, include it in the <code>callbacks</code> list when setting up your <code>TrainConfig</code>: <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import WriteMetrics\n\n# Create an instance of the WriteMetrics callback\nwrite_metrics_callback = WriteMetrics(on = \"val_batch_end\", called_every = 100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    # Print metrics every 1000 training epochs\n    print_every=1000,\n    # Add the custom callback that runs every 100 val_batch_end\n    callbacks=[write_metrics_callback]\n)\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def __init__(\n    self,\n    on: str | TrainingStage = \"idle\",\n    called_every: int = 1,\n    callback: CallbackFunction | None = None,\n    callback_condition: CallbackConditionFunction | None = None,\n    modify_optimize_result: CallbackFunction | dict[str, Any] | None = None,\n):\n    if not isinstance(called_every, int):\n        raise ValueError(\"called_every must be a positive integer or 0\")\n\n    self.callback: CallbackFunction | None = callback\n    self.on: str | TrainingStage = on\n    self.called_every: int = called_every\n    self.callback_condition = callback_condition or (lambda _: True)\n\n    if isinstance(modify_optimize_result, dict):\n        self.modify_optimize_result = (\n            lambda opt_res: opt_res.extra.update(modify_optimize_result) or opt_res\n        )\n    else:\n        self.modify_optimize_result = modify_optimize_result or (lambda opt_res: opt_res)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.callback.WriteMetrics.run_callback","title":"<code>run_callback(trainer, config, writer)</code>","text":"<p>Writes metrics using the writer.</p> PARAMETER DESCRIPTION <code>trainer</code> <p>The training object.</p> <p> TYPE: <code>Any</code> </p> <code>config</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>writer</code> <p>The writer object for logging.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/callback.py</code> <pre><code>def run_callback(self, trainer: Any, config: TrainConfig, writer: BaseWriter) -&gt; Any:\n    \"\"\"Writes metrics using the writer.\n\n    Args:\n        trainer (Any): The training object.\n        config (TrainConfig): The configuration object.\n        writer (BaseWriter ): The writer object for logging.\n    \"\"\"\n    opt_result = trainer.opt_result\n    writer.write(opt_result.iteration, opt_result.metrics)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer","title":"<code>BaseTrainer(model, optimizer, config, loss_fn='mse', optimize_step=optimize_step, train_dataloader=None, val_dataloader=None, test_dataloader=None, max_batches=None)</code>","text":"<p>Base class for training machine learning models using a given optimizer.</p> <p>The base class implements contextmanager for gradient based/free optimization, properties, property setters, input validations, callback decorator generator, and empty hooks for different training steps.</p> This class provides <ul> <li>Context managers for enabling/disabling gradient-based optimization</li> <li>Properties for managing models, optimizers, and dataloaders</li> <li>Input validations and a callback decorator generator</li> <li>Config and callback managers using the provided <code>TrainConfig</code></li> </ul> ATTRIBUTE DESCRIPTION <code>use_grad</code> <p>Indicates if gradients are used for optimization. Default is True.</p> <p> TYPE: <code>bool</code> </p> <code>model</code> <p>The neural network model.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The configuration settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> </p> <code>optimize_step</code> <p>Function for performing an optimization step.</p> <p> TYPE: <code>Callable</code> </p> <code>loss_fn</code> <p>loss function to use. Default loss function used is 'mse'</p> <p> TYPE: <code>Callable | str ]</code> </p> <code>num_training_batches</code> <p>Number of training batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_validation_batches</code> <p>Number of validation batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>num_test_batches</code> <p>Number of test batches. In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int</code> </p> <code>state</code> <p>Current state in the training process</p> <p> TYPE: <code>str</code> </p> <p>Initializes the BaseTrainer.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to train.</p> <p> TYPE: <code>Module</code> </p> <code>optimizer</code> <p>The optimizer for training.</p> <p> TYPE: <code>Optimizer | Optimizer | None</code> </p> <code>config</code> <p>The TrainConfig settings for training.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>loss_fn</code> <p>The loss function to use. str input to be specified to use a default loss function. currently supported loss functions: 'mse', 'cross_entropy'. If not specified, default mse loss will be used.</p> <p> TYPE: <code>str | Callable</code> DEFAULT: <code>'mse'</code> </p> <code>train_dataloader</code> <p>DataLoader for training data. If the model does not need data to evaluate loss, no dataset should be provided.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>Dataloader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>max_batches</code> <p>Maximum number of batches to process per epoch. This is only valid in case of finite TensorDataset dataloaders. if max_batches is not None, the maximum number of batches used will be min(max_batches, len(dataloader.dataset)) In case of InfiniteTensorDataset only 1 batch per epoch is used.</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | NGOptimizer | None,\n    config: TrainConfig,\n    loss_fn: str | Callable = \"mse\",\n    optimize_step: Callable = optimize_step,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n    max_batches: int | None = None,\n):\n    \"\"\"\n    Initializes the BaseTrainer.\n\n    Args:\n        model (nn.Module): The model to train.\n        optimizer (optim.Optimizer | NGOptimizer | None): The optimizer\n            for training.\n        config (TrainConfig): The TrainConfig settings for training.\n        loss_fn (str | Callable): The loss function to use.\n            str input to be specified to use a default loss function.\n            currently supported loss functions: 'mse', 'cross_entropy'.\n            If not specified, default mse loss will be used.\n        train_dataloader (Dataloader | DictDataLoader | None): DataLoader for training data.\n            If the model does not need data to evaluate loss, no dataset\n            should be provided.\n        val_dataloader (Dataloader | DictDataLoader | None): DataLoader for validation data.\n        test_dataloader (Dataloader | DictDataLoader | None): DataLoader for testing data.\n        max_batches (int | None): Maximum number of batches to process per epoch.\n            This is only valid in case of finite TensorDataset dataloaders.\n            if max_batches is not None, the maximum number of batches used will\n            be min(max_batches, len(dataloader.dataset))\n            In case of InfiniteTensorDataset only 1 batch per epoch is used.\n    \"\"\"\n    self._model: nn.Module\n    self._optimizer: optim.Optimizer | NGOptimizer | None\n    self._config: TrainConfig\n    self._train_dataloader: DataLoader | DictDataLoader | None = None\n    self._val_dataloader: DataLoader | DictDataLoader | None = None\n    self._test_dataloader: DataLoader | DictDataLoader | None = None\n\n    self.config = config\n    self.model = model\n    self.optimizer = optimizer\n    self.max_batches = max_batches\n\n    self.num_training_batches: int\n    self.num_validation_batches: int\n    self.num_test_batches: int\n\n    self.train_dataloader = train_dataloader\n    self.val_dataloader = val_dataloader\n    self.test_dataloader = test_dataloader\n\n    self.loss_fn: Callable = get_loss_fn(loss_fn)\n    self.optimize_step: Callable = optimize_step\n    self.ng_params: ng.p.Array\n    self.training_stage: TrainingStage = TrainingStage(\"idle\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.config","title":"<code>config: TrainConfig</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training configuration.</p> RETURNS DESCRIPTION <code>TrainConfig</code> <p>The configuration object.</p> <p> TYPE: <code>TrainConfig</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.model","title":"<code>model: nn.Module</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the model if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Module</code> <p>nn.Module: The model.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.optimizer","title":"<code>optimizer: optim.Optimizer | NGOptimizer | None</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimizer if set, otherwise raises an error.</p> RETURNS DESCRIPTION <code>Optimizer | Optimizer | None</code> <p>optim.Optimizer | NGOptimizer | None: The optimizer.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.test_dataloader","title":"<code>test_dataloader: DataLoader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the test DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for testing data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.train_dataloader","title":"<code>train_dataloader: DataLoader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the training DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for training data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.use_grad","title":"<code>use_grad: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the optimization framework for the trainer.</p> <p>use_grad = True : Gradient based optimization use_grad = False : Gradient free optimization</p> RETURNS DESCRIPTION <code>bool</code> <p>Bool value for using gradient.</p> <p> TYPE: <code>bool</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.val_dataloader","title":"<code>val_dataloader: DataLoader</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the validation DataLoader, validating its type.</p> RETURNS DESCRIPTION <code>DataLoader</code> <p>The DataLoader for validation data.</p> <p> TYPE: <code>DataLoader</code> </p>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.callback","title":"<code>callback(phase)</code>  <code>staticmethod</code>","text":"<p>Decorator for executing callbacks before and after a phase.</p> <p>Phase are different hooks during the training. list of valid phases is defined in Callbacks. We also update the current state of the training process in the callback decorator.</p> PARAMETER DESCRIPTION <code>phase</code> <p>The phase for which the callback is executed (e.g., \"train\", \"train_epoch\", \"train_batch\").</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>The decorated function.</p> <p> TYPE: <code>Callable</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@staticmethod\ndef callback(phase: str) -&gt; Callable:\n    \"\"\"\n    Decorator for executing callbacks before and after a phase.\n\n    Phase are different hooks during the training. list of valid\n    phases is defined in Callbacks.\n    We also update the current state of the training process in\n    the callback decorator.\n\n    Args:\n        phase (str): The phase for which the callback is executed (e.g., \"train\",\n            \"train_epoch\", \"train_batch\").\n\n    Returns:\n        Callable: The decorated function.\n    \"\"\"\n\n    def decorator(method: Callable) -&gt; Callable:\n        def wrapper(self: Any, *args: Any, **kwargs: Any) -&gt; Any:\n            start_event = f\"{phase}_start\"\n            end_event = f\"{phase}_end\"\n\n            self.training_stage = TrainingStage(start_event)\n            self.callback_manager.run_callbacks(trainer=self)\n            result = method(self, *args, **kwargs)\n\n            self.training_stage = TrainingStage(end_event)\n            # build_optimize_result method is defined in the trainer.\n            self.build_optimize_result(result)\n            self.callback_manager.run_callbacks(trainer=self)\n\n            return result\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.disable_grad_opt","title":"<code>disable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily disable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The Nevergrad optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef disable_grad_opt(self, optimizer: NGOptimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily disable gradient-based optimization.\n\n    Args:\n        optimizer (NGOptimizer): The Nevergrad optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = False\n        self.callback_manager.use_grad = False\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.enable_grad_opt","title":"<code>enable_grad_opt(optimizer=None)</code>","text":"<p>Context manager to temporarily enable gradient-based optimization.</p> PARAMETER DESCRIPTION <code>optimizer</code> <p>The PyTorch optimizer to use. If no optimizer is provided, default optimizer for trainer object will be used.</p> <p> TYPE: <code>Optimizer</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@contextmanager\ndef enable_grad_opt(self, optimizer: optim.Optimizer | None = None) -&gt; Iterator[None]:\n    \"\"\"\n    Context manager to temporarily enable gradient-based optimization.\n\n    Args:\n        optimizer (optim.Optimizer): The PyTorch optimizer to use.\n            If no optimizer is provided, default optimizer for trainer\n            object will be used.\n    \"\"\"\n    original_mode = self.use_grad\n    original_optimizer = self._optimizer\n    try:\n        self.use_grad = True\n        self.callback_manager.use_grad = True\n        self.optimizer = optimizer if optimizer else self.optimizer\n        yield\n    finally:\n        self.use_grad = original_mode\n        self.callback_manager.use_grad = original_mode\n        self.optimizer = original_optimizer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_test_batch_end","title":"<code>on_test_batch_end(test_batch_loss_metrics)</code>","text":"<p>Called at the end of each testing batch.</p> PARAMETER DESCRIPTION <code>test_batch_loss_metrics</code> <p>Metrics for the testing batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_end(self, test_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each testing batch.\n\n    Args:\n        test_batch_loss_metrics: Metrics for the testing batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_test_batch_start","title":"<code>on_test_batch_start(batch)</code>","text":"<p>Called at the start of each testing batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_test_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each testing batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_batch_end","title":"<code>on_train_batch_end(train_batch_loss_metrics)</code>","text":"<p>Called at the end of each training batch.</p> PARAMETER DESCRIPTION <code>train_batch_loss_metrics</code> <p>Metrics for the training batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_end(self, train_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each training batch.\n\n    Args:\n        train_batch_loss_metrics: Metrics for the training batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_batch_start","title":"<code>on_train_batch_start(batch)</code>","text":"<p>Called at the start of each training batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each training batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_end","title":"<code>on_train_end(train_losses, val_losses=None)</code>","text":"<p>Called at the end of training.</p> PARAMETER DESCRIPTION <code>train_losses</code> <p>Metrics for the training losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]]</code> </p> <code>val_losses</code> <p>Metrics for the validation losses. list    -&gt; list                  -&gt; tuples Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)</p> <p> TYPE: <code>list[list[tuple[Tensor, Any]]] | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_end(\n    self,\n    train_losses: list[list[tuple[torch.Tensor, Any]]],\n    val_losses: list[list[tuple[torch.Tensor, Any]]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Called at the end of training.\n\n    Args:\n        train_losses (list[list[tuple[torch.Tensor, Any]]]):\n            Metrics for the training losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Training Batches      -&gt; (loss, metrics)\n        val_losses (list[list[tuple[torch.Tensor, Any]]] | None):\n            Metrics for the validation losses.\n            list    -&gt; list                  -&gt; tuples\n            Epochs  -&gt; Validation Batches    -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_epoch_end","title":"<code>on_train_epoch_end(train_epoch_loss_metrics)</code>","text":"<p>Called at the end of each training epoch.</p> PARAMETER DESCRIPTION <code>train_epoch_loss_metrics</code> <p>Metrics for the training epoch losses. list                  -&gt; tuples Training Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_end(self, train_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each training epoch.\n\n    Args:\n        train_epoch_loss_metrics: Metrics for the training epoch losses.\n            list                  -&gt; tuples\n            Training Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_epoch_start","title":"<code>on_train_epoch_start()</code>","text":"<p>Called at the start of each training epoch.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each training epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_train_start","title":"<code>on_train_start()</code>","text":"<p>Called at the start of training.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_train_start(self) -&gt; None:\n    \"\"\"Called at the start of training.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_batch_end","title":"<code>on_val_batch_end(val_batch_loss_metrics)</code>","text":"<p>Called at the end of each validation batch.</p> PARAMETER DESCRIPTION <code>val_batch_loss_metrics</code> <p>Metrics for the validation batch loss. tuple of (loss, metrics)</p> <p> TYPE: <code>tuple[Tensor, Any]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_end(self, val_batch_loss_metrics: tuple[torch.Tensor, Any]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation batch.\n\n    Args:\n        val_batch_loss_metrics: Metrics for the validation batch loss.\n            tuple of (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_batch_start","title":"<code>on_val_batch_start(batch)</code>","text":"<p>Called at the start of each validation batch.</p> PARAMETER DESCRIPTION <code>batch</code> <p>A batch of data from the DataLoader. Typically a tuple containing input tensors and corresponding target tensors.</p> <p> TYPE: <code>tuple[Tensor, ...] | None</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_batch_start(self, batch: tuple[torch.Tensor, ...] | None) -&gt; None:\n    \"\"\"\n    Called at the start of each validation batch.\n\n    Args:\n        batch: A batch of data from the DataLoader. Typically a tuple containing\n            input tensors and corresponding target tensors.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_epoch_end","title":"<code>on_val_epoch_end(val_epoch_loss_metrics)</code>","text":"<p>Called at the end of each validation epoch.</p> PARAMETER DESCRIPTION <code>val_epoch_loss_metrics</code> <p>Metrics for the validation epoch loss. list                    -&gt; tuples Validation Batches      -&gt; (loss, metrics)</p> <p> TYPE: <code>list[tuple[Tensor, Any]]</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_end(self, val_epoch_loss_metrics: list[tuple[torch.Tensor, Any]]) -&gt; None:\n    \"\"\"\n    Called at the end of each validation epoch.\n\n    Args:\n        val_epoch_loss_metrics: Metrics for the validation epoch loss.\n            list                    -&gt; tuples\n            Validation Batches      -&gt; (loss, metrics)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.on_val_epoch_start","title":"<code>on_val_epoch_start()</code>","text":"<p>Called at the start of each validation epoch.</p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>def on_val_epoch_start(self) -&gt; None:\n    \"\"\"Called at the start of each validation epoch.\"\"\"\n    pass\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.train_utils.base_trainer.BaseTrainer.set_use_grad","title":"<code>set_use_grad(value)</code>  <code>classmethod</code>","text":"<p>Sets the global use_grad flag.</p> PARAMETER DESCRIPTION <code>value</code> <p>Whether to use gradient-based optimization.</p> <p> TYPE: <code>bool</code> </p> Source code in <code>qadence/ml_tools/train_utils/base_trainer.py</code> <pre><code>@classmethod\ndef set_use_grad(cls, value: bool) -&gt; None:\n    \"\"\"\n    Sets the global use_grad flag.\n\n    Args:\n        value (bool): Whether to use gradient-based optimization.\n    \"\"\"\n    if not isinstance(value, bool):\n        raise TypeError(\"use_grad must be a boolean value.\")\n    cls._use_grad = value\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for experiment tracking writers.</p> METHOD DESCRIPTION <code>open</code> <p>Opens the writer and sets up the logging environment.</p> <code>close</code> <p>Closes the writer and finalizes any ongoing logging processes.</p> <code>print_metrics</code> <p>Prints metrics and loss in a formatted manner.</p> <code>write</code> <p>Writes the optimization results to the tracking tool.</p> <code>log_hyperparams</code> <p>Logs the hyperparameters to the tracking tool.</p> <code>plot</code> <p>Logs model plots using provided plotting functions.</p> <code>log_model</code> <p>Logs the model and any relevant information.</p>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Closes the writer and finalizes logging.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Closes the writer and finalizes logging.\"\"\"\n    raise NotImplementedError(\"Writers must implement a close method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>  <code>abstractmethod</code>","text":"<p>Logs hyperparameters.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_hyperparams method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>  <code>abstractmethod</code>","text":"<p>Logs the model and associated data.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and associated data.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a log_model method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.open","title":"<code>open(config, iteration=None)</code>  <code>abstractmethod</code>","text":"<p>Opens the writer and prepares it for logging.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef open(self, config: TrainConfig, iteration: int | None = None) -&gt; Any:\n    \"\"\"\n    Opens the writer and prepares it for logging.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement an open method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>  <code>abstractmethod</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used to\n            generate plots.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a plot method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.print_metrics","title":"<code>print_metrics(result)</code>","text":"<p>Prints the metrics and loss in a readable format.</p> PARAMETER DESCRIPTION <code>result</code> <p>The optimization results to display.</p> <p> TYPE: <code>OptimizeResult</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def print_metrics(self, result: OptimizeResult) -&gt; None:\n    \"\"\"Prints the metrics and loss in a readable format.\n\n    Args:\n        result (OptimizeResult): The optimization results to display.\n    \"\"\"\n\n    # Find the key in result.metrics that contains \"loss\" (case-insensitive)\n    loss_key = next((k for k in result.metrics if \"loss\" in k.lower()), None)\n    if loss_key:\n        loss_value = result.metrics[loss_key]\n        msg = f\"Iteration {result.iteration: &gt;7} | {loss_key.title()}: {loss_value:.7f} -\"\n    else:\n        msg = f\"Iteration {result.iteration: &gt;7} | Loss: None -\"\n    msg += \" \".join([f\"{k}: {v:.7f}\" for k, v in result.metrics.items() if k != loss_key])\n    print(msg)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.BaseWriter.write","title":"<code>write(iteration, metrics)</code>  <code>abstractmethod</code>","text":"<p>Logs the results of the current iteration.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>@abstractmethod\ndef write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    raise NotImplementedError(\"Writers must implement a write method.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter","title":"<code>MLFlowWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to MLflow.</p> ATTRIBUTE DESCRIPTION <code>run</code> <p>The active MLflow run.</p> <p> TYPE: <code>Run</code> </p> <code>mlflow</code> <p>The MLflow module.</p> <p> TYPE: <code>ModuleType</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    try:\n        from mlflow.entities import Run\n    except ImportError:\n        raise ImportError(\n            \"mlflow is not installed. Please install qadence with the mlflow feature: \"\n            \"`pip install qadence[mlflow]`.\"\n        )\n\n    self.run: Run\n    self.mlflow: ModuleType\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.close","title":"<code>close()</code>","text":"<p>Closes the MLflow run.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the MLflow run.\"\"\"\n    if self.run:\n        self.mlflow.end_run()\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.get_signature_from_dataloader","title":"<code>get_signature_from_dataloader(model, dataloader)</code>","text":"<p>Infers the signature of the model based on the input data from the dataloader.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to use for inference.</p> <p> TYPE: <code>Module</code> </p> <code>dataloader</code> <p>DataLoader for model inputs.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Optional[Any]: The inferred signature, if available.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def get_signature_from_dataloader(\n    self, model: Module, dataloader: DataLoader | DictDataLoader | None\n) -&gt; Any:\n    \"\"\"\n    Infers the signature of the model based on the input data from the dataloader.\n\n    Args:\n        model (Module): The model to use for inference.\n        dataloader (DataLoader | DictDataLoader |  None): DataLoader for model inputs.\n\n    Returns:\n        Optional[Any]: The inferred signature, if available.\n    \"\"\"\n    from mlflow.models import infer_signature\n\n    if dataloader is None:\n        return None\n\n    xs: InputData\n    xs, *_ = next(iter(dataloader))\n    preds = model(xs)\n\n    if isinstance(xs, Tensor):\n        xs = xs.detach().cpu().numpy()\n        preds = preds.detach().cpu().numpy()\n        return infer_signature(xs, preds)\n\n    return None\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to MLflow.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to MLflow.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_params(hyperparams)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model and its signature to MLflow using the provided data loaders.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model and its signature to MLflow using the provided data loaders.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    if not self.mlflow:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n\n    signatures = self.get_signature_from_dataloader(model, train_dataloader)\n    self.mlflow.pytorch.log_model(model, artifact_path=\"model\", signature=signatures)\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the MLflow writer and initializes an MLflow run.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>mlflow</code> <p>The MLflow module instance.</p> <p> TYPE: <code>ModuleType | None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; ModuleType | None:\n    \"\"\"\n    Opens the MLflow writer and initializes an MLflow run.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        mlflow: The MLflow module instance.\n    \"\"\"\n    import mlflow\n\n    self.mlflow = mlflow\n    tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", \"\")\n    experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", str(uuid4()))\n    run_name = os.getenv(\"MLFLOW_RUN_NAME\", str(uuid4()))\n\n    if self.mlflow:\n        self.mlflow.set_tracking_uri(tracking_uri)\n\n        # Create or get the experiment\n        exp_filter_string = f\"name = '{experiment_name}'\"\n        experiments = self.mlflow.search_experiments(filter_string=exp_filter_string)\n        if not experiments:\n            self.mlflow.create_experiment(name=experiment_name)\n\n        self.mlflow.set_experiment(experiment_name)\n        self.run = self.mlflow.start_run(run_name=run_name, nested=False)\n\n    return self.mlflow\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.mlflow:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.mlflow.log_figure(fig, descr)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.MLFlowWriter.write","title":"<code>write(iteration, metrics)</code>","text":"<p>Logs the results of the current iteration to MLflow.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to MLflow.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    if self.mlflow:\n        self.mlflow.log_metrics(metrics, step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing.\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter","title":"<code>TensorBoardWriter()</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for logging to TensorBoard.</p> ATTRIBUTE DESCRIPTION <code>writer</code> <p>The TensorBoard SummaryWriter instance.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.writer = None\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.close","title":"<code>close()</code>","text":"<p>Closes the TensorBoard writer.</p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Closes the TensorBoard writer.\"\"\"\n    if self.writer:\n        self.writer.close()\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.log_hyperparams","title":"<code>log_hyperparams(hyperparams)</code>","text":"<p>Logs hyperparameters to TensorBoard.</p> PARAMETER DESCRIPTION <code>hyperparams</code> <p>A dictionary of hyperparameters to log.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_hyperparams(self, hyperparams: dict) -&gt; None:\n    \"\"\"\n    Logs hyperparameters to TensorBoard.\n\n    Args:\n        hyperparams (dict): A dictionary of hyperparameters to log.\n    \"\"\"\n    if self.writer:\n        self.writer.add_hparams(hyperparams, {})\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.log_model","title":"<code>log_model(model, train_dataloader=None, val_dataloader=None, test_dataloader=None)</code>","text":"<p>Logs the model.</p> <p>Currently not supported by TensorBoard.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to log.</p> <p> TYPE: <code>Module</code> </p> <code>train_dataloader</code> <p>DataLoader for training data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>val_dataloader</code> <p>DataLoader for validation data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> <code>test_dataloader</code> <p>DataLoader for testing data.</p> <p> TYPE: <code>DataLoader | DictDataLoader | None</code> DEFAULT: <code>None</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def log_model(\n    self,\n    model: Module,\n    train_dataloader: DataLoader | DictDataLoader | None = None,\n    val_dataloader: DataLoader | DictDataLoader | None = None,\n    test_dataloader: DataLoader | DictDataLoader | None = None,\n) -&gt; None:\n    \"\"\"\n    Logs the model.\n\n    Currently not supported by TensorBoard.\n\n    Args:\n        model (Module): The model to log.\n        train_dataloader (DataLoader | DictDataLoader |  None): DataLoader for training data.\n        val_dataloader (DataLoader | DictDataLoader |  None): DataLoader for validation data.\n        test_dataloader (DataLoader | DictDataLoader |  None): DataLoader for testing data.\n    \"\"\"\n    logger.warning(\"Model logging is not supported by tensorboard. No model will be logged.\")\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.open","title":"<code>open(config, iteration=None)</code>","text":"<p>Opens the TensorBoard writer.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object containing settings for logging.</p> <p> TYPE: <code>TrainConfig</code> </p> <code>iteration</code> <p>The iteration step to start logging from. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>SummaryWriter</code> <p>The initialized TensorBoard writer.</p> <p> TYPE: <code>SummaryWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def open(self, config: TrainConfig, iteration: int | None = None) -&gt; SummaryWriter:\n    \"\"\"\n    Opens the TensorBoard writer.\n\n    Args:\n        config: Configuration object containing settings for logging.\n        iteration (int, optional): The iteration step to start logging from.\n            Defaults to None.\n\n    Returns:\n        SummaryWriter: The initialized TensorBoard writer.\n    \"\"\"\n    log_dir = str(config.log_folder)\n    purge_step = iteration if isinstance(iteration, int) else None\n    self.writer = SummaryWriter(log_dir=log_dir, purge_step=purge_step)\n    return self.writer\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.plot","title":"<code>plot(model, iteration, plotting_functions)</code>","text":"<p>Logs plots of the model using provided plotting functions.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model to plot.</p> <p> TYPE: <code>Module</code> </p> <code>iteration</code> <p>The current iteration number.</p> <p> TYPE: <code>int</code> </p> <code>plotting_functions</code> <p>Functions used to generate plots.</p> <p> TYPE: <code>tuple[PlottingFunction, ...]</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def plot(\n    self,\n    model: Module,\n    iteration: int,\n    plotting_functions: tuple[PlottingFunction, ...],\n) -&gt; None:\n    \"\"\"\n    Logs plots of the model using provided plotting functions.\n\n    Args:\n        model (Module): The model to plot.\n        iteration (int): The current iteration number.\n        plotting_functions (tuple[PlottingFunction, ...]): Functions used\n            to generate plots.\n    \"\"\"\n    if self.writer:\n        for pf in plotting_functions:\n            descr, fig = pf(model, iteration)\n            self.writer.add_figure(descr, fig, global_step=iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.TensorBoardWriter.write","title":"<code>write(iteration, metrics)</code>","text":"<p>Logs the results of the current iteration to TensorBoard.</p> PARAMETER DESCRIPTION <code>iteration</code> <p>The current training iteration.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>A dictionary of metrics to log, where keys are metric names             and values are the corresponding metric values.</p> <p> TYPE: <code>dict</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def write(self, iteration: int, metrics: dict) -&gt; None:\n    \"\"\"\n    Logs the results of the current iteration to TensorBoard.\n\n    Args:\n        iteration (int): The current training iteration.\n        metrics (dict): A dictionary of metrics to log, where keys are metric names\n                        and values are the corresponding metric values.\n    \"\"\"\n    if self.writer:\n        for key, value in metrics.items():\n            self.writer.add_scalar(key, value, iteration)\n    else:\n        raise RuntimeError(\n            \"The writer is not initialized.\"\n            \"Please call the 'writer.open()' method before writing.\"\n        )\n</code></pre>"},{"location":"api/ml_tools/#qadence.ml_tools.callbacks.writer_registry.get_writer","title":"<code>get_writer(tracking_tool)</code>","text":"<p>Factory method to get the appropriate writer based on the tracking tool.</p> PARAMETER DESCRIPTION <code>tracking_tool</code> <p>The experiment tracking tool to use.</p> <p> TYPE: <code>ExperimentTrackingTool</code> </p> RETURNS DESCRIPTION <code>BaseWriter</code> <p>An instance of the appropriate writer.</p> <p> TYPE: <code>BaseWriter</code> </p> Source code in <code>qadence/ml_tools/callbacks/writer_registry.py</code> <pre><code>def get_writer(tracking_tool: ExperimentTrackingTool) -&gt; BaseWriter:\n    \"\"\"Factory method to get the appropriate writer based on the tracking tool.\n\n    Args:\n        tracking_tool (ExperimentTrackingTool): The experiment tracking tool to use.\n\n    Returns:\n        BaseWriter: An instance of the appropriate writer.\n    \"\"\"\n    writer_class = WRITER_REGISTRY.get(tracking_tool)\n    if writer_class:\n        return writer_class()\n    else:\n        raise ValueError(f\"Unsupported tracking tool: {tracking_tool}\")\n</code></pre>"},{"location":"api/models/","title":"Quantum models","text":"<p>               Bases: <code>QuantumModel</code></p> <p>Quantum neural network model for n-dimensional inputs.</p> <p>Examples: <pre><code>import torch\nfrom qadence import QuantumCircuit, QNN, Z\nfrom qadence import hea, feature_map, hamiltonian_factory, kron\n\n# create the circuit\nn_qubits, depth = 2, 4\nfm = kron(\n    feature_map(1, support=(0,), param=\"x\"),\n    feature_map(1, support=(1,), param=\"y\")\n)\nansatz = hea(n_qubits=n_qubits, depth=depth)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\nobs_base = hamiltonian_factory(n_qubits, detuning=Z)\n\n# the QNN will yield two outputs\nobs = [2.0 * obs_base, 4.0 * obs_base]\n\n# initialize and use the model\nqnn = QNN(circuit, obs, inputs=[\"x\", \"y\"])\ny = qnn(torch.rand(3, 2))\n</code></pre> <pre><code>tensor([[0.8634, 1.7269],\n        [0.8384, 1.6768],\n        [0.9240, 1.8479]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> <p>Initialize the QNN.</p> <p>The number of inputs is determined by the feature parameters in the input quantum circuit while the number of outputs is determined by how many observables are provided as input</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The quantum circuit to use for the QNN.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>The observable.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>optional configuration for the backend</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>inputs</code> <p>List that indicates the order of variables of the tensors that are passed to the model. Given input tensors <code>xs = torch.rand(batch_size, input_size:=2)</code> a QNN with <code>inputs=[\"t\", \"x\"]</code> will assign <code>t, x = xs[:,0], xs[:,1]</code>.</p> <p> TYPE: <code>list[Basic | str] | None</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode | str</code> DEFAULT: <code>AD</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock,\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    inputs: list[sympy.Basic | str] | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n):\n    \"\"\"Initialize the QNN.\n\n    The number of inputs is determined by the feature parameters in the input\n    quantum circuit while the number of outputs is determined by how many\n    observables are provided as input\n\n    Args:\n        circuit: The quantum circuit to use for the QNN.\n        observable: The observable.\n        backend: The chosen quantum backend.\n        diff_mode: The differentiation engine to use. Choices 'gpsr' or 'ad'.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        configuration: optional configuration for the backend\n        inputs: List that indicates the order of variables of the tensors that are passed\n            to the model. Given input tensors `xs = torch.rand(batch_size, input_size:=2)` a QNN\n            with `inputs=[\"t\", \"x\"]` will assign `t, x = xs[:,0], xs[:,1]`.\n        input_diff_mode: The differentiation mode for the input tensor.\n    \"\"\"\n    super().__init__(\n        circuit,\n        observable=observable,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        configuration=configuration,\n        noise=noise,\n    )\n    if self._observable is None:\n        raise ValueError(\"You need to provide at least one observable in the QNN constructor\")\n    if (inputs is not None) and (len(self.inputs) == len(inputs)):\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in inputs]  # type: ignore[union-attr]\n    elif (inputs is None) and len(self.inputs) &lt;= 1:\n        self.inputs = [sympy.symbols(x) if isinstance(x, str) else x for x in self.inputs]  # type: ignore[union-attr]\n    else:\n        raise ValueError(\n            \"\"\"\n            Your QNN has more than one input. Please provide a list of inputs in the order of\n            your tensor domain. For example, if you want to pass\n            `xs = torch.rand(batch_size, input_size:=3)` to you QNN, where\n            ```\n            t = x[:,0]\n            x = x[:,1]\n            y = x[:,2]\n            ```\n            you have to specify\n            ```\n            QNN(circuit, observable, inputs=[\"t\", \"x\", \"y\"])\n            ```\n            You can also pass a list of sympy symbols.\n        \"\"\"\n        )\n    self.format_to_dict = format_to_dict_fn(self.inputs)  # type: ignore[arg-type]\n    self.input_diff_mode = InputDiffMode(input_diff_mode)\n    if self.input_diff_mode == InputDiffMode.FD:\n        from qadence.backends.utils import finitediff\n\n        self.__derivative = finitediff\n    elif self.input_diff_mode == InputDiffMode.AD:\n        self.__derivative = _torch_derivative  # type: ignore[assignment]\n    else:\n        raise ValueError(f\"Unkown forward diff mode: {self.input_diff_mode}\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel","title":"<code>QuantumModel(circuit, observable=None, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, mitigation=None, configuration=None)</code>","text":"<p>               Bases: <code>Module</code></p> <p>The central class of qadence that executes <code>QuantumCircuit</code>s and make them differentiable.</p> <p>This class should be used as base class for any new quantum model supported in the qadence framework for information on the implementation of custom models see here.</p> <p>Example: <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit, RX, RY, Z, PI, chain, kron\nfrom qadence import FeatureParameter, VariationalParameter\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, theta), RY(1, theta)),\n    kron(RX(0, phi), RY(1, phi)),\n)\n\ncircuit = QuantumCircuit(2, block)\n\nobservable = Z(0) + Z(1)\n\nmodel = QuantumModel(circuit, observable)\nvalues = {\"phi\": torch.tensor([PI, PI/2]), \"theta\": torch.tensor([PI, PI/2])}\n\nwf = model.run(values)\nxs = model.sample(values, n_shots=100)\nex = model.expectation(values)\nprint(wf)\nprint(xs)\nprint(ex)\n</code></pre> <pre><code>tensor([[ 1.0000e+00+0.0000e+00j, -1.2246e-16+0.0000e+00j,\n          0.0000e+00+1.2246e-16j,  0.0000e+00-1.4998e-32j],\n        [ 4.9304e-32+0.0000e+00j,  2.2204e-16+0.0000e+00j,\n          0.0000e+00-2.2204e-16j,  0.0000e+00-1.0000e+00j]])\n[OrderedCounter({'00': 100}), OrderedCounter({'11': 100})]\ntensor([[ 2.],\n        [-2.]], requires_grad=True)\n</code></pre>  ```</p> <p>Initialize a generic QuantumModel instance.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>The circuit that is executed.</p> <p> TYPE: <code>QuantumCircuit</code> </p> <code>observable</code> <p>Optional observable(s) that are used only in the <code>expectation</code> method. You can also provide observables on the fly to the expectation call directly.</p> <p> TYPE: <code>list[AbstractBlock] | AbstractBlock | None</code> DEFAULT: <code>None</code> </p> <code>backend</code> <p>A backend for circuit execution.</p> <p> TYPE: <code>BackendName | str</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>A differentiability mode. Parameter shift based modes work on all backends. AD based modes only on PyTorch based backends.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Configuration for the backend.</p> <p> TYPE: <code>BackendConfiguration | dict | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>if the <code>diff_mode</code> argument is set to None</p> Source code in <code>qadence/model.py</code> <pre><code>def __init__(\n    self,\n    circuit: QuantumCircuit,\n    observable: list[AbstractBlock] | AbstractBlock | None = None,\n    backend: BackendName | str = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n):\n    \"\"\"Initialize a generic QuantumModel instance.\n\n    Arguments:\n        circuit: The circuit that is executed.\n        observable: Optional observable(s) that are used only in the `expectation` method. You\n            can also provide observables on the fly to the expectation call directly.\n        backend: A backend for circuit execution.\n        diff_mode: A differentiability mode. Parameter shift based modes work on all backends.\n            AD based modes only on PyTorch based backends.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        configuration: Configuration for the backend.\n        noise: A noise model to use.\n\n    Raises:\n        ValueError: if the `diff_mode` argument is set to None\n    \"\"\"\n    super().__init__()\n\n    if not isinstance(circuit, QuantumCircuit):\n        TypeError(\n            f\"The circuit should be of type '&lt;class QuantumCircuit&gt;'. Got {type(circuit)}.\"\n        )\n\n    if diff_mode is None:\n        raise ValueError(\"`diff_mode` cannot be `None` in a `QuantumModel`.\")\n\n    self.backend = backend_factory(\n        backend=backend, diff_mode=diff_mode, configuration=configuration\n    )\n\n    if isinstance(observable, list) or observable is None:\n        observable = observable\n    else:\n        observable = [observable]\n\n    def _is_feature_param(p: Parameter) -&gt; bool:\n        return not p.trainable and not p.is_number\n\n    if observable is None:\n        self.inputs = list(filter(_is_feature_param, circuit.unique_parameters))\n    else:\n        uparams = unique_parameters(chain(circuit.block, *observable))\n        self.inputs = list(filter(_is_feature_param, uparams))\n\n    conv = self.backend.convert(circuit, observable)\n    self.embedding_fn = conv.embedding_fn\n    self._circuit = conv.circuit\n    self._observable = conv.observable\n    self._backend_name = backend\n    self._diff_mode = diff_mode\n    self._measurement = measurement\n    self._noise = noise\n    self._mitigation = mitigation\n    self._params = nn.ParameterDict(\n        {\n            str(key): nn.Parameter(val, requires_grad=val.requires_grad)\n            for key, val in conv.params.items()\n        }\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.device","title":"<code>device: torch.device</code>  <code>property</code>","text":"<p>Get device.</p> RETURNS DESCRIPTION <code>device</code> <p>torch.device</p>"},{"location":"api/models/#qadence.model.QuantumModel.in_features","title":"<code>in_features: int</code>  <code>property</code>","text":"<p>Number of inputs.</p>"},{"location":"api/models/#qadence.model.QuantumModel.num_vparams","title":"<code>num_vparams: int</code>  <code>property</code>","text":"<p>The number of variational parameters.</p>"},{"location":"api/models/#qadence.model.QuantumModel.out_features","title":"<code>out_features: int | None</code>  <code>property</code>","text":"<p>Number of outputs.</p>"},{"location":"api/models/#qadence.model.QuantumModel.vals_vparams","title":"<code>vals_vparams: Tensor</code>  <code>property</code>","text":"<p>Dictionary with parameters which are actually updated during optimization.</p>"},{"location":"api/models/#qadence.model.QuantumModel.vparams","title":"<code>vparams: OrderedDict</code>  <code>property</code>","text":"<p>Variational parameters.</p>"},{"location":"api/models/#qadence.model.QuantumModel.assign_parameters","title":"<code>assign_parameters(values)</code>","text":"<p>Return the final, assigned circuit that is used in e.g. <code>backend.run</code>.</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Final, assigned circuit that is used in e.g. <code>backend.run</code></p> Source code in <code>qadence/model.py</code> <pre><code>def assign_parameters(self, values: dict[str, Tensor]) -&gt; Any:\n    \"\"\"Return the final, assigned circuit that is used in e.g. `backend.run`.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n\n    Returns:\n        Final, assigned circuit that is used in e.g. `backend.run`\n    \"\"\"\n    params = self.embedding_fn(self._params, values)\n    return self.backend.assign_parameters(self._circuit, params)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.circuit","title":"<code>circuit(circuit)</code>","text":"<p>Get backend-converted circuit.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>QuantumCircuit instance.</p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>Backend circuit.</p> Source code in <code>qadence/model.py</code> <pre><code>def circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Get backend-converted circuit.\n\n    Args:\n        circuit: QuantumCircuit instance.\n\n    Returns:\n        Backend circuit.\n    \"\"\"\n    return self.backend.circuit(circuit)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.expectation","title":"<code>expectation(values={}, observable=None, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute expectation using the given backend.</p> <p>Given an input state \\(|\\psi_0 \\rangle\\), a set of variational parameters \\(\\vec{\\theta}\\) and the unitary representation of the model \\(U(\\vec{\\theta})\\) we return \\(\\langle \\psi_0 | U(\\vec{\\theta}) | \\psi_0 \\rangle\\).</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>observable</code> <p>Observable part of the expectation.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable | None</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Optional input state.</p> <p> TYPE: <code>Optional[Tensor]</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>A mitigation protocol to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>when no observable is set.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor of shape n_batches x n_obs</p> Source code in <code>qadence/model.py</code> <pre><code>def expectation(\n    self,\n    values: dict[str, Tensor] = {},\n    observable: list[ConvertedObservable] | ConvertedObservable | None = None,\n    state: Optional[Tensor] = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    r\"\"\"Compute expectation using the given backend.\n\n\n\n    Given an input state $|\\psi_0 \\rangle$,\n    a set of variational parameters $\\vec{\\theta}$\n    and the unitary representation of the model $U(\\vec{\\theta})$\n    we return $\\langle \\psi_0 | U(\\vec{\\theta}) | \\psi_0 \\rangle$.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        observable: Observable part of the expectation.\n        state: Optional input state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: A mitigation protocol to use.\n        endianness: Storage convention for binary information.\n\n    Raises:\n        ValueError: when no observable is set.\n\n    Returns:\n        A torch.Tensor of shape n_batches x n_obs\n    \"\"\"\n    if observable is None:\n        if self._observable is None:\n            raise ValueError(\n                \"Provide an AbstractBlock as the observable to compute expectation.\"\n                \"Either pass a 'native_observable' directly to 'QuantumModel.expectation'\"\n                \"or pass a (non-native) '&lt;class AbstractBlock&gt;' to the 'QuantumModel.__init__'.\"\n            )\n        observable = self._observable\n\n    params = self.embedding_fn(self._params, values)\n    if measurement is None:\n        measurement = self._measurement\n    if noise is None:\n        noise = self._noise\n    else:\n        self._noise = noise\n    if mitigation is None:\n        mitigation = self._mitigation\n    return self.backend.expectation(\n        circuit=self._circuit,\n        observable=observable,\n        param_values=params,\n        state=state,\n        measurement=measurement,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Calls run method with arguments.</p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor representing output.</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/model.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Tensor:\n    \"\"\"Calls run method with arguments.\n\n    Returns:\n        Tensor: A torch.Tensor representing output.\n    \"\"\"\n    return self.run(*args, **kwargs)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.load","title":"<code>load(file_path, as_torch=False, map_location='cpu')</code>  <code>classmethod</code>","text":"<p>Load QuantumModel.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>File path to load model from.</p> <p> TYPE: <code>str | Path</code> </p> <code>as_torch</code> <p>Load parameters as torch tensor. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>map_location</code> <p>Location for loading. Defaults to \"cpu\".</p> <p> TYPE: <code>str | device</code> DEFAULT: <code>'cpu'</code> </p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel from file_path.</p> Source code in <code>qadence/model.py</code> <pre><code>@classmethod\ndef load(\n    cls, file_path: str | Path, as_torch: bool = False, map_location: str | torch.device = \"cpu\"\n) -&gt; QuantumModel:\n    \"\"\"Load QuantumModel.\n\n    Arguments:\n        file_path: File path to load model from.\n        as_torch: Load parameters as torch tensor. Defaults to False.\n        map_location (str | torch.device, optional): Location for loading. Defaults to \"cpu\".\n\n    Returns:\n        QuantumModel from file_path.\n    \"\"\"\n    qm_pt = {}\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if os.path.isdir(file_path):\n        from qadence.ml_tools.callbacks.saveload import get_latest_checkpoint_name\n\n        file_path = file_path / get_latest_checkpoint_name(file_path, \"model\")\n\n    try:\n        qm_pt = torch.load(file_path, map_location=map_location)\n    except Exception as e:\n        logger.error(f\"Unable to load QuantumModel due to {e}\")\n    return cls._from_dict(qm_pt, as_torch)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.load_params_from_dict","title":"<code>load_params_from_dict(d, strict=True)</code>","text":"<p>Copy parameters from dictionary into this QuantumModel.</p> <p>Unlike :meth:<code>~qadence.QuantumModel.from_dict</code>, this method does not create a new QuantumModel instance, but rather loads the parameters into the same QuantumModel. The behaviour of this method is similar to :meth:<code>~torch.nn.Module.load_state_dict</code>.</p> <p>The dictionary is assumed to have the format as saved via :meth:<code>~qadence.QuantumModel.to_dict</code></p> PARAMETER DESCRIPTION <code>d</code> <p>The dictionary</p> <p> TYPE: <code>dict</code> </p> <code>strict</code> <p>Whether to strictly enforce that the parameter keys in the dictionary and in the model match exactly. Default: <code>True</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>qadence/model.py</code> <pre><code>def load_params_from_dict(self, d: dict, strict: bool = True) -&gt; None:\n    \"\"\"Copy parameters from dictionary into this QuantumModel.\n\n    Unlike :meth:`~qadence.QuantumModel.from_dict`, this method does not create a new\n    QuantumModel instance, but rather loads the parameters into the same QuantumModel.\n    The behaviour of this method is similar to :meth:`~torch.nn.Module.load_state_dict`.\n\n    The dictionary is assumed to have the format as saved via\n    :meth:`~qadence.QuantumModel.to_dict`\n\n    Args:\n        d (dict): The dictionary\n        strict (bool, optional):\n            Whether to strictly enforce that the parameter keys in the dictionary and\n            in the model match exactly. Default: ``True``.\n    \"\"\"\n    param_dict = d[\"param_dict\"]\n    missing_keys = set(self._params.keys()) - set(param_dict.keys())\n    unexpected_keys = set(param_dict.keys()) - set(self._params.keys())\n\n    if strict:\n        error_msgs = []\n        if len(unexpected_keys) &gt; 0:\n            error_msgs.append(f\"Unexpected key(s) in dictionary: {unexpected_keys}\")\n        if len(missing_keys) &gt; 0:\n            error_msgs.append(f\"Missing key(s) in dictionary: {missing_keys}\")\n        if len(error_msgs) &gt; 0:\n            errors_string = \"\\n\\t\".join(error_msgs)\n            raise RuntimeError(\n                f\"Error(s) loading the parameter dictionary due to: \\n\\t{errors_string}\\n\"\n                \"This error was thrown because the `strict` argument is set `True`.\"\n                \"If you don't need the parameter keys of the dictionary to exactly match \"\n                \"the model parameters, set `strict=False`.\"\n            )\n\n    for n, param in param_dict.items():\n        try:\n            with torch.no_grad():\n                self._params[n].copy_(\n                    torch.nn.Parameter(param, requires_grad=param.requires_grad)\n                )\n        except Exception as e:\n            logger.warning(f\"Unable to load parameter {n} from dictionary due to {e}.\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.observable","title":"<code>observable(observable, n_qubits)</code>","text":"<p>Get backend observable.</p> PARAMETER DESCRIPTION <code>observable</code> <p>Observable block.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Any</code> <p>Backend observable.</p> Source code in <code>qadence/model.py</code> <pre><code>def observable(self, observable: AbstractBlock, n_qubits: int) -&gt; Any:\n    \"\"\"Get backend observable.\n\n    Args:\n        observable: Observable block.\n        n_qubits: Number of qubits\n\n    Returns:\n        Backend observable.\n    \"\"\"\n    return self.backend.observable(observable, n_qubits)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.overlap","title":"<code>overlap()</code>","text":"<p>Overlap of model.</p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>The overlap method is not implemented for this model.</p> Source code in <code>qadence/model.py</code> <pre><code>def overlap(self) -&gt; Tensor:\n    \"\"\"Overlap of model.\n\n    Raises:\n        NotImplementedError: The overlap method is not implemented for this model.\n    \"\"\"\n    raise NotImplementedError(\"The overlap method is not implemented for this model.\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.reset_vparams","title":"<code>reset_vparams(values)</code>","text":"<p>Reset all the variational parameters with a given list of values.</p> Source code in <code>qadence/model.py</code> <pre><code>def reset_vparams(self, values: Sequence) -&gt; None:\n    \"\"\"Reset all the variational parameters with a given list of values.\"\"\"\n    current_vparams = OrderedDict({k: v for k, v in self._params.items() if v.requires_grad})\n\n    assert (\n        len(values) == self.num_vparams\n    ), \"Pass an iterable with the values of all variational parameters\"\n    for i, k in enumerate(current_vparams.keys()):\n        current_vparams[k].data = torch.tensor([values[i]])\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.run","title":"<code>run(values=None, state=None, endianness=Endianness.BIG)</code>","text":"<p>Run model.</p> <p>Given an input state \\(| \\psi_0 \\rangle\\), a set of variational parameters \\(\\vec{\\theta}\\) and the unitary representation of the model \\(U(\\vec{\\theta})\\) we return \\(U(\\vec{\\theta}) | \\psi_0 \\rangle\\).</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Optional input state to apply model on.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor representing output.</p> Source code in <code>qadence/model.py</code> <pre><code>def run(\n    self,\n    values: dict[str, Tensor] = None,\n    state: Tensor | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    r\"\"\"Run model.\n\n    Given an input state $| \\psi_0 \\rangle$,\n    a set of variational parameters $\\vec{\\theta}$\n    and the unitary representation of the model $U(\\vec{\\theta})$\n    we return $U(\\vec{\\theta}) | \\psi_0 \\rangle$.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        state: Optional input state to apply model on.\n        endianness: Storage convention for binary information.\n\n    Returns:\n        A torch.Tensor representing output.\n    \"\"\"\n    if values is None:\n        values = {}\n\n    params = self.embedding_fn(self._params, values)\n\n    return self.backend.run(self._circuit, params, state=state, endianness=endianness)\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.sample","title":"<code>sample(values={}, n_shots=1000, state=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Obtain samples from model.</p> PARAMETER DESCRIPTION <code>values</code> <p>Values dict which contains values for the parameters.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>n_shots</code> <p>Observable part of the expectation.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>state</code> <p>Optional input state to apply model on.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>A mitigation protocol to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Storage convention for binary information.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>list[Counter]</code> <p>A list of Counter instances with the sample results.</p> Source code in <code>qadence/model.py</code> <pre><code>def sample(\n    self,\n    values: dict[str, torch.Tensor] = {},\n    n_shots: int = 1000,\n    state: torch.Tensor | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; list[Counter]:\n    \"\"\"Obtain samples from model.\n\n    Arguments:\n        values: Values dict which contains values for the parameters.\n        n_shots: Observable part of the expectation.\n        state: Optional input state to apply model on.\n        noise: A noise model to use.\n        mitigation: A mitigation protocol to use.\n        endianness: Storage convention for binary information.\n\n    Returns:\n        A list of Counter instances with the sample results.\n    \"\"\"\n    params = self.embedding_fn(self._params, values)\n    if noise is None:\n        noise = self._noise\n    if mitigation is None:\n        mitigation = self._mitigation\n    return self.backend.sample(\n        self._circuit,\n        params,\n        n_shots=n_shots,\n        state=state,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.save","title":"<code>save(folder, file_name='quantum_model.pt', save_params=True)</code>","text":"<p>Save model.</p> PARAMETER DESCRIPTION <code>folder</code> <p>Folder where model is saved.</p> <p> TYPE: <code>str | Path</code> </p> <code>file_name</code> <p>File name for saving model. Defaults to \"quantum_model.pt\".</p> <p> TYPE: <code>str</code> DEFAULT: <code>'quantum_model.pt'</code> </p> <code>save_params</code> <p>Save parameters if True. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If folder is not a directory.</p> Source code in <code>qadence/model.py</code> <pre><code>def save(\n    self, folder: str | Path, file_name: str = \"quantum_model.pt\", save_params: bool = True\n) -&gt; None:\n    \"\"\"Save model.\n\n    Arguments:\n        folder: Folder where model is saved.\n        file_name: File name for saving model. Defaults to \"quantum_model.pt\".\n        save_params: Save parameters if True. Defaults to True.\n\n    Raises:\n        FileNotFoundError: If folder is not a directory.\n    \"\"\"\n    if not os.path.isdir(folder):\n        raise FileNotFoundError\n    try:\n        torch.save(self._to_dict(save_params), folder / Path(file_name))\n    except Exception as e:\n        logger.error(f\"Unable to write QuantumModel to disk due to {e}\")\n</code></pre>"},{"location":"api/models/#qadence.model.QuantumModel.to","title":"<code>to(*args, **kwargs)</code>","text":"<p>Conversion method for device or types.</p> RETURNS DESCRIPTION <code>QuantumModel</code> <p>QuantumModel with conversions.</p> Source code in <code>qadence/model.py</code> <pre><code>def to(self, *args: Any, **kwargs: Any) -&gt; QuantumModel:\n    \"\"\"Conversion method for device or types.\n\n    Returns:\n        QuantumModel with conversions.\n    \"\"\"\n    from pyqtorch import QuantumCircuit as PyQCircuit\n\n    try:\n        if isinstance(self._circuit.native, PyQCircuit):\n            self._circuit.native = self._circuit.native.to(*args, **kwargs)\n            if self._observable is not None:\n                if isinstance(self._observable, ConvertedObservable):\n                    self._observable.native = self._observable.native.to(*args, **kwargs)\n                elif isinstance(self._observable, list):\n                    for obs in self._observable:\n                        obs.native = obs.native.to(*args, **kwargs)\n            self._params = self._params.to(\n                device=self._circuit.native.device,\n                dtype=(\n                    torch.float64\n                    if self._circuit.native.dtype == torch.cdouble\n                    else torch.float32\n                ),\n            )\n            logger.debug(f\"Moved {self} to {args}, {kwargs}.\")\n        else:\n            logger.debug(\"QuantumModel.to only supports pyqtorch.QuantumCircuits.\")\n    except Exception as e:\n        logger.warning(f\"Unable to move {self} to {args}, {kwargs} due to {e}.\")\n    return self\n</code></pre>"},{"location":"api/models/#qadence.ml_tools.models.QNN.forward","title":"<code>forward(values=None, state=None, measurement=None, noise=None, endianness=Endianness.BIG)</code>","text":"<p>Forward pass of the model.</p> <p>This returns the (differentiable) expectation value of the given observable operator defined in the constructor. Differently from the base QuantumModel class, the QNN accepts also a tensor as input for the forward pass. The tensor is expected to have shape: <code>n_batches x in_features</code> where <code>n_batches</code> is the number of data points and <code>in_features</code> is the dimensionality of the problem</p> <p>The output of the forward pass is the expectation value of the input observable(s). If a single observable is given, the output shape is <code>n_batches</code> while if multiple observables are given the output shape is instead <code>n_batches x n_observables</code></p> PARAMETER DESCRIPTION <code>values</code> <p>the values of the feature parameters</p> <p> TYPE: <code>dict[str, Tensor] | Tensor</code> DEFAULT: <code>None</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>optional measurement protocol. If None, use exact expectation value with a statevector simulator</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>a tensor with the expectation value of the observables passed in the constructor of the model</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>def forward(\n    self,\n    values: dict[str, Tensor] | Tensor = None,\n    state: Tensor | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; Tensor:\n    \"\"\"Forward pass of the model.\n\n    This returns the (differentiable) expectation value of the given observable\n    operator defined in the constructor. Differently from the base QuantumModel\n    class, the QNN accepts also a tensor as input for the forward pass. The\n    tensor is expected to have shape: `n_batches x in_features` where `n_batches`\n    is the number of data points and `in_features` is the dimensionality of the problem\n\n    The output of the forward pass is the expectation value of the input\n    observable(s). If a single observable is given, the output shape is\n    `n_batches` while if multiple observables are given the output shape\n    is instead `n_batches x n_observables`\n\n    Args:\n        values: the values of the feature parameters\n        state: Initial state.\n        measurement: optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n\n    Returns:\n        Tensor: a tensor with the expectation value of the observables passed\n            in the constructor of the model\n    \"\"\"\n    return self.expectation(\n        values, state=state, measurement=measurement, noise=noise, endianness=endianness\n    )\n</code></pre>"},{"location":"api/models/#qadence.ml_tools.models.QNN.from_configs","title":"<code>from_configs(register, obs_config, fm_config=FeatureMapConfig(), ansatz_config=AnsatzConfig(), backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD, measurement=None, noise=None, configuration=None, input_diff_mode=InputDiffMode.AD)</code>  <code>classmethod</code>","text":"<p>Create a QNN from a set of configurations.</p> PARAMETER DESCRIPTION <code>register</code> <p>The number of qubits or a register object.</p> <p> TYPE: <code>int | Register</code> </p> <code>obs_config</code> <p>The configuration(s) for the observable(s).</p> <p> TYPE: <code>list[ObservableConfig] | ObservableConfig</code> </p> <code>fm_config</code> <p>The configuration for the feature map. Defaults to no feature encoding block.</p> <p> TYPE: <code>FeatureMapConfig</code> DEFAULT: <code>FeatureMapConfig()</code> </p> <code>ansatz_config</code> <p>The configuration for the ansatz. Defaults to a single layer of hardware efficient ansatz.</p> <p> TYPE: <code>AnsatzConfig</code> DEFAULT: <code>AnsatzConfig()</code> </p> <code>backend</code> <p>The chosen quantum backend.</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> <code>diff_mode</code> <p>The differentiation engine to use. Choices are 'gpsr' or 'ad'.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>Noise</code> DEFAULT: <code>None</code> </p> <code>configuration</code> <p>Optional backend configuration.</p> <p> TYPE: <code>BackendConfiguration | dict</code> DEFAULT: <code>None</code> </p> <code>input_diff_mode</code> <p>The differentiation mode for the input tensor.</p> <p> TYPE: <code>InputDiffMode</code> DEFAULT: <code>AD</code> </p> RETURNS DESCRIPTION <code>QNN</code> <p>A QNN object.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the observable configuration is not provided.</p> <p>Example: <pre><code>import torch\nfrom qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\nfrom qadence.ml_tools import QNN\nfrom qadence.constructors import ObservableConfig\nfrom qadence.operations import Z\nfrom qadence.types import (\n    AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n)\n\nregister = 4\nobs_config = ObservableConfig(\n    detuning=Z,\n    scale=5.0,\n    shift=0.0,\n    transformation_type=ObservableTransform.SCALE,\n    trainable_transform=None,\n)\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs=[\"x\", \"y\"],\n    basis_set=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.CONSTANT,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nqnn = QNN.from_configs(\n    register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n)\n\nx = torch.rand(2, 2)\ny = qnn(x)\n</code></pre> <pre><code>tensor([[ 1.6359],\n        [-1.8182]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> </p> Source code in <code>qadence/ml_tools/models.py</code> <pre><code>@classmethod\ndef from_configs(\n    cls,\n    register: int | Register,\n    obs_config: Any,\n    fm_config: Any = FeatureMapConfig(),\n    ansatz_config: Any = AnsatzConfig(),\n    backend: BackendName = BackendName.PYQTORCH,\n    diff_mode: DiffMode = DiffMode.AD,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    configuration: BackendConfiguration | dict | None = None,\n    input_diff_mode: InputDiffMode | str = InputDiffMode.AD,\n) -&gt; QNN:\n    \"\"\"Create a QNN from a set of configurations.\n\n    Args:\n        register (int | Register): The number of qubits or a register object.\n        obs_config (list[ObservableConfig] | ObservableConfig): The configuration(s)\n            for the observable(s).\n        fm_config (FeatureMapConfig): The configuration for the feature map.\n            Defaults to no feature encoding block.\n        ansatz_config (AnsatzConfig): The configuration for the ansatz.\n            Defaults to a single layer of hardware efficient ansatz.\n        backend (BackendName): The chosen quantum backend.\n        diff_mode (DiffMode): The differentiation engine to use. Choices are\n            'gpsr' or 'ad'.\n        measurement (Measurements): Optional measurement protocol. If None,\n            use exact expectation value with a statevector simulator.\n        noise (Noise): A noise model to use.\n        configuration (BackendConfiguration | dict): Optional backend configuration.\n        input_diff_mode (InputDiffMode): The differentiation mode for the input tensor.\n\n    Returns:\n        A QNN object.\n\n    Raises:\n        ValueError: If the observable configuration is not provided.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence.ml_tools.config import AnsatzConfig, FeatureMapConfig\n    from qadence.ml_tools import QNN\n    from qadence.constructors import ObservableConfig\n    from qadence.operations import Z\n    from qadence.types import (\n        AnsatzType, BackendName, BasisSet, ObservableTransform, ReuploadScaling, Strategy\n    )\n\n    register = 4\n    obs_config = ObservableConfig(\n        detuning=Z,\n        scale=5.0,\n        shift=0.0,\n        transformation_type=ObservableTransform.SCALE,\n        trainable_transform=None,\n    )\n    fm_config = FeatureMapConfig(\n        num_features=2,\n        inputs=[\"x\", \"y\"],\n        basis_set=BasisSet.FOURIER,\n        reupload_scaling=ReuploadScaling.CONSTANT,\n        feature_range={\n            \"x\": (-1.0, 1.0),\n            \"y\": (0.0, 1.0),\n        },\n    )\n    ansatz_config = AnsatzConfig(\n        depth=2,\n        ansatz_type=AnsatzType.HEA,\n        ansatz_strategy=Strategy.DIGITAL,\n    )\n\n    qnn = QNN.from_configs(\n        register, obs_config, fm_config, ansatz_config, backend=BackendName.PYQTORCH\n    )\n\n    x = torch.rand(2, 2)\n    y = qnn(x)\n    print(str(y)) # markdown-exec: hide\n    ```\n    \"\"\"\n    from .constructors import build_qnn_from_configs\n\n    return build_qnn_from_configs(\n        register=register,\n        observable_config=obs_config,\n        fm_config=fm_config,\n        ansatz_config=ansatz_config,\n        backend=backend,\n        diff_mode=diff_mode,\n        measurement=measurement,\n        noise=noise,\n        configuration=configuration,\n        input_diff_mode=input_diff_mode,\n    )\n</code></pre>"},{"location":"api/noise/","title":"Noise","text":""},{"location":"api/noise/#noise-for-simulations","title":"Noise for simulations","text":""},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler","title":"<code>NoiseHandler(protocol, options=dict())</code>","text":"<p>A container for multiple sources of noise.</p> <p>Note <code>NoiseProtocol.ANALOG</code> and <code>NoiseProtocol.DIGITAL</code> sources cannot be both present. Also <code>NoiseProtocol.READOUT</code> can only be present once as the last noise sources, and only exclusively with <code>NoiseProtocol.DIGITAL</code> sources.</p> PARAMETER DESCRIPTION <code>protocol</code> <p>The protocol(s) applied. To be defined from <code>NoiseProtocol</code>.</p> <p> TYPE: <code>NoiseEnum | list[NoiseEnum]</code> </p> <code>options</code> <p>A list of options defining the protocol. For <code>NoiseProtocol.ANALOG</code>, options should contain a field <code>noise_probs</code>. For <code>NoiseProtocol.DIGITAL</code>, options should contain a field <code>error_probability</code>.</p> <p> TYPE: <code>dict | list[dict]</code> DEFAULT: <code>dict()</code> </p> <p>Examples:</p> <pre><code>    from qadence import NoiseProtocol, NoiseHandler\n\n    analog_options = {\"noise_probs\": 0.1}\n    digital_options = {\"error_probability\": 0.1}\n    readout_options = {\"error_probability\": 0.1, \"seed\": 0}\n\n    # single noise sources\n    analog_noise = NoiseHandler(NoiseProtocol.ANALOG.DEPOLARIZING, analog_options)\n    digital_depo_noise = NoiseHandler(NoiseProtocol.DIGITAL.DEPOLARIZING, digital_options)\n    readout_noise = NoiseHandler(NoiseProtocol.READOUT, readout_options)\n\n    # init from multiple sources\n    protocols: list = [NoiseProtocol.DIGITAL.DEPOLARIZING, NoiseProtocol.READOUT]\n    options: list = [digital_options, readout_noise]\n    noise_combination = NoiseHandler(protocols, options)\n\n    # Appending noise sources\n    noise_combination = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, digital_options)\n    noise_combination.append([digital_depo_noise, readout_noise])\n</code></pre> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def __init__(\n    self,\n    protocol: NoiseEnum | list[NoiseEnum],\n    options: dict | list[dict] = dict(),\n) -&gt; None:\n    self.protocol = protocol if isinstance(protocol, list) else [protocol]\n    self.options = options if isinstance(options, list) else [options] * len(self.protocol)\n    self.verify_all_protocols()\n</code></pre>"},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler.append","title":"<code>append(other)</code>","text":"<p>Append noises.</p> PARAMETER DESCRIPTION <code>other</code> <p>The noises to add.</p> <p> TYPE: <code>NoiseHandler | list[NoiseHandler]</code> </p> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def append(self, other: NoiseHandler | list[NoiseHandler]) -&gt; None:\n    \"\"\"Append noises.\n\n    Args:\n        other (NoiseHandler | list[NoiseHandler]): The noises to add.\n    \"\"\"\n    # To avoid overwriting the noise_sources list if an error is raised, make a copy\n    other_list = other if isinstance(other, list) else [other]\n    protocols = self.protocol[:]\n    options = self.options[:]\n\n    for noise in other_list:\n        protocols += noise.protocol\n        options += noise.options\n\n    # init may raise an error\n    temp_handler = NoiseHandler(protocols, options)\n    # if verify passes, replace protocols and options\n    self.protocol = temp_handler.protocol\n    self.options = temp_handler.options\n</code></pre>"},{"location":"api/noise/#qadence.noise.protocols.NoiseHandler.verify_all_protocols","title":"<code>verify_all_protocols()</code>","text":"<p>Make sure all protocols are correct in terms and their combination too.</p> Source code in <code>qadence/noise/protocols.py</code> <pre><code>def verify_all_protocols(self) -&gt; None:\n    \"\"\"Make sure all protocols are correct in terms and their combination too.\"\"\"\n\n    if len(self.protocol) == 0:\n        raise ValueError(\"NoiseHandler should be specified with one valid configuration.\")\n\n    if len(self.protocol) != len(self.options):\n        raise ValueError(\"Specify lists of same length when defining noises.\")\n\n    for protocol, option in zip(self.protocol, self.options):\n        self._verify_single_protocol(protocol, option)\n\n    types = [type(p) for p in self.protocol]\n    unique_types = set(types)\n    if NoiseProtocol.DIGITAL in unique_types and NoiseProtocol.ANALOG in unique_types:\n        raise ValueError(\"Cannot define a config with both Digital and Analog noises.\")\n\n    if NoiseProtocol.ANALOG in unique_types:\n        if NoiseProtocol.READOUT in unique_types:\n            raise ValueError(\"Cannot define a config with both READOUT and Analog noises.\")\n        if types.count(NoiseProtocol.ANALOG) &gt; 1:\n            raise ValueError(\"Multiple Analog Noises are not supported yet.\")\n\n    if NoiseProtocol.READOUT in unique_types:\n        if (\n            not isinstance(self.protocol[-1], NoiseProtocol.READOUT)\n            or types.count(NoiseProtocol.READOUT) &gt; 1\n        ):\n            raise ValueError(\"Only define a NoiseHandler with one READOUT as the last Noise.\")\n</code></pre>"},{"location":"api/operations/","title":"Operations","text":"<p>Operations are common <code>PrimitiveBlocks</code>, these are often called gates elsewhere.</p>"},{"location":"api/operations/#constant-blocks","title":"Constant blocks","text":"<p>CY gate not implemented</p>"},{"location":"api/operations/#qadence.operations.X","title":"<code>X(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The X gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.Y","title":"<code>Y(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Y gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.Z","title":"<code>Z(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Z gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.I","title":"<code>I(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The identity gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(self, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.H","title":"<code>H(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hadamard or H gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = (1 / np.sqrt(2)) * (X(target) + Z(target) - np.sqrt(2) * I(target))\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.S","title":"<code>S(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The S / Phase gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.SDagger","title":"<code>SDagger(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hermitian adjoint/conjugate transpose of the S / Phase gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.SWAP","title":"<code>SWAP(control, target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The SWAP gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    a11 = 0.5 * (Z(control) - I(control))\n    a22 = -0.5 * (Z(target) + I(target))\n    a12 = 0.5 * (chain(X(control), Z(control)) + X(control))\n    a21 = 0.5 * (chain(Z(target), X(target)) + X(target))\n    self.generator = (\n        kron(-1.0 * a22, a11) + kron(-1.0 * a11, a22) + kron(a12, a21) + kron(a21, a12)\n    )\n    super().__init__((control, target), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.T","title":"<code>T(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The T gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.TDagger","title":"<code>TDagger(target, noise=None)</code>","text":"<p>               Bases: <code>PrimitiveBlock</code></p> <p>The Hermitian adjoint/conjugate transpose of the T gate.</p> Source code in <code>qadence/operations/primitive.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CNOT","title":"<code>CNOT(control, target, noise=None)</code>","text":"<p>               Bases: <code>ControlBlock</code></p> <p>The CNot, or CX, gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(self, control: int, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    self.generator = kron(N(control), X(target) - I(target))\n    super().__init__((control,), X(target), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CZ","title":"<code>CZ(control, target, noise=None)</code>","text":"<p>               Bases: <code>MCZ</code></p> <p>The CZ gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(self, control: int, target: int, noise: NoiseHandler | None = None) -&gt; None:\n    super().__init__((control,), target, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CPHASE","title":"<code>CPHASE(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCPHASE</code></p> <p>The CPHASE gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#parametrized-blocks","title":"Parametrized blocks","text":""},{"location":"api/operations/#qadence.operations.RX","title":"<code>RX(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Rx gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    # TODO: should we give them more meaningful names? like 'angle'?\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = X(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.RY","title":"<code>RY(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Ry gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = Y(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.RZ","title":"<code>RZ(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Rz gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TParameter | ParamMap,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = (\n        parameter if isinstance(parameter, ParamMap) else ParamMap(parameter=parameter)\n    )\n    self.generator = Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRX","title":"<code>CRX(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRX</code></p> <p>The CRX gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRY","title":"<code>CRY(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRY</code></p> <p>The CRY gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self, control: int, target: int, parameter: TParameter, noise: NoiseHandler | None = None\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.CRZ","title":"<code>CRZ(control, target, parameter, noise=None)</code>","text":"<p>               Bases: <code>MCRZ</code></p> <p>The CRZ gate.</p> Source code in <code>qadence/operations/control_ops.py</code> <pre><code>def __init__(\n    self,\n    control: int,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n):\n    super().__init__((control,), target, parameter, noise=noise)\n</code></pre>"},{"location":"api/operations/#qadence.operations.PHASE","title":"<code>PHASE(target, parameter, noise=None)</code>","text":"<p>               Bases: <code>ParametricBlock</code></p> <p>The Parametric Phase / S gate.</p> Source code in <code>qadence/operations/parametric.py</code> <pre><code>def __init__(\n    self,\n    target: int,\n    parameter: Parameter | TNumber | sympy.Expr | str,\n    noise: NoiseHandler | None = None,\n) -&gt; None:\n    self.parameters = ParamMap(parameter=parameter)\n    self.generator = I(target) - Z(target)\n    super().__init__((target,), noise=noise)\n</code></pre>"},{"location":"api/operations/#hamiltonian-evolution","title":"Hamiltonian Evolution","text":"<p>AnalogSWAP should be turned into a proper analog block</p>"},{"location":"api/operations/#qadence.operations.HamEvo","title":"<code>HamEvo(generator, parameter, qubit_support=None, duration=None, noise_operators=list())</code>","text":"<p>               Bases: <code>TimeEvolutionBlock</code></p> <p>The Hamiltonian evolution operator U(t).</p> <p>For time-independent Hamiltonians the solution is exact:</p> <pre><code>U(t) = exp(-iGt)\n</code></pre> <p>where G represents an Hermitian generator, or Hamiltonian and t represents the time parameter. For time-dependent Hamiltonians, the solution is obtained by numerical integration of the Schrodinger equation.</p> PARAMETER DESCRIPTION <code>generator</code> <p>Hamiltonian generator, either symbolic as an AbstractBlock, or as a torch.Tensor or numpy.ndarray.</p> <p> TYPE: <code>Union[TGenerator, AbstractBlock]</code> </p> <code>parameter</code> <p>The time parameter for evolution operator. For the time-independent case, it represents the actual value for which the evolution will be evaluated. For the time-dependent case, it should be an instance of TimeParameter to signal the solver the variable that will be integrated over.</p> <p> TYPE: <code>TParameter</code> </p> <code>qubit_support</code> <p>The qubits on which the evolution will be performed on. Only required for generators that are not a composition of blocks.</p> <p> TYPE: <code>tuple[int, ...]</code> DEFAULT: <code>None</code> </p> <code>duration</code> <p>(optional) duration of the evolution in case of time-dependent generator. By default, a FeatureParameter with tag \"duration\" will be initialized, and the value will then be required in the values dict.</p> <p> TYPE: <code>TParameter | None</code> DEFAULT: <code>None</code> </p> <code>noise_operators</code> <p>(optional) the list of jump operators to use when using a shrodinger solver, allowing to perform noisy simulations.</p> <p> TYPE: <code>list[AbstractBlock]</code> DEFAULT: <code>list()</code> </p> <p>Examples:</p> <pre><code>from qadence import X, HamEvo, PI, add, run\nfrom qadence import FeatureParameter, TimeParameter\nimport torch\n\nn_qubits = 3\n\n# Hamiltonian as a block composition\nhamiltonian = add(X(i) for i in range(n_qubits))\nhevo = HamEvo(hamiltonian, parameter=torch.rand(2))\nstate = run(hevo)\n\n# Hamiltonian as a random matrix\nhamiltonian = torch.rand(2, 2, dtype=torch.complex128)\nhevo = HamEvo(hamiltonian, parameter=torch.rand(2), qubit_support=(0,))\nstate = run(hevo)\n\n# Time-dependent Hamiltonian\nt = TimeParameter(\"t\")\nhamiltonian = t * add(X(i) for i in range(n_qubits))\nhevo = HamEvo(hamiltonian, parameter=t)\nstate = run(hevo, values = {\"duration\": torch.tensor(1.0)})\n\n# Adding noise operators\nnoise_ops = [X(0)]\nhevo = HamEvo(hamiltonian, parameter=t, noise_operators=noise_ops)\n</code></pre> <pre><code>\n</code></pre> Source code in <code>qadence/operations/ham_evo.py</code> <pre><code>def __init__(\n    self,\n    generator: Union[TGenerator, AbstractBlock],\n    parameter: TParameter,\n    qubit_support: tuple[int, ...] = None,\n    duration: TParameter | None = None,\n    noise_operators: list[AbstractBlock] = list(),\n):\n    params = {}\n    if qubit_support is None and not isinstance(generator, AbstractBlock):\n        raise ValueError(\"You have to supply a qubit support for non-block generators.\")\n    super().__init__(qubit_support if qubit_support else generator.qubit_support)\n    if isinstance(generator, AbstractBlock):\n        qubit_support = generator.qubit_support\n        if generator.is_parametric:\n            params = {str(e): e for e in expressions(generator)}\n        if generator.is_time_dependent:\n            if isinstance(duration, str):\n                duration = Parameter(duration, trainable=False)\n            elif duration is None:\n                duration = Parameter(\"duration\", trainable=False)\n        if not generator.is_time_dependent and duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n    elif isinstance(generator, torch.Tensor):\n        if duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n        msg = \"Please provide a square generator.\"\n        if len(generator.shape) == 2:\n            assert generator.shape[0] == generator.shape[1], msg\n        elif len(generator.shape) == 3:\n            assert generator.shape[1] == generator.shape[2], msg\n            assert generator.shape[0] == 1, \"Qadence doesnt support batched generators.\"\n        else:\n            raise TypeError(\n                \"Only 2D or 3D generators are supported.\\\n                            In case of a 3D generator, the batch dim\\\n                            is expected to be at dim 0.\"\n            )\n        params = {str(generator.__hash__()): generator}\n    elif isinstance(generator, (sympy.Basic, sympy.Array)):\n        if duration is not None:\n            raise TypeError(\n                \"Duration argument is only supported for time-dependent generators.\"\n            )\n        params = {str(generator): generator}\n    else:\n        raise TypeError(\n            f\"Generator of type {type(generator)} not supported.\\\n                        If you're using a numpy.ndarray, please cast it to a torch tensor.\"\n        )\n    if duration is not None:\n        params = {\"duration\": Parameter(duration), **params}\n    params = {\"parameter\": Parameter(parameter), **params}\n    self.parameters = ParamMap(**params)\n    self.time_param = parameter\n    self.generator = generator\n    self.duration = duration\n\n    if len(noise_operators) &gt; 0:\n        if not all(\n            [\n                len(set(op.qubit_support + self.qubit_support) - set(self.qubit_support)) == 0\n                for op in noise_operators\n            ]\n        ):\n            raise ValueError(\n                \"Noise operators should be defined\"\n                \" over the same or a subset of the qubit support\"\n            )\n        if True in [op.is_parametric for op in noise_operators]:\n            raise ValueError(\"Parametric operators are not supported\")\n    self.noise_operators = noise_operators\n</code></pre>"},{"location":"api/operations/#qadence.operations.HamEvo.digital_decomposition","title":"<code>digital_decomposition(approximation=LTSOrder.ST4)</code>","text":"<p>Decompose the Hamiltonian evolution into digital gates.</p> PARAMETER DESCRIPTION <code>approximation</code> <p>Choose the type of decomposition. Defaults to \"st4\". Available types are: * 'basic' = apply first-order Trotter formula and decompose each term of     the exponential into digital gates. It is exact only if applied to an     operator whose terms are mutually commuting. * 'st2' = Trotter-Suzuki 2nd order formula for approximating non-commuting     Hamiltonians. * 'st4' = Trotter-Suzuki 4th order formula for approximating non-commuting     Hamiltonians.</p> <p> TYPE: <code>str</code> DEFAULT: <code>ST4</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>a block with the digital decomposition</p> <p> TYPE: <code>AbstractBlock</code> </p> Source code in <code>qadence/operations/ham_evo.py</code> <pre><code>def digital_decomposition(self, approximation: LTSOrder = LTSOrder.ST4) -&gt; AbstractBlock:\n    \"\"\"Decompose the Hamiltonian evolution into digital gates.\n\n    Args:\n        approximation (str, optional): Choose the type of decomposition. Defaults to \"st4\".\n            Available types are:\n            * 'basic' = apply first-order Trotter formula and decompose each term of\n                the exponential into digital gates. It is exact only if applied to an\n                operator whose terms are mutually commuting.\n            * 'st2' = Trotter-Suzuki 2nd order formula for approximating non-commuting\n                Hamiltonians.\n            * 'st4' = Trotter-Suzuki 4th order formula for approximating non-commuting\n                Hamiltonians.\n\n    Returns:\n        AbstractBlock: a block with the digital decomposition\n    \"\"\"\n\n    # psi(t) = exp(-i * H * t * psi0)\n    # psi(t) = exp(-i * lambda * t * psi0)\n    # H = sum(Paulin) + sum(Pauli1*Pauli2)\n    logger.info(\"Quantum simulation of the time-independent Schr\u00f6dinger equation.\")\n\n    blocks = []\n\n    # how to change the type/dict to enum effectively\n\n    # when there is a term including non-commuting matrices use st2 or st4\n\n    # 1) should check that the given generator respects the constraints\n    # single-qubit gates\n\n    assert isinstance(\n        self.generator, AbstractBlock\n    ), \"Only a generator represented as a block can be decomposed\"\n\n    if block_is_qubit_hamiltonian(self.generator):\n        try:\n            block_is_commuting_hamiltonian(self.generator)\n            approximation = LTSOrder.BASIC  # use the simpler approach if the H is commuting\n        except TypeError:\n            logger.warning(\n                \"\"\"Non-commuting terms in the Pauli operator.\n                The Suzuki-Trotter approximation is applied.\"\"\"\n            )\n\n        blocks.extend(\n            lie_trotter_suzuki(\n                block=self.generator,\n                parameter=self.parameters.parameter,\n                order=LTSOrder[approximation],\n            )\n        )\n\n        # 2) return an AbstractBlock instance with the set of gates\n        # resulting from the decomposition\n\n        return chain(*blocks)\n    else:\n        raise NotImplementedError(\n            \"The current digital decomposition can be applied only to Pauli Hamiltonians.\"\n        )\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogSWAP","title":"<code>AnalogSWAP(control, target, parameter=3 * PI / 4)</code>","text":"<p>               Bases: <code>HamEvo</code></p> <p>Single time-independent Hamiltonian evolution over a Rydberg Ising.</p> <p>hamiltonian yielding a SWAP (up to global phase).</p> <p>Derived from Bapat et al. where it is applied to XX-type Hamiltonian</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def __init__(self, control: int, target: int, parameter: TParameter = 3 * PI / 4):\n    rydberg_ising_hamiltonian_generator = (\n        4.0 * kron((I(control) - Z(control)) / 2.0, (I(target) - Z(target)) / 2.0)\n        + (2.0 / 3.0) * np.sqrt(2.0) * X(control)\n        + (2.0 / 3.0) * np.sqrt(2.0) * X(target)\n        + (1.0 + np.sqrt(5.0) / 3) * Z(control)\n        + (1.0 + np.sqrt(5.0) / 3) * Z(target)\n    )\n    super().__init__(rydberg_ising_hamiltonian_generator, parameter, (control, target))\n</code></pre>"},{"location":"api/operations/#analog-blocks","title":"Analog blocks","text":""},{"location":"api/operations/#qadence.operations.AnalogRX","title":"<code>AnalogRX(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog X rotation.</p> <p>Shorthand for <code>AnalogRot</code>:</p> <pre><code>\u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\nAnalogRot(duration=t, omega=\u03a9)\n</code></pre> PARAMETER DESCRIPTION <code>angle</code> <p>Rotation angle [rad]</p> <p> TYPE: <code>float | str | Parameter</code> </p> <code>qubit_support</code> <p>Defines the (local/global) qubit support</p> <p> TYPE: <code>str | QubitSupport | Tuple</code> DEFAULT: <code>'global'</code> </p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRX(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog X rotation.\n\n    Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n\n    ```python\n    \u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\n    AnalogRot(duration=t, omega=\u03a9)\n    ```\n\n    Arguments:\n        angle: Rotation angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n    return _analog_rot(angle, qubit_support, phase=0, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRY","title":"<code>AnalogRY(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog Y rotation.</p> <p>Shorthand for <code>AnalogRot</code>:</p> <p><pre><code>\u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\nAnalogRot(duration=t, omega=\u03a9, phase=-\u03c0/2)\n</code></pre> Arguments:     angle: Rotation angle [rad]     qubit_support: Defines the (local/global) qubit support</p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRY(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog Y rotation.\n\n    Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n\n    ```python\n    \u03c6=2.4; \u03a9=\u03c0; t = \u03c6/\u03a9 * 1000\n    AnalogRot(duration=t, omega=\u03a9, phase=-\u03c0/2)\n    ```\n    Arguments:\n        angle: Rotation angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n    return _analog_rot(angle, qubit_support, phase=-PI / 2, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRZ","title":"<code>AnalogRZ(angle, qubit_support='global', add_pattern=True)</code>","text":"<p>Analog Z rotation. Shorthand for <code>AnalogRot</code>: <pre><code>\u03c6=2.4; \u03b4=\u03c0; t = \u03c6/\u03b4 * 100)\nAnalogRot(duration=t, delta=\u03b4, phase=\u03c0/2)\n</code></pre></p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRZ(\n    angle: float | str | Parameter,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"Analog Z rotation. Shorthand for [`AnalogRot`][qadence.operations.AnalogRot]:\n    ```\n    \u03c6=2.4; \u03b4=\u03c0; t = \u03c6/\u03b4 * 100)\n    AnalogRot(duration=t, delta=\u03b4, phase=\u03c0/2)\n    ```\n    \"\"\"\n    q = _cast(QubitSupport, qubit_support)\n    alpha = _cast(Parameter, angle)\n    delta = PI\n    omega = 0\n    duration = alpha / delta * 1000\n    h_norm = sympy.sqrt(omega**2 + delta**2)\n    ps = ParamMap(\n        alpha=alpha, duration=duration, omega=omega, delta=delta, phase=0.0, h_norm=h_norm\n    )\n    return ConstantAnalogRotation(qubit_support=q, parameters=ps, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogRot","title":"<code>AnalogRot(duration, omega=0, delta=0, phase=0, qubit_support='global', add_pattern=True)</code>","text":"<p>General analog rotation operation.</p> PARAMETER DESCRIPTION <code>duration</code> <p>Duration of the rotation [ns].</p> <p> TYPE: <code>float | str | Parameter</code> </p> <code>omega</code> <p>Rotation frequency [rad/\u03bcs]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>delta</code> <p>Rotation frequency [rad/\u03bcs]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>phase</code> <p>Phase angle [rad]</p> <p> TYPE: <code>float | str | Parameter</code> DEFAULT: <code>0</code> </p> <code>qubit_support</code> <p>Defines the (local/global) qubit support</p> <p> TYPE: <code>str | QubitSupport | Tuple</code> DEFAULT: <code>'global'</code> </p> <code>add_pattern</code> <p>False disables the semi-local addressing pattern for the execution of this specific block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>ConstantAnalogRotation</code> <p>ConstantAnalogRotation</p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogRot(\n    duration: float | str | Parameter,\n    omega: float | str | Parameter = 0,\n    delta: float | str | Parameter = 0,\n    phase: float | str | Parameter = 0,\n    qubit_support: str | QubitSupport | Tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; ConstantAnalogRotation:\n    \"\"\"General analog rotation operation.\n\n    Arguments:\n        duration: Duration of the rotation [ns].\n        omega: Rotation frequency [rad/\u03bcs]\n        delta: Rotation frequency [rad/\u03bcs]\n        phase: Phase angle [rad]\n        qubit_support: Defines the (local/global) qubit support\n        add_pattern: False disables the semi-local addressing pattern\n            for the execution of this specific block.\n\n    Returns:\n        ConstantAnalogRotation\n    \"\"\"\n\n    if omega == 0 and delta == 0:\n        raise ValueError(\"Parameters omega and delta cannot both be 0.\")\n\n    q = _cast(QubitSupport, qubit_support)\n    duration = Parameter(duration)\n    omega = Parameter(omega)\n    delta = Parameter(delta)\n    phase = Parameter(phase)\n    h_norm = sympy.sqrt(omega**2 + delta**2)\n    alpha = duration * h_norm / 1000\n    ps = ParamMap(\n        alpha=alpha, duration=duration, omega=omega, delta=delta, phase=phase, h_norm=h_norm\n    )\n    return ConstantAnalogRotation(parameters=ps, qubit_support=q, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/operations/#qadence.operations.AnalogInteraction","title":"<code>AnalogInteraction(duration, qubit_support='global', add_pattern=True)</code>","text":"<p>Evolution of the interaction term for a register of qubits.</p> <p>Constructs a <code>InteractionBlock</code>.</p> PARAMETER DESCRIPTION <code>duration</code> <p>Time to evolve the interaction for in nanoseconds.</p> <p> TYPE: <code>TNumber | Basic</code> </p> <code>qubit_support</code> <p>Qubits the <code>InteractionBlock</code> is applied to. Can be either <code>\"global\"</code> to evolve the interaction block to all qubits or a tuple of integers.</p> <p> TYPE: <code>str | QubitSupport | tuple</code> DEFAULT: <code>'global'</code> </p> <code>add_pattern</code> <p>False disables the semi-local addressing pattern for the execution of this specific block.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>InteractionBlock</code> <p>a <code>InteractionBlock</code></p> Source code in <code>qadence/operations/analog.py</code> <pre><code>def AnalogInteraction(\n    duration: TNumber | sympy.Basic,\n    qubit_support: str | QubitSupport | tuple = \"global\",\n    add_pattern: bool = True,\n) -&gt; InteractionBlock:\n    \"\"\"Evolution of the interaction term for a register of qubits.\n\n    Constructs a [`InteractionBlock`][qadence.blocks.analog.InteractionBlock].\n\n    Arguments:\n        duration: Time to evolve the interaction for in nanoseconds.\n        qubit_support: Qubits the `InteractionBlock` is applied to. Can be either\n            `\"global\"` to evolve the interaction block to all qubits or a tuple of integers.\n        add_pattern: False disables the semi-local addressing pattern\n            for the execution of this specific block.\n\n    Returns:\n        a `InteractionBlock`\n    \"\"\"\n    q = _cast(QubitSupport, qubit_support)\n    ps = ParamMap(duration=duration)\n    return InteractionBlock(parameters=ps, qubit_support=q, add_pattern=add_pattern)\n</code></pre>"},{"location":"api/parameters/","title":"Parameters","text":""},{"location":"api/parameters/#parameters","title":"Parameters","text":""},{"location":"api/parameters/#qadence.parameters.ParamMap","title":"<code>ParamMap(**kwargs)</code>","text":"<p>Connects UUIDs of parameters to their expressions and names.</p> <p>This class is not user-facing and only needed for more complex block definitions. It provides convenient access to expressions/UUIDs/names needed in different backends.</p> PARAMETER DESCRIPTION <code>kwargs</code> <p>Parameters.</p> <p> TYPE: <code>str | TNumber | Tensor | Basic | Parameter</code> DEFAULT: <code>{}</code> </p> <p>Example: <pre><code>import sympy\nfrom qadence.parameters import ParamMap\n\n(x,y) = sympy.symbols(\"x y\")\nps = ParamMap(omega=2.0, duration=x+y)\n\nprint(f\"{ps.names() = }\")\nprint(f\"{ps.expressions() = }\")\nprint(f\"{ps.uuids() = }\")\n</code></pre> <pre><code>ps.names() = dict_keys(['omega', 'duration'])\nps.expressions() = dict_values([2.00000000000000, x + y])\nps.uuids() = dict_keys(['b07a96c5-cc8f-4488-91c5-ce2dcb4d42fe', '272e9bcb-08bd-4cee-9110-a1d472ad763e'])\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def __init__(self, **kwargs: str | TNumber | Tensor | Basic | Parameter):\n    self._name_dict: dict[str, tuple[str, Basic]] = {}\n    self._uuid_dict: dict[str, str] = {}\n    for name, v in kwargs.items():\n        param = v if isinstance(v, sympy.Basic) else Parameter(v)\n        uuid = str(uuid4())\n        self._name_dict[name] = (uuid, param)\n        self._uuid_dict[uuid] = param\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.Parameter","title":"<code>Parameter</code>","text":"<p>               Bases: <code>Symbol</code></p> <p>A wrapper on top of <code>sympy.Symbol</code>.</p> <p>Includes two additional keywords: <code>trainable</code> and <code>value</code>. This class is to define both feature parameter and variational parameters.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.trainable","title":"<code>trainable: bool</code>  <code>instance-attribute</code>","text":"<p>Trainable parameters are variational parameters.</p> <p>Non-trainable parameters are feature parameters.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.value","title":"<code>value: TNumber</code>  <code>instance-attribute</code>","text":"<p>(Initial) value of the parameter.</p>"},{"location":"api/parameters/#qadence.parameters.Parameter.__new__","title":"<code>__new__(name, **assumptions)</code>","text":"<p>Arguments:</p> <pre><code>name: When given a string only, the class\n    constructs a trainable Parameter with a a randomly initialized value.\n**assumptions: are passed on to the parent class `sympy.Symbol`. Two new assumption\n    kwargs are supported by this constructor: `trainable: bool`, and `value: TNumber`.\n</code></pre> <p>Example: <pre><code>from qadence.parameters import Parameter, VariationalParameter\n\ntheta = Parameter(\"theta\")\nprint(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\nassert not theta.is_number\n\n# you can specify both trainable/value in the constructor\ntheta = Parameter(\"theta\", trainable=True, value=2.0)\nprint(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n\n# VariationalParameter/FeatureParameter are constructing\n# trainable/untrainable Parameters\ntheta = VariationalParameter(\"theta\", value=2.0)\nassert theta == Parameter(\"theta\", trainable=True, value=2.0)\n\n# When provided with a numeric type, Parameter constructs a sympy numeric type\":\nconstant_zero = Parameter(0)\nassert constant_zero.is_number\n\n# When passed a Parameter or a sympy expression, it just returns it.\nexpr = Parameter(\"x\") * Parameter(\"y\")\nprint(f\"{expr=} : {expr.free_symbols}\")\n</code></pre> <pre><code>theta: trainable=True value=0.8610640931814965\ntheta: trainable=True value=2.0\nexpr=x*y : {x, y}\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def __new__(\n    cls, name: str | TNumber | Tensor | Basic | Parameter, **assumptions: Any\n) -&gt; Parameter | Basic | Expr | Array:\n    \"\"\"\n    Arguments:\n\n        name: When given a string only, the class\n            constructs a trainable Parameter with a a randomly initialized value.\n        **assumptions: are passed on to the parent class `sympy.Symbol`. Two new assumption\n            kwargs are supported by this constructor: `trainable: bool`, and `value: TNumber`.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.parameters import Parameter, VariationalParameter\n\n    theta = Parameter(\"theta\")\n    print(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n    assert not theta.is_number\n\n    # you can specify both trainable/value in the constructor\n    theta = Parameter(\"theta\", trainable=True, value=2.0)\n    print(f\"{theta}: trainable={theta.trainable} value={theta.value}\")\n\n    # VariationalParameter/FeatureParameter are constructing\n    # trainable/untrainable Parameters\n    theta = VariationalParameter(\"theta\", value=2.0)\n    assert theta == Parameter(\"theta\", trainable=True, value=2.0)\n\n    # When provided with a numeric type, Parameter constructs a sympy numeric type\":\n    constant_zero = Parameter(0)\n    assert constant_zero.is_number\n\n    # When passed a Parameter or a sympy expression, it just returns it.\n    expr = Parameter(\"x\") * Parameter(\"y\")\n    print(f\"{expr=} : {expr.free_symbols}\")\n    ```\n    \"\"\"\n    p: Parameter\n    if isinstance(name, get_args(TNumber)):\n        return sympify(name)\n    elif isinstance(name, Tensor):\n        if name.numel() == 1:\n            return sympify(name)\n        else:\n            return Array(name.detach().numpy())\n    elif isinstance(name, Parameter):\n        p = super().__new__(cls, name.name, **assumptions)\n        p.name = name.name\n        p.trainable = name.trainable\n        p.value = name.value\n        p.is_time = name.is_time\n        return p\n    elif isinstance(name, (Basic, Expr)):\n        if name.is_number:\n            return sympify(evaluate(name))\n        return name\n    elif isinstance(name, str):\n        p = super().__new__(cls, name, **assumptions)\n        p.trainable = assumptions.get(\"trainable\", True)\n        p.value = assumptions.get(\"value\", None)\n        p.is_time = assumptions.get(\"is_time\", False)\n        if p.value is None:\n            p.value = rand(1).item()\n        return p\n    else:\n        raise TypeError(f\"Parameter does not support type {type(name)}\")\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.FeatureParameter","title":"<code>FeatureParameter(name, **kwargs)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=False)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def FeatureParameter(name: str, **kwargs: Any) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=False)`.\"\"\"\n    return Parameter(name, trainable=False, **kwargs)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.TimeParameter","title":"<code>TimeParameter(name)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=False, is_time=True)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def TimeParameter(name: str) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=False, is_time=True)`.\"\"\"\n    return Parameter(name, trainable=False, is_time=True)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.VariationalParameter","title":"<code>VariationalParameter(name, **kwargs)</code>","text":"<p>Shorthand for <code>Parameter(..., trainable=True)</code>.</p> Source code in <code>qadence/parameters.py</code> <pre><code>def VariationalParameter(name: str, **kwargs: Any) -&gt; Parameter:\n    \"\"\"Shorthand for `Parameter(..., trainable=True)`.\"\"\"\n    return Parameter(name, trainable=True, **kwargs)\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.evaluate","title":"<code>evaluate(expr, values=None, as_torch=False)</code>","text":"<p>Arguments:</p> <pre><code>expr: An expression consisting of Parameters.\nvalues: values dict which contains values for the Parameters,\n    if empty, Parameter.value will be used.\nas_torch: Whether to retrieve a torch-differentiable expression result.\n</code></pre> <p>Example: <pre><code>from qadence.parameters import Parameter, evaluate\n\nexpr = Parameter(\"x\") * Parameter(\"y\")\n\n# Unless specified, Parameter initialized random values\n# Lets evaluate this expression and see what the result is\nres = evaluate(expr)\nprint(res)\n\n# We can also evaluate the expr using a custom dict\nd = {\"x\": 1, \"y\":2}\nres = evaluate(expr, d)\nprint(res)\n\n# Lastly, if we want a differentiable result, lets put the as_torch flag\nres = evaluate(expr, d, as_torch=True)\nprint(res)\n</code></pre> <pre><code>0.44177768378369486\n2.0\ntensor([2])\n</code></pre> </p> Source code in <code>qadence/parameters.py</code> <pre><code>def evaluate(expr: Expr, values: dict | None = None, as_torch: bool = False) -&gt; TNumber | Tensor:\n    \"\"\"\n    Arguments:\n\n        expr: An expression consisting of Parameters.\n        values: values dict which contains values for the Parameters,\n            if empty, Parameter.value will be used.\n        as_torch: Whether to retrieve a torch-differentiable expression result.\n\n    Example:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.parameters import Parameter, evaluate\n\n    expr = Parameter(\"x\") * Parameter(\"y\")\n\n    # Unless specified, Parameter initialized random values\n    # Lets evaluate this expression and see what the result is\n    res = evaluate(expr)\n    print(res)\n\n    # We can also evaluate the expr using a custom dict\n    d = {\"x\": 1, \"y\":2}\n    res = evaluate(expr, d)\n    print(res)\n\n    # Lastly, if we want a differentiable result, lets put the as_torch flag\n    res = evaluate(expr, d, as_torch=True)\n    print(res)\n    ```\n    \"\"\"\n    res: Basic\n    res_value: TNumber | Tensor\n    query: dict[Parameter, TNumber | Tensor] = dict()\n    values = values or dict()\n    if isinstance(expr, Array):\n        return Tensor(expr.tolist())\n    else:\n        if not expr.is_number:\n            for s in expr.free_symbols:\n                if s.name in values.keys():\n                    query[s] = values[s.name]\n                elif hasattr(s, \"value\"):\n                    query[s] = s.value\n                else:\n                    raise ValueError(f\"No value provided for symbol {s.name}\")\n        if as_torch:\n            res_value = make_differentiable(expr)(**{s.name: tensor(v) for s, v in query.items()})\n        else:\n            res = expr.subs(query)\n            res_value = sympy_to_numeric(res)\n        return res_value\n</code></pre>"},{"location":"api/parameters/#qadence.parameters.extract_original_param_entry","title":"<code>extract_original_param_entry(param)</code>","text":"<p>Given an Expression, what was the original \"param\" given by the user? It is either.</p> <p>going to be a numeric value, or a sympy Expression (in case a string was given, it was converted via Parameter(\"string\").</p> Source code in <code>qadence/parameters.py</code> <pre><code>def extract_original_param_entry(\n    param: Expr,\n) -&gt; TNumber | Tensor | Expr:\n    \"\"\"\n    Given an Expression, what was the original \"param\" given by the user? It is either.\n\n    going to be a numeric value, or a sympy Expression (in case a string was given,\n    it was converted via Parameter(\"string\").\n    \"\"\"\n    return param if not param.is_number else evaluate(param)\n</code></pre>"},{"location":"api/parameters/#parameter-embedding","title":"Parameter embedding","text":""},{"location":"api/parameters/#qadence.blocks.embedding.embedding","title":"<code>embedding(block, to_gate_params=False, engine=Engine.TORCH)</code>","text":"<p>Construct embedding function which maps user-facing parameters to either expression-level.</p> <p>parameters or gate-level parameters. The constructed embedding function has the signature:</p> <pre><code> embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n</code></pre> <p>which means that it maps the variational parameter dict <code>params</code> and the feature parameter dict <code>inputs</code> to one new parameter dict <code>embedded_dict</code> which holds all parameters that are needed to execute a circuit on a given backend. There are two different modes for this mapping:</p> <ul> <li>Expression-level parameters: For AD-based optimization. For every unique expression we end   up with one entry in the embedded dict:   <code>len(embedded_dict) == len(unique_parameter_expressions)</code>.</li> <li>Gate-level parameters: For PSR-based optimization or real devices. One parameter for each   gate parameter, regardless if they are based on the same expression. <code>len(embedded_dict) ==   len(parametric_gates)</code>. This is needed because PSR requires to shift the angles of every   gate where the same parameter appears.</li> </ul> PARAMETER DESCRIPTION <code>block</code> <p>parametrized block into which we want to embed parameters.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>to_gate_params</code> <p>A boolean flag whether to generate gate-level parameters or expression-level parameters.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>tuple[ParamDictType, Callable[[ParamDictType, ParamDictType], ParamDictType]]</code> <p>A tuple with variational parameter dict and the embedding function.</p> Source code in <code>qadence/blocks/embedding.py</code> <pre><code>def embedding(\n    block: AbstractBlock, to_gate_params: bool = False, engine: Engine = Engine.TORCH\n) -&gt; tuple[ParamDictType, Callable[[ParamDictType, ParamDictType], ParamDictType],]:\n    \"\"\"Construct embedding function which maps user-facing parameters to either *expression-level*.\n\n    parameters or *gate-level* parameters. The constructed embedding function has the signature:\n\n         embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n\n    which means that it maps the *variational* parameter dict `params` and the *feature* parameter\n    dict `inputs` to one new parameter dict `embedded_dict` which holds all parameters that are\n    needed to execute a circuit on a given backend. There are two different *modes* for this\n    mapping:\n\n    - *Expression-level* parameters: For AD-based optimization. For every unique expression we end\n      up with one entry in the embedded dict:\n      `len(embedded_dict) == len(unique_parameter_expressions)`.\n    - *Gate-level* parameters: For PSR-based optimization or real devices. One parameter for each\n      gate parameter, regardless if they are based on the same expression. `len(embedded_dict) ==\n      len(parametric_gates)`. This is needed because PSR requires to shift the angles of **every**\n      gate where the same parameter appears.\n\n    Arguments:\n        block: parametrized block into which we want to embed parameters.\n        to_gate_params: A boolean flag whether to generate gate-level parameters or\n            expression-level parameters.\n\n    Returns:\n        A tuple with variational parameter dict and the embedding function.\n    \"\"\"\n    concretize_parameter = _concretize_parameter(engine)\n    if engine == Engine.TORCH:\n        cast_dtype = tensor\n    else:\n        from jax.numpy import array\n\n        cast_dtype = array\n\n    unique_expressions = unique(expressions(block))\n    unique_symbols = [p for p in unique(parameters(block)) if not isinstance(p, sympy.Array)]\n    unique_const_matrices = [e for e in unique_expressions if isinstance(e, sympy.Array)]\n    unique_expressions = [e for e in unique_expressions if not isinstance(e, sympy.Array)]\n\n    # NOTE\n    # there are 3 kinds of parameters in qadence\n    # - non-trainable which are considered as inputs for classical data\n    # - trainable which are the variational parameters to be optimized\n    # - fixed: which are non-trainable parameters with fixed value (e.g. pi/2)\n    #\n    # both non-trainable and trainable parameters can have the same element applied\n    # to different operations in the quantum circuit, e.g. assigning the same parameter\n    # to multiple gates.\n    non_numeric_symbols = [p for p in unique_symbols if not p.is_number]\n    trainable_symbols = [p for p in non_numeric_symbols if p.trainable]\n    constant_expressions = [expr for expr in unique_expressions if expr.is_number]\n    # we dont need to care about constant symbols if they are contained in an symbolic expression\n    # we only care about gate params which are ONLY a constant\n\n    embeddings: dict[sympy.Expr, DifferentiableExpression] = {\n        expr: make_differentiable(expr=expr, engine=engine)\n        for expr in unique_expressions\n        if not expr.is_number\n    }\n\n    uuid_to_expr = uuid_to_expression(block)\n\n    def embedding_fn(params: ParamDictType, inputs: ParamDictType) -&gt; ParamDictType:\n        embedded_params: dict[sympy.Expr, ArrayLike] = {}\n        for expr, fn in embeddings.items():\n            angle: ArrayLike\n            values = {}\n            for symbol in expr.free_symbols:\n                if symbol.name in inputs:\n                    value = inputs[symbol.name]\n                elif symbol.name in params:\n                    value = params[symbol.name]\n                else:\n                    if symbol.is_time:\n                        value = tensor(1.0)\n                    else:\n                        msg_trainable = \"Trainable\" if symbol.trainable else \"Non-trainable\"\n                        raise KeyError(\n                            f\"{msg_trainable} parameter '{symbol.name}' not found in the \"\n                            f\"inputs list: {list(inputs.keys())} nor the \"\n                            f\"params list: {list(params.keys())}.\"\n                        )\n                values[symbol.name] = value\n            angle = fn(**values)\n            # do not reshape parameters which are multi-dimensional\n            # tensors, such as for example generator matrices\n            if not len(angle.squeeze().shape) &gt; 1:\n                angle = angle.reshape(-1)\n            embedded_params[expr] = angle\n\n        for e in constant_expressions + unique_const_matrices:\n            embedded_params[e] = params[stringify(e)]\n\n        if to_gate_params:\n            gate_lvl_params: ParamDictType = {}\n            for uuid, e in uuid_to_expr.items():\n                gate_lvl_params[uuid] = embedded_params[e]\n            return gate_lvl_params\n        else:\n            embedded_params.update(inputs)\n            for k, v in params.items():\n                if k not in embedded_params:\n                    embedded_params[k] = v\n            out = {\n                stringify(k)\n                if not isinstance(k, str)\n                else k: as_tensor(v)[None]\n                if as_tensor(v).ndim == 0\n                else v\n                for k, v in embedded_params.items()\n            }\n            return out\n\n    params: ParamDictType\n    params = {\n        p.name: concretize_parameter(value=p.value, trainable=True) for p in trainable_symbols\n    }\n    params.update(\n        {\n            stringify(expr): concretize_parameter(value=evaluate(expr), trainable=False)\n            for expr in constant_expressions\n        }\n    )\n    params.update(\n        {\n            stringify(expr): cast_dtype(nparray(expr.tolist(), dtype=npcdouble))\n            for expr in unique_const_matrices\n        }\n    )\n    return params, embedding_fn\n</code></pre>"},{"location":"api/quantumcircuit/","title":"QuantumCircuit","text":""},{"location":"api/quantumcircuit/#quantumcircuit","title":"QuantumCircuit","text":"<p>The abstract <code>QuantumCircuit</code> is the key object in Qadence, as it is what can be executed.</p>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit","title":"<code>QuantumCircuit(support, *blocks)</code>  <code>dataclass</code>","text":"<p>Am abstract QuantumCircuit instance.</p> <p>It needs to be passed to a quantum backend for execution.</p> <p>Arguments:</p> <pre><code>support: `Register` or number of qubits. If an integer is provided, a register is\n    constructed with `Register.all_to_all(x)`\n*blocks: (Possibly multiple) blocks to construct the circuit from.\n</code></pre> Source code in <code>qadence/circuit.py</code> <pre><code>def __init__(self, support: int | Register, *blocks: AbstractBlock):\n    \"\"\"\n    Arguments:\n\n        support: `Register` or number of qubits. If an integer is provided, a register is\n            constructed with `Register.all_to_all(x)`\n        *blocks: (Possibly multiple) blocks to construct the circuit from.\n    \"\"\"\n    self.block = chain(*blocks) if len(blocks) != 1 else blocks[0]\n    self.register = Register(support) if isinstance(support, int) else support\n\n    global_block = isinstance(self.block, AnalogBlock) and self.block.qubit_support.is_global\n    if not global_block and len(self.block) and self.block.n_qubits &gt; self.register.n_qubits:\n        raise ValueError(\n            f\"Register with {self.register.n_qubits} qubits is too small for the \"\n            f\"given block with {self.block.n_qubits} qubits\"\n        )\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.unique_parameters","title":"<code>unique_parameters: list[Parameter]</code>  <code>property</code>","text":"<p>Return the unique parameters in the circuit.</p> <p>These parameters are the actual user-facing parameters which can be assigned by the user. Multiple gates can contain the same unique parameter</p> RETURNS DESCRIPTION <code>list[Parameter]</code> <p>list[Parameter]: List of unique parameters in the circuit</p>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.dagger","title":"<code>dagger()</code>","text":"<p>Reverse the QuantumCircuit by calling dagger on the block.</p> Source code in <code>qadence/circuit.py</code> <pre><code>def dagger(self) -&gt; QuantumCircuit:\n    \"\"\"Reverse the QuantumCircuit by calling dagger on the block.\"\"\"\n    return QuantumCircuit(self.n_qubits, self.block.dagger())\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.get_blocks_by_tag","title":"<code>get_blocks_by_tag(tag)</code>","text":"<p>Extract one or more blocks using the human-readable tag.</p> <p>This function recursively explores all composite blocks to find all the occurrences of a certain tag in the blocks.</p> PARAMETER DESCRIPTION <code>tag</code> <p>the tag to look for</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>list[AbstractBlock]</code> <p>list[AbstractBlock]: The block(s) corresponding to the given tag</p> Source code in <code>qadence/circuit.py</code> <pre><code>def get_blocks_by_tag(self, tag: str) -&gt; list[AbstractBlock]:\n    \"\"\"Extract one or more blocks using the human-readable tag.\n\n    This function recursively explores all composite blocks to find\n    all the occurrences of a certain tag in the blocks.\n\n    Args:\n        tag (str): the tag to look for\n\n    Returns:\n        list[AbstractBlock]: The block(s) corresponding to the given tag\n    \"\"\"\n\n    def _get_block(block: AbstractBlock) -&gt; list[AbstractBlock]:\n        blocks = []\n        if block.tag == tag:\n            blocks += [block]\n        if isinstance(block, CompositeBlock):\n            blocks += flatten(*[_get_block(b) for b in block.blocks])\n        return blocks\n\n    return _get_block(self.block)\n</code></pre>"},{"location":"api/quantumcircuit/#qadence.circuit.QuantumCircuit.parameters","title":"<code>parameters()</code>","text":"<p>Extract all parameters for primitive blocks in the circuit.</p> <p>Notice that this function returns all the unique Parameters used in the quantum circuit. These can correspond to constants too.</p> RETURNS DESCRIPTION <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>List[tuple[Parameter]]: A list of tuples containing the Parameter</p> <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>instance of each of the primitive blocks in the circuit or, if the <code>flatten</code></p> <code>list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]</code> <p>flag is set to True, a flattened list of all circuit parameters</p> Source code in <code>qadence/circuit.py</code> <pre><code>def parameters(self) -&gt; list[Parameter | Basic] | list[tuple[Parameter | Basic, ...]]:\n    \"\"\"Extract all parameters for primitive blocks in the circuit.\n\n    Notice that this function returns all the unique Parameters used\n    in the quantum circuit. These can correspond to constants too.\n\n    Returns:\n        List[tuple[Parameter]]: A list of tuples containing the Parameter\n        instance of each of the primitive blocks in the circuit or, if the `flatten`\n        flag is set to True, a flattened list of all circuit parameters\n    \"\"\"\n    return parameters(self.block)\n</code></pre>"},{"location":"api/register/","title":"Register","text":""},{"location":"api/register/#quantum-registers","title":"Quantum Registers","text":""},{"location":"api/register/#qadence.register.Register","title":"<code>Register(support, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>","text":"<p>A register of qubits including 2D coordinates.</p> <p>Instantiating the Register class directly is only recommended for building custom registers. For most uses where a predefined lattice is desired it is recommended to use the various class methods available, e.g. <code>Register.triangular_lattice</code>.</p> PARAMETER DESCRIPTION <code>support</code> <p>A NetworkX graph or number of qubits. Nodes can include a <code>\"pos\"</code> attribute such that e.g.: <code>graph.nodes = {0: {\"pos\": (2,3)}, 1: {\"pos\": (0,0)}, ...}</code> which will be used in backends that need qubit coordinates. Passing a number of qubits calls <code>Register.all_to_all(n_qubits)</code>.</p> <p> TYPE: <code>Graph | int</code> </p> <code>spacing</code> <p>Value set as the distance between the two closest qubits. The spacing argument is also available for all the class method constructors.</p> <p> TYPE: <code>float | None</code> DEFAULT: <code>1.0</code> </p> <p>Examples: <pre><code>from qadence import Register\n\nreg_all = Register.all_to_all(n_qubits = 4)\nreg_line = Register.line(n_qubits = 4)\nreg_circle = Register.circle(n_qubits = 4)\nreg_squre = Register.square(qubits_side = 2)\nreg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\nreg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\nreg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n</code></pre> </p> Source code in <code>qadence/register.py</code> <pre><code>def __init__(\n    self,\n    support: nx.Graph | int,\n    spacing: float | None = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n):\n    \"\"\"\n    A register of qubits including 2D coordinates.\n\n    Instantiating the Register class directly is only recommended for building custom registers.\n    For most uses where a predefined lattice is desired it is recommended to use the various\n    class methods available, e.g. `Register.triangular_lattice`.\n\n    Arguments:\n        support: A NetworkX graph or number of qubits. Nodes can include a `\"pos\"` attribute\n            such that e.g.: `graph.nodes = {0: {\"pos\": (2,3)}, 1: {\"pos\": (0,0)}, ...}` which\n            will be used in backends that need qubit coordinates. Passing a number of qubits\n            calls `Register.all_to_all(n_qubits)`.\n        spacing: Value set as the distance between the two closest qubits. The spacing\n            argument is also available for all the class method constructors.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\"\n    from qadence import Register\n\n    reg_all = Register.all_to_all(n_qubits = 4)\n    reg_line = Register.line(n_qubits = 4)\n    reg_circle = Register.circle(n_qubits = 4)\n    reg_squre = Register.square(qubits_side = 2)\n    reg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\n    reg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\n    reg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n    ```\n    \"\"\"\n    if device_specs is not None and not isinstance(device_specs, RydbergDevice):\n        raise ValueError(\"Device specs are not valid. Please pass a `RydbergDevice` instance.\")\n\n    self.device_specs = device_specs\n\n    self.graph = support if isinstance(support, nx.Graph) else alltoall_graph(support)\n\n    if spacing is not None and self.min_distance != 0.0:\n        _scale_node_positions(self.graph, self.min_distance, spacing)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.all_node_pairs","title":"<code>all_node_pairs: EdgeView</code>  <code>property</code>","text":"<p>Return a list of all possible qubit pairs in the register.</p>"},{"location":"api/register/#qadence.register.Register.coords","title":"<code>coords: dict</code>  <code>property</code>","text":"<p>Return the dictionary of qubit coordinates.</p>"},{"location":"api/register/#qadence.register.Register.distances","title":"<code>distances: dict</code>  <code>property</code>","text":"<p>Return a dictionary of distances for all qubit pairs in the register.</p>"},{"location":"api/register/#qadence.register.Register.edge_distances","title":"<code>edge_distances: dict</code>  <code>property</code>","text":"<p>Return a dictionary of distances for the qubit pairs that are.</p> <p>connected by an edge in the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.edges","title":"<code>edges: EdgeView</code>  <code>property</code>","text":"<p>Return the EdgeView of the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.min_distance","title":"<code>min_distance: float</code>  <code>property</code>","text":"<p>Return the minimum distance between two qubts in the register.</p>"},{"location":"api/register/#qadence.register.Register.n_qubits","title":"<code>n_qubits: int</code>  <code>property</code>","text":"<p>Total number of qubits in the register.</p>"},{"location":"api/register/#qadence.register.Register.nodes","title":"<code>nodes: NodeView</code>  <code>property</code>","text":"<p>Return the NodeView of the underlying NetworkX graph.</p>"},{"location":"api/register/#qadence.register.Register.support","title":"<code>support: set</code>  <code>property</code>","text":"<p>Return the set of qubits in the register.</p>"},{"location":"api/register/#qadence.register.Register.all_to_all","title":"<code>all_to_all(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a register with an all-to-all connectivity graph.</p> <p>The graph is projected onto a 2D space and the qubit coordinates are set using a spring layout algorithm.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef all_to_all(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a register with an all-to-all connectivity graph.\n\n    The graph is projected\n    onto a 2D space and the qubit coordinates are set using a spring layout algorithm.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    return cls(alltoall_graph(n_qubits), spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.circle","title":"<code>circle(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a circle register.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef circle(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a circle register.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    graph = nx.grid_2d_graph(n_qubits, 1, periodic=True)\n    graph = nx.relabel_nodes(graph, {(i, 0): i for i in range(n_qubits)})\n    coords = nx.circular_layout(graph)\n    values = {i: {\"pos\": pos} for i, pos in coords.items()}\n    nx.set_node_attributes(graph, values)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.draw","title":"<code>draw(show=True)</code>","text":"<p>Draw the underlying NetworkX graph representing the register.</p> Source code in <code>qadence/register.py</code> <pre><code>def draw(self, show: bool = True) -&gt; None:\n    \"\"\"Draw the underlying NetworkX graph representing the register.\"\"\"\n    coords = {i: n[\"pos\"] for i, n in self.graph.nodes.items()}\n    nx.draw(self.graph, with_labels=True, pos=coords)\n    if show:\n        plt.gcf().show()\n</code></pre>"},{"location":"api/register/#qadence.register.Register.from_coordinates","title":"<code>from_coordinates(coords, lattice=LatticeTopology.ARBITRARY, spacing=None, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a register from a list of qubit coordinates.</p> <p>Each node is added to the underlying graph with the respective coordinates, but the edges are left empty.</p> PARAMETER DESCRIPTION <code>coords</code> <p>List of qubit coordinate tuples.</p> <p> TYPE: <code>list[tuple]</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef from_coordinates(\n    cls,\n    coords: list[tuple],\n    lattice: LatticeTopology | str = LatticeTopology.ARBITRARY,\n    spacing: float | None = None,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a register from a list of qubit coordinates.\n\n    Each node is added to the underlying\n    graph with the respective coordinates, but the edges are left empty.\n\n    Arguments:\n        coords: List of qubit coordinate tuples.\n    \"\"\"\n    graph = nx.Graph()\n    for i, pos in enumerate(coords):\n        graph.add_node(i, pos=pos)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.honeycomb_lattice","title":"<code>honeycomb_lattice(n_cells_row, n_cells_col, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a honeycomb lattice register.</p> <p>Each cell is an hexagon made up of six qubits.</p> PARAMETER DESCRIPTION <code>n_cells_row</code> <p>Number of cells in each row.</p> <p> TYPE: <code>int</code> </p> <code>n_cells_col</code> <p>Number of cells in each column.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef honeycomb_lattice(\n    cls,\n    n_cells_row: int,\n    n_cells_col: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a honeycomb lattice register.\n\n    Each cell is an hexagon made up of six qubits.\n\n    Arguments:\n        n_cells_row: Number of cells in each row.\n        n_cells_col: Number of cells in each column.\n    \"\"\"\n    graph = nx.hexagonal_lattice_graph(n_cells_row, n_cells_col)\n    graph = nx.relabel_nodes(graph, {(i, j): k for k, (i, j) in enumerate(graph.nodes)})\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.line","title":"<code>line(n_qubits, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a line register.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Total number of qubits.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef line(\n    cls,\n    n_qubits: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a line register.\n\n    Arguments:\n        n_qubits: Total number of qubits.\n    \"\"\"\n    return cls(line_graph(n_qubits), spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.rescale_coords","title":"<code>rescale_coords(scaling)</code>","text":"<p>Rescale the coordinates of all qubits in the register.</p> PARAMETER DESCRIPTION <code>scaling</code> <p>Scaling value.</p> <p> TYPE: <code>float</code> </p> Source code in <code>qadence/register.py</code> <pre><code>def rescale_coords(self, scaling: float) -&gt; Register:\n    \"\"\"\n    Rescale the coordinates of all qubits in the register.\n\n    Arguments:\n        scaling: Scaling value.\n    \"\"\"\n    g = deepcopy(self.graph)\n    _scale_node_positions(g, min_distance=1.0, spacing=scaling)\n    return Register(g, spacing=None, device_specs=self.device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.square","title":"<code>square(qubits_side, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a square register.</p> PARAMETER DESCRIPTION <code>qubits_side</code> <p>Number of qubits on one side of the square.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef square(\n    cls,\n    qubits_side: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a square register.\n\n    Arguments:\n        qubits_side: Number of qubits on one side of the square.\n    \"\"\"\n    n_points = 4 * (qubits_side - 1)\n\n    def gen_points() -&gt; np.ndarray:\n        rotate_left = np.array([[0.0, -1.0], [1.0, 0.0]])\n        increment = np.array([0.0, 1.0])\n\n        points = [np.array([0.0, 0.0])]\n        counter = 1\n        while len(points) &lt; n_points:\n            points.append(points[-1] + increment)\n\n            counter = (counter + 1) % qubits_side\n            if counter == 0:\n                increment = rotate_left.dot(increment)\n                counter = 1\n        points = np.array(points)  # type: ignore[assignment]\n        points -= np.mean(points, axis=0)\n\n        return points  # type: ignore[return-value]\n\n    graph = nx.grid_2d_graph(n_points, 1, periodic=True)\n    graph = nx.relabel_nodes(graph, {(i, 0): i for i in range(n_points)})\n    values = {i: {\"pos\": point} for i, point in zip(graph.nodes, gen_points())}\n    nx.set_node_attributes(graph, values)\n    return cls(graph, spacing, device_specs)\n</code></pre>"},{"location":"api/register/#qadence.register.Register.triangular_lattice","title":"<code>triangular_lattice(n_cells_row, n_cells_col, spacing=1.0, device_specs=DEFAULT_DEVICE)</code>  <code>classmethod</code>","text":"<p>Build a triangular lattice register.</p> <p>Each cell is a triangle made up of three qubits.</p> PARAMETER DESCRIPTION <code>n_cells_row</code> <p>Number of cells in each row.</p> <p> TYPE: <code>int</code> </p> <code>n_cells_col</code> <p>Number of cells in each column.</p> <p> TYPE: <code>int</code> </p> Source code in <code>qadence/register.py</code> <pre><code>@classmethod\ndef triangular_lattice(\n    cls,\n    n_cells_row: int,\n    n_cells_col: int,\n    spacing: float = 1.0,\n    device_specs: RydbergDevice = DEFAULT_DEVICE,\n) -&gt; Register:\n    \"\"\"\n    Build a triangular lattice register.\n\n    Each cell is a triangle made up of three qubits.\n\n    Arguments:\n        n_cells_row: Number of cells in each row.\n        n_cells_col: Number of cells in each column.\n    \"\"\"\n    return cls(triangular_lattice_graph(n_cells_row, n_cells_col), spacing, device_specs)\n</code></pre>"},{"location":"api/serialization/","title":"Serialization","text":""},{"location":"api/serialization/#serialization","title":"Serialization","text":""},{"location":"api/serialization/#qadence.serialization.SerializationModel","title":"<code>SerializationModel(d=dict())</code>  <code>dataclass</code>","text":"<p>A serialization model class to serialize data from <code>QuantumModel</code>s,.</p> <p><code>torch.nn.Module</code> and similar structures. The data included in the serialization logic includes: the <code>AbstractBlock</code> and its children classes, <code>QuantumCircuit</code>, <code>Register</code>, and <code>sympy</code> expressions (including <code>Parameter</code> class from <code>qadence.parameters</code>).</p> <p>A children class must define the <code>value</code> attribute type and how to handle it, since it is the main property for the class to be used by the serialization process. For instance:</p> <pre><code>@dataclass\nclass QuantumCircuitSerialization(SerializationModel):\n    value: QuantumCircuit = dataclass_field(init=False)\n\n    def __post_init__(self) -&gt; None:\n        self.value = (\n            QuantumCircuit._from_dict(self.d)\n            if isinstance(self.d, dict)\n            else self.d\n        )\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.deserialize","title":"<code>deserialize(d, as_torch=False)</code>","text":"<p>Supported Types:</p> <p>AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module Deserializes a dict to one of the supported types.</p> PARAMETER DESCRIPTION <code>d</code> <p>A dict containing a serialized object.</p> <p> TYPE: <code>dict</code> </p> <code>as_torch</code> <p>Whether to transform to torch for the deserialized object.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Returns:     AbstractBlock, QuantumCircuit, QuantumModel, Register, torch.nn.Module.</p> <p>Examples: <pre><code>import torch\nfrom qadence import serialize, deserialize, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nblock_dict = serialize(myblock)\nprint(block_dict)\n\n## Lets use myblock in a QuantumCircuit and serialize it.\n\nqc = QuantumCircuit(n_qubits, myblock)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n## Finally, let's wrap it in a QuantumModel\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n# Lets check if the loaded QuantumModel returns the same expectation\nassert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n</code></pre> <pre><code>{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': 'HEA', 'blocks': [{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('91865968-30ab-420e-bdd5-f6adbdca392b', {'name': 'theta_0', 'expression': \"Parameter('theta_0')\", 'symbols': {'theta_0': {'name': 'theta_0', 'trainable': 'True', 'value': '0.03638394619672025'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('eb32bdde-cb88-47ae-bb1e-7e2ea70855cf', {'name': 'theta_1', 'expression': \"Parameter('theta_1')\", 'symbols': {'theta_1': {'name': 'theta_1', 'trainable': 'True', 'value': '0.4410628096099821'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RY', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('cc69725a-061b-40d3-9657-c75b43af7fbd', {'name': 'theta_2', 'expression': \"Parameter('theta_2')\", 'symbols': {'theta_2': {'name': 'theta_2', 'trainable': 'True', 'value': '0.45068742636843784'}}})}}, 'noise': None}, {'type': 'RY', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('dd45f01b-44f8-4ad1-96cc-905b0d74eb57', {'name': 'theta_3', 'expression': \"Parameter('theta_3')\", 'symbols': {'theta_3': {'name': 'theta_3', 'trainable': 'True', 'value': '0.041073875853981257'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('7a02a67c-8ee5-4334-b489-d032965ab4e9', {'name': 'theta_4', 'expression': \"Parameter('theta_4')\", 'symbols': {'theta_4': {'name': 'theta_4', 'trainable': 'True', 'value': '0.8181221623032562'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('7b8d0fc8-6ee5-4165-a904-2238e976bf84', {'name': 'theta_5', 'expression': \"Parameter('theta_5')\", 'symbols': {'theta_5': {'name': 'theta_5', 'trainable': 'True', 'value': '0.19151406076785604'}}})}}, 'noise': None}]}]}, {'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'CNOT', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'X', 'qubit_support': (1,), 'tag': None, 'noise': None}], 'noise': None}]}]}]}\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def deserialize(d: dict, as_torch: bool = False) -&gt; SUPPORTED_TYPES:\n    \"\"\"\n    Supported Types:\n\n    AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module\n    Deserializes a dict to one of the supported types.\n\n    Arguments:\n        d (dict): A dict containing a serialized object.\n        as_torch (bool): Whether to transform to torch for the deserialized object.\n    Returns:\n        AbstractBlock, QuantumCircuit, QuantumModel, Register, torch.nn.Module.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence import serialize, deserialize, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    block_dict = serialize(myblock)\n    print(block_dict)\n\n    ## Lets use myblock in a QuantumCircuit and serialize it.\n\n    qc = QuantumCircuit(n_qubits, myblock)\n    qc_dict = serialize(qc)\n    qc_deserialized = deserialize(qc_dict)\n    assert qc == qc_deserialized\n\n    ## Finally, let's wrap it in a QuantumModel\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\n    qm_dict = serialize(qm)\n    qm_deserialized = deserialize(qm_dict)\n    # Lets check if the loaded QuantumModel returns the same expectation\n    assert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n    ```\n    \"\"\"\n    obj: SerializationModel\n    if d.get(\"expression\"):\n        obj = ExpressionSerialization(d)\n    elif d.get(\"block\") and d.get(\"register\"):\n        obj = QuantumCircuitSerialization(d)\n    elif d.get(\"graph\"):\n        obj = RegisterSerialization(d)\n    elif d.get(\"type\"):\n        obj = BlockTypeSerialization(d)\n    else:\n        obj = ModelSerialization(d, as_torch=as_torch)\n    return obj.value\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.load","title":"<code>load(file_path, map_location='cpu')</code>","text":"<p>Same as serialize/deserialize but for storing/loading files.</p> <p>Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register Loads a .json or .pt file to one of the supported types.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>The name of the file.</p> <p> TYPE: <code>str</code> </p> <code>map_location</code> <p>In case of a .pt file, on which device to load the object (cpu,cuda).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'cpu'</code> </p> <p>Returns:     A object of type AbstractBlock, QuantumCircuit, QuantumModel, Register.</p> <p>Examples: <pre><code>import torch\nfrom pathlib import Path\nimport os\n\nfrom qadence import save, load, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nqc = QuantumCircuit(n_qubits, myblock)\n# Lets store the circuit in a json file\nsave(qc, '.', 'circ')\nloaded_qc = load(Path('circ.json'))\nqc == loaded_qc\nos.remove('circ.json')\n## Let's wrap it in a QuantumModel and store that\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\nsave(qm, folder= '.',file_name= 'quantum_model')\nqm_loaded = load('quantum_model.json')\nos.remove('quantum_model.json')\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def load(file_path: str | Path, map_location: str = \"cpu\") -&gt; SUPPORTED_TYPES:\n    \"\"\"\n    Same as serialize/deserialize but for storing/loading files.\n\n    Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register\n    Loads a .json or .pt file to one of the supported types.\n\n    Arguments:\n        file_path (str): The name of the file.\n        map_location (str): In case of a .pt file, on which device to load the object (cpu,cuda).\n    Returns:\n        A object of type AbstractBlock, QuantumCircuit, QuantumModel, Register.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from pathlib import Path\n    import os\n\n    from qadence import save, load, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    qc = QuantumCircuit(n_qubits, myblock)\n    # Lets store the circuit in a json file\n    save(qc, '.', 'circ')\n    loaded_qc = load(Path('circ.json'))\n    qc == loaded_qc\n    os.remove('circ.json')\n    ## Let's wrap it in a QuantumModel and store that\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n    save(qm, folder= '.',file_name= 'quantum_model')\n    qm_loaded = load('quantum_model.json')\n    os.remove('quantum_model.json')\n    ```\n    \"\"\"\n    d = {}\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n    if not os.path.exists(file_path):\n        logger.error(f\"File {file_path} not found.\")\n        raise FileNotFoundError\n    FORMAT = file_extension(file_path)\n    _, _, load_fn, _ = FORMAT_DICT[FORMAT]  # type: ignore[index]\n    try:\n        d = load_fn(file_path, map_location)\n        logger.debug(f\"Successfully loaded {d} from {file_path}.\")\n    except Exception as e:\n        logger.error(f\"Unable to load Object from {file_path} due to {e}\")\n    return deserialize(d)\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.parse_expr_fn","title":"<code>parse_expr_fn(code)</code>","text":"<p>A parsing expressions function that checks whether a given code is valid on.</p> <p>the parsing grammar. The grammar is defined to be compatible with <code>sympy</code> expressions, such as <code>Float('-0.33261030434342942', precision=53)</code>, while avoiding code injection such as <code>2*3</code> or <code>__import__('os').system('ls -la')</code>.</p> PARAMETER DESCRIPTION <code>code</code> <p>code to be parsed and checked.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>Boolean indicating whether the code matches the defined grammar or not.</p> Source code in <code>qadence/serialization.py</code> <pre><code>def parse_expr_fn(code: str) -&gt; bool:\n    \"\"\"\n    A parsing expressions function that checks whether a given code is valid on.\n\n    the parsing grammar. The grammar is defined to be compatible with `sympy`\n    expressions, such as `Float('-0.33261030434342942', precision=53)`, while\n    avoiding code injection such as `2*3` or `__import__('os').system('ls -la')`.\n\n    Args:\n        code (str): code to be parsed and checked.\n\n    Returns:\n        Boolean indicating whether the code matches the defined grammar or not.\n    \"\"\"\n\n    parser = _parsing_serialize_expr\n    try:\n        parser.parse(code)\n    except NoMatch:\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.save","title":"<code>save(obj, folder, file_name='', format=SerializationFormat.JSON)</code>","text":"<p>Same as serialize/deserialize but for storing/loading files.</p> <p>Supported types: AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module Saves a qadence object to a json/.pt.</p> PARAMETER DESCRIPTION <code>obj</code> <pre><code>Either AbstractBlock, QuantumCircuit, QuantumModel, Register.\n</code></pre> <p> TYPE: <code>AbstractBlock | QuantumCircuit | QuantumModel | Register</code> </p> <code>file_name</code> <p>The name of the file.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>format</code> <p>The type of file to save.</p> <p> TYPE: <code>str</code> DEFAULT: <code>JSON</code> </p> <p>Returns:     None.</p> <p>Examples: <pre><code>import torch\nfrom pathlib import Path\nimport os\n\nfrom qadence import save, load, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nqc = QuantumCircuit(n_qubits, myblock)\n# Lets store the circuit in a json file\nsave(qc, '.', 'circ')\nloaded_qc = load(Path('circ.json'))\nqc == loaded_qc\nos.remove('circ.json')\n## Let's wrap it in a QuantumModel and store that\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\nsave(qm, folder= '.',file_name= 'quantum_model')\nqm_loaded = load('quantum_model.json')\nos.remove('quantum_model.json')\n</code></pre> <pre><code>\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def save(\n    obj: SUPPORTED_TYPES,\n    folder: str | Path,\n    file_name: str = \"\",\n    format: SerializationFormat = SerializationFormat.JSON,\n) -&gt; None:\n    \"\"\"\n    Same as serialize/deserialize but for storing/loading files.\n\n    Supported types:\n    AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module\n    Saves a qadence object to a json/.pt.\n\n    Arguments:\n        obj (AbstractBlock | QuantumCircuit | QuantumModel | Register):\n                Either AbstractBlock, QuantumCircuit, QuantumModel, Register.\n        file_name (str): The name of the file.\n        format (str): The type of file to save.\n    Returns:\n        None.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from pathlib import Path\n    import os\n\n    from qadence import save, load, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    qc = QuantumCircuit(n_qubits, myblock)\n    # Lets store the circuit in a json file\n    save(qc, '.', 'circ')\n    loaded_qc = load(Path('circ.json'))\n    qc == loaded_qc\n    os.remove('circ.json')\n    ## Let's wrap it in a QuantumModel and store that\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n    save(qm, folder= '.',file_name= 'quantum_model')\n    qm_loaded = load('quantum_model.json')\n    os.remove('quantum_model.json')\n    ```\n    \"\"\"\n    if not isinstance(obj, get_args(SUPPORTED_TYPES)):\n        logger.error(f\"Serialization of object type {type(obj)} not supported.\")\n    folder = Path(folder)\n    if not folder.is_dir():\n        logger.error(NotADirectoryError)\n    if file_name == \"\":\n        file_name = type(obj).__name__\n    try:\n        suffix, save_fn, _, save_params = FORMAT_DICT[format]\n        d = serialize(obj, save_params)\n        file_path = folder / Path(file_name + suffix)\n        save_fn(d, file_path)\n        logger.debug(f\"Successfully saved {obj} from to {folder}.\")\n    except Exception as e:\n        logger.error(f\"Unable to write {type(obj)} to disk due to {e}\")\n</code></pre>"},{"location":"api/serialization/#qadence.serialization.serialize","title":"<code>serialize(obj, save_params=False)</code>","text":"<p>Supported Types:</p> <p>AbstractBlock | QuantumCircuit | QuantumModel | torch.nn.Module | Register | Module Serializes a qadence object to a dictionary.</p> PARAMETER DESCRIPTION <code>obj</code> <p> TYPE: <code>AbstractBlock | QuantumCircuit | QuantumModel | Register | Module</code> </p> <p>Returns:     A dict.</p> <p>Examples: <pre><code>import torch\nfrom qadence import serialize, deserialize, hea, hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel\n\nn_qubits = 2\nmyblock = hea(n_qubits=n_qubits, depth=1)\nblock_dict = serialize(myblock)\nprint(block_dict)\n\n## Lets use myblock in a QuantumCircuit and serialize it.\n\nqc = QuantumCircuit(n_qubits, myblock)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n## Finally, let's wrap it in a QuantumModel\nobs = hamiltonian_factory(n_qubits, detuning = Z)\nqm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n# Lets check if the loaded QuantumModel returns the same expectation\nassert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n</code></pre> <pre><code>{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': 'HEA', 'blocks': [{'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('65b3f2f6-4389-4559-88a1-1d7abf082ed2', {'name': 'theta_0', 'expression': \"Parameter('theta_0')\", 'symbols': {'theta_0': {'name': 'theta_0', 'trainable': 'True', 'value': '0.6989043180715112'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('3dac6e78-58d1-4a11-a1d6-18442e4a2d35', {'name': 'theta_1', 'expression': \"Parameter('theta_1')\", 'symbols': {'theta_1': {'name': 'theta_1', 'trainable': 'True', 'value': '0.6384918497393969'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RY', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('5d6c43de-4c98-47b7-b9a9-2a30e75e6d09', {'name': 'theta_2', 'expression': \"Parameter('theta_2')\", 'symbols': {'theta_2': {'name': 'theta_2', 'trainable': 'True', 'value': '0.5116847846883059'}}})}}, 'noise': None}, {'type': 'RY', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('961e37e4-920b-43d3-b254-ef89718488ab', {'name': 'theta_3', 'expression': \"Parameter('theta_3')\", 'symbols': {'theta_3': {'name': 'theta_3', 'trainable': 'True', 'value': '0.7920525595818162'}}})}}, 'noise': None}]}, {'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'RX', 'qubit_support': (0,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('26c16eb9-2c69-46b8-b57c-4e6731c0f37a', {'name': 'theta_4', 'expression': \"Parameter('theta_4')\", 'symbols': {'theta_4': {'name': 'theta_4', 'trainable': 'True', 'value': '0.9888183631572813'}}})}}, 'noise': None}, {'type': 'RX', 'qubit_support': (1,), 'tag': None, 'parameters': {'_name_dict': {'parameter': ('42043050-d1f9-4c36-95a9-7a13e168d6cc', {'name': 'theta_5', 'expression': \"Parameter('theta_5')\", 'symbols': {'theta_5': {'name': 'theta_5', 'trainable': 'True', 'value': '0.13235765857339998'}}})}}, 'noise': None}]}]}, {'type': 'ChainBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'KronBlock', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'CNOT', 'qubit_support': (0, 1), 'tag': None, 'blocks': [{'type': 'X', 'qubit_support': (1,), 'tag': None, 'noise': None}], 'noise': None}]}]}]}\n</code></pre> </p> Source code in <code>qadence/serialization.py</code> <pre><code>def serialize(obj: SUPPORTED_TYPES, save_params: bool = False) -&gt; dict:\n    \"\"\"\n    Supported Types:\n\n    AbstractBlock | QuantumCircuit | QuantumModel | torch.nn.Module | Register | Module\n    Serializes a qadence object to a dictionary.\n\n    Arguments:\n        obj (AbstractBlock | QuantumCircuit | QuantumModel | Register | torch.nn.Module):\n    Returns:\n        A dict.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    import torch\n    from qadence import serialize, deserialize, hea, hamiltonian_factory, Z\n    from qadence import QuantumCircuit, QuantumModel\n\n    n_qubits = 2\n    myblock = hea(n_qubits=n_qubits, depth=1)\n    block_dict = serialize(myblock)\n    print(block_dict)\n\n    ## Lets use myblock in a QuantumCircuit and serialize it.\n\n    qc = QuantumCircuit(n_qubits, myblock)\n    qc_dict = serialize(qc)\n    qc_deserialized = deserialize(qc_dict)\n    assert qc == qc_deserialized\n\n    ## Finally, let's wrap it in a QuantumModel\n    obs = hamiltonian_factory(n_qubits, detuning = Z)\n    qm = QuantumModel(qc, obs, backend='pyqtorch', diff_mode='ad')\n\n    qm_dict = serialize(qm)\n    qm_deserialized = deserialize(qm_dict)\n    # Lets check if the loaded QuantumModel returns the same expectation\n    assert torch.isclose(qm.expectation({}), qm_deserialized.expectation({}))\n    ```\n    \"\"\"\n    if not isinstance(obj, get_args(SUPPORTED_TYPES)):\n        logger.error(TypeError(f\"Serialization of object type {type(obj)} not supported.\"))\n\n    d: dict = dict()\n    try:\n        if isinstance(obj, core.Expr):\n            symb_dict = dict()\n            expr_dict = {\"name\": str(obj), \"expression\": srepr(obj)}\n            symbs: set[Parameter | core.Basic] = obj.free_symbols\n            if symbs:\n                symb_dict = {\"symbols\": {str(s): s._to_dict() for s in symbs}}\n            d = {**expr_dict, **symb_dict}\n        else:\n            if hasattr(obj, \"_to_dict\"):\n                model_to_dict: Callable = obj._to_dict\n                d = (\n                    model_to_dict(save_params)\n                    if isinstance(obj, torch.nn.Module)\n                    else model_to_dict()\n                )\n            elif hasattr(obj, \"state_dict\"):\n                d = {type(obj).__name__: obj.state_dict()}\n            else:\n                raise ValueError(f\"Cannot serialize object {obj}.\")\n    except Exception as e:\n        logger.error(f\"Serialization of object {obj} failed due to {e}\")\n    return d\n</code></pre>"},{"location":"api/states/","title":"State preparation","text":""},{"location":"api/states/#state-preparation-routines","title":"State Preparation Routines","text":""},{"location":"api/states/#qadence.states.density_mat","title":"<code>density_mat(state)</code>","text":"<p>Computes the density matrix from a pure state vector.</p> PARAMETER DESCRIPTION <code>state</code> <p>The pure state vector :math:<code>|\\psi\\rangle</code>.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>The density matrix :math:<code>\\rho = |\\psi \\rangle \\langle\\psi|</code>.</p> <p> TYPE: <code>DensityMatrix</code> </p> Source code in <code>qadence/states.py</code> <pre><code>def density_mat(state: Tensor) -&gt; DensityMatrix:\n    \"\"\"\n    Computes the density matrix from a pure state vector.\n\n    Arguments:\n        state: The pure state vector :math:`|\\\\psi\\\\rangle`.\n\n    Returns:\n        Tensor: The density matrix :math:`\\\\rho = |\\psi \\\\rangle \\\\langle\\\\psi|`.\n    \"\"\"\n    if isinstance(state, DensityMatrix):\n        return state\n    return DensityMatrix(torch.einsum(\"bi,bj-&gt;bij\", (state, state.conj())))\n</code></pre>"},{"location":"api/states/#qadence.states.ghz_block","title":"<code>ghz_block(n_qubits)</code>","text":"<p>Generates the abstract ghz state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ChainBlock</code> <p>A ChainBlock representing the GHZ state.</p> <p>Examples: <pre><code>from qadence.states import ghz_block\n\nblock = ghz_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 CNOT(0, 1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def ghz_block(n_qubits: int) -&gt; ChainBlock:\n    \"\"\"\n    Generates the abstract ghz state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A ChainBlock representing the GHZ state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import ghz_block\n\n    block = ghz_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    cnots = chain(CNOT(i - 1, i) for i in range(1, n_qubits))\n    return chain(H(0), cnots)\n</code></pre>"},{"location":"api/states/#qadence.states.ghz_state","title":"<code>ghz_state(n_qubits, batch_size=1)</code>","text":"<p>Creates a GHZ state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>How many bitstrings to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import ghz_state\n\nprint(ghz_state(n_qubits=2, batch_size=2))\n</code></pre> <pre><code>tensor([[0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j],\n        [0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def ghz_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Creates a GHZ state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): How many bitstrings to use.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import ghz_state\n\n    print(ghz_state(n_qubits=2, batch_size=2))\n    ```\n    \"\"\"\n    norm = 1 / torch.sqrt(torch.tensor(2))\n    return norm * (zero_state(n_qubits, batch_size) + one_state(n_qubits, batch_size))\n</code></pre>"},{"location":"api/states/#qadence.states.is_normalized","title":"<code>is_normalized(wf, atol=NORMALIZATION_ATOL)</code>","text":"<p>Checks if a wave function is normalized.</p> PARAMETER DESCRIPTION <code>wf</code> <p>The wave function as a torch tensor.</p> <p> TYPE: <code>Tensor</code> </p> <code>atol</code> <p>The tolerance.</p> <p> TYPE: <code>float) </code> DEFAULT: <code>NORMALIZATION_ATOL</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>A bool.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, is_normalized\n\nprint(is_normalized(uniform_state(2)))\n</code></pre> <pre><code>True\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def is_normalized(wf: Tensor, atol: float = NORMALIZATION_ATOL) -&gt; bool:\n    \"\"\"\n    Checks if a wave function is normalized.\n\n    Arguments:\n        wf (torch.Tensor): The wave function as a torch tensor.\n        atol (float) : The tolerance.\n\n    Returns:\n        A bool.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, is_normalized\n\n    print(is_normalized(uniform_state(2)))\n    ```\n    \"\"\"\n    if wf.dim() == 1:\n        wf = wf.unsqueeze(0)\n    sum_probs: Tensor = (wf.abs() ** 2).sum(dim=1)\n    ones = torch.ones_like(sum_probs)\n    return torch.allclose(sum_probs, ones, rtol=0.0, atol=atol)  # type: ignore[no-any-return]\n</code></pre>"},{"location":"api/states/#qadence.states.normalize","title":"<code>normalize(wf)</code>","text":"<p>Normalizes a wavefunction or batch of wave functions.</p> PARAMETER DESCRIPTION <code>wf</code> <p>Normalized wavefunctions.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, normalize\n\nprint(normalize(uniform_state(2, 2)))\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j],\n        [0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def normalize(wf: Tensor) -&gt; Tensor:\n    \"\"\"\n    Normalizes a wavefunction or batch of wave functions.\n\n    Arguments:\n        wf (torch.Tensor): Normalized wavefunctions.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, normalize\n\n    print(normalize(uniform_state(2, 2)))\n    ```\n    \"\"\"\n    if wf.dim() == 1:\n        return wf / torch.sqrt((wf.abs() ** 2).sum())\n    else:\n        return wf / torch.sqrt((wf.abs() ** 2).sum(1)).unsqueeze(1)\n</code></pre>"},{"location":"api/states/#qadence.states.one_block","title":"<code>one_block(n_qubits)</code>","text":"<p>Generates the abstract one state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the one state.</p> <p>Examples: <pre><code>from qadence.states import one_block\n\nblock = one_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def one_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract one state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the one state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import one_block\n\n    block = one_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(X, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.one_state","title":"<code>one_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the one state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import one_state\n\nstate = one_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def one_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the one state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): The batch size.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import one_state\n\n    state = one_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    bitstring = \"1\" * n_qubits\n    return _state_from_bitstring(bitstring, batch_size)\n</code></pre>"},{"location":"api/states/#qadence.states.pmf","title":"<code>pmf(wf)</code>","text":"<p>Converts a wave function into a torch Distribution.</p> PARAMETER DESCRIPTION <code>wf</code> <p>The wave function as a torch tensor.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Distribution</code> <p>A torch.distributions.Distribution.</p> <p>Examples: <pre><code>from qadence.states import uniform_state, pmf\n\nprint(pmf(uniform_state(2)).probs)\n</code></pre> <pre><code>tensor([[0.2500, 0.2500, 0.2500, 0.2500]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def pmf(wf: Tensor) -&gt; Distribution:\n    \"\"\"\n    Converts a wave function into a torch Distribution.\n\n    Arguments:\n        wf (torch.Tensor): The wave function as a torch tensor.\n\n    Returns:\n        A torch.distributions.Distribution.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state, pmf\n\n    print(pmf(uniform_state(2)).probs)\n    ```\n    \"\"\"\n    return Categorical(torch.abs(torch.pow(wf, 2)))\n</code></pre>"},{"location":"api/states/#qadence.states.product_block","title":"<code>product_block(bitstring)</code>","text":"<p>Creates an abstract product state from a bitstring.</p> PARAMETER DESCRIPTION <code>bitstring</code> <p>A bitstring.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the product state.</p> <p>Examples: <pre><code>from qadence.states import product_block\n\nprint(product_block(\"1100\"))\n</code></pre> <pre><code>KronBlock(0,1,2,3)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u251c\u2500\u2500 I(2)\n\u2514\u2500\u2500 I(3)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def product_block(bitstring: str) -&gt; KronBlock:\n    \"\"\"\n    Creates an abstract product state from a bitstring.\n\n    Arguments:\n        bitstring (str): A bitstring.\n\n    Returns:\n        A KronBlock representing the product state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import product_block\n\n    print(product_block(\"1100\"))\n    ```\n    \"\"\"\n    return _block_from_bitstring(bitstring)\n</code></pre>"},{"location":"api/states/#qadence.states.product_state","title":"<code>product_state(bitstring, batch_size=1, endianness=Endianness.BIG, backend=BackendName.PYQTORCH)</code>","text":"<p>Creates a product state from a bitstring.</p> PARAMETER DESCRIPTION <code>bitstring</code> <p>A bitstring.</p> <p> TYPE: <code>str</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int) </code> DEFAULT: <code>1</code> </p> <code>backend</code> <p>The backend to use. Default is \"pyqtorch\".</p> <p> TYPE: <code>BackendName</code> DEFAULT: <code>PYQTORCH</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import product_state\n\nprint(product_state(\"1100\", backend=\"pyqtorch\"))\nprint(product_state(\"1100\", backend=\"horqrux\"))\n</code></pre> <pre><code>tensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j,\n         1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j\n 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>@singledispatch\ndef product_state(\n    bitstring: str,\n    batch_size: int = 1,\n    endianness: Endianness = Endianness.BIG,\n    backend: BackendName = BackendName.PYQTORCH,\n) -&gt; ArrayLike:\n    \"\"\"\n    Creates a product state from a bitstring.\n\n    Arguments:\n        bitstring (str): A bitstring.\n        batch_size (int) : Batch size.\n        backend (BackendName): The backend to use. Default is \"pyqtorch\".\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import product_state\n\n    print(product_state(\"1100\", backend=\"pyqtorch\"))\n    print(product_state(\"1100\", backend=\"horqrux\"))\n    ```\n    \"\"\"\n    if batch_size:\n        logger.debug(\n            \"The input `batch_size` is going to be deprecated. \"\n            \"For now, default batch_size is set to 1.\"\n        )\n    return run(product_block(bitstring), backend=backend, endianness=endianness)\n</code></pre>"},{"location":"api/states/#qadence.states.rand_bitstring","title":"<code>rand_bitstring(N)</code>","text":"<p>Creates a random bistring.</p> PARAMETER DESCRIPTION <code>N</code> <p>The length of the bitstring.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A string.</p> <p>Examples: <pre><code>from qadence.states import rand_bitstring\n\nprint(rand_bitstring(N=8))\n</code></pre> <pre><code>10111011\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_bitstring(N: int) -&gt; str:\n    \"\"\"\n    Creates a random bistring.\n\n    Arguments:\n        N (int): The length of the bitstring.\n\n    Returns:\n        A string.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_bitstring\n\n    print(rand_bitstring(N=8))\n    ```\n    \"\"\"\n    return \"\".join(str(random.randint(0, 1)) for _ in range(N))\n</code></pre>"},{"location":"api/states/#qadence.states.rand_product_block","title":"<code>rand_product_block(n_qubits)</code>","text":"<p>Creates a block representing a random abstract product state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the product state.</p> <p>Examples: <pre><code>from qadence.states import rand_product_block\n\nprint(rand_product_block(n_qubits=2))\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 I(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_product_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Creates a block representing a random abstract product state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the product state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_product_block\n\n    print(rand_product_block(n_qubits=2))\n    ```\n    \"\"\"\n    return product_block(rand_bitstring(n_qubits))\n</code></pre>"},{"location":"api/states/#qadence.states.rand_product_state","title":"<code>rand_product_state(n_qubits, batch_size=1)</code>","text":"<p>Creates a random product state.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>How many bitstrings to use.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import rand_product_state\n\nprint(rand_product_state(n_qubits=2, batch_size=2))\n</code></pre> <pre><code>tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def rand_product_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Creates a random product state.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): How many bitstrings to use.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import rand_product_state\n\n    print(rand_product_state(n_qubits=2, batch_size=2))\n    ```\n    \"\"\"\n    wf_batch = torch.zeros(batch_size, 2**n_qubits, dtype=DTYPE)\n    rand_pos = torch.randint(0, 2**n_qubits, (batch_size,))\n    wf_batch[torch.arange(batch_size), rand_pos] = torch.tensor(1.0 + 0j, dtype=DTYPE)\n    return wf_batch\n</code></pre>"},{"location":"api/states/#qadence.states.random_state","title":"<code>random_state(n_qubits, batch_size=1, backend=BackendName.PYQTORCH, type=StateGeneratorType.HAAR_MEASURE_FAST)</code>","text":"<p>Generates a random state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>backend</code> <p>The backend to use.</p> <p> TYPE: <code>str</code> DEFAULT: <code>PYQTORCH</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>type</code> <p>StateGeneratorType.</p> <p> DEFAULT: <code>HAAR_MEASURE_FAST</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import random_state, StateGeneratorType\nfrom qadence.states import random_state, is_normalized, pmf\nfrom qadence.types import BackendName\nfrom torch.distributions import Distribution\n\n### We have the following options:\nprint([g.value for g in StateGeneratorType])\n\nn_qubits = 2\n# The default is StateGeneratorType.HAAR_MEASURE_FAST\nstate = random_state(n_qubits=n_qubits)\nprint(state)\n\n### Lets initialize a state using random rotations, i.e., StateGeneratorType.RANDOM_ROTATIONS.\nrandom = random_state(n_qubits=n_qubits, type=StateGeneratorType.RANDOM_ROTATIONS)\nprint(random)\n</code></pre> <pre><code>['RandomRotations', 'HaarMeasureFast', 'HaarMeasureSlow']\ntensor([[-0.0075-0.0796j, -0.6158-0.1680j,  0.6531-0.2728j, -0.0566-0.2864j]])\ntensor([[0.8141+0.5786j, 0.0000+0.0000j, 0.0288-0.0405j, 0.0000+0.0000j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def random_state(\n    n_qubits: int,\n    batch_size: int = 1,\n    backend: str = BackendName.PYQTORCH,\n    type: StateGeneratorType = StateGeneratorType.HAAR_MEASURE_FAST,\n) -&gt; Tensor:\n    \"\"\"\n    Generates a random state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        backend (str): The backend to use.\n        batch_size (int): The batch size.\n        type : StateGeneratorType.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import random_state, StateGeneratorType\n    from qadence.states import random_state, is_normalized, pmf\n    from qadence.types import BackendName\n    from torch.distributions import Distribution\n\n    ### We have the following options:\n    print([g.value for g in StateGeneratorType])\n\n    n_qubits = 2\n    # The default is StateGeneratorType.HAAR_MEASURE_FAST\n    state = random_state(n_qubits=n_qubits)\n    print(state)\n\n    ### Lets initialize a state using random rotations, i.e., StateGeneratorType.RANDOM_ROTATIONS.\n    random = random_state(n_qubits=n_qubits, type=StateGeneratorType.RANDOM_ROTATIONS)\n    print(random)\n    ```\n    \"\"\"\n\n    if type == StateGeneratorType.HAAR_MEASURE_FAST:\n        state = concat(tuple(_rand_haar_fast(n_qubits) for _ in range(batch_size)), dim=0)\n    elif type == StateGeneratorType.HAAR_MEASURE_SLOW:\n        state = concat(tuple(_rand_haar_slow(n_qubits) for _ in range(batch_size)), dim=0)\n    elif type == StateGeneratorType.RANDOM_ROTATIONS:\n        state = run(_abstract_random_state(n_qubits, batch_size))  # type: ignore\n    assert all(list(map(is_normalized, state)))\n    return state\n</code></pre>"},{"location":"api/states/#qadence.states.uniform_block","title":"<code>uniform_block(n_qubits)</code>","text":"<p>Generates the abstract uniform state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the uniform state.</p> <p>Examples: <pre><code>from qadence.states import uniform_block\n\nblock = uniform_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 H(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def uniform_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract uniform state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the uniform state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_block\n\n    block = uniform_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(H, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.uniform_state","title":"<code>uniform_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the uniform state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import uniform_state\n\nstate = uniform_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def uniform_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the uniform state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n        batch_size (int): The batch size.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import uniform_state\n\n    state = uniform_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    norm = 1 / torch.sqrt(torch.tensor(2**n_qubits))\n    return norm * torch.ones(batch_size, 2**n_qubits, dtype=DTYPE)\n</code></pre>"},{"location":"api/states/#qadence.states.zero_block","title":"<code>zero_block(n_qubits)</code>","text":"<p>Generates the abstract zero state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>KronBlock</code> <p>A KronBlock representing the zero state.</p> <p>Examples: <pre><code>from qadence.states import zero_block\n\nblock = zero_block(n_qubits=2)\nprint(block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 I(0)\n\u2514\u2500\u2500 I(1)\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def zero_block(n_qubits: int) -&gt; KronBlock:\n    \"\"\"\n    Generates the abstract zero state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits.\n\n    Returns:\n        A KronBlock representing the zero state.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import zero_block\n\n    block = zero_block(n_qubits=2)\n    print(block)\n    ```\n    \"\"\"\n    return _from_op(I, n_qubits=n_qubits)\n</code></pre>"},{"location":"api/states/#qadence.states.zero_state","title":"<code>zero_state(n_qubits, batch_size=1)</code>","text":"<p>Generates the zero state for a specified number of qubits.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>The number of qubits for which the zero state is to be generated.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>The batch size for the zero state.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>A torch.Tensor.</p> <p>Examples: <pre><code>from qadence.states import zero_state\n\nstate = zero_state(n_qubits=2)\nprint(state)\n</code></pre> <pre><code>tensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> </p> Source code in <code>qadence/states.py</code> <pre><code>def zero_state(n_qubits: int, batch_size: int = 1) -&gt; Tensor:\n    \"\"\"\n    Generates the zero state for a specified number of qubits.\n\n    Arguments:\n        n_qubits (int): The number of qubits for which the zero state is to be generated.\n        batch_size (int): The batch size for the zero state.\n\n    Returns:\n        A torch.Tensor.\n\n    Examples:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence.states import zero_state\n\n    state = zero_state(n_qubits=2)\n    print(state)\n    ```\n    \"\"\"\n    bitstring = \"0\" * n_qubits\n    return _state_from_bitstring(bitstring, batch_size)\n</code></pre>"},{"location":"api/transpile/","title":"Transpilation","text":"<p>Contains functions that operate on blocks and circuits to <code>transpile</code> them to new blocks/circuits.</p>"},{"location":"api/transpile/#qadence.transpile.transpile.transpile","title":"<code>transpile(*fs)</code>","text":"<p><code>AbstractBlock</code> or <code>QuantumCircuit</code> transpilation.</p> <p>Compose functions that accept a circuit/block and returns a circuit/block.</p> PARAMETER DESCRIPTION <code>*fs</code> <p>composable functions that either map blocks to blocks (<code>Callable[[AbstractBlock], AbstractBlock]</code>) or circuits to circuits (<code>Callable[[QuantumCircuit], QuantumCircuit]</code>).</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Callable</code> <p>Composed function.</p> <p>Examples:</p> <p>Flatten a block of nested chains and krons: <pre><code>from qadence import *\nfrom qadence.transpile import transpile, flatten, scale_primitive_blocks_only\n\nb = chain(2 * chain(chain(X(0), Y(0))), kron(kron(X(0), X(1))))\nprint(b)\n\n# both flatten and scale_primitive_blocks_only are functions that accept and\n# return a block\nt = transpile(flatten, scale_primitive_blocks_only)(b)\nprint(t)\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 [mul: 2] \n\u2502   \u2514\u2500\u2500 ChainBlock(0)\n\u2502       \u2514\u2500\u2500 ChainBlock(0)\n\u2502           \u251c\u2500\u2500 X(0)\n\u2502           \u2514\u2500\u2500 Y(0)\n\u2514\u2500\u2500 KronBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u251c\u2500\u2500 X(0)\n        \u2514\u2500\u2500 X(1)\n\nChainBlock(0,1)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u251c\u2500\u2500 Y(0)\n\u2514\u2500\u2500 KronBlock(0,1)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 X(1)\n</code></pre> </p> <p>We also proved a decorator to easily turn a function <code>Callable[[AbstractBlock], AbstractBlock]</code> into a <code>Callable[[QuantumCircuit], QuantumCircuit]</code> to be used in circuit transpilation. <pre><code>from qadence import *\nfrom qadence.transpile import transpile, blockfn_to_circfn, flatten\n\n# We want to pass this circuit to `transpile` instead of a block,\n# so we need functions that map from a circuit to a circuit.\ncirc = QuantumCircuit(2, chain(chain(X(0), chain(X(1)))))\n\n@blockfn_to_circfn\ndef fn(block):\n    # un-decorated function accepts a block and returns a block\n    return block * block\n\ntransp = transpile(\n    # the decorated function accepts a circuit and returns a circuit\n    fn,\n    # already existing functions can also be decorated\n    blockfn_to_circfn(flatten)\n)\nprint(transp(circ))\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 X(0)\n\u2502   \u2514\u2500\u2500 X(1)\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 X(1)\n</code></pre> </p> Source code in <code>qadence/transpile/transpile.py</code> <pre><code>def transpile(*fs: Callable) -&gt; Callable:\n    \"\"\"`AbstractBlock` or `QuantumCircuit` transpilation.\n\n    Compose functions that\n    accept a circuit/block and returns a circuit/block.\n\n    Arguments:\n        *fs: composable functions that either map blocks to blocks\n            (`Callable[[AbstractBlock], AbstractBlock]`)\n            or circuits to circuits (`Callable[[QuantumCircuit], QuantumCircuit]`).\n\n    Returns:\n        Composed function.\n\n    Examples:\n\n    Flatten a block of nested chains and krons:\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import *\n    from qadence.transpile import transpile, flatten, scale_primitive_blocks_only\n\n    b = chain(2 * chain(chain(X(0), Y(0))), kron(kron(X(0), X(1))))\n    print(b)\n    print() # markdown-exec: hide\n\n    # both flatten and scale_primitive_blocks_only are functions that accept and\n    # return a block\n    t = transpile(flatten, scale_primitive_blocks_only)(b)\n    print(t)\n    ```\n\n    We also proved a decorator to easily turn a function `Callable[[AbstractBlock], AbstractBlock]`\n    into a `Callable[[QuantumCircuit], QuantumCircuit]` to be used in circuit transpilation.\n    ```python exec=\"on\" source=\"material-block\" result=\"json\"\n    from qadence import *\n    from qadence.transpile import transpile, blockfn_to_circfn, flatten\n\n    # We want to pass this circuit to `transpile` instead of a block,\n    # so we need functions that map from a circuit to a circuit.\n    circ = QuantumCircuit(2, chain(chain(X(0), chain(X(1)))))\n\n    @blockfn_to_circfn\n    def fn(block):\n        # un-decorated function accepts a block and returns a block\n        return block * block\n\n    transp = transpile(\n        # the decorated function accepts a circuit and returns a circuit\n        fn,\n        # already existing functions can also be decorated\n        blockfn_to_circfn(flatten)\n    )\n    print(transp(circ))\n    ```\n    \"\"\"\n    return lambda x: reduce(lambda acc, f: f(acc), reversed(fs), x)\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.chain_single_qubit_ops","title":"<code>chain_single_qubit_ops(block)</code>","text":"<p>Transpile a chain of krons into a kron of chains of single qubit operations.</p> <p>Examples: <pre><code>from qadence import hea\nfrom qadence.transpile.block import chain_single_qubit_ops\n\n# Consider a single HEA layer\nblock = hea(2,1)\nprint(block)\n\n# After applying chain_single_qubit_ops, we get:\nprint(chain_single_qubit_ops(block))\n</code></pre> <pre><code>ChainBlock(0,1) [tag: HEA]\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n\u2502   \u2502   \u2514\u2500\u2500 RX(1) [params: ['theta_1']]\n\u2502   \u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_2']]\n\u2502   \u2502   \u2514\u2500\u2500 RY(1) [params: ['theta_3']]\n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 RX(0) [params: ['theta_4']]\n\u2502       \u2514\u2500\u2500 RX(1) [params: ['theta_5']]\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u2514\u2500\u2500 CNOT(0, 1)\nChainBlock(0,1)\n\u251c\u2500\u2500 KronBlock(0,1)\n\u2502   \u251c\u2500\u2500 ChainBlock(0)\n\u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n\u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_2']]\n\u2502   \u2502   \u2514\u2500\u2500 RX(0) [params: ['theta_4']]\n\u2502   \u2514\u2500\u2500 ChainBlock(1)\n\u2502       \u251c\u2500\u2500 RX(1) [params: ['theta_1']]\n\u2502       \u251c\u2500\u2500 RY(1) [params: ['theta_3']]\n\u2502       \u2514\u2500\u2500 RX(1) [params: ['theta_5']]\n\u2514\u2500\u2500 ChainBlock(0,1)\n    \u2514\u2500\u2500 KronBlock(0,1)\n        \u2514\u2500\u2500 CNOT(0, 1)\n</code></pre></p> Source code in <code>qadence/transpile/block.py</code> <pre><code>def chain_single_qubit_ops(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Transpile a chain of krons into a kron of chains of single qubit operations.\n\n    Examples:\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import hea\n    from qadence.transpile.block import chain_single_qubit_ops\n\n    # Consider a single HEA layer\n    block = hea(2,1)\n    print(block)\n\n    # After applying chain_single_qubit_ops, we get:\n    print(chain_single_qubit_ops(block))\n    ```\n    \"\"\"\n    if is_chain_of_primitivekrons(block):\n        try:\n            return kron(*map(lambda bs: chain(*bs), zip(*block)))  # type: ignore[misc]\n        except Exception as e:\n            logger.debug(\n                f\"Unable to transpile {block} using chain_single_qubit_ops\\\n                         due to {e}. Returning original circuit.\"\n            )\n            return block\n\n    elif isinstance(block, CompositeBlock):\n        return _construct(type(block), tuple(chain_single_qubit_ops(b) for b in block.blocks))\n    else:\n        return block\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.scale_primitive_blocks_only","title":"<code>scale_primitive_blocks_only(block, scale=None)</code>","text":"<p>Push the scale all the way down into the leaves of the block tree.</p> <p>When given a scaled CompositeBlock consisting of several PrimitiveBlocks.</p> PARAMETER DESCRIPTION <code>block</code> <p>The block to be transpiled.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>scale</code> <p>An optional scale parameter. Only to be used for recursive calls internally.</p> <p> TYPE: <code>Basic</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>AbstractBlock</code> <p>A block of the same type where the scales have been moved into the subblocks.</p> <p> TYPE: <code>AbstractBlock</code> </p> <p>Examples:</p> <p>There are two different cases: <code>ChainBlock</code>s/<code>KronBlock</code>s: Only the first subblock needs to be scaled because chains/krons represent multiplications. <pre><code>from qadence import chain, X, RX\nfrom qadence.transpile import scale_primitive_blocks_only\nb = 2 * chain(X(0), RX(0, \"theta\"))\nprint(b)\n# After applying scale_primitive_blocks_only\nprint(scale_primitive_blocks_only(b))\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 ChainBlock(0)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\nChainBlock(0)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 RX(0) [params: ['theta']]\n</code></pre></p> <p><code>AddBlock</code>s: Consider 2 * add(X(0), RX(0, \"theta\")).  The scale needs to be added to all subblocks.  We get add(2 * X(0), 2 * RX(0, \"theta\")). <pre><code>from qadence import add, X, RX\nfrom qadence.transpile import scale_primitive_blocks_only\nb = 2 * add(X(0), RX(0, \"theta\"))\nprint(b)\n# After applying scale_primitive_blocks_only\nprint(scale_primitive_blocks_only(b))\n</code></pre> <pre><code>[mul: 2] \n\u2514\u2500\u2500 AddBlock(0)\n    \u251c\u2500\u2500 X(0)\n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\nAddBlock(0)\n\u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 [mul: 2.000] \n    \u2514\u2500\u2500 RX(0) [params: ['theta']]\n</code></pre></p> Source code in <code>qadence/transpile/block.py</code> <pre><code>@singledispatch\ndef scale_primitive_blocks_only(block: AbstractBlock, scale: sympy.Basic = None) -&gt; AbstractBlock:\n    \"\"\"Push the scale all the way down into the leaves of the block tree.\n\n    When given a scaled CompositeBlock consisting of several PrimitiveBlocks.\n\n    Arguments:\n        block: The block to be transpiled.\n        scale: An optional scale parameter. Only to be used for recursive calls internally.\n\n    Returns:\n        AbstractBlock: A block of the same type where the scales have been moved into the subblocks.\n\n    Examples:\n\n    There are two different cases:\n    `ChainBlock`s/`KronBlock`s: Only the first subblock needs to be scaled because chains/krons\n    represent multiplications.\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import chain, X, RX\n    from qadence.transpile import scale_primitive_blocks_only\n    b = 2 * chain(X(0), RX(0, \"theta\"))\n    print(b)\n    # After applying scale_primitive_blocks_only\n    print(scale_primitive_blocks_only(b))\n    ```\n\n    `AddBlock`s: Consider 2 * add(X(0), RX(0, \"theta\")).  The scale needs to be added to all\n    subblocks.  We get add(2 * X(0), 2 * RX(0, \"theta\")).\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence import add, X, RX\n    from qadence.transpile import scale_primitive_blocks_only\n    b = 2 * add(X(0), RX(0, \"theta\"))\n    print(b)\n    # After applying scale_primitive_blocks_only\n    print(scale_primitive_blocks_only(b))\n    ```\n    \"\"\"\n    raise NotImplementedError(f\"scale_primitive_blocks_only is not implemented for {type(block)}\")\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.set_trainable","title":"<code>set_trainable(blocks, value=True, inplace=True)</code>","text":"<p>Set the trainability of all parameters in a block to a given value.</p> PARAMETER DESCRIPTION <code>blocks</code> <p>Block or list of blocks for which to set the trainable attribute</p> <p> TYPE: <code>AbstractBlock | list[AbstractBlock]</code> </p> <code>value</code> <p>The value of the trainable attribute to assign to the input blocks</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>inplace</code> <p>Whether to modify the block(s) in place or not. Currently, only</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>if the <code>inplace</code> argument is set to False, the function will raise  this exception</p> RETURNS DESCRIPTION <code>AbstractBlock | list[AbstractBlock]</code> <p>AbstractBlock | list[AbstractBlock]: the input block or list of blocks with the trainable attribute set to the given value</p> Source code in <code>qadence/transpile/block.py</code> <pre><code>def set_trainable(\n    blocks: AbstractBlock | list[AbstractBlock], value: bool = True, inplace: bool = True\n) -&gt; AbstractBlock | list[AbstractBlock]:\n    \"\"\"Set the trainability of all parameters in a block to a given value.\n\n    Args:\n        blocks (AbstractBlock | list[AbstractBlock]): Block or list of blocks for which\n            to set the trainable attribute\n        value (bool, optional): The value of the trainable attribute to assign to the input blocks\n        inplace (bool, optional): Whether to modify the block(s) in place or not. Currently, only\n\n    Raises:\n        NotImplementedError: if the `inplace` argument is set to False, the function will\n            raise  this exception\n\n    Returns:\n        AbstractBlock | list[AbstractBlock]: the input block or list of blocks with the trainable\n            attribute set to the given value\n    \"\"\"\n\n    if isinstance(blocks, AbstractBlock):\n        blocks = [blocks]\n\n    if inplace:\n        for block in blocks:\n            params: list[sympy.Basic] = parameters(block)\n            for p in params:\n                if not p.is_number:\n                    p.trainable = value\n    else:\n        raise NotImplementedError(\"Not inplace set_trainable is not yet available\")\n\n    return blocks if len(blocks) &gt; 1 else blocks[0]\n</code></pre>"},{"location":"api/transpile/#qadence.transpile.block.validate","title":"<code>validate(block)</code>","text":"<p>Moves a block from global to local qubit numbers by adding PutBlocks.</p> <p>Reassigns qubit locations appropriately.</p>"},{"location":"api/transpile/#qadence.transpile.block.validate--example","title":"Example","text":"<pre><code>from qadence.blocks import chain\nfrom qadence.operations import X\nfrom qadence.transpile import validate\n\nx = chain(chain(X(0)), chain(X(1)))\nprint(x)\nprint(validate(x))\n</code></pre> <pre><code>ChainBlock(0,1)\n\u251c\u2500\u2500 ChainBlock(0)\n\u2502   \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 ChainBlock(1)\n    \u2514\u2500\u2500 X(1)\nChainBlock(0,1)\n\u251c\u2500\u2500 put on (0)\n\u2502   \u2514\u2500\u2500 ChainBlock(0)\n\u2502       \u2514\u2500\u2500 put on (0)\n\u2502           \u2514\u2500\u2500 X(0)\n\u2514\u2500\u2500 put on (1)\n    \u2514\u2500\u2500 ChainBlock(0)\n        \u2514\u2500\u2500 put on (0)\n            \u2514\u2500\u2500 X(0)\n</code></pre> Source code in <code>qadence/transpile/block.py</code> <pre><code>def validate(block: AbstractBlock) -&gt; AbstractBlock:\n    \"\"\"Moves a block from global to local qubit numbers by adding PutBlocks.\n\n    Reassigns qubit locations appropriately.\n\n    # Example\n    ```python exec=\"on\" source=\"above\" result=\"json\"\n    from qadence.blocks import chain\n    from qadence.operations import X\n    from qadence.transpile import validate\n\n    x = chain(chain(X(0)), chain(X(1)))\n    print(x)\n    print(validate(x))\n    ```\n    \"\"\"\n    vblock: AbstractBlock\n    from qadence.transpile import reassign\n\n    if isinstance(block, ControlBlock):\n        vblock = deepcopy(block)\n        b: AbstractBlock\n        (b,) = block.blocks\n        b = reassign(b, {i: i - min(b.qubit_support) for i in b.qubit_support})\n        b = validate(b)\n        vblock.blocks = (b,)  # type: ignore[assignment]\n\n    elif isinstance(block, CompositeBlock):\n        blocks = []\n        for b in block.blocks:\n            mi, ma = min(b.qubit_support), max(b.qubit_support)\n            nb = reassign(b, {i: i - min(b.qubit_support) for i in b.qubit_support})\n            nb = validate(nb)\n            nb = PutBlock(nb, tuple(range(mi, ma + 1)))\n            blocks.append(nb)\n        try:\n            vblock = _construct(type(block), tuple(blocks))\n        except AssertionError as e:\n            if str(e) == \"Make sure blocks act on distinct qubits!\":\n                vblock = chain(*blocks)\n            else:\n                raise e\n\n    elif isinstance(block, PrimitiveBlock):\n        vblock = deepcopy(block)\n\n    else:\n        raise NotImplementedError\n\n    vblock.tag = block.tag\n    return vblock\n</code></pre>"},{"location":"api/types/","title":"Types","text":""},{"location":"api/types/#qadence-types","title":"Qadence Types","text":""},{"location":"api/types/#qadence.types.TArray","title":"<code>TArray = Union[Iterable, Tensor, np.ndarray]</code>  <code>module-attribute</code>","text":"<p>Union of common array types.</p>"},{"location":"api/types/#qadence.types.TGenerator","title":"<code>TGenerator = Union[Tensor, sympy.Array, sympy.Basic]</code>  <code>module-attribute</code>","text":"<p>Union of torch tensors and numpy arrays.</p>"},{"location":"api/types/#qadence.types.TNumber","title":"<code>TNumber = Union[int, float, complex, np.int64, np.float64]</code>  <code>module-attribute</code>","text":"<p>Union of python and numpy numeric types.</p>"},{"location":"api/types/#qadence.types.TParameter","title":"<code>TParameter = Union[TNumber, Tensor, sympy.Basic, str]</code>  <code>module-attribute</code>","text":"<p>Union of numbers, tensors, and parameter types.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo","title":"<code>AlgoHEvo</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Hamiltonian Evolution algorithms that can be used by the backend.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.EIG","title":"<code>EIG = 'EIG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using Hamiltonian diagonalization.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.EXP","title":"<code>EXP = 'EXP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using torch.matrix_exp on the generator matrix.</p>"},{"location":"api/types/#qadence.types.AlgoHEvo.RK4","title":"<code>RK4 = 'RK4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>4th order Runge-Kutta approximation.</p>"},{"location":"api/types/#qadence.types.AnalogNoise","title":"<code>AnalogNoise</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of noise protocol.</p>"},{"location":"api/types/#qadence.types.AnsatzType","title":"<code>AnsatzType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Ansatz types for variational circuits.</p>"},{"location":"api/types/#qadence.types.AnsatzType.HEA","title":"<code>HEA = 'hea'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hardware-efficient ansatz.</p>"},{"location":"api/types/#qadence.types.AnsatzType.IIA","title":"<code>IIA = 'iia'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Identity-Initialised Ansatz.</p>"},{"location":"api/types/#qadence.types.BasisSet","title":"<code>BasisSet</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Basis set for feature maps.</p>"},{"location":"api/types/#qadence.types.BasisSet.CHEBYSHEV","title":"<code>CHEBYSHEV = 'Chebyshev'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Chebyshev polynomials of the first kind.</p>"},{"location":"api/types/#qadence.types.BasisSet.FOURIER","title":"<code>FOURIER = 'Fourier'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fourier basis set.</p>"},{"location":"api/types/#qadence.types.DeviceType","title":"<code>DeviceType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Supported types of devices for Pulser backend.</p>"},{"location":"api/types/#qadence.types.DeviceType.IDEALIZED","title":"<code>IDEALIZED = 'IdealDevice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Idealized device, least realistic.</p>"},{"location":"api/types/#qadence.types.DeviceType.REALISTIC","title":"<code>REALISTIC = 'RealisticDevice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Device with realistic specs.</p>"},{"location":"api/types/#qadence.types.Endianness","title":"<code>Endianness</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The endianness convention to use.</p>"},{"location":"api/types/#qadence.types.Endianness.BIG","title":"<code>BIG = 'Big'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use Big endianness.</p>"},{"location":"api/types/#qadence.types.Endianness.LITTLE","title":"<code>LITTLE = 'Little'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use little endianness.</p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool","title":"<code>ExperimentTrackingTool</code>","text":"<p>               Bases: <code>StrEnum</code></p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool.MLFLOW","title":"<code>MLFLOW = 'mlflow'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the ml-flow experiment tracker.</p>"},{"location":"api/types/#qadence.types.ExperimentTrackingTool.TENSORBOARD","title":"<code>TENSORBOARD = 'tensorboard'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the tensorboard experiment tracker.</p>"},{"location":"api/types/#qadence.types.FigFormat","title":"<code>FigFormat</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available output formats for exporting visualized circuits to a file.</p>"},{"location":"api/types/#qadence.types.FigFormat.PDF","title":"<code>PDF = 'PDF'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PDF format.</p>"},{"location":"api/types/#qadence.types.FigFormat.PNG","title":"<code>PNG = 'PNG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>PNG format.</p>"},{"location":"api/types/#qadence.types.FigFormat.SVG","title":"<code>SVG = 'SVG'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>SVG format.</p>"},{"location":"api/types/#qadence.types.GenDAQC","title":"<code>GenDAQC</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>The type of interaction for the DAQC transform.</p>"},{"location":"api/types/#qadence.types.GenDAQC.NN","title":"<code>NN = 'NN'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NN</p>"},{"location":"api/types/#qadence.types.GenDAQC.ZZ","title":"<code>ZZ = 'ZZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ZZ</p>"},{"location":"api/types/#qadence.types.InputDiffMode","title":"<code>InputDiffMode</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Derivative modes w.r.t inputs of UFAs.</p>"},{"location":"api/types/#qadence.types.InputDiffMode.AD","title":"<code>AD = 'ad'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Reverse automatic differentiation.</p>"},{"location":"api/types/#qadence.types.InputDiffMode.FD","title":"<code>FD = 'fd'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Central finite differencing.</p>"},{"location":"api/types/#qadence.types.Interaction","title":"<code>Interaction</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Interaction types used in.</p> <ul> <li><code>RydbergDevice</code>.</li> <li><code>hamiltonian_factory</code>.</li> </ul>"},{"location":"api/types/#qadence.types.Interaction.NN","title":"<code>NN = 'NN'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NN-Ising Interaction, N=(I-Z)/2.</p>"},{"location":"api/types/#qadence.types.Interaction.XY","title":"<code>XY = 'XY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>XY Interaction.</p>"},{"location":"api/types/#qadence.types.Interaction.XYZ","title":"<code>XYZ = 'XYZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>XYZ Interaction.</p>"},{"location":"api/types/#qadence.types.Interaction.ZZ","title":"<code>ZZ = 'ZZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ZZ-Ising Interaction.</p>"},{"location":"api/types/#qadence.types.LTSOrder","title":"<code>LTSOrder</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Lie-Trotter-Suzuki approximation order.</p>"},{"location":"api/types/#qadence.types.LTSOrder.BASIC","title":"<code>BASIC = 'BASIC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Basic.</p>"},{"location":"api/types/#qadence.types.LTSOrder.ST2","title":"<code>ST2 = 'ST2'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ST2.</p>"},{"location":"api/types/#qadence.types.LTSOrder.ST4","title":"<code>ST4 = 'ST4'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>ST4.</p>"},{"location":"api/types/#qadence.types.LatticeTopology","title":"<code>LatticeTopology</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Lattice topologies to choose from for the register.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.ALL_TO_ALL","title":"<code>ALL_TO_ALL = 'all_to_all'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>All to all- connected lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.ARBITRARY","title":"<code>ARBITRARY = 'arbitrary'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Arbitrarily-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.CIRCLE","title":"<code>CIRCLE = 'circle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Circular lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.HONEYCOMB_LATTICE","title":"<code>HONEYCOMB_LATTICE = 'honeycomb_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Honeycomb-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.LINE","title":"<code>LINE = 'line'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Line-format lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.RECTANGULAR_LATTICE","title":"<code>RECTANGULAR_LATTICE = 'rectangular_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Rectangular-shaped lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.SQUARE","title":"<code>SQUARE = 'square'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Square lattice.</p>"},{"location":"api/types/#qadence.types.LatticeTopology.TRIANGULAR_LATTICE","title":"<code>TRIANGULAR_LATTICE = 'triangular_lattice'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Triangular-shaped shape.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy","title":"<code>MultivariateStrategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Multivariate strategy for feature maps.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy.PARALLEL","title":"<code>PARALLEL = 'Parallel'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parallel strategy.</p>"},{"location":"api/types/#qadence.types.MultivariateStrategy.SERIES","title":"<code>SERIES = 'Series'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Serial strategy.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol","title":"<code>NoiseProtocol()</code>  <code>dataclass</code>","text":"<p>Type of noise protocol.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.ANALOG","title":"<code>ANALOG = AnalogNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied in analog blocks.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.DIGITAL","title":"<code>DIGITAL = DigitalNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied to digital blocks.</p>"},{"location":"api/types/#qadence.types.NoiseProtocol.READOUT","title":"<code>READOUT = ReadoutNoise</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Noise applied on outputs of quantum programs.</p>"},{"location":"api/types/#qadence.types.ObservableTransform","title":"<code>ObservableTransform</code>","text":"<p>Observable transformation type.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.NONE","title":"<code>NONE = 'none'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>No transformation.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.RANGE","title":"<code>RANGE = 'range'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the given values as min and max.</p>"},{"location":"api/types/#qadence.types.ObservableTransform.SCALE","title":"<code>SCALE = 'scale'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the given values as scale and shift.</p>"},{"location":"api/types/#qadence.types.OpName","title":"<code>OpName</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>A list of all available of digital-analog operations.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGENTANG","title":"<code>ANALOGENTANG = 'AnalogEntanglement'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog entanglement operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGINTERACTION","title":"<code>ANALOGINTERACTION = 'AnalogInteraction'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog interaction operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRX","title":"<code>ANALOGRX = 'AnalogRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RX operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRY","title":"<code>ANALOGRY = 'AnalogRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RY operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGRZ","title":"<code>ANALOGRZ = 'AnalogRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog RZ operation.</p>"},{"location":"api/types/#qadence.types.OpName.ANALOGSWAP","title":"<code>ANALOGSWAP = 'AnalogSWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The analog SWAP operation.</p>"},{"location":"api/types/#qadence.types.OpName.CNOT","title":"<code>CNOT = 'CNOT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The CNOT gate.</p>"},{"location":"api/types/#qadence.types.OpName.CPHASE","title":"<code>CPHASE = 'CPHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The controlled PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRX","title":"<code>CRX = 'CRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRY","title":"<code>CRY = 'CRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Controlled RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.CRZ","title":"<code>CRZ = 'CRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.CSWAP","title":"<code>CSWAP = 'CSWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Control SWAP gate.</p>"},{"location":"api/types/#qadence.types.OpName.CZ","title":"<code>CZ = 'CZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The CZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.ENTANGLE","title":"<code>ENTANGLE = 'entangle'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The entanglement operation.</p>"},{"location":"api/types/#qadence.types.OpName.H","title":"<code>H = 'H'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Hadamard gate.</p>"},{"location":"api/types/#qadence.types.OpName.HAMEVO","title":"<code>HAMEVO = 'HamEvo'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Hamiltonian Evolution operation.</p>"},{"location":"api/types/#qadence.types.OpName.I","title":"<code>I = 'I'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Identity gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCPHASE","title":"<code>MCPHASE = 'MCPHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRX","title":"<code>MCRX = 'MCRX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRY","title":"<code>MCRY = 'MCRY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCRZ","title":"<code>MCRZ = 'MCRZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.MCZ","title":"<code>MCZ = 'MCZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Multicontrol CZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.N","title":"<code>N = 'N'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The N = (1/2)(I-Z) operator.</p>"},{"location":"api/types/#qadence.types.OpName.PHASE","title":"<code>PHASE = 'PHASE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PHASE gate.</p>"},{"location":"api/types/#qadence.types.OpName.PROJ","title":"<code>PROJ = 'Projector'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The projector operation.</p>"},{"location":"api/types/#qadence.types.OpName.RX","title":"<code>RX = 'RX'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RX gate.</p>"},{"location":"api/types/#qadence.types.OpName.RY","title":"<code>RY = 'RY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RY gate.</p>"},{"location":"api/types/#qadence.types.OpName.RZ","title":"<code>RZ = 'RZ'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The RZ gate.</p>"},{"location":"api/types/#qadence.types.OpName.S","title":"<code>S = 'S'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The S gate.</p>"},{"location":"api/types/#qadence.types.OpName.SDAGGER","title":"<code>SDAGGER = 'SDagger'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The S dagger gate.</p>"},{"location":"api/types/#qadence.types.OpName.SWAP","title":"<code>SWAP = 'SWAP'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The SWAP gate.</p>"},{"location":"api/types/#qadence.types.OpName.T","title":"<code>T = 'T'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The T gate.</p>"},{"location":"api/types/#qadence.types.OpName.TDAGGER","title":"<code>TDAGGER = 'TDagger'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The T dagger gate.</p>"},{"location":"api/types/#qadence.types.OpName.TOFFOLI","title":"<code>TOFFOLI = 'Toffoli'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Toffoli gate.</p>"},{"location":"api/types/#qadence.types.OpName.U","title":"<code>U = 'U'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The U gate.</p>"},{"location":"api/types/#qadence.types.OpName.X","title":"<code>X = 'X'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The X gate.</p>"},{"location":"api/types/#qadence.types.OpName.Y","title":"<code>Y = 'Y'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Y gate.</p>"},{"location":"api/types/#qadence.types.OpName.Z","title":"<code>Z = 'Z'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Z gate.</p>"},{"location":"api/types/#qadence.types.OpName.ZERO","title":"<code>ZERO = 'Zero'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The zero gate.</p>"},{"location":"api/types/#qadence.types.OverlapMethod","title":"<code>OverlapMethod</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Overlap Methods to choose from.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.COMPUTE_UNCOMPUTE","title":"<code>COMPUTE_UNCOMPUTE = 'compute_uncompute'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Compute-uncompute.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.EXACT","title":"<code>EXACT = 'exact'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exact.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.HADAMARD_TEST","title":"<code>HADAMARD_TEST = 'hadamard_test'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hadamard-test.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.JENSEN_SHANNON","title":"<code>JENSEN_SHANNON = 'jensen_shannon'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Jensen-shannon.</p>"},{"location":"api/types/#qadence.types.OverlapMethod.SWAP_TEST","title":"<code>SWAP_TEST = 'swap_test'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Swap-test.</p>"},{"location":"api/types/#qadence.types.ParameterType","title":"<code>ParameterType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Parameter types available in qadence.</p>"},{"location":"api/types/#qadence.types.ParameterType.FEATURE","title":"<code>FEATURE = 'Feature'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>FeatureParameters act as input and are not trainable.</p>"},{"location":"api/types/#qadence.types.ParameterType.FIXED","title":"<code>FIXED = 'Fixed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fixed/ constant parameters are neither trainable nor act as input.</p>"},{"location":"api/types/#qadence.types.ParameterType.VARIATIONAL","title":"<code>VARIATIONAL = 'Variational'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>VariationalParameters are trainable.</p>"},{"location":"api/types/#qadence.types.QubitSupportType","title":"<code>QubitSupportType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Qubit support types.</p>"},{"location":"api/types/#qadence.types.QubitSupportType.GLOBAL","title":"<code>GLOBAL = 'global'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use global qubit support.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise","title":"<code>ReadoutNoise</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Type of readout protocol.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise.CORRELATED","title":"<code>CORRELATED = 'Correlated Readout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Using a confusion matrix (2n, 2n) for corrupting bitstrings values.</p>"},{"location":"api/types/#qadence.types.ReadoutNoise.INDEPENDENT","title":"<code>INDEPENDENT = 'Independent Readout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Simple readout protocols where each qubit is corrupted independently.</p>"},{"location":"api/types/#qadence.types.ResultType","title":"<code>ResultType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available data types for generating certain results.</p>"},{"location":"api/types/#qadence.types.ResultType.NUMPY","title":"<code>NUMPY = 'Numpy'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Numpy Array Type.</p>"},{"location":"api/types/#qadence.types.ResultType.STRING","title":"<code>STRING = 'String'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>String Type.</p>"},{"location":"api/types/#qadence.types.ResultType.TORCH","title":"<code>TORCH = 'Torch'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Torch Tensor Type.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling","title":"<code>ReuploadScaling</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Scaling for data reuploads in feature maps.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.CONSTANT","title":"<code>CONSTANT = 'Constant'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Constant scaling.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.EXP","title":"<code>EXP = 'Exponential'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Exponentially increasing scaling.</p>"},{"location":"api/types/#qadence.types.ReuploadScaling.TOWER","title":"<code>TOWER = 'Tower'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Linearly increasing scaling.</p>"},{"location":"api/types/#qadence.types.SerializationFormat","title":"<code>SerializationFormat</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Available serialization formats for circuits.</p>"},{"location":"api/types/#qadence.types.SerializationFormat.JSON","title":"<code>JSON = 'JSON'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The Json format.</p>"},{"location":"api/types/#qadence.types.SerializationFormat.PT","title":"<code>PT = 'PT'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The PT format used by Torch.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType","title":"<code>StateGeneratorType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Methods to generate random states.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.HAAR_MEASURE_FAST","title":"<code>HAAR_MEASURE_FAST = 'HaarMeasureFast'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>HaarMeasure.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.HAAR_MEASURE_SLOW","title":"<code>HAAR_MEASURE_SLOW = 'HaarMeasureSlow'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>HaarMeasure non-optimized version.</p>"},{"location":"api/types/#qadence.types.StateGeneratorType.RANDOM_ROTATIONS","title":"<code>RANDOM_ROTATIONS = 'RandomRotations'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random Rotations.</p>"},{"location":"api/types/#qadence.types.StrEnum","title":"<code>StrEnum</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p>"},{"location":"api/types/#qadence.types.StrEnum.__str__","title":"<code>__str__()</code>","text":"<p>Used when dumping enum fields in a schema.</p> Source code in <code>qadence/types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Used when dumping enum fields in a schema.\"\"\"\n    ret: str = self.value\n    return ret\n</code></pre>"},{"location":"api/types/#qadence.types.Strategy","title":"<code>Strategy</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Computing paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.ANALOG","title":"<code>ANALOG = 'Analog'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the analog paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.BDAQC","title":"<code>BDAQC = 'bDAQC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the banged digital-analog QC paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.DIGITAL","title":"<code>DIGITAL = 'Digital'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the digital paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.RYDBERG","title":"<code>RYDBERG = 'Rydberg'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the Rydberg QC paradigm.</p>"},{"location":"api/types/#qadence.types.Strategy.SDAQC","title":"<code>SDAQC = 'sDAQC'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use the step-wise digital-analog QC paradigm.</p>"},{"location":"api/types/#qadence.types.TensorType","title":"<code>TensorType</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Tensor Types for converting blocks to tensors.</p>"},{"location":"api/types/#qadence.types.TensorType.DENSE","title":"<code>DENSE = 'Dense'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a block to a dense tensor.</p>"},{"location":"api/types/#qadence.types.TensorType.SPARSE","title":"<code>SPARSE = 'Sparse'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a observable block to a sparse tensor.</p>"},{"location":"api/types/#qadence.types.TensorType.SPARSEDIAGONAL","title":"<code>SPARSEDIAGONAL = 'SparseDiagonal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Convert a diagonal observable block to a sparse diagonal if possible.</p>"},{"location":"api/backends/backend/","title":"Abstract backend","text":""},{"location":"api/backends/backend/#qadence.backend.Backend","title":"<code>Backend(name, supports_ad, support_bp, supports_adjoint, is_remote, with_measurements, native_endianness, engine, with_noise, config)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The abstract class that defines the interface for the backends.</p> ATTRIBUTE DESCRIPTION <code>name</code> <p>backend unique string identifier</p> <p> TYPE: <code>BackendName</code> </p> <code>supports_ad</code> <p>whether or not the backend has a native autograd</p> <p> TYPE: <code>bool</code> </p> <code>supports_bp</code> <p>whether or not the backend has a native backprop</p> <p> TYPE: <code>bool</code> </p> <code>supports_adjoint</code> <p>Does the backend support native adjoint differentation.</p> <p> TYPE: <code>bool</code> </p> <code>is_remote</code> <p>whether computations are executed locally or remotely on this backend, useful when using cloud platforms where credentials are needed for example.</p> <p> TYPE: <code>bool</code> </p> <code>with_measurements</code> <p>whether it supports counts or not</p> <p> TYPE: <code>bool</code> </p> <code>with_noise</code> <p>whether to add realistic noise or not</p> <p> TYPE: <code>bool</code> </p> <code>native_endianness</code> <p>The native endianness of the backend</p> <p> TYPE: <code>Endianness</code> </p> <code>engine</code> <p>The underlying (native) automatic differentiation engine of the backend.</p> <p> TYPE: <code>Engine</code> </p>"},{"location":"api/backends/backend/#qadence.backend.Backend.circuit","title":"<code>circuit(circuit)</code>  <code>abstractmethod</code>","text":"<p>Converts an abstract <code>QuantumCircuit</code> to the native backend representation.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A circuit, for example: <code>QuantumCircuit(2, X(0))</code></p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>A converted circuit <code>c</code>. You can access the original, arbstract circuit via <code>c.abstract</code></p> <code>ConvertedCircuit</code> <p>and the converted (or backend native) circuit via <code>c.native</code>.</p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Converts an abstract `QuantumCircuit` to the native backend representation.\n\n    Arguments:\n        circuit: A circuit, for example: `QuantumCircuit(2, X(0))`\n\n    Returns:\n        A converted circuit `c`. You can access the original, arbstract circuit via `c.abstract`\n        and the converted (or backend *native*) circuit via `c.native`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.convert","title":"<code>convert(circuit, observable=None)</code>","text":"<p>Convert an abstract circuit and an optional observable to their native representation.</p> <p>Additionally, this function constructs an embedding function which maps from user-facing parameters to device parameters (read more on parameter embedding here).</p> Source code in <code>qadence/backend.py</code> <pre><code>def convert(\n    self, circuit: QuantumCircuit, observable: list[AbstractBlock] | AbstractBlock | None = None\n) -&gt; Converted:\n    \"\"\"Convert an abstract circuit and an optional observable to their native representation.\n\n    Additionally, this function constructs an embedding function which maps from\n    user-facing parameters to device parameters (read more on parameter embedding\n    [here][qadence.blocks.embedding.embedding]).\n    \"\"\"\n\n    def check_observable(obs_obj: Any) -&gt; AbstractBlock:\n        if isinstance(obs_obj, QubitOperator):\n            from qadence.blocks.manipulate import from_openfermion\n\n            assert len(obs_obj.terms) &gt; 0, \"Make sure to give a non-empty qubit hamiltonian\"\n\n            return from_openfermion(obs_obj)\n\n        elif isinstance(obs_obj, (CompositeBlock, PrimitiveBlock, ScaleBlock)):\n            from qadence.blocks.utils import block_is_qubit_hamiltonian\n\n            assert block_is_qubit_hamiltonian(\n                obs_obj\n            ), \"Make sure the QubitHamiltonian consists only of Pauli operators X, Y, Z, I\"\n            return obs_obj\n        raise TypeError(\n            \"qubit_hamiltonian should be a Pauli-like AbstractBlock or a QubitOperator\"\n        )\n\n    conv_circ = self.circuit(circuit)\n    circ_params, circ_embedding_fn = embedding(\n        conv_circ.abstract.block, self.config._use_gate_params, self.engine\n    )\n    params = circ_params\n    if observable is not None:\n        observable = observable if isinstance(observable, list) else [observable]\n        conv_obs = []\n        obs_embedding_fn_list = []\n\n        for obs in observable:\n            obs = check_observable(obs)\n            c_obs = self.observable(obs, max(circuit.n_qubits, obs.n_qubits))\n            obs_params, obs_embedding_fn = embedding(\n                c_obs.abstract, self.config._use_gate_params, self.engine\n            )\n            params.update(obs_params)\n            obs_embedding_fn_list.append(obs_embedding_fn)\n            conv_obs.append(c_obs)\n\n        def embedding_fn_dict(a: dict, b: dict) -&gt; dict:\n            embedding_dict = circ_embedding_fn(a, b)\n            for o in obs_embedding_fn_list:\n                embedding_dict.update(o(a, b))\n            return embedding_dict\n\n        return Converted(conv_circ, conv_obs, embedding_fn_dict, params)\n\n    def embedding_fn(a: dict, b: dict) -&gt; dict:\n        return circ_embedding_fn(a, b)\n\n    return Converted(conv_circ, None, embedding_fn, params)\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>  <code>abstractmethod</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.observable","title":"<code>observable(observable, n_qubits)</code>  <code>abstractmethod</code>","text":"<p>Converts an abstract observable (which is just an <code>AbstractBlock</code>) to the native backend.</p> <p>representation.</p> PARAMETER DESCRIPTION <code>observable</code> <p>An observable.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits the observable covers. This is typically <code>circuit.n_qubits</code>.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>ConvertedObservable</code> <p>A converted observable <code>o</code>. You can access the original, arbstract observable via</p> <code>ConvertedObservable</code> <p><code>o.abstract</code> and the converted (or backend native) observable via <code>o.native</code>.</p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef observable(self, observable: AbstractBlock, n_qubits: int) -&gt; ConvertedObservable:\n    \"\"\"Converts an abstract observable (which is just an `AbstractBlock`) to the native backend.\n\n    representation.\n\n    Arguments:\n        observable: An observable.\n        n_qubits: Number of qubits the observable covers. This is typically `circuit.n_qubits`.\n\n    Returns:\n        A converted observable `o`. You can access the original, arbstract observable via\n        `o.abstract` and the converted (or backend *native*) observable via `o.native`.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.run","title":"<code>run(circuit, param_values={}, state=None, endianness=Endianness.BIG, *args, **kwargs)</code>","text":"<p>Run a circuit and return the resulting wave function.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>dict[str, ArrayLike]</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>Tensor | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting wavefunction.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> RETURNS DESCRIPTION <code>ArrayLike</code> <p>A list of Counter objects where each key represents a bitstring</p> <code>ArrayLike</code> <p>and its value the number of times it has been sampled from the given wave function.</p> Source code in <code>qadence/backend.py</code> <pre><code>def run(\n    self,\n    circuit: ConvertedCircuit,\n    param_values: dict[str, ArrayLike] = {},\n    state: Tensor | None = None,\n    endianness: Endianness = Endianness.BIG,\n    *args: Any,\n    **kwargs: Any,\n) -&gt; ArrayLike:\n    \"\"\"Run a circuit and return the resulting wave function.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        endianness: Endianness of the resulting wavefunction.\n\n    Returns:\n        A list of Counter objects where each key represents a bitstring\n        and its value the number of times it has been sampled from the given wave function.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.Backend.sample","title":"<code>sample(circuit, param_values={}, n_shots=1000, state=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>  <code>abstractmethod</code>","text":"<p>Sample bit strings.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>dict[str, Tensor]</code> DEFAULT: <code>{}</code> </p> <code>n_shots</code> <p>Number of shots to sample.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>An error mitigation protocol to apply.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>@abstractmethod\ndef sample(\n    self,\n    circuit: ConvertedCircuit,\n    param_values: dict[str, Tensor] = {},\n    n_shots: int = 1000,\n    state: ArrayLike | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; list[Counter]:\n    \"\"\"Sample bit strings.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        n_shots: Number of shots to sample.\n        state: Initial state.\n        noise: A noise model to use.\n        mitigation: An error mitigation protocol to apply.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration","title":"<code>BackendConfiguration(_use_gate_params=True, use_sparse_observable=False, use_gradient_checkpointing=False, use_single_qubit_composition=False, transpilation_passes=None)</code>  <code>dataclass</code>","text":""},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration.available_options","title":"<code>available_options()</code>","text":"<p>Return as a string the available fields with types of the configuration.</p> RETURNS DESCRIPTION <code>str</code> <p>a string with all the available fields, one per line</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/backend.py</code> <pre><code>def available_options(self) -&gt; str:\n    \"\"\"Return as a string the available fields with types of the configuration.\n\n    Returns:\n        str: a string with all the available fields, one per line\n    \"\"\"\n    conf_msg = \"\"\n    for _field in fields(self):\n        if not _field.name.startswith(\"_\"):\n            conf_msg += (\n                f\"Name: {_field.name} - Type: {_field.type} - Default value: {_field.default}\\n\"\n            )\n    return conf_msg\n</code></pre>"},{"location":"api/backends/backend/#qadence.backend.BackendConfiguration.get_param_name","title":"<code>get_param_name(blk)</code>","text":"<p>Return parameter names for the current backend.</p> <p>Depending on which backend is in use this function returns either UUIDs or expressions of parameters.</p> Source code in <code>qadence/backend.py</code> <pre><code>def get_param_name(self, blk: AbstractBlock) -&gt; Tuple[str, ...]:\n    \"\"\"Return parameter names for the current backend.\n\n    Depending on which backend is in use this\n    function returns either UUIDs or expressions of parameters.\n    \"\"\"\n    param_ids: Tuple\n    # FIXME: better type hiearchy?\n    types = (TimeEvolutionBlock, ParametricBlock, ConstantAnalogRotation, InteractionBlock)\n    if not isinstance(blk, types):\n        raise TypeError(f\"Can not infer param name from {type(blk)}\")\n    else:\n        if self._use_gate_params:\n            param_ids = tuple(blk.parameters.uuids())\n        else:\n            param_ids = tuple(map(stringify, blk.parameters.expressions()))\n    return param_ids\n</code></pre>"},{"location":"api/backends/differentiable/","title":"DifferentiableBackend","text":""},{"location":"api/backends/differentiable/#qadence.engines.torch.differentiable_backend.DifferentiableBackend","title":"<code>DifferentiableBackend(backend, diff_mode=DiffMode.AD, **psr_args)</code>","text":"<p>               Bases: <code>DifferentiableBackend</code></p> <p>A class which wraps a QuantumBackend with the automatic differentation engine TORCH.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of the QuantumBackend type perform execution.</p> <p> TYPE: <code>Backend</code> </p> <code>diff_mode</code> <p>A differentiable mode supported by the differentiation engine.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>**psr_args</code> <p>Arguments that will be passed on to <code>DifferentiableExpectation</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>{}</code> </p> Source code in <code>qadence/engines/torch/differentiable_backend.py</code> <pre><code>def __init__(\n    self,\n    backend: QuantumBackend,\n    diff_mode: DiffMode = DiffMode.AD,\n    **psr_args: int | float | None,\n) -&gt; None:\n    super().__init__(backend=backend, engine=Engine.TORCH, diff_mode=diff_mode)\n    self.psr_args = psr_args\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.torch.differentiable_backend.DifferentiableBackend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>observable</code> <p>A converted observable as returned by <code>backend.observable</code>.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>The error mitigation to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/engines/torch/differentiable_backend.py</code> <pre><code>def expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        observable: A converted observable as returned by `backend.observable`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: The error mitigation to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    observable = observable if isinstance(observable, list) else [observable]\n    differentiable_expectation = DifferentiableExpectation(\n        backend=self.backend,\n        circuit=circuit,\n        observable=observable,\n        param_values=param_values,\n        state=state,\n        measurement=measurement,\n        noise=noise,\n        mitigation=mitigation,\n        endianness=endianness,\n    )\n\n    if self.diff_mode == DiffMode.AD:\n        expectation = differentiable_expectation.ad\n    elif self.diff_mode == DiffMode.ADJOINT:\n        expectation = differentiable_expectation.adjoint\n    else:\n        try:\n            fns = get_gpsr_fns()\n            psr_fn = fns[self.diff_mode]\n        except KeyError:\n            raise ValueError(f\"{self.diff_mode} differentiation mode is not supported\")\n        expectation = partial(differentiable_expectation.psr, psr_fn=psr_fn, **self.psr_args)\n    return expectation()\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.jax.differentiable_backend.DifferentiableBackend","title":"<code>DifferentiableBackend(backend, diff_mode=DiffMode.AD, **psr_args)</code>","text":"<p>               Bases: <code>DifferentiableBackend</code></p> <p>A class which wraps a QuantumBackend with the automatic differentation engine JAX.</p> PARAMETER DESCRIPTION <code>backend</code> <p>An instance of the QuantumBackend type perform execution.</p> <p> TYPE: <code>Backend</code> </p> <code>diff_mode</code> <p>A differentiable mode supported by the differentiation engine.</p> <p> TYPE: <code>DiffMode</code> DEFAULT: <code>AD</code> </p> <code>**psr_args</code> <p>Arguments that will be passed on to <code>DifferentiableExpectation</code>.</p> <p> TYPE: <code>int | float | None</code> DEFAULT: <code>{}</code> </p> Source code in <code>qadence/engines/jax/differentiable_backend.py</code> <pre><code>def __init__(\n    self,\n    backend: Backend,\n    diff_mode: DiffMode = DiffMode.AD,\n    **psr_args: int | float | None,\n) -&gt; None:\n    super().__init__(backend=backend, engine=Engine.JAX, diff_mode=diff_mode)\n    self.psr_args = psr_args\n</code></pre>"},{"location":"api/backends/differentiable/#qadence.engines.jax.differentiable_backend.DifferentiableBackend.expectation","title":"<code>expectation(circuit, observable, param_values={}, state=None, measurement=None, noise=None, mitigation=None, endianness=Endianness.BIG)</code>","text":"<p>Compute the expectation value of the <code>circuit</code> with the given <code>observable</code>.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>A converted circuit as returned by <code>backend.circuit</code>.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>observable</code> <p>A converted observable as returned by <code>backend.observable</code>.</p> <p> TYPE: <code>list[ConvertedObservable] | ConvertedObservable</code> </p> <code>param_values</code> <p>Already embedded parameters of the circuit. See <code>embedding</code> for more info.</p> <p> TYPE: <code>ParamDictType</code> DEFAULT: <code>{}</code> </p> <code>state</code> <p>Initial state.</p> <p> TYPE: <code>ArrayLike | None</code> DEFAULT: <code>None</code> </p> <code>measurement</code> <p>Optional measurement protocol. If None, use exact expectation value with a statevector simulator.</p> <p> TYPE: <code>Measurements | None</code> DEFAULT: <code>None</code> </p> <code>noise</code> <p>A noise model to use.</p> <p> TYPE: <code>NoiseHandler | None</code> DEFAULT: <code>None</code> </p> <code>mitigation</code> <p>The error mitigation to use.</p> <p> TYPE: <code>Mitigations | None</code> DEFAULT: <code>None</code> </p> <code>endianness</code> <p>Endianness of the resulting bit strings.</p> <p> TYPE: <code>Endianness</code> DEFAULT: <code>BIG</code> </p> Source code in <code>qadence/engines/jax/differentiable_backend.py</code> <pre><code>def expectation(\n    self,\n    circuit: ConvertedCircuit,\n    observable: list[ConvertedObservable] | ConvertedObservable,\n    param_values: ParamDictType = {},\n    state: ArrayLike | None = None,\n    measurement: Measurements | None = None,\n    noise: NoiseHandler | None = None,\n    mitigation: Mitigations | None = None,\n    endianness: Endianness = Endianness.BIG,\n) -&gt; ArrayLike:\n    \"\"\"Compute the expectation value of the `circuit` with the given `observable`.\n\n    Arguments:\n        circuit: A converted circuit as returned by `backend.circuit`.\n        observable: A converted observable as returned by `backend.observable`.\n        param_values: _**Already embedded**_ parameters of the circuit. See\n            [`embedding`][qadence.blocks.embedding.embedding] for more info.\n        state: Initial state.\n        measurement: Optional measurement protocol. If None, use\n            exact expectation value with a statevector simulator.\n        noise: A noise model to use.\n        mitigation: The error mitigation to use.\n        endianness: Endianness of the resulting bit strings.\n    \"\"\"\n    observable = observable if isinstance(observable, list) else [observable]\n\n    if self.diff_mode == DiffMode.AD:\n        expectation = self.backend.expectation(circuit, observable, param_values, state)\n    else:\n        expectation = DifferentiableExpectation(\n            backend=self.backend,\n            circuit=circuit,\n            observable=observable,\n            param_values=param_values,\n            state=state,\n            measurement=measurement,\n            noise=noise,\n            mitigation=mitigation,\n            endianness=endianness,\n        ).psr()\n    return expectation\n</code></pre>"},{"location":"api/backends/pulser/","title":"Pulser","text":"<p>The Pulser backend features a basic integration with the pulse-level programming interface Pulser. This backend offers for now few simple operations which are translated into a valid, non time-dependent pulse sequence. In particular, one has access to:</p> <ul> <li>analog rotations: <code>AnalogRx</code> and <code>AnalogRy</code> blocks</li> <li>free evolution blocks (basically no pulse, just interaction): <code>AnalogWait</code> block</li> <li>a block for creating entangled states: <code>AnalogEntanglement</code></li> <li>digital rotation <code>Rx</code> and <code>Ry</code></li> </ul>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend.Backend","title":"<code>Backend(name=BackendName.PULSER, supports_ad=False, support_bp=False, supports_adjoint=False, is_remote=False, with_measurements=True, native_endianness=Endianness.BIG, engine=Engine.TORCH, with_noise=False, config=Configuration())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Backend</code></p> <p>The Pulser backend.</p>"},{"location":"api/backends/pulser/#qadence.backends.pulser.backend.create_register","title":"<code>create_register(register)</code>","text":"<p>Convert Qadence Register to Pulser Register.</p> Source code in <code>qadence/backends/pulser/backend.py</code> <pre><code>def create_register(register: Register) -&gt; PulserRegister:\n    \"\"\"Convert Qadence Register to Pulser Register.\"\"\"\n    coords = np.array(list(register.coords.values()))\n    return PulserRegister.from_coordinates(coords)\n</code></pre>"},{"location":"api/backends/pyqtorch/","title":"PyQTorch","text":"<p>Fast differentiable statevector emulator based on PyTorch. The code is open source, hosted on Github and maintained by Pasqal.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend","title":"<code>Backend(name=BackendName.PYQTORCH, supports_ad=True, support_bp=True, supports_adjoint=True, is_remote=False, with_measurements=True, native_endianness=Endianness.BIG, engine=Engine.TORCH, with_noise=False, config=Configuration())</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Backend</code></p> <p>PyQTorch backend.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend.circuit","title":"<code>circuit(circuit)</code>","text":"<p>Return the converted circuit.</p> <p>Note that to get a representation with noise, noise should be passed within the config.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Original circuit</p> <p> TYPE: <code>QuantumCircuit</code> </p> RETURNS DESCRIPTION <code>ConvertedCircuit</code> <p>ConvertedCircuit instance for backend.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def circuit(self, circuit: QuantumCircuit) -&gt; ConvertedCircuit:\n    \"\"\"Return the converted circuit.\n\n    Note that to get a representation with noise, noise\n    should be passed within the config.\n\n    Args:\n        circuit (QuantumCircuit): Original circuit\n\n    Returns:\n        ConvertedCircuit: ConvertedCircuit instance for backend.\n    \"\"\"\n    passes = self.config.transpilation_passes\n    if passes is None:\n        passes = default_passes(self.config)\n\n    original_circ = circuit\n    if len(passes) &gt; 0:\n        circuit = transpile(*passes)(circuit)\n    # Setting noise in the circuit.\n    if self.config.noise:\n        set_noise(circuit, self.config.noise)\n\n    ops = convert_block(circuit.block, n_qubits=circuit.n_qubits, config=self.config)\n    readout_noise = (\n        convert_readout_noise(circuit.n_qubits, self.config.noise)\n        if self.config.noise\n        else None\n    )\n    if self.config.dropout_probability == 0:\n        native = pyq.QuantumCircuit(\n            circuit.n_qubits,\n            ops,\n            readout_noise,\n        )\n    else:\n        native = pyq.DropoutQuantumCircuit(\n            circuit.n_qubits,\n            ops,\n            readout_noise,\n            dropout_prob=self.config.dropout_probability,\n            dropout_mode=self.config.dropout_mode,\n        )\n    return ConvertedCircuit(native=native, abstract=circuit, original=original_circ)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.Backend.convert","title":"<code>convert(circuit, observable=None)</code>","text":"<p>Convert an abstract circuit and an optional observable to their native representation.</p> <p>Additionally, this function constructs an embedding function which maps from user-facing parameters to device parameters (read more on parameter embedding here).</p> Source code in <code>qadence/backend.py</code> <pre><code>def convert(\n    self, circuit: QuantumCircuit, observable: list[AbstractBlock] | AbstractBlock | None = None\n) -&gt; Converted:\n    \"\"\"Convert an abstract circuit and an optional observable to their native representation.\n\n    Additionally, this function constructs an embedding function which maps from\n    user-facing parameters to device parameters (read more on parameter embedding\n    [here][qadence.blocks.embedding.embedding]).\n    \"\"\"\n\n    def check_observable(obs_obj: Any) -&gt; AbstractBlock:\n        if isinstance(obs_obj, QubitOperator):\n            from qadence.blocks.manipulate import from_openfermion\n\n            assert len(obs_obj.terms) &gt; 0, \"Make sure to give a non-empty qubit hamiltonian\"\n\n            return from_openfermion(obs_obj)\n\n        elif isinstance(obs_obj, (CompositeBlock, PrimitiveBlock, ScaleBlock)):\n            from qadence.blocks.utils import block_is_qubit_hamiltonian\n\n            assert block_is_qubit_hamiltonian(\n                obs_obj\n            ), \"Make sure the QubitHamiltonian consists only of Pauli operators X, Y, Z, I\"\n            return obs_obj\n        raise TypeError(\n            \"qubit_hamiltonian should be a Pauli-like AbstractBlock or a QubitOperator\"\n        )\n\n    conv_circ = self.circuit(circuit)\n    circ_params, circ_embedding_fn = embedding(\n        conv_circ.abstract.block, self.config._use_gate_params, self.engine\n    )\n    params = circ_params\n    if observable is not None:\n        observable = observable if isinstance(observable, list) else [observable]\n        conv_obs = []\n        obs_embedding_fn_list = []\n\n        for obs in observable:\n            obs = check_observable(obs)\n            c_obs = self.observable(obs, max(circuit.n_qubits, obs.n_qubits))\n            obs_params, obs_embedding_fn = embedding(\n                c_obs.abstract, self.config._use_gate_params, self.engine\n            )\n            params.update(obs_params)\n            obs_embedding_fn_list.append(obs_embedding_fn)\n            conv_obs.append(c_obs)\n\n        def embedding_fn_dict(a: dict, b: dict) -&gt; dict:\n            embedding_dict = circ_embedding_fn(a, b)\n            for o in obs_embedding_fn_list:\n                embedding_dict.update(o(a, b))\n            return embedding_dict\n\n        return Converted(conv_circ, conv_obs, embedding_fn_dict, params)\n\n    def embedding_fn(a: dict, b: dict) -&gt; dict:\n        return circ_embedding_fn(a, b)\n\n    return Converted(conv_circ, None, embedding_fn, params)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_block_and_readout_noises","title":"<code>set_block_and_readout_noises(circuit, noise, config)</code>","text":"<p>Add noise on blocks and readout on circuit.</p> <p>We first start by adding noise to the abstract blocks. Then we do a conversion to their native representation. Finally, we add readout.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>noise</code> <p>Noise to add.</p> <p> TYPE: <code>NoiseHandler | None</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_block_and_readout_noises(\n    circuit: ConvertedCircuit, noise: NoiseHandler | None, config: Configuration\n) -&gt; None:\n    \"\"\"Add noise on blocks and readout on circuit.\n\n    We first start by adding noise to the abstract blocks. Then we do a conversion to their\n    native representation. Finally, we add readout.\n\n    Args:\n        circuit (ConvertedCircuit): Input circuit.\n        noise (NoiseHandler | None): Noise to add.\n    \"\"\"\n    if noise:\n        set_noise(circuit, noise)\n        set_noise_abstract_to_native(circuit, config)\n        set_readout_noise(circuit, noise)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_noise_abstract_to_native","title":"<code>set_noise_abstract_to_native(circuit, config)</code>","text":"<p>Set noise in native blocks from the abstract ones with noise.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input converted circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_noise_abstract_to_native(circuit: ConvertedCircuit, config: Configuration) -&gt; None:\n    \"\"\"Set noise in native blocks from the abstract ones with noise.\n\n    Args:\n        circuit (ConvertedCircuit): Input converted circuit.\n    \"\"\"\n    ops = convert_block(circuit.abstract.block, n_qubits=circuit.native.n_qubits, config=config)\n    circuit.native = pyq.QuantumCircuit(circuit.native.n_qubits, ops, circuit.native.readout_noise)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.backend.set_readout_noise","title":"<code>set_readout_noise(circuit, noise)</code>","text":"<p>Set readout noise in place in native.</p> PARAMETER DESCRIPTION <code>circuit</code> <p>Input converted circuit.</p> <p> TYPE: <code>ConvertedCircuit</code> </p> <code>noise</code> <p>Noise.</p> <p> TYPE: <code>NoiseHandler | None</code> </p> Source code in <code>qadence/backends/pyqtorch/backend.py</code> <pre><code>def set_readout_noise(circuit: ConvertedCircuit, noise: NoiseHandler) -&gt; None:\n    \"\"\"Set readout noise in place in native.\n\n    Args:\n        circuit (ConvertedCircuit):  Input converted circuit.\n        noise (NoiseHandler | None): Noise.\n    \"\"\"\n    readout = convert_readout_noise(circuit.abstract.n_qubits, noise)\n    if readout:\n        circuit.native.readout_noise = readout\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration","title":"<code>Configuration(_use_gate_params=True, use_sparse_observable=False, use_gradient_checkpointing=False, use_single_qubit_composition=False, transpilation_passes=None, algo_hevo=AlgoHEvo.EXP, ode_solver=SolverType.DP5_SE, n_steps_hevo=100, loop_expectation=False, noise=None, dropout_probability=0.0, dropout_mode=DropoutMode.ROTATIONAL)</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BackendConfiguration</code></p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.algo_hevo","title":"<code>algo_hevo: AlgoHEvo = AlgoHEvo.EXP</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine which kind of Hamiltonian evolution algorithm to use.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.dropout_mode","title":"<code>dropout_mode: DropoutMode = DropoutMode.ROTATIONAL</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of quantum dropout to perform.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.dropout_probability","title":"<code>dropout_probability: float = 0.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Quantum dropout probability (0 means no dropout).</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.loop_expectation","title":"<code>loop_expectation: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>When computing batches of expectation values, only allocate one wavefunction.</p> <p>Loop over the batch of parameters to only allocate a single wavefunction at any given time.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.n_steps_hevo","title":"<code>n_steps_hevo: int = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Default number of steps for the Hamiltonian evolution.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.noise","title":"<code>noise: NoiseHandler | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>NoiseHandler containing readout noise applied in backend.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.ode_solver","title":"<code>ode_solver: SolverType = SolverType.DP5_SE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Determine which ODE solver to use for time-dependent blocks.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.use_gradient_checkpointing","title":"<code>use_gradient_checkpointing: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Use gradient checkpointing.</p> <p>Recommended for higher-order optimization tasks.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.config.Configuration.use_single_qubit_composition","title":"<code>use_single_qubit_composition: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Composes chains of single qubit gates into a single matmul if possible.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.supported_gates","title":"<code>supported_gates = list(set(OpName.list()) - set([OpName.TDAGGER]))</code>  <code>module-attribute</code>","text":"<p>The set of supported gates.</p> <p>Tdagger is currently not supported.</p>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_block","title":"<code>convert_block(block, n_qubits=None, config=None)</code>","text":"<p>Convert block to native Pyqtorch representation.</p> PARAMETER DESCRIPTION <code>block</code> <p>Block to convert.</p> <p> TYPE: <code>AbstractBlock</code> </p> <code>n_qubits</code> <p>Number of qubits. Defaults to None.</p> <p> TYPE: <code>int</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>Backend configuration instance. Defaults to None.</p> <p> TYPE: <code>Configuration</code> DEFAULT: <code>None</code> </p> RAISES DESCRIPTION <code>NotImplementedError</code> <p>For non supported blocks.</p> RETURNS DESCRIPTION <code>Sequence[Module | Tensor | str | Expr]</code> <p>Sequence[Module | Tensor | str | sympy.Expr]: List of native operations.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_block(\n    block: AbstractBlock,\n    n_qubits: int = None,\n    config: Configuration = None,\n) -&gt; Sequence[Module | Tensor | str | sympy.Expr]:\n    \"\"\"Convert block to native Pyqtorch representation.\n\n    Args:\n        block (AbstractBlock): Block to convert.\n        n_qubits (int, optional): Number of qubits. Defaults to None.\n        config (Configuration, optional): Backend configuration instance. Defaults to None.\n\n    Raises:\n        NotImplementedError: For non supported blocks.\n\n    Returns:\n        Sequence[Module | Tensor | str | sympy.Expr]: List of native operations.\n    \"\"\"\n    if isinstance(block, (Tensor, str, sympy.Expr)):  # case for hamevo generators\n        if isinstance(block, Tensor):\n            block = block.permute(1, 2, 0)  # put batch size in the back\n        return [block]\n    qubit_support = block.qubit_support\n    if n_qubits is None:\n        n_qubits = max(qubit_support) + 1\n\n    if config is None:\n        config = Configuration()\n\n    noise: NoiseHandler | None = None\n    if hasattr(block, \"noise\") and block.noise:\n        noise = convert_digital_noise(block.noise)\n\n    if isinstance(block, ScaleBlock):\n        scaled_ops = convert_block(block.block, n_qubits, config)\n        scale = extract_parameter(block, config=config)\n\n        # replace underscore by dot when underscore is between two numbers in string\n        if isinstance(scale, str):\n            scale = replace_underscore_floats(scale)\n\n        if isinstance(scale, str) and not config._use_gate_params:\n            param = sympy_to_pyq(sympy.parse_expr(scale))\n        else:\n            param = scale\n\n        return [pyq.Scale(pyq.Sequence(scaled_ops), param)]\n\n    elif isinstance(block, TimeEvolutionBlock):\n        duration = block.duration  # type: ignore [attr-defined]\n        if getattr(block.generator, \"is_time_dependent\", False):\n            config._use_gate_params = False\n            duration = config.get_param_name(block)[1]\n            generator = convert_block(block.generator, config=config)[0]  # type: ignore [arg-type]\n        elif isinstance(block.generator, sympy.Basic):\n            generator = config.get_param_name(block)[1]\n\n        elif isinstance(block.generator, Tensor):\n            m = block.generator.to(dtype=cdouble)\n            generator = convert_block(\n                MatrixBlock(\n                    m,\n                    qubit_support=qubit_support,\n                    check_unitary=False,\n                    check_hermitian=True,\n                )\n            )[0]\n        else:\n            generator = convert_block(block.generator, n_qubits, config)[0]  # type: ignore[arg-type]\n        time_param = config.get_param_name(block)[0]\n\n        # convert noise operators here\n        noise_operators: list = [\n            convert_block(noise_block, config=config)[0] for noise_block in block.noise_operators\n        ]\n        if len(noise_operators) &gt; 0:\n            # squeeze batch size for noise operators\n            noise_operators = [\n                pyq_op.tensor(full_support=qubit_support).squeeze(-1) for pyq_op in noise_operators\n            ]\n\n        return [\n            pyq.HamiltonianEvolution(\n                qubit_support=qubit_support,\n                generator=generator,\n                time=time_param,\n                cache_length=0,\n                duration=duration,\n                solver=config.ode_solver,\n                steps=config.n_steps_hevo,\n                noise=noise_operators if len(noise_operators) &gt; 0 else None,\n            )\n        ]\n\n    elif isinstance(block, MatrixBlock):\n        return [pyq.primitives.Primitive(block.matrix, block.qubit_support, noise=noise)]\n    elif isinstance(block, CompositeBlock):\n        ops = list(flatten(*(convert_block(b, n_qubits, config) for b in block.blocks)))\n        if isinstance(block, AddBlock):\n            return [pyq.Add(ops)]  # add\n        elif is_single_qubit_chain(block) and config.use_single_qubit_composition:\n            return [pyq.Merge(ops)]  # for chains of single qubit ops on the same qubit\n        else:\n            return [pyq.Sequence(ops)]  # for kron and chain\n    elif isinstance(block, tuple(non_unitary_gateset)):\n        if isinstance(block, ProjectorBlock):\n            projector = getattr(pyq, block.name)\n            if block.name == OpName.N:\n                return [projector(target=qubit_support, noise=noise)]\n            else:\n                return [\n                    projector(\n                        qubit_support=qubit_support,\n                        ket=block.ket,\n                        bra=block.bra,\n                        noise=noise,\n                    )\n                ]\n        else:\n            return [getattr(pyq, block.name)(qubit_support[0])]\n    elif isinstance(block, tuple(single_qubit_gateset)):\n        pyq_cls = getattr(pyq, block.name)\n        if isinstance(block, ParametricBlock):\n            if isinstance(block, U):\n                op = pyq_cls(\n                    qubit_support[0],\n                    *config.get_param_name(block),\n                    noise=noise,\n                )\n            else:\n                param = extract_parameter(block, config)\n                op = pyq_cls(qubit_support[0], param, noise=noise)\n        else:\n            op = pyq_cls(qubit_support[0], noise=noise)  # type: ignore [attr-defined]\n        return [op]\n    elif isinstance(block, tuple(two_qubit_gateset)):\n        pyq_cls = getattr(pyq, block.name)\n        if isinstance(block, ParametricBlock):\n            op = pyq_cls(\n                qubit_support[0],\n                qubit_support[1],\n                extract_parameter(block, config),\n                noise=noise,\n            )\n        else:\n            op = pyq_cls(\n                qubit_support[0], qubit_support[1], noise=noise  # type: ignore [attr-defined]\n            )\n        return [op]\n    elif isinstance(block, tuple(three_qubit_gateset) + tuple(multi_qubit_gateset)):\n        block_name = block.name[1:] if block.name.startswith(\"M\") else block.name\n        pyq_cls = getattr(pyq, block_name)\n        if isinstance(block, ParametricBlock):\n            op = pyq_cls(\n                qubit_support[:-1],\n                qubit_support[-1],\n                extract_parameter(block, config),\n                noise=noise,\n            )\n        else:\n            if \"CSWAP\" in block_name:\n                op = pyq_cls(\n                    qubit_support[:-2], qubit_support[-2:], noise=noise  # type: ignore [attr-defined]\n                )\n            else:\n                op = pyq_cls(\n                    qubit_support[:-1], qubit_support[-1], noise=noise  # type: ignore [attr-defined]\n                )\n        return [op]\n    else:\n        raise NotImplementedError(\n            f\"Non supported operation of type {type(block)}. \"\n            \"In case you are trying to run an `AnalogBlock`, make sure you \"\n            \"specify the `device_specs` in your `Register` first.\"\n        )\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_digital_noise","title":"<code>convert_digital_noise(noise)</code>","text":"<p>Convert the digital noise into pyqtorch NoiseProtocol.</p> PARAMETER DESCRIPTION <code>noise</code> <p>Noise to convert.</p> <p> TYPE: <code>NoiseHandler</code> </p> RETURNS DESCRIPTION <code>DigitalNoiseProtocol | None</code> <p>pyq.noise.DigitalNoiseProtocol | None: Pyqtorch native noise protocol if there are any digital noise protocols.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_digital_noise(noise: NoiseHandler) -&gt; pyq.noise.DigitalNoiseProtocol | None:\n    \"\"\"Convert the digital noise into pyqtorch NoiseProtocol.\n\n    Args:\n        noise (NoiseHandler): Noise to convert.\n\n    Returns:\n        pyq.noise.DigitalNoiseProtocol | None: Pyqtorch native noise protocol\n            if there are any digital noise protocols.\n    \"\"\"\n    digital_part = noise.filter(NoiseProtocol.DIGITAL)\n    if digital_part is None:\n        return None\n    return pyq.noise.DigitalNoiseProtocol(\n        [\n            pyq.noise.DigitalNoiseProtocol(proto, option.get(\"error_probability\"))\n            for proto, option in zip(digital_part.protocol, digital_part.options)\n        ]\n    )\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.convert_readout_noise","title":"<code>convert_readout_noise(n_qubits, noise)</code>","text":"<p>Convert the readout noise into pyqtorch ReadoutNoise.</p> PARAMETER DESCRIPTION <code>n_qubits</code> <p>Number of qubits</p> <p> TYPE: <code>int</code> </p> <code>noise</code> <p>Noise to convert.</p> <p> TYPE: <code>NoiseHandler</code> </p> RETURNS DESCRIPTION <code>ReadoutNoise | None</code> <p>pyq.noise.ReadoutNoise | None: Pyqtorch native ReadoutNoise instance if readout is is noise.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def convert_readout_noise(n_qubits: int, noise: NoiseHandler) -&gt; pyq.noise.ReadoutNoise | None:\n    \"\"\"Convert the readout noise into pyqtorch ReadoutNoise.\n\n    Args:\n        n_qubits (int): Number of qubits\n        noise (NoiseHandler):  Noise to convert.\n\n    Returns:\n        pyq.noise.ReadoutNoise | None: Pyqtorch native ReadoutNoise instance\n            if readout is is noise.\n    \"\"\"\n    readout_part = noise.filter(NoiseProtocol.READOUT)\n    if readout_part is None:\n        return None\n\n    if readout_part.protocol[0] == NoiseProtocol.READOUT.INDEPENDENT:\n        return pyq.noise.ReadoutNoise(n_qubits, **readout_part.options[0])\n    else:\n        return pyq.noise.CorrelatedReadoutNoise(**readout_part.options[0])\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.extract_parameter","title":"<code>extract_parameter(block, config)</code>","text":"<p>Extract the parameter as string or its tensor value.</p> PARAMETER DESCRIPTION <code>block</code> <p>Block to extract parameter from.</p> <p> TYPE: <code>ScaleBlock | ParametricBlock</code> </p> <code>config</code> <p>Configuration instance.</p> <p> TYPE: <code>Configuration</code> </p> RETURNS DESCRIPTION <code>str | Tensor</code> <p>str | Tensor: Parameter value or symbol.</p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def extract_parameter(block: ScaleBlock | ParametricBlock, config: Configuration) -&gt; str | Tensor:\n    \"\"\"Extract the parameter as string or its tensor value.\n\n    Args:\n        block (ScaleBlock | ParametricBlock): Block to extract parameter from.\n        config (Configuration): Configuration instance.\n\n    Returns:\n        str | Tensor: Parameter value or symbol.\n    \"\"\"\n    if not block.is_parametric:\n        tensor_val = tensor([block.parameters.parameter], dtype=complex64)\n        return (\n            tensor([block.parameters.parameter], dtype=float64)\n            if torch.all(tensor_val.imag == 0)\n            else tensor_val\n        )\n\n    return config.get_param_name(block)[0]\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.replace_underscore_floats","title":"<code>replace_underscore_floats(s)</code>","text":"<p>Replace underscores with periods for all floats in given string.</p> <p>Needed for correct parsing of string by sympy parser.</p> PARAMETER DESCRIPTION <code>s</code> <p>string expression</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>transformed string expression</p> <p> TYPE: <code>str</code> </p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def replace_underscore_floats(s: str) -&gt; str:\n    \"\"\"Replace underscores with periods for all floats in given string.\n\n    Needed for correct parsing of string by sympy parser.\n\n    Args:\n        s (str): string expression\n\n    Returns:\n        str: transformed string expression\n    \"\"\"\n\n    # Regular expression to match floats written with underscores instead of dots\n    float_with_underscore_pattern = r\"\"\"\n        (?&lt;!\\w)            # Negative lookbehind to ensure not part of a word\n        -?                 # Optional negative sign\n        \\d+                # One or more digits (before underscore)\n        _                  # The underscore acting as decimal separator\n        \\d+                # One or more digits (after underscore)\n        ([eE][-+]?\\d+)?    # Optional exponent part for scientific notation\n        (?!\\w)             # Negative lookahead to ensure not part of a word\n    \"\"\"\n\n    # Function to replace the underscore with a dot\n    def underscore_to_dot(match: re.Match) -&gt; Any:\n        return match.group(0).replace(\"_\", \".\")\n\n    # Compile the regular expression\n    pattern = re.compile(float_with_underscore_pattern, re.VERBOSE)\n\n    return pattern.sub(underscore_to_dot, s)\n</code></pre>"},{"location":"api/backends/pyqtorch/#qadence.backends.pyqtorch.convert_ops.sympy_to_pyq","title":"<code>sympy_to_pyq(expr)</code>","text":"<p>Convert sympy expression to pyqtorch ConcretizedCallable object.</p> PARAMETER DESCRIPTION <code>expr</code> <p>sympy expression</p> <p> TYPE: <code>Expr</code> </p> RETURNS DESCRIPTION <code>ConcretizedCallable</code> <p>expression encoded as ConcretizedCallable</p> <p> TYPE: <code>ConcretizedCallable | Tensor</code> </p> Source code in <code>qadence/backends/pyqtorch/convert_ops.py</code> <pre><code>def sympy_to_pyq(expr: sympy.Expr) -&gt; ConcretizedCallable | Tensor:\n    \"\"\"Convert sympy expression to pyqtorch ConcretizedCallable object.\n\n    Args:\n        expr (sympy.Expr): sympy expression\n\n    Returns:\n        ConcretizedCallable: expression encoded as ConcretizedCallable\n    \"\"\"\n\n    # base case - independent argument\n    if len(expr.args) == 0:\n        try:\n            res = torch.as_tensor(float(expr))\n        except Exception as e:\n            res = str(expr)\n\n            if \"/\" in res:  # Found a rational\n                res = torch.as_tensor(float(sympy.Rational(res).evalf()))\n        return res\n\n    # Recursively iterate through current function arguments\n    all_results = []\n    for arg in expr.args:\n        res = sympy_to_pyq(arg)\n        all_results.append(res)\n\n    # deal with multi-argument (&gt;2) sympy functions: converting to nested\n    # ConcretizedCallable objects\n    if len(all_results) &gt; 2:\n\n        def fn(x: str | ConcretizedCallable, y: str | ConcretizedCallable) -&gt; Callable:\n            return partial(ConcretizedCallable, call_name=SYMPY_TO_PYQ_MAPPING[expr.func])(  # type: ignore [no-any-return]\n                abstract_args=[x, y]\n            )\n\n        concretized_callable = reduce(fn, all_results)\n    else:\n        concretized_callable = ConcretizedCallable(SYMPY_TO_PYQ_MAPPING[expr.func], all_results)\n    return concretized_callable\n</code></pre>"},{"location":"content/backends/","title":"Backends","text":"<p>Backends allow execution of Qadence abstract quantum circuits. They could be chosen from a variety of simulators, emulators and hardware and can enable circuit differentiability. The primary way to interact and configure a backend is via the high-level API <code>QuantumModel</code>.</p> <p>Not all backends are equivalent</p> <p>Not all backends support the same set of operations, especially while executing analog blocks. Qadence will throw descriptive errors in such cases.</p>"},{"location":"content/backends/#execution-backends","title":"Execution backends","text":"<p>PyQTorch: An efficient, large-scale simulator designed for quantum machine learning, seamlessly integrated with the popular PyTorch deep learning framework for automatic differentiability. It also offers analog computing for time-(in)dependent pulses. See <code>PyQTorchBackend</code>.</p> <p>Pulser: A Python library for pulse-level/analog control of neutral atom devices. Execution via QuTiP. See <code>PulserBackend</code>.</p> <p>More: Proprietary Qadence extensions provide more high-performance backends based on tensor networks or differentiation engines. For more enquiries, please contact: <code>info@pasqal.com</code>.</p>"},{"location":"content/backends/#differentiation-backend","title":"Differentiation backend","text":"<p>The <code>DifferentiableBackend</code> class enables different differentiation modes for the given backend. This can be chosen from two types:</p> <ul> <li>Automatic differentiation (AD): available for PyTorch based backends (PyQTorch).</li> <li>Parameter Shift Rules (PSR): available for all backends. See this section for more information on differentiability and PSR.</li> </ul> <p>In practice, only a <code>diff_mode</code> should be provided in the <code>QuantumModel</code>. Please note that <code>diff_mode</code> defaults to <code>None</code>:</p> <pre><code>import sympy\nimport torch\nfrom qadence import Parameter, RX, RZ, Z, CNOT, QuantumCircuit, QuantumModel, chain, BackendName, DiffMode\n\nx = Parameter(\"x\", trainable=False)\ny = Parameter(\"y\", trainable=False)\nfm = chain(\n    RX(0, 3 * x),\n    RX(0, x),\n    RZ(1, sympy.exp(y)),\n    RX(0, 3.14),\n    RZ(1, \"theta\")\n)\n\nansatz = CNOT(0, 1)\nblock = chain(fm, ansatz)\n\ncircuit = QuantumCircuit(2, block)\n\nobservable = Z(0)\n\n# DiffMode.GPSR is available for any backend.\n# DiffMode.AD is only available for natively differentiable backends.\nmodel = QuantumModel(circuit, observable, backend=BackendName.PYQTORCH, diff_mode=DiffMode.GPSR)\n\n# Get some values for the feature parameters.\nvalues = {\"x\": (x := torch.tensor([0.5], requires_grad=True)), \"y\": torch.tensor([0.1])}\n\n# Compute expectation.\nexp = model.expectation(values)\n\n# Differentiate the expectation wrt x.\ndexp_dx = torch.autograd.grad(exp, x, torch.ones_like(exp))\n</code></pre> <pre><code>dexp_dx = (tensor([3.6398]),)\n</code></pre>"},{"location":"content/backends/#low-level-backend_factory-interface","title":"Low-level <code>backend_factory</code> interface","text":"<p>Every backend in Qadence inherits from the abstract <code>Backend</code> class: <code>Backend</code> and implement the following methods:</p> <ul> <li><code>run</code>: propagate the initial state according to the quantum circuit and return the final wavefunction object.</li> <li><code>sample</code>: sample from a circuit.</li> <li><code>expectation</code>: computes the expectation of a circuit given an observable.</li> <li><code>convert</code>: convert the abstract <code>QuantumCircuit</code> object to its backend-native representation including a backend specific parameter embedding function.</li> </ul> <p>Backends are purely functional objects which take as input the values for the circuit parameters and return the desired output from a call to a method. In order to use a backend directly, embedded parameters must be supplied as they are returned by the backend specific embedding function.</p> <p>Here is a simple demonstration of the use of the PyQTorch backend to execute a circuit in non-differentiable mode:</p> <pre><code>from qadence import QuantumCircuit, FeatureParameter, RX, RZ, CNOT, hea, chain\n\n# Construct a feature map.\nx = FeatureParameter(\"x\")\nz = FeatureParameter(\"y\")\nfm = chain(RX(0, 3 * x), RZ(1, z), CNOT(0, 1))\n\n# Construct a circuit with an hardware-efficient ansatz.\ncircuit = QuantumCircuit(3, fm, hea(3,1))\n</code></pre> <p>The abstract <code>QuantumCircuit</code> can now be converted to its native representation via the PyQTorch backend.</p> <pre><code>from qadence import backend_factory\n\n# Use only PyQtorch in non-differentiable mode:\nbackend = backend_factory(\"pyqtorch\")\n\n# The `Converted` object\n# (contains a `ConvertedCircuit` with the original and native representation)\nconv = backend.convert(circuit)\n</code></pre> <pre><code>conv.circuit.original = ChainBlock(0,1,2)\n\u251c\u2500\u2500 ChainBlock(0,1)\n\u2502   \u251c\u2500\u2500 RX(0) [params: ['3*x']]\n\u2502   \u251c\u2500\u2500 RZ(1) [params: ['y']]\n\u2502   \u2514\u2500\u2500 CNOT(0, 1)\n\u2514\u2500\u2500 ChainBlock(0,1,2) [tag: HEA]\n    \u251c\u2500\u2500 ChainBlock(0,1,2)\n    \u2502   \u251c\u2500\u2500 KronBlock(0,1,2)\n    \u2502   \u2502   \u251c\u2500\u2500 RX(0) [params: ['theta_0']]\n    \u2502   \u2502   \u251c\u2500\u2500 RX(1) [params: ['theta_1']]\n    \u2502   \u2502   \u2514\u2500\u2500 RX(2) [params: ['theta_2']]\n    \u2502   \u251c\u2500\u2500 KronBlock(0,1,2)\n    \u2502   \u2502   \u251c\u2500\u2500 RY(0) [params: ['theta_3']]\n    \u2502   \u2502   \u251c\u2500\u2500 RY(1) [params: ['theta_4']]\n    \u2502   \u2502   \u2514\u2500\u2500 RY(2) [params: ['theta_5']]\n    \u2502   \u2514\u2500\u2500 KronBlock(0,1,2)\n    \u2502       \u251c\u2500\u2500 RX(0) [params: ['theta_6']]\n    \u2502       \u251c\u2500\u2500 RX(1) [params: ['theta_7']]\n    \u2502       \u2514\u2500\u2500 RX(2) [params: ['theta_8']]\n    \u2514\u2500\u2500 ChainBlock(0,1,2)\n        \u251c\u2500\u2500 KronBlock(0,1)\n        \u2502   \u2514\u2500\u2500 CNOT(0, 1)\n        \u2514\u2500\u2500 KronBlock(1,2)\n            \u2514\u2500\u2500 CNOT(1, 2)\nconv.circuit.native = QuantumCircuit(\n  (operations): ModuleList(\n    (0): Sequence(\n      (operations): ModuleList(\n        (0): Sequence(\n          (operations): ModuleList(\n            (0): RX(target: (0,), param: bda01c07-c9ef-4fd6-a633-c90038977783)\n            (1): RZ(target: (1,), param: 12f41cf5-c0d9-4ebd-9b06-b9dfcbbc7179)\n            (2): CNOT(control: (0,), target: (1,))\n          )\n        )\n        (1): Sequence(\n          (operations): ModuleList(\n            (0): Sequence(\n              (operations): ModuleList(\n                (0): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (0,), param: b808b10a-e85f-4e2c-a919-5fc80bb7ed67)\n                    (1): RY(target: (0,), param: 31e57b6f-4c49-431a-91dc-611287604af2)\n                    (2): RX(target: (0,), param: fd671271-5606-48e2-8833-452de1fb1b79)\n                  )\n                )\n                (1): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (1,), param: d1759e5b-f59b-40bc-a668-91abff68845b)\n                    (1): RY(target: (1,), param: 98e829f7-4d55-4312-8767-eaed19b38ddd)\n                    (2): RX(target: (1,), param: ba038256-7e5d-4a2e-890e-feb1a4d323df)\n                  )\n                )\n                (2): Merge(\n                  (operations): ModuleList(\n                    (0): RX(target: (2,), param: 0e90f2ff-8b01-43fc-8508-64397963a4c0)\n                    (1): RY(target: (2,), param: 25b6f153-50e8-448c-824d-367c2fe59be5)\n                    (2): RX(target: (2,), param: 4cf2ccb4-4652-4a2c-9e30-27fb3a9659f1)\n                  )\n                )\n              )\n            )\n            (1): Sequence(\n              (operations): ModuleList(\n                (0): Sequence(\n                  (operations): ModuleList(\n                    (0): CNOT(control: (0,), target: (1,))\n                  )\n                )\n                (1): Sequence(\n                  (operations): ModuleList(\n                    (0): CNOT(control: (1,), target: (2,))\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n  )\n)\n</code></pre> <p>Additionally, <code>Converted</code> contains all fixed and variational parameters, as well as an embedding function which accepts feature parameters to construct a dictionary of circuit native parameters. These are needed as each backend uses a different representation of the circuit parameters:</p> <pre><code>import torch\n\n# Contains fixed parameters and variational (from the HEA)\nconv.params\n\ninputs = {\"x\": torch.tensor([1., 1.]), \"y\":torch.tensor([2., 2.])}\n\n# get all circuit parameters (including feature params)\nembedded = conv.embedding_fn(conv.params, inputs)\n</code></pre> <pre><code>conv.params = {\n  theta_0: tensor([0.1209], requires_grad=True)\n  theta_4: tensor([0.2303], requires_grad=True)\n  theta_7: tensor([0.3370], requires_grad=True)\n  theta_5: tensor([0.1556], requires_grad=True)\n  theta_1: tensor([0.9662], requires_grad=True)\n  theta_3: tensor([0.0068], requires_grad=True)\n  theta_2: tensor([0.7121], requires_grad=True)\n  theta_6: tensor([0.2225], requires_grad=True)\n  theta_8: tensor([0.6760], requires_grad=True)\n}\nembedded = {\n  bda01c07-c9ef-4fd6-a633-c90038977783: tensor([3., 3.], grad_fn=&lt;ViewBackward0&gt;)\n  12f41cf5-c0d9-4ebd-9b06-b9dfcbbc7179: tensor([2., 2.])\n  b808b10a-e85f-4e2c-a919-5fc80bb7ed67: tensor([0.1209], grad_fn=&lt;ViewBackward0&gt;)\n  31e57b6f-4c49-431a-91dc-611287604af2: tensor([0.0068], grad_fn=&lt;ViewBackward0&gt;)\n  fd671271-5606-48e2-8833-452de1fb1b79: tensor([0.2225], grad_fn=&lt;ViewBackward0&gt;)\n  d1759e5b-f59b-40bc-a668-91abff68845b: tensor([0.9662], grad_fn=&lt;ViewBackward0&gt;)\n  98e829f7-4d55-4312-8767-eaed19b38ddd: tensor([0.2303], grad_fn=&lt;ViewBackward0&gt;)\n  ba038256-7e5d-4a2e-890e-feb1a4d323df: tensor([0.3370], grad_fn=&lt;ViewBackward0&gt;)\n  0e90f2ff-8b01-43fc-8508-64397963a4c0: tensor([0.7121], grad_fn=&lt;ViewBackward0&gt;)\n  25b6f153-50e8-448c-824d-367c2fe59be5: tensor([0.1556], grad_fn=&lt;ViewBackward0&gt;)\n  4cf2ccb4-4652-4a2c-9e30-27fb3a9659f1: tensor([0.6760], grad_fn=&lt;ViewBackward0&gt;)\n}\n</code></pre> <p>With the embedded parameters, <code>QuantumModel</code> methods are accessible:</p> <pre><code>output = backend.run(conv.circuit, embedded)\nprint(f\"{output = }\")\n</code></pre> <pre><code>output = tensor([[ 0.0989-0.0053j,  0.0055-0.0829j,  0.0491+0.0685j, -0.0741+0.0679j,\n         -0.5184-0.2958j, -0.2980+0.4020j,  0.3327+0.1941j, -0.1826+0.4216j],\n        [ 0.0989-0.0053j,  0.0055-0.0829j,  0.0491+0.0685j, -0.0741+0.0679j,\n         -0.5184-0.2958j, -0.2980+0.4020j,  0.3327+0.1941j, -0.1826+0.4216j]],\n       grad_fn=&lt;TBackward0&gt;)\n</code></pre>"},{"location":"content/backends/#lower-level-the-backend-representation","title":"Lower-level: the <code>Backend</code> representation","text":"<p>If there is a requirement to work with a specific backend, it is possible to access directly the native circuit. For example, should one wish to use PyQtorch noise features directly instead of using the <code>NoiseHandler</code> interface from Qadence:</p> <pre><code>from pyqtorch.noise import Depolarizing\n\ninputs = {\"x\": torch.rand(1), \"y\":torch.rand(1)}\nembedded = conv.embedding_fn(conv.params, inputs)\n\n# Define a noise channel on qubit 0\nnoise = Depolarizing(0, error_probability=0.1)\n\n# Add noise to circuit\nconv.circuit.native.operations.append(noise)\n</code></pre> <p>When running With noise, one can see that the output is a density matrix:</p> <pre><code>density_result = backend.run(conv.circuit, embedded)\nprint(density_result.shape)\n</code></pre> <pre><code>torch.Size([1, 8, 8])\n</code></pre>"},{"location":"content/block_system/","title":"Block system","text":"<p>Quantum programs in Qadence are constructed using a block-system, with an emphasis on composability of primitive blocks to obtain larger, composite blocks. This functional approach is different from other frameworks which follow a more object-oriented way to construct circuits and express programs.</p>"},{"location":"content/block_system/#primitive-blocks","title":"Primitive blocks","text":"<p>A <code>PrimitiveBlock</code> represents a digital or an analog time-evolution quantum operation applied to a qubit support. Programs can always be decomposed down into a sequence of <code>PrimitiveBlock</code> elements.</p> <p>Two canonical examples of digital primitive blocks are the parametrized <code>RX</code> and the <code>CNOT</code> gates:</p> <pre><code>from qadence import chain, RX, CNOT\n\nrx = RX(0, 0.5)\ncnot = CNOT(0, 1)\n\nblock = chain(rx, cnot)\n</code></pre> %3 f4200e97429c48bc82c0fe7896556598 0 16b5daa4d0f54f13acb711fdd4e1f69d RX(0.5) f4200e97429c48bc82c0fe7896556598--16b5daa4d0f54f13acb711fdd4e1f69d 59b6983dc2324d1682873215e2cac39c 1 9acedb75e50f438792ad55a4f7896e2a 16b5daa4d0f54f13acb711fdd4e1f69d--9acedb75e50f438792ad55a4f7896e2a 6caec729b67241da8a9f995ce34ba50c 9acedb75e50f438792ad55a4f7896e2a--6caec729b67241da8a9f995ce34ba50c cdc160bd294e4276ae30c7ebf57b5b56 8bcf9d845587406fb1d10b1108e10bc2 59b6983dc2324d1682873215e2cac39c--8bcf9d845587406fb1d10b1108e10bc2 e7468e5d7eca45c3ae78650383a25fa6 X 8bcf9d845587406fb1d10b1108e10bc2--e7468e5d7eca45c3ae78650383a25fa6 e7468e5d7eca45c3ae78650383a25fa6--9acedb75e50f438792ad55a4f7896e2a e7468e5d7eca45c3ae78650383a25fa6--cdc160bd294e4276ae30c7ebf57b5b56 <p>A list of all available primitive operations can be found here.</p> How to visualize blocks <p>There are two ways to display blocks in a Python interpreter: either as a tree in ASCII format using <code>print</code>:</p> <pre><code>from qadence import X, Y, kron\n\nkron_block = kron(X(0), Y(1))\nprint(kron_block)\n</code></pre> <pre><code>KronBlock(0,1)\n\u251c\u2500\u2500 X(0)\n\u2514\u2500\u2500 Y(1)\n</code></pre> <p>Or using the visualization package:</p> <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nkron_block = kron(X(0), Y(1))\n# display(kron_block)\n</code></pre> %3 af355b7421674eba8dd1ebc0d0c7342e 0 e5a348c1895e457bb7eaae61d554c5c1 X af355b7421674eba8dd1ebc0d0c7342e--e5a348c1895e457bb7eaae61d554c5c1 551dd656a68c4603bb03227d3c4b2b91 1 8f3b65c957d745a6a85ad5089487f288 e5a348c1895e457bb7eaae61d554c5c1--8f3b65c957d745a6a85ad5089487f288 ba8d1a5a3a804276928eed55bc9e00c6 e8f7e5c303dd41a8b2a8616133e8efd6 Y 551dd656a68c4603bb03227d3c4b2b91--e8f7e5c303dd41a8b2a8616133e8efd6 e8f7e5c303dd41a8b2a8616133e8efd6--ba8d1a5a3a804276928eed55bc9e00c6"},{"location":"content/block_system/#composite-blocks","title":"Composite Blocks","text":"<p>Programs can be expressed by composing blocks to result in a larger <code>CompositeBlock</code> using three fundamental operations: chain, kron, and add.</p> <ul> <li>chain applies a set of blocks in sequence, which can have overlapping qubit supports, and results in a <code>ChainBlock</code> type. It is akin to applying a matrix product of the sub-blocks, and can also be used with the <code>*</code> operator.</li> <li>kron applies a set of blocks in parallel, requiring disjoint qubit support, and results in a <code>KronBlock</code> type. This is akin to applying a tensor product of the sub-blocks, and can also be used with the <code>@</code> operator.</li> <li>add performs a direct sum of the operators, and results in an <code>AddBlock</code> type. Blocks constructed this way are typically non-unitary, as is the case for Hamiltonians which can be constructed through sums of Pauli strings. Addition can also be performed directly with the <code>+</code> operator.</li> </ul> <pre><code>from qadence import X, Y, chain, kron\n\nchain_0 = chain(X(0), Y(0))\nchain_1 = chain(X(1), Y(1))\n\nkron_block = kron(chain_0, chain_1)\n</code></pre> %3 b7e55bcefa5847ffb8b206a6922971bd 0 ca6f2a23e04b4db98fbc594e0283f75e X b7e55bcefa5847ffb8b206a6922971bd--ca6f2a23e04b4db98fbc594e0283f75e bdf44919e8fd429e8a61f2f66d16a552 1 b5ee21d3de8341388beeef79608ad347 Y ca6f2a23e04b4db98fbc594e0283f75e--b5ee21d3de8341388beeef79608ad347 63bb92e17aad42ef85acbe19020c74e9 b5ee21d3de8341388beeef79608ad347--63bb92e17aad42ef85acbe19020c74e9 001117446c444f5287e349c1ea75dc28 365491e6a65b4720b3792214fc439246 X bdf44919e8fd429e8a61f2f66d16a552--365491e6a65b4720b3792214fc439246 cec1117da1c54c9ebbe63e50a0bc63fb Y 365491e6a65b4720b3792214fc439246--cec1117da1c54c9ebbe63e50a0bc63fb cec1117da1c54c9ebbe63e50a0bc63fb--001117446c444f5287e349c1ea75dc28 <p>All composition functions support list comprehension syntax. Below we exemplify the creation of an XY Hamiltonian for qubits laid out on a line.</p> <pre><code>from qadence import X, Y, add\n\ndef xy_int(i: int, j: int):\n    return (1/2) * (X(i)@X(j) + Y(i)@Y(j))\n\nn_qubits = 3\n\nxy_ham = add(xy_int(i, i+1) for i in range(n_qubits-1))\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 AddBlock(0,1)\n\u2502       \u251c\u2500\u2500 KronBlock(0,1)\n\u2502       \u2502   \u251c\u2500\u2500 X(0)\n\u2502       \u2502   \u2514\u2500\u2500 X(1)\n\u2502       \u2514\u2500\u2500 KronBlock(0,1)\n\u2502           \u251c\u2500\u2500 Y(0)\n\u2502           \u2514\u2500\u2500 Y(1)\n\u2514\u2500\u2500 [mul: 0.500] \n    \u2514\u2500\u2500 AddBlock(1,2)\n        \u251c\u2500\u2500 KronBlock(1,2)\n        \u2502   \u251c\u2500\u2500 X(1)\n        \u2502   \u2514\u2500\u2500 X(2)\n        \u2514\u2500\u2500 KronBlock(1,2)\n            \u251c\u2500\u2500 Y(1)\n            \u2514\u2500\u2500 Y(2)\n</code></pre> <p>Qadence blocks can be directly translated to matrix form by calling <code>block.tensor()</code>. Note that first dimension is the batch dimension, following PyTorch conventions. This becomes relevant if the block are parameterized and batched input values are passed, as we will see later.</p> <pre><code>from qadence import X, Y\n\nxy = (1/2) * (X(0)@X(1) + Y(0)@Y(1))\n\nprint(xy.tensor().real)\n</code></pre> <pre><code>tensor([[[0., 0., 0., 0.],\n         [0., 0., 1., 0.],\n         [0., 1., 0., 0.],\n         [0., 0., 0., 0.]]])\n</code></pre> <p>For a final example of the flexibility of functional block composition, below is an implementation of the Quantum Fourier Transform on an arbitrary qubit support.</p> <pre><code>from qadence import H, CPHASE, PI, chain, kron\n\ndef qft_layer(qs: tuple, l: int):\n    cphases = chain(CPHASE(qs[j], qs[l], PI/2**(j-l)) for j in range(l+1, len(qs)))\n    return H(qs[l]) * cphases\n\ndef qft(qs: tuple):\n    return chain(qft_layer(qs, l) for l in range(len(qs)))\n</code></pre> %3 2cdf5e2dce3b400e81e61ddc899027cb 0 15034f690e2149afb95bf5d415608804 H 2cdf5e2dce3b400e81e61ddc899027cb--15034f690e2149afb95bf5d415608804 d6e56e36977f44cdbf22c1482890b477 1 a08ed97defdd4496b0493e35eb2d70b0 PHASE(1.571) 15034f690e2149afb95bf5d415608804--a08ed97defdd4496b0493e35eb2d70b0 dcdfe921e2774f3ab08e03ccbde05800 PHASE(0.785) a08ed97defdd4496b0493e35eb2d70b0--dcdfe921e2774f3ab08e03ccbde05800 5cfbaf7fa61f459ba6234622b2a0ba44 a08ed97defdd4496b0493e35eb2d70b0--5cfbaf7fa61f459ba6234622b2a0ba44 71d85d4798b6431da8da2463cd168d7b dcdfe921e2774f3ab08e03ccbde05800--71d85d4798b6431da8da2463cd168d7b 39187da2f9eb4ba5945f2f8aa6d45740 dcdfe921e2774f3ab08e03ccbde05800--39187da2f9eb4ba5945f2f8aa6d45740 aedf5d0a71984603aae8b48c5a279a8d 71d85d4798b6431da8da2463cd168d7b--aedf5d0a71984603aae8b48c5a279a8d ce90892734104c35a6b39aff5019f0ea aedf5d0a71984603aae8b48c5a279a8d--ce90892734104c35a6b39aff5019f0ea baaaf9b5d9654b7a82d6d382a45c293c ce90892734104c35a6b39aff5019f0ea--baaaf9b5d9654b7a82d6d382a45c293c 0a09c2b6d5144328bb5640b85370f384 25ef6f983add4d7da842128dbd8fa2f4 d6e56e36977f44cdbf22c1482890b477--25ef6f983add4d7da842128dbd8fa2f4 e086441580244d9b800e88ddbc05807f 2 25ef6f983add4d7da842128dbd8fa2f4--5cfbaf7fa61f459ba6234622b2a0ba44 5c3c2f30c039427ebd4745a9f901fab5 5cfbaf7fa61f459ba6234622b2a0ba44--5c3c2f30c039427ebd4745a9f901fab5 c1a9d146a19f42f183c4e32814cb4e9d H 5c3c2f30c039427ebd4745a9f901fab5--c1a9d146a19f42f183c4e32814cb4e9d 79d004e7fd974bc8820d1f9e3a58cf0e PHASE(1.571) c1a9d146a19f42f183c4e32814cb4e9d--79d004e7fd974bc8820d1f9e3a58cf0e bcef05a4083848ac889eb40832248577 79d004e7fd974bc8820d1f9e3a58cf0e--bcef05a4083848ac889eb40832248577 1f2491fe6b92400688984cc77827f9f8 79d004e7fd974bc8820d1f9e3a58cf0e--1f2491fe6b92400688984cc77827f9f8 bcef05a4083848ac889eb40832248577--0a09c2b6d5144328bb5640b85370f384 9515a3412e7c41fea6f9853101be390b 8956f602c24c483c8aaa2e8a625d6649 e086441580244d9b800e88ddbc05807f--8956f602c24c483c8aaa2e8a625d6649 39ce98580bae4c00bdca28b5e1195a0b 8956f602c24c483c8aaa2e8a625d6649--39ce98580bae4c00bdca28b5e1195a0b 39ce98580bae4c00bdca28b5e1195a0b--39187da2f9eb4ba5945f2f8aa6d45740 45b751d4b5b74f87bd369e83932cd1fa 39187da2f9eb4ba5945f2f8aa6d45740--45b751d4b5b74f87bd369e83932cd1fa 45b751d4b5b74f87bd369e83932cd1fa--1f2491fe6b92400688984cc77827f9f8 7c11c7a83a124b28bc309364e701b6d9 H 1f2491fe6b92400688984cc77827f9f8--7c11c7a83a124b28bc309364e701b6d9 7c11c7a83a124b28bc309364e701b6d9--9515a3412e7c41fea6f9853101be390b <p>Other functionalities are directly built in the block system. For example, the inverse operation can be created with the <code>dagger()</code> method.</p> <pre><code>qft_inv = qft((0, 1, 2)).dagger()\n</code></pre> %3 1682155a510c4035b0928988e9aa6f4f 0 25d1daf476ee4289b7cccf587dbb4798 1682155a510c4035b0928988e9aa6f4f--25d1daf476ee4289b7cccf587dbb4798 a3a860e7b81a428e847e61a80668f0ce 1 6552ccd494044a17a3a3d23af567a733 25d1daf476ee4289b7cccf587dbb4798--6552ccd494044a17a3a3d23af567a733 4e66efe1a8d540d390a4317b3dae0e18 6552ccd494044a17a3a3d23af567a733--4e66efe1a8d540d390a4317b3dae0e18 f869b7132fff4b87857d6fad05fb74c2 PHASE(-0.785) 4e66efe1a8d540d390a4317b3dae0e18--f869b7132fff4b87857d6fad05fb74c2 ba64a8442ca64486a8cafed5eb5cef92 PHASE(-1.571) f869b7132fff4b87857d6fad05fb74c2--ba64a8442ca64486a8cafed5eb5cef92 9571a4142ae8422cb5077693213707e3 f869b7132fff4b87857d6fad05fb74c2--9571a4142ae8422cb5077693213707e3 43eee2e11afe411f8fda67fbfaa44cdf H ba64a8442ca64486a8cafed5eb5cef92--43eee2e11afe411f8fda67fbfaa44cdf f3aab38ed661447bba5c7c037a7a7b7b ba64a8442ca64486a8cafed5eb5cef92--f3aab38ed661447bba5c7c037a7a7b7b 96f36dfd92184f89ad6b09c60eac5012 43eee2e11afe411f8fda67fbfaa44cdf--96f36dfd92184f89ad6b09c60eac5012 804b2edaec0f4ed3857d2389e9001dc6 a461a6da4c214c49809ff260539297c1 a3a860e7b81a428e847e61a80668f0ce--a461a6da4c214c49809ff260539297c1 0056f759fce643a39a5bd56ec8efe375 2 5bb417a06ea94926bbdf7cac9d0e4c5d PHASE(-1.571) a461a6da4c214c49809ff260539297c1--5bb417a06ea94926bbdf7cac9d0e4c5d 9cc1c9237d964481a2c6f555f6c46e6c H 5bb417a06ea94926bbdf7cac9d0e4c5d--9cc1c9237d964481a2c6f555f6c46e6c 8eb89145dd8e4d2c99dc896ee5715fc5 5bb417a06ea94926bbdf7cac9d0e4c5d--8eb89145dd8e4d2c99dc896ee5715fc5 2094f6c1d7914b4199c439da92b641fc 9cc1c9237d964481a2c6f555f6c46e6c--2094f6c1d7914b4199c439da92b641fc 2094f6c1d7914b4199c439da92b641fc--f3aab38ed661447bba5c7c037a7a7b7b d59e7428752b4e84a0460863f2637754 f3aab38ed661447bba5c7c037a7a7b7b--d59e7428752b4e84a0460863f2637754 d59e7428752b4e84a0460863f2637754--804b2edaec0f4ed3857d2389e9001dc6 5d0ec0a91c7d42dc99ad753ffb3a30f9 51907076602e46e98838bf7a36ae0a2c H 0056f759fce643a39a5bd56ec8efe375--51907076602e46e98838bf7a36ae0a2c 51907076602e46e98838bf7a36ae0a2c--8eb89145dd8e4d2c99dc896ee5715fc5 abcdaf5d3d7844869c75c0f13950287c 8eb89145dd8e4d2c99dc896ee5715fc5--abcdaf5d3d7844869c75c0f13950287c abcdaf5d3d7844869c75c0f13950287c--9571a4142ae8422cb5077693213707e3 2097a4187de64b45953b43667c5bc347 9571a4142ae8422cb5077693213707e3--2097a4187de64b45953b43667c5bc347 3202dedaec8a4111bf90fbc4a0f2b172 2097a4187de64b45953b43667c5bc347--3202dedaec8a4111bf90fbc4a0f2b172 3202dedaec8a4111bf90fbc4a0f2b172--5d0ec0a91c7d42dc99ad753ffb3a30f9"},{"location":"content/block_system/#digital-analog-composition","title":"Digital-analog composition","text":"<p>In Qadence, analog operations are first-class citizens. An analog operation is one whose unitary is best described by the evolution of some hermitian generator, or Hamiltonian, acting on an arbitrary number of qubits. Qadence provides the <code>HamEvo</code> class to initialize analog operations. For a time-independent generator \\(\\mathcal{H}\\) and some time variable \\(t\\), <code>HamEvo(H, t)</code> represents the evolution operator \\(\\exp(-i\\mathcal{H}t)\\).</p> <p>Analog operations constitute a generalization of digital operations, and all digital operations can also be represented as the evolution of some hermitian generator. For example, the <code>RX</code> gate is the evolution of <code>X</code>.</p> <pre><code>from qadence import X, RX, HamEvo, PI\nfrom torch import allclose\n\nangle = PI/2\n\nblock_digital = RX(0, angle)\n\nblock_analog = HamEvo(0.5*X(0), angle)\n\nprint(allclose(block_digital.tensor(), block_analog.tensor()))\n</code></pre> <pre><code>True\n</code></pre> <p>As seen in the previous section, arbitrary Hamiltonians can be constructed using Pauli operators. Their evolution can be combined with other arbitrary digital operations and incorporated into any quantum program.</p> <pre><code>from qadence import X, Y, RX, HamEvo\nfrom qadence import add, kron, PI\n\ndef xy_int(i: int, j: int):\n    return (1/2) * (X(i)@X(j) + Y(i)@Y(j))\n\nn_qubits = 3\n\nxy_ham = add(xy_int(i, i+1) for i in range(n_qubits-1))\n\nanalog_evo = HamEvo(xy_ham, 1.0)\n\ndigital_block = kron(RX(i, i*PI/2) for i in range(n_qubits))\n\nprogram = digital_block * analog_evo * digital_block\n</code></pre> %3 cluster_a2239b3da7274a209a7bbff055f7cd62 5cf1a3e664d543f8b10ff519697915f9 0 58eaf86f3cd8468d81e806741d8433e4 RX(0.0) 5cf1a3e664d543f8b10ff519697915f9--58eaf86f3cd8468d81e806741d8433e4 96af6ffcc7194d408e161ae28e0f0d3c 1 3ceb2840bfd247ea83e84a77b3e9fe72 HamEvo 58eaf86f3cd8468d81e806741d8433e4--3ceb2840bfd247ea83e84a77b3e9fe72 74cefb93d2564548894a9b64f18f510b RX(0.0) 3ceb2840bfd247ea83e84a77b3e9fe72--74cefb93d2564548894a9b64f18f510b 79153110bb49499799ae40149122dd8f 74cefb93d2564548894a9b64f18f510b--79153110bb49499799ae40149122dd8f 0a184eb47cec446a9f392baaed159770 8d6c23095d7b49a79e6f79045367a3ea RX(1.571) 96af6ffcc7194d408e161ae28e0f0d3c--8d6c23095d7b49a79e6f79045367a3ea ccc98c3b7d384a44a3be39ca539ee496 2 5cf26fbc41614381a47fbc8bf480c495 t = 1.000 8d6c23095d7b49a79e6f79045367a3ea--5cf26fbc41614381a47fbc8bf480c495 c30efd4497f54019bad421ad63e5447d RX(1.571) 5cf26fbc41614381a47fbc8bf480c495--c30efd4497f54019bad421ad63e5447d c30efd4497f54019bad421ad63e5447d--0a184eb47cec446a9f392baaed159770 76e3c46821084cbda3c1215b8ee565cd 672df9f05ef04aba89a6dd1e1a682598 RX(3.142) ccc98c3b7d384a44a3be39ca539ee496--672df9f05ef04aba89a6dd1e1a682598 7b4a626ef8074c6cb4ca865954110af9 672df9f05ef04aba89a6dd1e1a682598--7b4a626ef8074c6cb4ca865954110af9 d506801e94f7477b81f2997c77448216 RX(3.142) 7b4a626ef8074c6cb4ca865954110af9--d506801e94f7477b81f2997c77448216 d506801e94f7477b81f2997c77448216--76e3c46821084cbda3c1215b8ee565cd"},{"location":"content/block_system/#block-execution","title":"Block execution","text":"<p>To quickly run block operations and access wavefunctions, samples or expectation values of observables, one can use the convenience functions <code>run</code>, <code>sample</code> and <code>expectation</code>.</p> <pre><code>from qadence import kron, add, H, Z, run, sample, expectation\n\nn_qubits = 2\n\n# Prepares a uniform state\nh_block = kron(H(i) for i in range(n_qubits))\n\nwf = run(h_block)\n\nxs = sample(h_block, n_shots=1000)\n\nobs = add(Z(i) for i in range(n_qubits))\nex = expectation(h_block, obs)\n</code></pre> <pre><code>wf = tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\nxs = [OrderedCounter({'00': 260, '01': 250, '10': 250, '11': 240})]\nex = tensor([[0.]])\n</code></pre>"},{"location":"content/block_system/#execution-via-quantumcircuit-and-quantummodel","title":"Execution via <code>QuantumCircuit</code> and <code>QuantumModel</code>","text":"<p>More fine-grained control and better performance is provided via the high-level <code>QuantumModel</code> abstraction. Quantum programs in Qadence are constructed in two steps:</p> <ol> <li>Build a <code>QuantumCircuit</code> which ties together a composite block and a register.</li> <li>Define a <code>QuantumModel</code> which differentiates, compiles and executes the circuit.</li> </ol> <p>Execution of more complex Qadence programs will be explored in the next tutorials.</p>"},{"location":"content/block_system/#adding-noise-to-gates","title":"Adding noise to gates","text":"<p>It is possible to add noise to gates. Please refer to the noise tutorial here.</p>"},{"location":"content/hamiltonians/","title":"Constructing arbitrary Hamiltonians","text":"<p>At the heart of digital-analog quantum computing is the description and execution of analog blocks, which represent a set of interacting qubits under some interaction Hamiltonian. For this purpose, Qadence relies on the <code>hamiltonian_factory</code> function to create arbitrary Hamiltonian blocks to be used as generators of <code>HamEvo</code> or as observables to be measured.</p>"},{"location":"content/hamiltonians/#arbitrary-all-to-all-hamiltonians","title":"Arbitrary all-to-all Hamiltonians","text":"<p>Arbitrary all-to-all interaction Hamiltonians can be easily created by passing the number of qubits in the first argument. The type of <code>interaction</code> can be chosen from the available ones in the <code>Interaction</code> enum type.</p> <pre><code>from qadence import hamiltonian_factory\nfrom qadence import N, X, Y, Z\nfrom qadence import Interaction\n\nn_qubits = 3\n\nhamilt = hamiltonian_factory(n_qubits, interaction=Interaction.ZZ)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(2)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 Z(1)\n        \u2514\u2500\u2500 Z(2)\n</code></pre> <p>Alternatively, a custom interaction function can also be defined. The input should be two integer indices \\(i\\) and \\(j\\) and it should return a composition of pauli terms representing the interaction between qubits \\(i\\) and \\(j\\):</p> <pre><code>def custom_int(i: int, j: int):\n    return X(i) @ X(j) + Y(i) @ Y(j)\n\nn_qubits = 2\n\nhamilt = hamiltonian_factory(n_qubits, interaction=custom_int)\n</code></pre> <pre><code>AddBlock(0,1)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 AddBlock(0,1)\n        \u251c\u2500\u2500 KronBlock(0,1)\n        \u2502   \u251c\u2500\u2500 X(0)\n        \u2502   \u2514\u2500\u2500 X(1)\n        \u2514\u2500\u2500 KronBlock(0,1)\n            \u251c\u2500\u2500 Y(0)\n            \u2514\u2500\u2500 Y(1)\n</code></pre> <p>Single-qubit terms can also be added by passing the respective operator directly to the <code>detuning</code> argument. For example, the total magnetization is commonly used as an observable to be measured:</p> <pre><code>total_mag = hamiltonian_factory(n_qubits, detuning = Z)\n</code></pre> <pre><code>AddBlock(0,1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 Z(0)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 Z(1)\n</code></pre> <p>For further customization, arbitrary coefficients can be passed as arrays to the <code>interaction_strength</code> and <code>detuning_strength</code> arguments for the two-qubits and single-qubit terms respectively.</p> <pre><code>n_qubits = 3\n\nhamilt = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.ZZ,\n    detuning=Z,\n    interaction_strength=[0.5, 0.2, 0.1],\n    detuning_strength=[0.1, 0.5, -0.3]\n)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 0.100] \n\u2502   \u2514\u2500\u2500 Z(0)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: -0.30] \n\u2502   \u2514\u2500\u2500 Z(2)\n\u251c\u2500\u2500 [mul: 0.500] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(1)\n\u251c\u2500\u2500 [mul: 0.200] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 Z(0)\n\u2502       \u2514\u2500\u2500 Z(2)\n\u2514\u2500\u2500 [mul: 0.100] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 Z(1)\n        \u2514\u2500\u2500 Z(2)\n</code></pre> <p>Ordering interaction strengths matters</p> <p>When passing interaction strengths as an array, the ordering must be identical to the one obtained from the <code>edges</code> property of a Qadence <code>Register</code>:</p> <pre><code>from qadence import Register\n\nprint(Register(n_qubits).edges)\n</code></pre> <pre><code>[(0, 1), (0, 2), (1, 2)]\n</code></pre> <p>For one more example, let's create a transverse-field Ising model,</p> <pre><code>n_qubits = 4\nn_edges = int(0.5 * n_qubits * (n_qubits - 1))\n\nz_terms = [1.0] * n_qubits\nzz_terms = [2.0] * n_edges\n\nzz_ham = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.ZZ,\n    detuning=Z,\n    interaction_strength=zz_terms,\n    detuning_strength=z_terms\n)\n\nx_terms = [-1.0] * n_qubits\nx_ham = hamiltonian_factory(n_qubits, detuning = X, detuning_strength = x_terms)\n\ntransverse_ising = zz_ham + x_ham\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 AddBlock(0,1,2,3)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(0)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(1)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2502   \u2514\u2500\u2500 Z(3)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(1)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502   \u2502       \u251c\u2500\u2500 Z(0)\n\u2502   \u2502       \u2514\u2500\u2500 Z(3)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502   \u2502       \u251c\u2500\u2500 Z(1)\n\u2502   \u2502       \u2514\u2500\u2500 Z(2)\n\u2502   \u251c\u2500\u2500 [mul: 2.000] \n\u2502   \u2502   \u2514\u2500\u2500 KronBlock(1,3)\n\u2502   \u2502       \u251c\u2500\u2500 Z(1)\n\u2502   \u2502       \u2514\u2500\u2500 Z(3)\n\u2502   \u2514\u2500\u2500 [mul: 2.000] \n\u2502       \u2514\u2500\u2500 KronBlock(2,3)\n\u2502           \u251c\u2500\u2500 Z(2)\n\u2502           \u2514\u2500\u2500 Z(3)\n\u2514\u2500\u2500 AddBlock(0,1,2,3)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(0)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(1)\n    \u251c\u2500\u2500 [mul: -1.00] \n    \u2502   \u2514\u2500\u2500 X(2)\n    \u2514\u2500\u2500 [mul: -1.00] \n        \u2514\u2500\u2500 X(3)\n</code></pre> <p>Random interaction coefficients</p> <p>Random interaction coefficients can be chosen between -1 and 1 by simply passing <code>random_strength = True</code> instead of <code>detuning_strength</code> and <code>interaction_strength</code>.</p>"},{"location":"content/hamiltonians/#arbitrary-hamiltonian-topologies","title":"Arbitrary Hamiltonian topologies","text":"<p>Arbitrary interaction topologies can be created using the Qadence <code>Register</code>. Simply pass the register with the desired topology as the first argument to the <code>hamiltonian_factory</code>:</p> <pre><code>from qadence import Register\n\nreg = Register.square(qubits_side=2)\n\nsquare_hamilt = hamiltonian_factory(reg, interaction=Interaction.NN)\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(3)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(2)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(2,3)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(3)\n</code></pre>"},{"location":"content/hamiltonians/#adding-variational-parameters","title":"Adding variational parameters","text":"<p>Finally, fully parameterized Hamiltonians can be created by passing a string to the strength arguments, and used to prefix the name of the variational parameters.</p> <pre><code>n_qubits = 3\n\nnn_ham = hamiltonian_factory(\n    n_qubits,\n    interaction=Interaction.NN,\n    detuning=N,\n    interaction_strength=\"c\",\n    detuning_strength=\"d\"\n)\n</code></pre> <pre><code>AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: d_0] \n\u2502   \u2514\u2500\u2500 N(0)\n\u251c\u2500\u2500 [mul: d_1] \n\u2502   \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: d_2] \n\u2502   \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: c_01] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: c_02] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(2)\n\u2514\u2500\u2500 [mul: c_12] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 N(1)\n        \u2514\u2500\u2500 N(2)\n</code></pre> <p>Alternatively, fully customizable sympy functions can be passed in an array using the Qadence parameters. Furthermore, the <code>use_all_node_pairs = True</code> option can be passed so that interactions are created for every single node pair in the register, irrespectively of the topology of the edges. This is useful for creating Hamiltonians that depend on qubit distance.</p> <pre><code>from qadence import VariationalParameter, Register\n\n# Square register of 4 qubits with a dimensionless distance of 8.0\nreg = Register.square(2, spacing = 8.0)\n\n# Get the distances between all pairs of qubits\ndistance_dict = reg.distances\n\n# Create interaction strength with variational parameter and 1/r term\nstrength_list = []\nfor node_pair in reg.all_node_pairs:\n    param = VariationalParameter(\"x\" + f\"_{node_pair[0]}{node_pair[1]}\")\n    dist_factor = reg.distances[node_pair]\n    strength_list.append(param / dist_factor)\n\nnn_ham = hamiltonian_factory(\n    reg,\n    interaction=Interaction.NN,\n    interaction_strength=strength_list,\n    use_all_node_pairs=True,\n)\n</code></pre> <pre><code>AddBlock(0,1,2,3)\n\u251c\u2500\u2500 [mul: 0.125*x_01] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(1)\n\u251c\u2500\u2500 [mul: 0.088*x_02] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: 0.125*x_03] \n\u2502   \u2514\u2500\u2500 KronBlock(0,3)\n\u2502       \u251c\u2500\u2500 N(0)\n\u2502       \u2514\u2500\u2500 N(3)\n\u251c\u2500\u2500 [mul: 0.125*x_12] \n\u2502   \u2514\u2500\u2500 KronBlock(1,2)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(2)\n\u251c\u2500\u2500 [mul: 0.088*x_13] \n\u2502   \u2514\u2500\u2500 KronBlock(1,3)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(3)\n\u2514\u2500\u2500 [mul: 0.125*x_23] \n    \u2514\u2500\u2500 KronBlock(2,3)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(3)\n</code></pre>"},{"location":"content/overlap/","title":"Wavefunction overlaps","text":"<p>Qadence offers convenience functions for computing the overlap between the wavefunctions generated by two quantum circuits \\(U\\) and \\(W\\) as:</p> \\[ S = |\\langle \\psi_U | \\psi_W \\rangle|^2 \\quad \\textrm{where} \\quad \\psi_U = U|\\psi_0\\rangle \\] <p>Here is an example on how to compute the overlap between two very simple parametric circuits consisting of a single <code>RX</code> rotation on different qubits. The overlap is expected to be non-zero only when the rotation angle is different from \\(\\pi \\; \\textrm{mod}\\; 2\\pi\\) for both rotations:</p> <pre><code>import numpy as np\nfrom torch import tensor\nfrom qadence import Overlap, OverlapMethod, QuantumCircuit, H, RX, X, FeatureParameter, hea, PI\n\n\n# Create two quantum circuits\n# with a single qubit rotation on two random qubits\nn_qubits = 4\nqubits = np.random.choice(n_qubits, n_qubits, replace=False)\n\nphi = FeatureParameter(\"phi\")\ncircuit_bra = QuantumCircuit(n_qubits, RX(qubits[0], phi))\n\npsi = FeatureParameter(\"psi\")\ncircuit_ket = QuantumCircuit(n_qubits, RX(qubits[1], psi))\n\n# Values for the feature parameters\nvalues_bra = {\"phi\": tensor([PI / 2, PI])}\nvalues_ket = {\"psi\": tensor([PI / 2, PI])}\n\n# Calculate overlap by assigning values to the given bra and ket circuits\novrlp = Overlap(circuit_bra, circuit_ket)\novrlp = ovrlp(bra_param_values=values_bra, ket_param_values=values_ket)\n</code></pre> <pre><code>Overlap with exact method:\n tensor([[2.5000e-01, 1.8747e-33],\n        [1.8747e-33, 1.4058e-65]])\n</code></pre> <p>The <code>Overlap</code> class above inherits from <code>QuantumModel</code> and is executed through its inherited forward method for the given input parameter values. By default, the overlap is computed exactly by performing the dot product of the wavefunction propagated from bra and ket circuits.</p> <p>However, it is possible to choose a different method from the <code>OverlapMethod</code> enumeration to be passed via the <code>overlap_method</code> argument in the <code>Overlap</code> initializer. Currently, one can choose from:</p> <ul> <li><code>EXACT</code>: exact computation using the wavefunction matrix representation. Does not work with real devices since it assumes access to the complete qubit system wavefunction.</li> <li><code>COMPUTE_UNCOMPUTE</code>: exact or sampling-based computation using bra \\(U\\) and ket \\(W^{\\dagger}\\) unitaries.</li> <li><code>SWAP_TEST</code>: exact or sampling-based computation using the SWAP test method.</li> <li><code>HADAMARD_TEST</code>: exact or sampling-based computation using the Hadamard test method.</li> <li><code>JENSEN_SHANNON</code>: compute the overlap using the Jensen-Shannon divergence of the two probability distributions obtained by sampling the propagated circuits. This will yield a different result than the other methods.</li> </ul> <p>All methods (except for the <code>EXACT</code> method) take an optional <code>n_shots</code> argument which can be used to perform shot-based calculations.</p> <p>Warning</p> <p>If you select a finite number of shots, the overlap is not differentiable. Therefore, it cannot be used as output of a quantum model if gradients are required.</p> <pre><code># Calculate overlap with SWAP test\novrlp = Overlap(circuit_bra, circuit_ket, method=OverlapMethod.SWAP_TEST)\novrlp_ha = ovrlp(values_bra, values_ket)\n\n# Calculate overlap with SWAP test\n# using a finite number of shots\novrlp = Overlap(circuit_bra, circuit_ket, method=OverlapMethod.SWAP_TEST)\novrlp_ha = ovrlp(values_bra, values_ket, n_shots=10_000)\n</code></pre> <pre><code>Overlap with SWAP test:\n tensor([[ 2.5000e-01, -3.3307e-16],\n        [-3.3307e-16, -4.4409e-16]])\nOverlap with SWAP test with finite number of shots:\n tensor([[0.2298, 0.0078],\n        [0.0150, 0.0140]])\n</code></pre>"},{"location":"content/parameters/","title":"Parametric programs","text":"<p>Qadence provides a flexible parameter system built on top of Sympy. Parameters can be of different types:</p> <ul> <li>Fixed parameter: a constant with a fixed, non-trainable value (e.g. \\(\\dfrac{\\pi}{2}\\)).</li> <li>Variational parameter: a trainable parameter which will be automatically picked up by the optimizer.</li> <li>Feature parameter: a non-trainable parameter which can be used to pass input values.</li> </ul>"},{"location":"content/parameters/#fixed-parameters","title":"Fixed parameters","text":"<p>Passing fixed parameters to blocks can be done by simply passing a Python numeric type or a <code>torch.Tensor</code>.</p> <pre><code>import torch\nfrom qadence import RX, run, PI\n\nwf = run(RX(0, torch.tensor(PI)))\n\nwf = run(RX(0, PI))\n</code></pre> <pre><code>wf = tensor([[6.1232e-17+0.j, 0.0000e+00-1.j]])\nwf = tensor([[6.1232e-17+0.j, 0.0000e+00-1.j]])\n</code></pre>"},{"location":"content/parameters/#variational-parameters","title":"Variational parameters","text":"<p>To parametrize a block a <code>VariationalParameter</code> instance is required. In most cases Qadence also accepts a Python string, which will be used to automatically initialize a <code>VariationalParameter</code>:</p> <pre><code>from qadence import RX, run, VariationalParameter\n\nblock = RX(0, VariationalParameter(\"theta\"))\nblock = RX(0, \"theta\")  # Equivalent\n\nwf = run(block)\n</code></pre> <pre><code>wf = tensor([[0.9998+0.0000j, 0.0000-0.0179j]])\n</code></pre> <p>By calling <code>run</code>, a random value for <code>\"theta\"</code> is initialized at execution. In a <code>QuantumModel</code>, variational parameters are stored in the underlying model parameter dictionary.</p>"},{"location":"content/parameters/#feature-parameters","title":"Feature parameters","text":"<p>A <code>FeatureParameter</code> type can also be used. It requires an input value or a batch of values. In most cases, Qadence accepts a <code>values</code> dictionary to set the input of feature parameters.</p> <pre><code>from torch import tensor\nfrom qadence import RX, PI, run, FeatureParameter\n\nblock = RX(0, FeatureParameter(\"phi\"))\n\nwf = run(block, values = {\"phi\": tensor([PI, PI/2])})\n</code></pre> <pre><code>wf = tensor([[6.1232e-17+0.0000j, 0.0000e+00-1.0000j],\n        [7.0711e-01+0.0000j, 0.0000e+00-0.7071j]])\n</code></pre> <p>Since a batch of input values was passed, the <code>run</code> function returns a batch of output states. Note that <code>FeatureParameter(\"x\")</code> and <code>VariationalParameter(\"x\")</code> are simply aliases for <code>Parameter(\"x\", trainable = False)</code> and <code>Parameter(\"x\", trainable = True)</code>.</p>"},{"location":"content/parameters/#multiparameter-expressions-and-analog-integration","title":"Multiparameter expressions and analog integration","text":"<p>The integration with Sympy becomes useful when one wishes to write arbitrary parameter compositions. Parameters can also be used as scaling coefficients in the block system, which is essential when defining arbitrary analog operations.</p> <pre><code>from torch import tensor\nfrom qadence import RX, Z, HamEvo, PI\nfrom qadence import VariationalParameter, FeatureParameter, run\nfrom sympy import sin\n\ntheta, phi = VariationalParameter(\"theta\"), FeatureParameter(\"phi\")\n\n# Arbitrary parameter composition\nexpr = PI * sin(theta + phi)\n\n# Use as unitary gate arguments\ngate = RX(0, expr)\n\n# Or as scaling coefficients for Hermitian operators\nh_op = expr * (Z(0) @ Z(1))\n\nwf = run(gate * HamEvo(h_op, 1.0), values = {\"phi\": tensor(PI)})\n</code></pre> <pre><code>wf = tensor([[0.9388+0.3065j, 0.0000+0.0000j, 0.0488+0.1493j, 0.0000+0.0000j]])\n</code></pre>"},{"location":"content/parameters/#parameter-redundancy","title":"Parameter redundancy","text":"<p>Parameters are uniquely defined by their name and redundancy is allowed in composite blocks to assign the same value to different blocks. This is useful, for example, when defining layers of rotation gates typically used as feature maps.</p> <pre><code>from torch import tensor\nfrom qadence import RY, PI, run, kron, FeatureParameter\n\nn_qubits = 3\n\nparam = FeatureParameter(\"phi\")\n\nblock = kron(RY(i, (i+1) * param) for i in range(n_qubits))\n\nwf = run(block, values = {\"phi\": tensor(PI)})\n</code></pre> <pre><code>wf = tensor([[ 1.1248e-32+0.j,  6.1232e-17+0.j, -1.3775e-48+0.j, -7.4988e-33+0.j,\n          1.8370e-16+0.j,  1.0000e+00+0.j, -2.2496e-32+0.j, -1.2246e-16+0.j]])\n</code></pre>"},{"location":"content/parameters/#parametrized-circuits","title":"Parametrized circuits","text":"<p>Let's look at a final example of an arbitrary composition of digital and analog parameterized blocks:</p> <pre><code>import sympy\nfrom qadence import RX, RY, RZ, CNOT, CPHASE, Z, HamEvo\nfrom qadence import run, chain, add, kron, FeatureParameter, VariationalParameter, PI\n\nn_qubits = 3\n\nphi = FeatureParameter(\"\u03a6\")\ntheta = VariationalParameter(\"\u03b8\")\n\nrotation_block = kron(\n    RX(0, phi/theta),\n    RY(1, theta*2),\n    RZ(2, sympy.cos(phi))\n)\ndigital_entangler = CNOT(0, 1) * CPHASE(1, 2, PI)\n\nhamiltonian = add(theta * (Z(i) @ Z(i+1)) for i in range(n_qubits-1))\n\nanalog_evo = HamEvo(hamiltonian, phi)\n\nprogram = chain(rotation_block, digital_entangler, analog_evo)\n</code></pre> %3 cluster_3c162995d077406cab6854c374e053c5 79c19cb22f1b4dc489dc8adcf04c0f1d 0 a3fd1a6a8c7648889e6815131c729ad8 RX(\u03a6/\u03b8) 79c19cb22f1b4dc489dc8adcf04c0f1d--a3fd1a6a8c7648889e6815131c729ad8 ea29da16d0744d29a6d0fd8eafef7782 1 80964c0cf5974601a7d52517cbd2e694 a3fd1a6a8c7648889e6815131c729ad8--80964c0cf5974601a7d52517cbd2e694 9ce5466846e1441e843b2dce4c30260e 80964c0cf5974601a7d52517cbd2e694--9ce5466846e1441e843b2dce4c30260e 1d893f92cf54408d98e41c94db404e11 HamEvo 9ce5466846e1441e843b2dce4c30260e--1d893f92cf54408d98e41c94db404e11 cedbf9fc713a4c0f99acd105475577c8 1d893f92cf54408d98e41c94db404e11--cedbf9fc713a4c0f99acd105475577c8 01224581639f42f5a0368d3151df0d2c 637afc3e93714a3eb084ba18a0464ca7 RY(2*\u03b8) ea29da16d0744d29a6d0fd8eafef7782--637afc3e93714a3eb084ba18a0464ca7 6ab99669d6984e5db6330b52bfbeb61a 2 c4bfb5d042de439f80074c94e52a1ad9 X 637afc3e93714a3eb084ba18a0464ca7--c4bfb5d042de439f80074c94e52a1ad9 c4bfb5d042de439f80074c94e52a1ad9--80964c0cf5974601a7d52517cbd2e694 a4d9a4492ee04d309a9ce1cd3fa09def c4bfb5d042de439f80074c94e52a1ad9--a4d9a4492ee04d309a9ce1cd3fa09def 588d90b970954ebb8cf0389bcfe5f632 t = \u03a6 a4d9a4492ee04d309a9ce1cd3fa09def--588d90b970954ebb8cf0389bcfe5f632 588d90b970954ebb8cf0389bcfe5f632--01224581639f42f5a0368d3151df0d2c 5213f088bb58466cbb98d9f69b846de7 bbb0fb1e474f4920aa69a00bd4d197fe RZ(cos(\u03a6)) 6ab99669d6984e5db6330b52bfbeb61a--bbb0fb1e474f4920aa69a00bd4d197fe 2d855d276d7b4d84ad9239bdb7984231 bbb0fb1e474f4920aa69a00bd4d197fe--2d855d276d7b4d84ad9239bdb7984231 5f9a197fd40d473aaa83689812011cfc PHASE(3.142) 2d855d276d7b4d84ad9239bdb7984231--5f9a197fd40d473aaa83689812011cfc 5f9a197fd40d473aaa83689812011cfc--a4d9a4492ee04d309a9ce1cd3fa09def c2bedff551f9470182463fc3e8f6570f 5f9a197fd40d473aaa83689812011cfc--c2bedff551f9470182463fc3e8f6570f c2bedff551f9470182463fc3e8f6570f--5213f088bb58466cbb98d9f69b846de7 <p>Please note the different colors for the parametrization with different types. The default palette assigns blue for <code>VariationalParameter</code>, green for <code>FeatureParameter</code>, orange for numeric values, and shaded red for non-parametric gates.</p>"},{"location":"content/qml_constructors/","title":"Quantum machine learning constructors","text":"<p>Besides the arbitrary Hamiltonian constructors, Qadence also provides a complete set of program constructors useful for digital-analog quantum machine learning programs.</p>"},{"location":"content/qml_constructors/#feature-maps","title":"Feature maps","text":"<p>The <code>feature_map</code> function can easily create several types of data-encoding blocks. The two main types of feature maps use a Fourier basis or a Chebyshev basis.</p> <pre><code>from qadence import feature_map, BasisSet, chain\nfrom qadence.draw import display\n\nn_qubits = 3\n\nfourier_fm = feature_map(n_qubits, fm_type=BasisSet.FOURIER)\n\nchebyshev_fm = feature_map(n_qubits, fm_type=BasisSet.CHEBYSHEV)\n\nblock = chain(fourier_fm, chebyshev_fm)\n</code></pre> %3 cluster_0fa486a966e4451f9941b75c79a5a5b8 Constant Chebyshev FM cluster_ec6b1ba57f784d00b2ab1514071a8b9b Constant Fourier FM 1de7c28925214f96a220e79653cd8384 0 73549c250ab54eabaaee451cb624b7ff RX(phi) 1de7c28925214f96a220e79653cd8384--73549c250ab54eabaaee451cb624b7ff ce97348fb6b74727aa0834b412be38e0 1 082ed5d69d114c79bd825a7dd18e7085 RX(acos(phi)) 73549c250ab54eabaaee451cb624b7ff--082ed5d69d114c79bd825a7dd18e7085 cd824e5c20cc4ca2bcda466c7982a862 082ed5d69d114c79bd825a7dd18e7085--cd824e5c20cc4ca2bcda466c7982a862 f090b8b6ddb14b9a917be843fdd97042 a5a808d162774ef6816025a058d3be61 RX(phi) ce97348fb6b74727aa0834b412be38e0--a5a808d162774ef6816025a058d3be61 f6c66a48062c4ea180a03c372e6fea86 2 b78cbbdaf9e9442197bb6acad342e7c6 RX(acos(phi)) a5a808d162774ef6816025a058d3be61--b78cbbdaf9e9442197bb6acad342e7c6 b78cbbdaf9e9442197bb6acad342e7c6--f090b8b6ddb14b9a917be843fdd97042 578b9a5d96ba495388d7769941ca5d3b 4c668b83ccff47378b498a388ca7787a RX(phi) f6c66a48062c4ea180a03c372e6fea86--4c668b83ccff47378b498a388ca7787a 6f6f972e85544a87af20921b9e0bbbb2 RX(acos(phi)) 4c668b83ccff47378b498a388ca7787a--6f6f972e85544a87af20921b9e0bbbb2 6f6f972e85544a87af20921b9e0bbbb2--578b9a5d96ba495388d7769941ca5d3b <p>A custom encoding function can also be passed with <code>sympy</code></p> <pre><code>from sympy import asin, Function\n\nn_qubits = 3\n\n# Using a pre-defined sympy Function\ncustom_fm_0 = feature_map(n_qubits, fm_type=asin)\n\n# Creating a custom function\ndef custom_fn(x):\n    return asin(x) + x**2\n\ncustom_fm_1 = feature_map(n_qubits, fm_type=custom_fn)\n\nblock = chain(custom_fm_0, custom_fm_1)\n</code></pre> %3 cluster_926febe0ce384c528515c5adf6e5088f Constant &lt;function custom_fn at 0x7fcf4ca76680&gt; FM cluster_d6d66808bf4348ca9400e52b5957fc0e Constant asin FM d0bedc3301e64bf0a693ad83cc2c1916 0 1ed4cb4e2d7e4a628a8ca55153eca84f RX(asin(phi)) d0bedc3301e64bf0a693ad83cc2c1916--1ed4cb4e2d7e4a628a8ca55153eca84f 9c31f5ffbbb64371b60778ef0baa3224 1 b5f6962c11a94b979885baa101999868 RX(phi**2 + asin(phi)) 1ed4cb4e2d7e4a628a8ca55153eca84f--b5f6962c11a94b979885baa101999868 475e53d87d4749e586fe574cb9c6d872 b5f6962c11a94b979885baa101999868--475e53d87d4749e586fe574cb9c6d872 ecf7972404b0435dbbf3eea55761665d 458b27ad2f0a4ca2bac5fa2ca36309eb RX(asin(phi)) 9c31f5ffbbb64371b60778ef0baa3224--458b27ad2f0a4ca2bac5fa2ca36309eb 982bb5cf0f8b4de9900c864a9544bfb9 2 f99a6bdf0ede4ff2bb5e9cb4e3b5c6d5 RX(phi**2 + asin(phi)) 458b27ad2f0a4ca2bac5fa2ca36309eb--f99a6bdf0ede4ff2bb5e9cb4e3b5c6d5 f99a6bdf0ede4ff2bb5e9cb4e3b5c6d5--ecf7972404b0435dbbf3eea55761665d a422794284834cfcb468db7508af5749 c3e3a2b6285d4b1cb74651f55a4c1f3d RX(asin(phi)) 982bb5cf0f8b4de9900c864a9544bfb9--c3e3a2b6285d4b1cb74651f55a4c1f3d c09ca3b2d0284af4a9c6512f234e365f RX(phi**2 + asin(phi)) c3e3a2b6285d4b1cb74651f55a4c1f3d--c09ca3b2d0284af4a9c6512f234e365f c09ca3b2d0284af4a9c6512f234e365f--a422794284834cfcb468db7508af5749 <p>Furthermore, the <code>reupload_scaling</code> argument can be used to change the scaling applied to each qubit in the support of the feature map. The default scalings can be chosen from the <code>ReuploadScaling</code> enumeration.</p> <pre><code>from qadence import ReuploadScaling\nfrom qadence.draw import display\n\nn_qubits = 5\n\n# Default constant value\nfm_constant = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.CONSTANT)\n\n# Linearly increasing scaling\nfm_tower = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.TOWER)\n\n# Exponentially increasing scaling\nfm_exp = feature_map(n_qubits, fm_type=BasisSet.FOURIER, reupload_scaling=ReuploadScaling.EXP)\n\nblock = chain(fm_constant, fm_tower, fm_exp)\n</code></pre> %3 cluster_7c1e569ac8e34247b3d5b69eee5d8781 Exponential Fourier FM cluster_fd467a1774154cf6892f062d22ea0d50 Constant Fourier FM cluster_811cd7c584784df7984c6dbd64bb8119 Tower Fourier FM 989f6b933cbe4acca90cac90d4276a12 0 4143763e833840fdbbb9fa9372178f2f RX(phi) 989f6b933cbe4acca90cac90d4276a12--4143763e833840fdbbb9fa9372178f2f b48a54526b9d4163b512912bee6f3442 1 44c46ebac4d145ab8726c9c5433ebf1a RX(1.0*phi) 4143763e833840fdbbb9fa9372178f2f--44c46ebac4d145ab8726c9c5433ebf1a 7ca8a1fa52d64de8972fcf362d57d4c8 RX(1.0*phi) 44c46ebac4d145ab8726c9c5433ebf1a--7ca8a1fa52d64de8972fcf362d57d4c8 8147280081f745d88567ff39d39af2a1 7ca8a1fa52d64de8972fcf362d57d4c8--8147280081f745d88567ff39d39af2a1 bbb792d6779844bfbe9af2360ecb4bec 81ddf966e9504ecabf611a745a319a14 RX(phi) b48a54526b9d4163b512912bee6f3442--81ddf966e9504ecabf611a745a319a14 12b2acd41cbe49d4afefaf41c059bb74 2 640f5bf0d59043989374193680e6f76f RX(2.0*phi) 81ddf966e9504ecabf611a745a319a14--640f5bf0d59043989374193680e6f76f 1a95fd311f7241bdb5f7868f231fec84 RX(2.0*phi) 640f5bf0d59043989374193680e6f76f--1a95fd311f7241bdb5f7868f231fec84 1a95fd311f7241bdb5f7868f231fec84--bbb792d6779844bfbe9af2360ecb4bec 3532f646a5f44ef29ac4e96075709378 32bf59dfac0246cabe2964d692d9a6c6 RX(phi) 12b2acd41cbe49d4afefaf41c059bb74--32bf59dfac0246cabe2964d692d9a6c6 8e316292b69647d291cb2b1c81f06d7a 3 ce94308908344990ba0354fe5d1e1ede RX(3.0*phi) 32bf59dfac0246cabe2964d692d9a6c6--ce94308908344990ba0354fe5d1e1ede a973dd786a6b4d7e894bd84610f16ae6 RX(4.0*phi) ce94308908344990ba0354fe5d1e1ede--a973dd786a6b4d7e894bd84610f16ae6 a973dd786a6b4d7e894bd84610f16ae6--3532f646a5f44ef29ac4e96075709378 d8e8c062943043649c97d451822dc6f6 87624da355b14625ae5dc8e98a7c6472 RX(phi) 8e316292b69647d291cb2b1c81f06d7a--87624da355b14625ae5dc8e98a7c6472 b9cc5b4bfbf044e087616376dab8c8e7 4 3914ff36b0024163892f8e03596335bb RX(4.0*phi) 87624da355b14625ae5dc8e98a7c6472--3914ff36b0024163892f8e03596335bb 2e7b44e3945f4877957d606c826326a9 RX(8.0*phi) 3914ff36b0024163892f8e03596335bb--2e7b44e3945f4877957d606c826326a9 2e7b44e3945f4877957d606c826326a9--d8e8c062943043649c97d451822dc6f6 4201a0651a604f579620a4b4338639c6 964dd49f86514a4bb69bd24fbb7153f9 RX(phi) b9cc5b4bfbf044e087616376dab8c8e7--964dd49f86514a4bb69bd24fbb7153f9 7a875d2b0bc84ae9ad8cd755ffbd7d2a RX(5.0*phi) 964dd49f86514a4bb69bd24fbb7153f9--7a875d2b0bc84ae9ad8cd755ffbd7d2a 51695d7e9a63446f909e36ce11ccde9e RX(16.0*phi) 7a875d2b0bc84ae9ad8cd755ffbd7d2a--51695d7e9a63446f909e36ce11ccde9e 51695d7e9a63446f909e36ce11ccde9e--4201a0651a604f579620a4b4338639c6 <p>A custom scaling can also be defined with a function with an <code>int</code> input and <code>int</code> or <code>float</code> output.</p> <pre><code>n_qubits = 5\n\ndef custom_scaling(i: int) -&gt; int | float:\n    \"\"\"Sqrt(i+1)\"\"\"\n    return (i+1) ** (0.5)\n\n# Custom scaling function\nfm_custom = feature_map(n_qubits, fm_type=BasisSet.CHEBYSHEV, reupload_scaling=custom_scaling)\n</code></pre> %3 f29d05770ee842e9936f851ad5b1d341 0 db73f76216034c7eadf03842587c2df2 RX(1.0*acos(phi)) f29d05770ee842e9936f851ad5b1d341--db73f76216034c7eadf03842587c2df2 6360006926e045268687811024ab9be8 1 6cffff1c3442454a95880b742a94677b db73f76216034c7eadf03842587c2df2--6cffff1c3442454a95880b742a94677b 4b1bc03b3a7e46f99b4226a5fea1749f 7103a1f0e773407c9d67eafd9cf60cc6 RX(1.414*acos(phi)) 6360006926e045268687811024ab9be8--7103a1f0e773407c9d67eafd9cf60cc6 e636aa0796754e8f9be16921c856cfff 2 7103a1f0e773407c9d67eafd9cf60cc6--4b1bc03b3a7e46f99b4226a5fea1749f 19082ff801dd420c9f39a88938b5f82b 7006bb6dd6644c86802642f7cce8c345 RX(1.732*acos(phi)) e636aa0796754e8f9be16921c856cfff--7006bb6dd6644c86802642f7cce8c345 114fbac20cf440baadc4ba789e355e80 3 7006bb6dd6644c86802642f7cce8c345--19082ff801dd420c9f39a88938b5f82b 8cdd1f45ad4a479d949cd37825cd0b2f 6a2c3830858743e797e3626dd2f14137 RX(2.0*acos(phi)) 114fbac20cf440baadc4ba789e355e80--6a2c3830858743e797e3626dd2f14137 5c1e7c9f9978468eac4da247b0a6d2a2 4 6a2c3830858743e797e3626dd2f14137--8cdd1f45ad4a479d949cd37825cd0b2f 2702a265648342c283958735d09bce05 e738e7462b904105bd2b337805b8d12b RX(2.236*acos(phi)) 5c1e7c9f9978468eac4da247b0a6d2a2--e738e7462b904105bd2b337805b8d12b e738e7462b904105bd2b337805b8d12b--2702a265648342c283958735d09bce05 <p>To add a trainable parameter that multiplies the feature parameter inside the encoding function, simply pass a <code>param_prefix</code> string:</p> <pre><code>n_qubits = 5\n\nfm_trainable = feature_map(\n    n_qubits,\n    fm_type=BasisSet.FOURIER,\n    reupload_scaling=ReuploadScaling.EXP,\n    param_prefix = \"w\",\n)\n</code></pre> %3 f0e85824fb1d4615b05f0c46799adb3e 0 979dc2015f0e4f958b73771ac648d117 RX(1.0*phi*w\u2080) f0e85824fb1d4615b05f0c46799adb3e--979dc2015f0e4f958b73771ac648d117 34bc6562e7614c598c3c08d032feecfb 1 8125901ca1184fd3a9d038649aa40d2d 979dc2015f0e4f958b73771ac648d117--8125901ca1184fd3a9d038649aa40d2d 47a32b5da983455fbaff11fd69ae7e51 4a14d7ff15d1437b8d60dbf6c20cd498 RX(2.0*phi*w\u2081) 34bc6562e7614c598c3c08d032feecfb--4a14d7ff15d1437b8d60dbf6c20cd498 11762968778e4fb7ad869139a801a41f 2 4a14d7ff15d1437b8d60dbf6c20cd498--47a32b5da983455fbaff11fd69ae7e51 ac0adaef78ee400d842d9383c5012029 2d02ebcdeb164933b298fe2f2d903eb1 RX(4.0*phi*w\u2082) 11762968778e4fb7ad869139a801a41f--2d02ebcdeb164933b298fe2f2d903eb1 3caebe68cb8241d6b87e6aa7ce512560 3 2d02ebcdeb164933b298fe2f2d903eb1--ac0adaef78ee400d842d9383c5012029 05f9fbc07efe4ccdacc9004179338ef1 64fe49189c24437c95779d8797742355 RX(8.0*phi*w\u2083) 3caebe68cb8241d6b87e6aa7ce512560--64fe49189c24437c95779d8797742355 976158e2dc8b4ae4ba3c6ea115e56d50 4 64fe49189c24437c95779d8797742355--05f9fbc07efe4ccdacc9004179338ef1 9380f93edf64413d9b8dde2e062740b7 e559575d5ee84763a753b361c857424a RX(16.0*phi*w\u2084) 976158e2dc8b4ae4ba3c6ea115e56d50--e559575d5ee84763a753b361c857424a e559575d5ee84763a753b361c857424a--9380f93edf64413d9b8dde2e062740b7 <p>Note that for the Fourier feature map, the encoding function is simply \\(f(x)=x\\). For other cases, like the Chebyshev <code>acos()</code> encoding, the trainable parameter may cause the feature value to be outside the domain of the encoding function. This will eventually be fixed by adding range constraints to trainable parameters in Qadence.</p> <p>A full description of the remaining arguments can be found in the <code>feature_map</code> API reference. We provide an example below.</p> <pre><code>from qadence import RY\n\nn_qubits = 5\n\n# Custom scaling function\nfm_full = feature_map(\n    n_qubits = n_qubits,\n    support = tuple(reversed(range(n_qubits))), # Reverse the qubit support to run the scaling from bottom to top\n    param = \"x\", # Change the name of the parameter\n    op = RY, # Change the rotation gate between RX, RY, RZ or PHASE\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.EXP,\n    feature_range = (-1.0, 2.0), # Range from which the input data comes from\n    target_range = (1.0, 3.0), # Range the encoder assumes as the natural range\n    multiplier = 5.0, # Extra multiplier, which can also be a Parameter\n    param_prefix = \"w\", # Add trainable parameters\n)\n</code></pre> %3 890aefdad64a4b368ed2cf0d0fd168a8 0 d1977820411e4290bfc68cf4f2fe2113 RY(80.0*acos(w\u2084*(0.667*x + 1.667))) 890aefdad64a4b368ed2cf0d0fd168a8--d1977820411e4290bfc68cf4f2fe2113 da1bf877b0f8431d8421d6cf21144a78 1 56341025f264485eb4be63268c631e7e d1977820411e4290bfc68cf4f2fe2113--56341025f264485eb4be63268c631e7e 36f1cf8be2904536becec73dfce07f6f bc18b0bd665448a1ba3a09a7d54212a0 RY(40.0*acos(w\u2083*(0.667*x + 1.667))) da1bf877b0f8431d8421d6cf21144a78--bc18b0bd665448a1ba3a09a7d54212a0 920e0020ce0646d89b1e96d716012b3b 2 bc18b0bd665448a1ba3a09a7d54212a0--36f1cf8be2904536becec73dfce07f6f dc6b5b43b7264a82ac5209cd98e67a7b 68c2d9c1a3344b4d8bdcc3b51e8a026e RY(20.0*acos(w\u2082*(0.667*x + 1.667))) 920e0020ce0646d89b1e96d716012b3b--68c2d9c1a3344b4d8bdcc3b51e8a026e 48c131c3b8994f3e8b11a8245bd0ba38 3 68c2d9c1a3344b4d8bdcc3b51e8a026e--dc6b5b43b7264a82ac5209cd98e67a7b 540cd1048b9c4c9b97a285eeaf0f5a02 fa524f9adf5748f98e59c93476d6d2c5 RY(10.0*acos(w\u2081*(0.667*x + 1.667))) 48c131c3b8994f3e8b11a8245bd0ba38--fa524f9adf5748f98e59c93476d6d2c5 b06981ae756a4b75b138a7c0c59859c1 4 fa524f9adf5748f98e59c93476d6d2c5--540cd1048b9c4c9b97a285eeaf0f5a02 00cf3b8c232541f093783fa8eece891a 12c70b4298204eea9c03865f60fe7bf0 RY(5.0*acos(w\u2080*(0.667*x + 1.667))) b06981ae756a4b75b138a7c0c59859c1--12c70b4298204eea9c03865f60fe7bf0 12c70b4298204eea9c03865f60fe7bf0--00cf3b8c232541f093783fa8eece891a"},{"location":"content/qml_constructors/#hardware-efficient-ansatz","title":"Hardware-efficient ansatz","text":"<p>Ansatze blocks for quantum machine-learning are typically built following the Hardware-Efficient Ansatz formalism (HEA). Both fully digital and digital-analog HEAs can easily be built with the <code>hea</code> function. By default, the digital version is returned:</p> <pre><code>from qadence import hea\nfrom qadence.draw import display\n\nn_qubits = 3\ndepth = 2\n\nansatz = hea(n_qubits, depth)\n</code></pre> %3 4f96b6eb06ae4e79b40a699e4785c6a2 0 8c4b088c11a34bd68bbeb44c4b1822d1 RX(theta\u2080) 4f96b6eb06ae4e79b40a699e4785c6a2--8c4b088c11a34bd68bbeb44c4b1822d1 cd87536052bb4a1b8a9bcb5607dfe111 1 f755b1f5543a4ebb8e7242c2fdee5f17 RY(theta\u2083) 8c4b088c11a34bd68bbeb44c4b1822d1--f755b1f5543a4ebb8e7242c2fdee5f17 a175892caf20449dbdefcf834c8cd759 RX(theta\u2086) f755b1f5543a4ebb8e7242c2fdee5f17--a175892caf20449dbdefcf834c8cd759 02f09c52b0244ad48a9f5acc3de42519 a175892caf20449dbdefcf834c8cd759--02f09c52b0244ad48a9f5acc3de42519 b43d721e6a8d4c22b35a5a988ab04561 02f09c52b0244ad48a9f5acc3de42519--b43d721e6a8d4c22b35a5a988ab04561 83c3196864804e1abdc3a4f2dc6c9ecd RX(theta\u2089) b43d721e6a8d4c22b35a5a988ab04561--83c3196864804e1abdc3a4f2dc6c9ecd 6dab748060284a8892d57481d49805db RY(theta\u2081\u2082) 83c3196864804e1abdc3a4f2dc6c9ecd--6dab748060284a8892d57481d49805db 351f317a5518480092fa5680b2eb32e4 RX(theta\u2081\u2085) 6dab748060284a8892d57481d49805db--351f317a5518480092fa5680b2eb32e4 a4fcf0a711bd431bb1a00d89f75c472d 351f317a5518480092fa5680b2eb32e4--a4fcf0a711bd431bb1a00d89f75c472d 7620dd3860784372b747b278ea48463a a4fcf0a711bd431bb1a00d89f75c472d--7620dd3860784372b747b278ea48463a 305a709469874d37a90dbb804bc7c96a 7620dd3860784372b747b278ea48463a--305a709469874d37a90dbb804bc7c96a b12ae94e089347b294d75022da59fc87 60d56557ba504bc08d039badc52647a8 RX(theta\u2081) cd87536052bb4a1b8a9bcb5607dfe111--60d56557ba504bc08d039badc52647a8 3cf246738dd84d29afb27c27de745ecb 2 ae3fee59b08d449ea26b8e5b80188374 RY(theta\u2084) 60d56557ba504bc08d039badc52647a8--ae3fee59b08d449ea26b8e5b80188374 348d9db18ece421cbea4a619050a21c6 RX(theta\u2087) ae3fee59b08d449ea26b8e5b80188374--348d9db18ece421cbea4a619050a21c6 ac3cb03be1d2499fb2125dd8b244aadf X 348d9db18ece421cbea4a619050a21c6--ac3cb03be1d2499fb2125dd8b244aadf ac3cb03be1d2499fb2125dd8b244aadf--02f09c52b0244ad48a9f5acc3de42519 eb582c9b49e442c68e0de1ee6773b2b3 ac3cb03be1d2499fb2125dd8b244aadf--eb582c9b49e442c68e0de1ee6773b2b3 9c85aca87f1e4e889acda0e12e976b63 RX(theta\u2081\u2080) eb582c9b49e442c68e0de1ee6773b2b3--9c85aca87f1e4e889acda0e12e976b63 ad6482ba770c479793140564bd1c10e1 RY(theta\u2081\u2083) 9c85aca87f1e4e889acda0e12e976b63--ad6482ba770c479793140564bd1c10e1 73101f825f78425a85c33e47334e1818 RX(theta\u2081\u2086) ad6482ba770c479793140564bd1c10e1--73101f825f78425a85c33e47334e1818 a417da33cbe047c8839ea96b453e968c X 73101f825f78425a85c33e47334e1818--a417da33cbe047c8839ea96b453e968c a417da33cbe047c8839ea96b453e968c--a4fcf0a711bd431bb1a00d89f75c472d bb11d2c4248f45c094dafd70a661f5cd a417da33cbe047c8839ea96b453e968c--bb11d2c4248f45c094dafd70a661f5cd bb11d2c4248f45c094dafd70a661f5cd--b12ae94e089347b294d75022da59fc87 e9342b454e414d46856da21f618ddfaf d4c9743f7b284f6eb51689efb6317c1b RX(theta\u2082) 3cf246738dd84d29afb27c27de745ecb--d4c9743f7b284f6eb51689efb6317c1b 7e9f2e09a6ed4488ae5416248a220fa7 RY(theta\u2085) d4c9743f7b284f6eb51689efb6317c1b--7e9f2e09a6ed4488ae5416248a220fa7 4e06706bced34e0d967c5b49996b7d9e RX(theta\u2088) 7e9f2e09a6ed4488ae5416248a220fa7--4e06706bced34e0d967c5b49996b7d9e d45a9c13f77f4a7d831324944033cbc6 4e06706bced34e0d967c5b49996b7d9e--d45a9c13f77f4a7d831324944033cbc6 eff6c9549da1426eaaf33ae99b90d506 X d45a9c13f77f4a7d831324944033cbc6--eff6c9549da1426eaaf33ae99b90d506 eff6c9549da1426eaaf33ae99b90d506--eb582c9b49e442c68e0de1ee6773b2b3 207c44345099490b99019763ea6ad46d RX(theta\u2081\u2081) eff6c9549da1426eaaf33ae99b90d506--207c44345099490b99019763ea6ad46d 4bd8d7eb26d04fac82643104f461abd0 RY(theta\u2081\u2084) 207c44345099490b99019763ea6ad46d--4bd8d7eb26d04fac82643104f461abd0 184ce93c71924beeb4effda95ac8c6e4 RX(theta\u2081\u2087) 4bd8d7eb26d04fac82643104f461abd0--184ce93c71924beeb4effda95ac8c6e4 6e5ca5a8a70b406d907547c1319ec5b1 184ce93c71924beeb4effda95ac8c6e4--6e5ca5a8a70b406d907547c1319ec5b1 46692beb8d6a475b96aa85bd8cbd8f6b X 6e5ca5a8a70b406d907547c1319ec5b1--46692beb8d6a475b96aa85bd8cbd8f6b 46692beb8d6a475b96aa85bd8cbd8f6b--bb11d2c4248f45c094dafd70a661f5cd 46692beb8d6a475b96aa85bd8cbd8f6b--e9342b454e414d46856da21f618ddfaf <p>As seen above, the rotation layers are automatically parameterized, and the prefix <code>\"theta\"</code> can be changed with the <code>param_prefix</code> argument.</p> <p>Furthermore, both the single-qubit rotations and the two-qubit entangler can be customized with the <code>operations</code> and <code>entangler</code> argument. The operations can be passed as a list of single-qubit rotations, while the entangler should be either <code>CNOT</code>, <code>CZ</code>, <code>CRX</code>, <code>CRY</code>, <code>CRZ</code> or <code>CPHASE</code>.</p> <pre><code>from qadence import RX, RY, CPHASE\n\nansatz = hea(\n    n_qubits=n_qubits,\n    depth=depth,\n    param_prefix=\"phi\",\n    operations=[RX, RY, RX],\n    entangler=CPHASE\n)\n</code></pre> %3 48fdae57e48c4fc3881eac609bbdbf2c 0 d917c6f668fe4daba158d7ff580e6509 RX(phi\u2080) 48fdae57e48c4fc3881eac609bbdbf2c--d917c6f668fe4daba158d7ff580e6509 c4aa4ee1e0b244e2b50e2dd234379fd0 1 9285a05ecc624217bb06a591a70c813c RY(phi\u2083) d917c6f668fe4daba158d7ff580e6509--9285a05ecc624217bb06a591a70c813c 0bb530411f654d10b2662ff1be6b4e7c RX(phi\u2086) 9285a05ecc624217bb06a591a70c813c--0bb530411f654d10b2662ff1be6b4e7c 888f435779254563a93d5a213b992427 0bb530411f654d10b2662ff1be6b4e7c--888f435779254563a93d5a213b992427 4ac4d2819dae4bc6b732a9746b462251 888f435779254563a93d5a213b992427--4ac4d2819dae4bc6b732a9746b462251 bfc0d72f18e346ffbe6f1766eb72ce9f RX(phi\u2089) 4ac4d2819dae4bc6b732a9746b462251--bfc0d72f18e346ffbe6f1766eb72ce9f f7230cc085564585beb5e161e536a1ae RY(phi\u2081\u2082) bfc0d72f18e346ffbe6f1766eb72ce9f--f7230cc085564585beb5e161e536a1ae 7ed4ec620fad4429aa2536c770dacfa0 RX(phi\u2081\u2085) f7230cc085564585beb5e161e536a1ae--7ed4ec620fad4429aa2536c770dacfa0 adee14eac15741aa9c14585e95c3fbb8 7ed4ec620fad4429aa2536c770dacfa0--adee14eac15741aa9c14585e95c3fbb8 a6fb997041174350a2bbbb240d60aeb1 adee14eac15741aa9c14585e95c3fbb8--a6fb997041174350a2bbbb240d60aeb1 4afc5a57e1ef433bb8821b340d6d5a70 a6fb997041174350a2bbbb240d60aeb1--4afc5a57e1ef433bb8821b340d6d5a70 3362f842d8f543a087960e21908091e8 e0409e0bc11c4b73ba1490043161dc40 RX(phi\u2081) c4aa4ee1e0b244e2b50e2dd234379fd0--e0409e0bc11c4b73ba1490043161dc40 d14ffa027ba042ffb3da1522b354d4cd 2 ef3ec181f45f4db8bfff47fe59785ba1 RY(phi\u2084) e0409e0bc11c4b73ba1490043161dc40--ef3ec181f45f4db8bfff47fe59785ba1 5e20236cbcd242c1a51788cd420fd147 RX(phi\u2087) ef3ec181f45f4db8bfff47fe59785ba1--5e20236cbcd242c1a51788cd420fd147 587ae7c3bfd544f2bb265f8cfc87f1ab PHASE(phi_ent\u2080) 5e20236cbcd242c1a51788cd420fd147--587ae7c3bfd544f2bb265f8cfc87f1ab 587ae7c3bfd544f2bb265f8cfc87f1ab--888f435779254563a93d5a213b992427 34ab688740424618a0548d191b3b38c6 587ae7c3bfd544f2bb265f8cfc87f1ab--34ab688740424618a0548d191b3b38c6 b77708af66d1448e8106fa4ba1b300ed RX(phi\u2081\u2080) 34ab688740424618a0548d191b3b38c6--b77708af66d1448e8106fa4ba1b300ed 2bddba35687649fba921765258fc43f0 RY(phi\u2081\u2083) b77708af66d1448e8106fa4ba1b300ed--2bddba35687649fba921765258fc43f0 1240f00805b54c9d98bcf2d7389fcdae RX(phi\u2081\u2086) 2bddba35687649fba921765258fc43f0--1240f00805b54c9d98bcf2d7389fcdae 443cefb90a324047af8f919a2e2f61e7 PHASE(phi_ent\u2082) 1240f00805b54c9d98bcf2d7389fcdae--443cefb90a324047af8f919a2e2f61e7 443cefb90a324047af8f919a2e2f61e7--adee14eac15741aa9c14585e95c3fbb8 ec6d269d4fc04996be58d066042b780c 443cefb90a324047af8f919a2e2f61e7--ec6d269d4fc04996be58d066042b780c ec6d269d4fc04996be58d066042b780c--3362f842d8f543a087960e21908091e8 ca1c409aa2e844b9b21fd9a5be2314fb cadd50fce39a4183b30f102fd45eb4b5 RX(phi\u2082) d14ffa027ba042ffb3da1522b354d4cd--cadd50fce39a4183b30f102fd45eb4b5 bf4c7f837b084625b54e9bc50f7fc5df RY(phi\u2085) cadd50fce39a4183b30f102fd45eb4b5--bf4c7f837b084625b54e9bc50f7fc5df c5d7076d4f12401c9adba3a85d51931a RX(phi\u2088) bf4c7f837b084625b54e9bc50f7fc5df--c5d7076d4f12401c9adba3a85d51931a db900c0463154ca2be32d63add14458e c5d7076d4f12401c9adba3a85d51931a--db900c0463154ca2be32d63add14458e 1abafa2ebe274631851751326729db68 PHASE(phi_ent\u2081) db900c0463154ca2be32d63add14458e--1abafa2ebe274631851751326729db68 1abafa2ebe274631851751326729db68--34ab688740424618a0548d191b3b38c6 494405c055e54ebd852c949f90d2b8ea RX(phi\u2081\u2081) 1abafa2ebe274631851751326729db68--494405c055e54ebd852c949f90d2b8ea ef7ca60aceae4704b57cca13e630b64d RY(phi\u2081\u2084) 494405c055e54ebd852c949f90d2b8ea--ef7ca60aceae4704b57cca13e630b64d d4eb33cbf2bd409fbe3401fcc46c8ff2 RX(phi\u2081\u2087) ef7ca60aceae4704b57cca13e630b64d--d4eb33cbf2bd409fbe3401fcc46c8ff2 7b3585d5504f4ee487f7d294d1653029 d4eb33cbf2bd409fbe3401fcc46c8ff2--7b3585d5504f4ee487f7d294d1653029 7ea876a6e97d485294c805eddfc243df PHASE(phi_ent\u2083) 7b3585d5504f4ee487f7d294d1653029--7ea876a6e97d485294c805eddfc243df 7ea876a6e97d485294c805eddfc243df--ec6d269d4fc04996be58d066042b780c 7ea876a6e97d485294c805eddfc243df--ca1c409aa2e844b9b21fd9a5be2314fb <p>Having a truly hardware-efficient ansatz means that the entangling operation can be chosen according to each device's native interactions. Besides digital operations, in Qadence it is also possible to build digital-analog HEAs with the entanglement produced by the natural evolution of a set of interacting qubits, as natively implemented in neutral atom devices. As with other digital-analog functions, this can be controlled with the <code>strategy</code> argument which can be chosen from the <code>Strategy</code> enum type. Currently, only <code>Strategy.DIGITAL</code> and <code>Strategy.SDAQC</code> are available. By default, calling <code>strategy = Strategy.SDAQC</code> will use a global entangling Hamiltonian with Ising-like \\(NN\\) interactions and constant interaction strength,</p> <pre><code>from qadence import Strategy\n\nansatz = hea(\n    n_qubits,\n    depth=depth,\n    strategy=Strategy.SDAQC\n)\n</code></pre> %3 cluster_5a7d2c4c9d474981ba2a181523e73484 cluster_a7cf5af3c06e43dea234d4a30b69ed0b 89f877713a1341d089008133ee8bae29 0 d578aad02c524697ab7342f1f3201dfa RX(theta\u2080) 89f877713a1341d089008133ee8bae29--d578aad02c524697ab7342f1f3201dfa 17d08eb7081d4da5a570885f3e4d1377 1 a81514ae642043e683a6c71f14e9654b RY(theta\u2083) d578aad02c524697ab7342f1f3201dfa--a81514ae642043e683a6c71f14e9654b 64983baa4241448d94c2d192f11f1e86 RX(theta\u2086) a81514ae642043e683a6c71f14e9654b--64983baa4241448d94c2d192f11f1e86 24d362e251f849028b4cd8ac69639108 HamEvo 64983baa4241448d94c2d192f11f1e86--24d362e251f849028b4cd8ac69639108 7e96c7b523754d0bbe1ca985cd4f1afb RX(theta\u2089) 24d362e251f849028b4cd8ac69639108--7e96c7b523754d0bbe1ca985cd4f1afb b84e3248601349859f0b0fbb82fe8713 RY(theta\u2081\u2082) 7e96c7b523754d0bbe1ca985cd4f1afb--b84e3248601349859f0b0fbb82fe8713 c729148e6ff244858597ff2922633cdc RX(theta\u2081\u2085) b84e3248601349859f0b0fbb82fe8713--c729148e6ff244858597ff2922633cdc ff2dc7cfd09f4f9b87f799375c1e0fac HamEvo c729148e6ff244858597ff2922633cdc--ff2dc7cfd09f4f9b87f799375c1e0fac 8811d42fa9344f098d100d6e2a7583ef ff2dc7cfd09f4f9b87f799375c1e0fac--8811d42fa9344f098d100d6e2a7583ef d81043f6d00a41a4ac1b35b417d1cad4 8218f43ad82640748c178f048eef09c6 RX(theta\u2081) 17d08eb7081d4da5a570885f3e4d1377--8218f43ad82640748c178f048eef09c6 cdd75e2d53084b8fb356a37d3631a7a2 2 1f4ba04e08824dae90bc63b6f7958d05 RY(theta\u2084) 8218f43ad82640748c178f048eef09c6--1f4ba04e08824dae90bc63b6f7958d05 4ee3001a5641415cb68020e8ee920553 RX(theta\u2087) 1f4ba04e08824dae90bc63b6f7958d05--4ee3001a5641415cb68020e8ee920553 ce42cfffca3441a6bd6c03def0029925 t = theta_t\u2080 4ee3001a5641415cb68020e8ee920553--ce42cfffca3441a6bd6c03def0029925 be6a6b23d5764e469bc4379616364007 RX(theta\u2081\u2080) ce42cfffca3441a6bd6c03def0029925--be6a6b23d5764e469bc4379616364007 9b9711bea9c74d0cb4d81ce04d104d5d RY(theta\u2081\u2083) be6a6b23d5764e469bc4379616364007--9b9711bea9c74d0cb4d81ce04d104d5d d32b92dce4734266a9c35df8c4ca8178 RX(theta\u2081\u2086) 9b9711bea9c74d0cb4d81ce04d104d5d--d32b92dce4734266a9c35df8c4ca8178 342c7e5a87cd4a8f883a296a8c1ef788 t = theta_t\u2081 d32b92dce4734266a9c35df8c4ca8178--342c7e5a87cd4a8f883a296a8c1ef788 342c7e5a87cd4a8f883a296a8c1ef788--d81043f6d00a41a4ac1b35b417d1cad4 7f1c25cb14ad49cea5c2e290ca935a1d 805b800b67774a1fba528404aa7e77d5 RX(theta\u2082) cdd75e2d53084b8fb356a37d3631a7a2--805b800b67774a1fba528404aa7e77d5 fce158fdf9bf432eac288325857ce90d RY(theta\u2085) 805b800b67774a1fba528404aa7e77d5--fce158fdf9bf432eac288325857ce90d d45b1d796eb641d1be59b6ef03f759b5 RX(theta\u2088) fce158fdf9bf432eac288325857ce90d--d45b1d796eb641d1be59b6ef03f759b5 7427a1c8d64347f28daa80c34aff59fb d45b1d796eb641d1be59b6ef03f759b5--7427a1c8d64347f28daa80c34aff59fb f196114e5ea741999f1bdc5de8f659c4 RX(theta\u2081\u2081) 7427a1c8d64347f28daa80c34aff59fb--f196114e5ea741999f1bdc5de8f659c4 129cc7b14a604bcf8bada04775e12ae8 RY(theta\u2081\u2084) f196114e5ea741999f1bdc5de8f659c4--129cc7b14a604bcf8bada04775e12ae8 a3f51632491f4f4687037507f917fa4e RX(theta\u2081\u2087) 129cc7b14a604bcf8bada04775e12ae8--a3f51632491f4f4687037507f917fa4e 234369791c9e45408247a799b5d38eeb a3f51632491f4f4687037507f917fa4e--234369791c9e45408247a799b5d38eeb 234369791c9e45408247a799b5d38eeb--7f1c25cb14ad49cea5c2e290ca935a1d <p>Note that, by default, only the time-parameter is automatically parameterized when building a digital-analog HEA. However, as described in the Hamiltonians tutorial, arbitrary interaction Hamiltonians can be easily built with the <code>hamiltonian_factory</code> function, with both customized or fully parameterized interactions, and these can be directly passed as the <code>entangler</code> for a customizable digital-analog HEA.</p> <pre><code>from qadence import hamiltonian_factory, Interaction, N, Register, hea\n\n# Build a parameterized neutral-atom Hamiltonian following a honeycomb_lattice:\nregister = Register.honeycomb_lattice(1, 1)\n\nentangler = hamiltonian_factory(\n    register,\n    interaction=Interaction.NN,\n    detuning=N,\n    interaction_strength=\"e\",\n    detuning_strength=\"n\"\n)\n\n# Build a fully parameterized Digital-Analog HEA:\nn_qubits = register.n_qubits\ndepth = 2\n\nansatz = hea(\n    n_qubits=register.n_qubits,\n    depth=depth,\n    operations=[RX, RY, RX],\n    entangler=entangler,\n    strategy=Strategy.SDAQC\n)\n</code></pre> %3 cluster_ec879d4caf084e938958ecc8e9b6025d cluster_30a0c0f00edc478587e1db82138c11e6 e06915b13f634dab83ab8a1798a48a46 0 97354ffcf9d8480e9e332898753d8b5f RX(theta\u2080) e06915b13f634dab83ab8a1798a48a46--97354ffcf9d8480e9e332898753d8b5f 671b5c15058c4a4d86781b834991adfa 1 082e025a60dd407ba64d923ca6e6acfa RY(theta\u2086) 97354ffcf9d8480e9e332898753d8b5f--082e025a60dd407ba64d923ca6e6acfa 4e7c9d09a04a41b2822fb7d2a6621e4a RX(theta\u2081\u2082) 082e025a60dd407ba64d923ca6e6acfa--4e7c9d09a04a41b2822fb7d2a6621e4a 2bf47042c4a3434aa5e8b6a5467bc2ad 4e7c9d09a04a41b2822fb7d2a6621e4a--2bf47042c4a3434aa5e8b6a5467bc2ad 21f07b937f2e4067a15b2022f8cac23b RX(theta\u2081\u2088) 2bf47042c4a3434aa5e8b6a5467bc2ad--21f07b937f2e4067a15b2022f8cac23b c2b94c91d2924c0c90c4b031f57923df RY(theta\u2082\u2084) 21f07b937f2e4067a15b2022f8cac23b--c2b94c91d2924c0c90c4b031f57923df 5af6df012bd843db8b9aa0513e66bbf2 RX(theta\u2083\u2080) c2b94c91d2924c0c90c4b031f57923df--5af6df012bd843db8b9aa0513e66bbf2 06fbba17c3824ce1b4e00e8f80c2abf5 5af6df012bd843db8b9aa0513e66bbf2--06fbba17c3824ce1b4e00e8f80c2abf5 581b1edae5324656a98daee52e854692 06fbba17c3824ce1b4e00e8f80c2abf5--581b1edae5324656a98daee52e854692 9a8044ed72e1441aadb12e8d5549f066 da52d90de8e841a580d34d312a2c7fcf RX(theta\u2081) 671b5c15058c4a4d86781b834991adfa--da52d90de8e841a580d34d312a2c7fcf 49a6df7b3a14447d81ffbb67310d2afd 2 0dd2249c7f6e451db1a95bc40756c74d RY(theta\u2087) da52d90de8e841a580d34d312a2c7fcf--0dd2249c7f6e451db1a95bc40756c74d 030781ebe5d64c58ba910971c6687d9a RX(theta\u2081\u2083) 0dd2249c7f6e451db1a95bc40756c74d--030781ebe5d64c58ba910971c6687d9a 1209afa22c1340cbbe47971d549e8b17 030781ebe5d64c58ba910971c6687d9a--1209afa22c1340cbbe47971d549e8b17 8ded048de5c74e9f89a743a312e984aa RX(theta\u2081\u2089) 1209afa22c1340cbbe47971d549e8b17--8ded048de5c74e9f89a743a312e984aa da4c88e3ac694985b2f0a17cc19cea29 RY(theta\u2082\u2085) 8ded048de5c74e9f89a743a312e984aa--da4c88e3ac694985b2f0a17cc19cea29 e69a8ac3a5ee49b489cf0e51cf142b87 RX(theta\u2083\u2081) da4c88e3ac694985b2f0a17cc19cea29--e69a8ac3a5ee49b489cf0e51cf142b87 eb4a3f351408421281cfb1fe21b288c1 e69a8ac3a5ee49b489cf0e51cf142b87--eb4a3f351408421281cfb1fe21b288c1 eb4a3f351408421281cfb1fe21b288c1--9a8044ed72e1441aadb12e8d5549f066 c7408a3a0209460eb4db062ef986c5b1 26efeff497e544618477d53095ade47d RX(theta\u2082) 49a6df7b3a14447d81ffbb67310d2afd--26efeff497e544618477d53095ade47d 9e86cb92d0dd4f65b18b0d6a3c1721de 3 230be976076e4e00bcd946e4c8e692c0 RY(theta\u2088) 26efeff497e544618477d53095ade47d--230be976076e4e00bcd946e4c8e692c0 d90efeb4cc5343bf86de69120dd771fa RX(theta\u2081\u2084) 230be976076e4e00bcd946e4c8e692c0--d90efeb4cc5343bf86de69120dd771fa b2333fc7e1804f1eb0d75354befaebaa HamEvo d90efeb4cc5343bf86de69120dd771fa--b2333fc7e1804f1eb0d75354befaebaa 933b76c93a6c499189a2da2f6a858096 RX(theta\u2082\u2080) b2333fc7e1804f1eb0d75354befaebaa--933b76c93a6c499189a2da2f6a858096 6f3708af7acf4f77ab6b054c8fec1f5f RY(theta\u2082\u2086) 933b76c93a6c499189a2da2f6a858096--6f3708af7acf4f77ab6b054c8fec1f5f 846279cabfee41ec8bf27a85418749d2 RX(theta\u2083\u2082) 6f3708af7acf4f77ab6b054c8fec1f5f--846279cabfee41ec8bf27a85418749d2 e559f286c5fe46ef86dadd10a25468e0 HamEvo 846279cabfee41ec8bf27a85418749d2--e559f286c5fe46ef86dadd10a25468e0 e559f286c5fe46ef86dadd10a25468e0--c7408a3a0209460eb4db062ef986c5b1 221bcb8ce49c4e62aeca170f4a4acca1 2cde677d70904cc0bb772a51d5912e70 RX(theta\u2083) 9e86cb92d0dd4f65b18b0d6a3c1721de--2cde677d70904cc0bb772a51d5912e70 8362a20fe15e469eb5dc1d3a1c01f0c4 4 0224e10439fe4946b283af9ead50d0cd RY(theta\u2089) 2cde677d70904cc0bb772a51d5912e70--0224e10439fe4946b283af9ead50d0cd b92cfdb13bcf477aae30a29988d85291 RX(theta\u2081\u2085) 0224e10439fe4946b283af9ead50d0cd--b92cfdb13bcf477aae30a29988d85291 ce71eb728fb045819747d1a1b37f1474 t = theta_t\u2080 b92cfdb13bcf477aae30a29988d85291--ce71eb728fb045819747d1a1b37f1474 262835a879ff400bb72813fcd029d2e3 RX(theta\u2082\u2081) ce71eb728fb045819747d1a1b37f1474--262835a879ff400bb72813fcd029d2e3 277b0b89cc764784ad9b9a13790f602f RY(theta\u2082\u2087) 262835a879ff400bb72813fcd029d2e3--277b0b89cc764784ad9b9a13790f602f 0e667c313330413da2080948c6273f58 RX(theta\u2083\u2083) 277b0b89cc764784ad9b9a13790f602f--0e667c313330413da2080948c6273f58 e0c9e4794ef24551b4df3dceac247226 t = theta_t\u2081 0e667c313330413da2080948c6273f58--e0c9e4794ef24551b4df3dceac247226 e0c9e4794ef24551b4df3dceac247226--221bcb8ce49c4e62aeca170f4a4acca1 8c10e95ab78c4a738f8cb09b4d48a1a8 89efd23d381c49a9b88d423061194340 RX(theta\u2084) 8362a20fe15e469eb5dc1d3a1c01f0c4--89efd23d381c49a9b88d423061194340 82987ada59bf41659fd436cd39fa963c 5 8d800b66d5294765ae4dfe21db228764 RY(theta\u2081\u2080) 89efd23d381c49a9b88d423061194340--8d800b66d5294765ae4dfe21db228764 b8d96de2f26e4e28b80d93fa9c9e26aa RX(theta\u2081\u2086) 8d800b66d5294765ae4dfe21db228764--b8d96de2f26e4e28b80d93fa9c9e26aa b74136a6925c48d18560cd3d2958e8fa b8d96de2f26e4e28b80d93fa9c9e26aa--b74136a6925c48d18560cd3d2958e8fa 51042c9dbbec4fc89641d5539ae1a82b RX(theta\u2082\u2082) b74136a6925c48d18560cd3d2958e8fa--51042c9dbbec4fc89641d5539ae1a82b 10716020095a429295f80641ba9bd11a RY(theta\u2082\u2088) 51042c9dbbec4fc89641d5539ae1a82b--10716020095a429295f80641ba9bd11a 87f67f2c34364fb890d34eab8b521b70 RX(theta\u2083\u2084) 10716020095a429295f80641ba9bd11a--87f67f2c34364fb890d34eab8b521b70 30035eb66b504323af09aa07043daae8 87f67f2c34364fb890d34eab8b521b70--30035eb66b504323af09aa07043daae8 30035eb66b504323af09aa07043daae8--8c10e95ab78c4a738f8cb09b4d48a1a8 61dca2e532444c359c2996c6030f2f6f 13337821198b4a55944f4fcec9b45aed RX(theta\u2085) 82987ada59bf41659fd436cd39fa963c--13337821198b4a55944f4fcec9b45aed f2314028ba894d398bd2165d8964d5af RY(theta\u2081\u2081) 13337821198b4a55944f4fcec9b45aed--f2314028ba894d398bd2165d8964d5af 29e9ab30c9124e2486242d863c031ff2 RX(theta\u2081\u2087) f2314028ba894d398bd2165d8964d5af--29e9ab30c9124e2486242d863c031ff2 0f272b4972924a1aaa0a349ac79e124f 29e9ab30c9124e2486242d863c031ff2--0f272b4972924a1aaa0a349ac79e124f 07736834858949c0ad87378fdd9c2835 RX(theta\u2082\u2083) 0f272b4972924a1aaa0a349ac79e124f--07736834858949c0ad87378fdd9c2835 9a5c67b6758347bf9f8e2ccc2c03937b RY(theta\u2082\u2089) 07736834858949c0ad87378fdd9c2835--9a5c67b6758347bf9f8e2ccc2c03937b 73a2a07c1e8a4af9bbe71ca8a3e6a95e RX(theta\u2083\u2085) 9a5c67b6758347bf9f8e2ccc2c03937b--73a2a07c1e8a4af9bbe71ca8a3e6a95e 0f4ad6a1b59041958e68160e0d87f1bc 73a2a07c1e8a4af9bbe71ca8a3e6a95e--0f4ad6a1b59041958e68160e0d87f1bc 0f4ad6a1b59041958e68160e0d87f1bc--61dca2e532444c359c2996c6030f2f6f"},{"location":"content/qml_constructors/#identity-initialized-ansatz","title":"Identity-initialized ansatz","text":"<p>It is widely known that parametrized quantum circuits are characterized by barren plateaus, where the gradient becomes exponentially small in the number of qubits. Here we include one of many techniques that have been proposed in recent years to mitigate this effect and facilitate <code>QNN</code>s training: Grant et al. showed that initializing the weights of a <code>QNN</code> so that each block of the circuit evaluates to identity reduces the effect of barren plateaus in the initial stage of training. In a similar fashion to <code>hea</code>, such circuit can be created via calling the associated function, <code>identity_initialized_ansatz</code>:</p> <pre><code>from qadence.constructors import identity_initialized_ansatz\nfrom qadence.draw import display\n\nn_qubits = 3\ndepth = 2\n\nansatz = identity_initialized_ansatz(n_qubits, depth)\n</code></pre> %3 cluster_3bba864a77bf49bd870dbafeeec3e539 BPMA-1 cluster_396b03a547f04c16be0a22b18a3333a4 BPMA-0 f66de7718e794f5e9c1af5a0d20be9f7 0 e39107545bda45398e9d954bc6c0ad8e RX(iia_\u03b1\u2080\u2080) f66de7718e794f5e9c1af5a0d20be9f7--e39107545bda45398e9d954bc6c0ad8e 89ab8188efd74e3e8a20d22c442afdf7 1 09f7045df2b84115ae5aa7830339d301 RY(iia_\u03b1\u2080\u2083) e39107545bda45398e9d954bc6c0ad8e--09f7045df2b84115ae5aa7830339d301 a78f2fe14f764ccf8d02c3d6f18b65cf 09f7045df2b84115ae5aa7830339d301--a78f2fe14f764ccf8d02c3d6f18b65cf 623958d7bd1a46b7bfaa3f1e91977c93 a78f2fe14f764ccf8d02c3d6f18b65cf--623958d7bd1a46b7bfaa3f1e91977c93 c6ae0b3be3f14aeaa58daaa1f145c2be RX(iia_\u03b3\u2080\u2080) 623958d7bd1a46b7bfaa3f1e91977c93--c6ae0b3be3f14aeaa58daaa1f145c2be 9230c5e71a5c4a60b327c6fff230db7e c6ae0b3be3f14aeaa58daaa1f145c2be--9230c5e71a5c4a60b327c6fff230db7e 4af1f48ec529449f8b4318c5531a8eac 9230c5e71a5c4a60b327c6fff230db7e--4af1f48ec529449f8b4318c5531a8eac b72475f9fa3d46c9a373bd8af2c0fb92 RY(iia_\u03b2\u2080\u2083) 4af1f48ec529449f8b4318c5531a8eac--b72475f9fa3d46c9a373bd8af2c0fb92 0857a3d6fa04499d90e5137c3f48c5b5 RX(iia_\u03b2\u2080\u2080) b72475f9fa3d46c9a373bd8af2c0fb92--0857a3d6fa04499d90e5137c3f48c5b5 6465274fc2384cfbb7aa009c56228450 RX(iia_\u03b1\u2081\u2080) 0857a3d6fa04499d90e5137c3f48c5b5--6465274fc2384cfbb7aa009c56228450 fe9338894abc46cb801862598ccda0f6 RY(iia_\u03b1\u2081\u2083) 6465274fc2384cfbb7aa009c56228450--fe9338894abc46cb801862598ccda0f6 0e4612d337a34c84bd0f1e124ce1ba1b fe9338894abc46cb801862598ccda0f6--0e4612d337a34c84bd0f1e124ce1ba1b 8eb3285e90294494a3afb9e40c2a0605 0e4612d337a34c84bd0f1e124ce1ba1b--8eb3285e90294494a3afb9e40c2a0605 55a323061a3f496bb04cfa590bc538d6 RX(iia_\u03b3\u2081\u2080) 8eb3285e90294494a3afb9e40c2a0605--55a323061a3f496bb04cfa590bc538d6 625d92e9c61c42ed84ae314ebc0c7278 55a323061a3f496bb04cfa590bc538d6--625d92e9c61c42ed84ae314ebc0c7278 57d4136ec0a54776b41d80493688a479 625d92e9c61c42ed84ae314ebc0c7278--57d4136ec0a54776b41d80493688a479 6c32ad7ca90b41e1ad0b000ba60ca073 RY(iia_\u03b2\u2081\u2083) 57d4136ec0a54776b41d80493688a479--6c32ad7ca90b41e1ad0b000ba60ca073 cbf3770c1dbf4338a7df1275d8f6542e RX(iia_\u03b2\u2081\u2080) 6c32ad7ca90b41e1ad0b000ba60ca073--cbf3770c1dbf4338a7df1275d8f6542e 84d7472c6ec540b7829e799e0b999b29 cbf3770c1dbf4338a7df1275d8f6542e--84d7472c6ec540b7829e799e0b999b29 aef8392d023e470b82ad63d4ecbad4ec 9fc075de700d4bc7a9413bd1b3425bfc RX(iia_\u03b1\u2080\u2081) 89ab8188efd74e3e8a20d22c442afdf7--9fc075de700d4bc7a9413bd1b3425bfc 0bde8114d3164460b539f4bc155bdc99 2 11b353c912224af79afacfe3b00f7a3f RY(iia_\u03b1\u2080\u2084) 9fc075de700d4bc7a9413bd1b3425bfc--11b353c912224af79afacfe3b00f7a3f af3c87a6fe65489faea167e6291d0c62 X 11b353c912224af79afacfe3b00f7a3f--af3c87a6fe65489faea167e6291d0c62 af3c87a6fe65489faea167e6291d0c62--a78f2fe14f764ccf8d02c3d6f18b65cf b9d4aaa6b4b64df9abc3bf062b3894c5 af3c87a6fe65489faea167e6291d0c62--b9d4aaa6b4b64df9abc3bf062b3894c5 d78c949ea86f477ca872b0c2e4958467 RX(iia_\u03b3\u2080\u2081) b9d4aaa6b4b64df9abc3bf062b3894c5--d78c949ea86f477ca872b0c2e4958467 167e50e6ff7b47d5ac03fd653295bf7f d78c949ea86f477ca872b0c2e4958467--167e50e6ff7b47d5ac03fd653295bf7f a03d52a7dd8c4ae1ad775cd66111df66 X 167e50e6ff7b47d5ac03fd653295bf7f--a03d52a7dd8c4ae1ad775cd66111df66 a03d52a7dd8c4ae1ad775cd66111df66--4af1f48ec529449f8b4318c5531a8eac 4b9cab5ae18045bcb349981512a1b263 RY(iia_\u03b2\u2080\u2084) a03d52a7dd8c4ae1ad775cd66111df66--4b9cab5ae18045bcb349981512a1b263 f1653bfad5934c6ab887591671a404c8 RX(iia_\u03b2\u2080\u2081) 4b9cab5ae18045bcb349981512a1b263--f1653bfad5934c6ab887591671a404c8 dcdeedba5cc347c4b2eb71b5490cd4b7 RX(iia_\u03b1\u2081\u2081) f1653bfad5934c6ab887591671a404c8--dcdeedba5cc347c4b2eb71b5490cd4b7 da9902728517494b82efa305fb3d5a95 RY(iia_\u03b1\u2081\u2084) dcdeedba5cc347c4b2eb71b5490cd4b7--da9902728517494b82efa305fb3d5a95 aef2fe071490422d9ef949c9583bf51e X da9902728517494b82efa305fb3d5a95--aef2fe071490422d9ef949c9583bf51e aef2fe071490422d9ef949c9583bf51e--0e4612d337a34c84bd0f1e124ce1ba1b 2320df9093a74e119e09f912f603f1c9 aef2fe071490422d9ef949c9583bf51e--2320df9093a74e119e09f912f603f1c9 109b8c81f419450b81b02b4490b4e25c RX(iia_\u03b3\u2081\u2081) 2320df9093a74e119e09f912f603f1c9--109b8c81f419450b81b02b4490b4e25c e5399b1f3eca458d8e9b27ba5bce3956 109b8c81f419450b81b02b4490b4e25c--e5399b1f3eca458d8e9b27ba5bce3956 84d364371f5a454c93d64c076d8e98c0 X e5399b1f3eca458d8e9b27ba5bce3956--84d364371f5a454c93d64c076d8e98c0 84d364371f5a454c93d64c076d8e98c0--57d4136ec0a54776b41d80493688a479 a029236b74514907abf272ae27155123 RY(iia_\u03b2\u2081\u2084) 84d364371f5a454c93d64c076d8e98c0--a029236b74514907abf272ae27155123 c717e23e9f1d4f0393e7aa36fe0b3266 RX(iia_\u03b2\u2081\u2081) a029236b74514907abf272ae27155123--c717e23e9f1d4f0393e7aa36fe0b3266 c717e23e9f1d4f0393e7aa36fe0b3266--aef8392d023e470b82ad63d4ecbad4ec b4380cd296134dc6a6a129df679209a9 1903dc9b4ab9413585989d26b7752979 RX(iia_\u03b1\u2080\u2082) 0bde8114d3164460b539f4bc155bdc99--1903dc9b4ab9413585989d26b7752979 f24a012459d345c68a4280bbca6927c8 RY(iia_\u03b1\u2080\u2085) 1903dc9b4ab9413585989d26b7752979--f24a012459d345c68a4280bbca6927c8 abc007db87cd466d8b4bfdf1460271ad f24a012459d345c68a4280bbca6927c8--abc007db87cd466d8b4bfdf1460271ad 98ba1c057513452c87c9710b1d1d259d X abc007db87cd466d8b4bfdf1460271ad--98ba1c057513452c87c9710b1d1d259d 98ba1c057513452c87c9710b1d1d259d--b9d4aaa6b4b64df9abc3bf062b3894c5 e0d9ed088644437badc870a67889ea14 RX(iia_\u03b3\u2080\u2082) 98ba1c057513452c87c9710b1d1d259d--e0d9ed088644437badc870a67889ea14 eaac7f7c7dfd4f508dd4c6ae69d869a8 X e0d9ed088644437badc870a67889ea14--eaac7f7c7dfd4f508dd4c6ae69d869a8 eaac7f7c7dfd4f508dd4c6ae69d869a8--167e50e6ff7b47d5ac03fd653295bf7f 02424d47be7b44b9b6da1f53eddd122c eaac7f7c7dfd4f508dd4c6ae69d869a8--02424d47be7b44b9b6da1f53eddd122c 2c83bd2bfc0d4ffaae6d05c743d67175 RY(iia_\u03b2\u2080\u2085) 02424d47be7b44b9b6da1f53eddd122c--2c83bd2bfc0d4ffaae6d05c743d67175 7ba0c6965ee74ac3bffe0eac6f06a57f RX(iia_\u03b2\u2080\u2082) 2c83bd2bfc0d4ffaae6d05c743d67175--7ba0c6965ee74ac3bffe0eac6f06a57f ac611a2e254b4902b1c9488db8f8a6d6 RX(iia_\u03b1\u2081\u2082) 7ba0c6965ee74ac3bffe0eac6f06a57f--ac611a2e254b4902b1c9488db8f8a6d6 86f2d404aaf749b68463b1f7edfe4189 RY(iia_\u03b1\u2081\u2085) ac611a2e254b4902b1c9488db8f8a6d6--86f2d404aaf749b68463b1f7edfe4189 514567da9ec14464b8d931f738710080 86f2d404aaf749b68463b1f7edfe4189--514567da9ec14464b8d931f738710080 be26c80e2e3f4f3cb0a6a187d989d636 X 514567da9ec14464b8d931f738710080--be26c80e2e3f4f3cb0a6a187d989d636 be26c80e2e3f4f3cb0a6a187d989d636--2320df9093a74e119e09f912f603f1c9 ad5724e5bf5a4cda8a15921e48cddd9e RX(iia_\u03b3\u2081\u2082) be26c80e2e3f4f3cb0a6a187d989d636--ad5724e5bf5a4cda8a15921e48cddd9e f2df417ab2eb4b6ab96e7dd2ced051ab X ad5724e5bf5a4cda8a15921e48cddd9e--f2df417ab2eb4b6ab96e7dd2ced051ab f2df417ab2eb4b6ab96e7dd2ced051ab--e5399b1f3eca458d8e9b27ba5bce3956 e5265d5b9fed4a71b92e3eff4726b668 f2df417ab2eb4b6ab96e7dd2ced051ab--e5265d5b9fed4a71b92e3eff4726b668 cbd1184a5c62497c8bb0795986a1adbe RY(iia_\u03b2\u2081\u2085) e5265d5b9fed4a71b92e3eff4726b668--cbd1184a5c62497c8bb0795986a1adbe 05df34c9ad6a44cc96765c3b9f0d56c4 RX(iia_\u03b2\u2081\u2082) cbd1184a5c62497c8bb0795986a1adbe--05df34c9ad6a44cc96765c3b9f0d56c4 05df34c9ad6a44cc96765c3b9f0d56c4--b4380cd296134dc6a6a129df679209a9"},{"location":"content/quantummodels/","title":"Quantum models","text":"<p>A quantum program can be expressed and executed using the <code>QuantumModel</code> type. It serves three primary purposes:</p> <p>Parameter handling: by conveniently handling and embedding the two parameter types that Qadence supports: feature and variational (see more details in the previous section).</p> <p>Differentiability: by enabling a differentiable backend that supports two differentiable modes: automatic differentiation (AD) and parameter shift rules (PSR). The former is used general differentiation in statevector simulators based on PyTorch and JAX. The latter is a quantum specific method used to differentiate gate parameters, and is enabled for all backends.</p> <p>Execution: by defining which backend the program is expected to be executed on. Qadence supports circuit compilation to the native backend representation.</p> <p>Backends</p> <p>The goal is for quantum models to be executed seemlessly on a number of different purpose backends: simulators, emulators or real hardware. By default, Qadence executes on the PyQTorch backend which implements a state vector simulator. Currently, this is the most feature rich backend. The Pulser backend is being developed, and currently supports a more limited set of functionalities (pulse sequences on programmable neutral atom arrays). The Horqrux backend, built on JAX, is also available, but currently not supported with the <code>QuantumModel</code> interface. For more information see the backend section.</p> <p>The base <code>QuantumModel</code> exposes the following methods:</p> <ul> <li><code>QuantumModel.run()</code>: To extract the wavefunction after circuit execution. Not supported by all backends.</li> <li><code>QuantumModel.sample()</code>: Sample a bitstring from the resulting quantum state after circuit execution. Supported by all backends.</li> <li><code>QuantumModel.expectation()</code>: Compute the expectation value of an observable.</li> </ul> <p>Every <code>QuantumModel</code> is an instance of a <code>torch.nn.Module</code> that enables differentiability for its <code>expectation</code> method. For statevector simulators, AD also works for the statevector itself.</p> <p>To construct a <code>QuantumModel</code>, the program block must first be initialized into a <code>QuantumCircuit</code> instance by combining it with a <code>Register</code>. An integer number can also be passed for the total number of qubits, which instantiates a <code>Register</code> automatically. The qubit register also includes topological information on the qubit layout, essential for digital-analog computations. However, we will explore that in a later tutorial. For now, let's construct a simple parametrized quantum circuit.</p> <pre><code>from qadence import QuantumCircuit, RX, RY, chain, kron\nfrom qadence import FeatureParameter, VariationalParameter\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, theta), RY(1, theta)),\n    kron(RX(0, phi), RY(1, phi)),\n)\n\ncircuit = QuantumCircuit(2, block)\nunique_params = circuit.unique_parameters\n</code></pre> <pre><code>unique_params = [theta, phi]\n</code></pre> <p>The model can then be instantiated. Similarly to the direct execution functions shown in the previous tutorial, the <code>run</code>, <code>sample</code> and <code>expectation</code> methods are available directly from the model.</p> <pre><code>import torch\nfrom qadence import QuantumModel, PI, Z\n\nobservable = Z(0) + Z(1)\n\nmodel = QuantumModel(circuit, observable)\n\nvalues = {\"phi\": torch.tensor([PI, PI/2])}\n\nwf = model.run(values)\nxs = model.sample(values, n_shots=100)\nex = model.expectation(values)\n</code></pre> <pre><code>wf = tensor([[ 0.0690+0.0000j, -0.2535+0.0000j,  0.0000+0.2535j,  0.0000-0.9310j],\n        [ 0.2465+0.0000j,  0.4310+0.0000j,  0.0000-0.4310j,  0.0000-0.7535j]])\nxs = [OrderedCounter({'11': 89, '10': 6, '01': 4, '00': 1}), OrderedCounter({'11': 50, '10': 25, '01': 16, '00': 9})]\nex = tensor([[-1.7239],\n        [-1.0139]])\n</code></pre> <p>By default, the <code>forward</code> method of <code>QuantumModel</code> calls <code>model.run()</code>. To define custom quantum models, the best way is to inherit from <code>QuantumModel</code> and override the <code>forward</code> method, as typically done with custom PyTorch Modules.</p> <p>The <code>QuantumModel</code> class provides convenience methods to manipulate parameters. Being a <code>torch.nn.Module</code>, all torch methods are also available.</p> <pre><code># To pass onto a torch optimizer\nparameter_generator = model.parameters()\n\n# Number of variational parameters\nnum_vparams = model.num_vparams\n\n# Dictionary to easily inspect variational parameter values\nvparams_values = model.vparams\n</code></pre> <pre><code>vparams_values = OrderedDict([('theta', tensor([0.5317]))])\n</code></pre>"},{"location":"content/quantummodels/#model-output","title":"Model output","text":"<p>The output of a quantum model is typically encoded in the measurement of an expectation value. In Qadence, one way to customize the number of outputs is by batching the number of observables at model creation by passing a list of blocks.</p> <pre><code>from torch import tensor\nfrom qadence import chain, kron, VariationalParameter, FeatureParameter\nfrom qadence import QuantumModel, QuantumCircuit, PI, Z, RX, CNOT\n\ntheta = VariationalParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, phi), RX(1, phi)),\n    CNOT(0, 1)\n)\n\ncircuit = QuantumCircuit(2, block)\n\nmodel = QuantumModel(circuit, [Z(0), Z(0) + Z(1)])\n\nvalues = {\"phi\": tensor(PI)}\n\nex = model.expectation(values)\n</code></pre> <pre><code>ex = tensor([[-1.0000e+00, -7.4988e-33]])\n</code></pre> <p>As mentioned in the previous tutorial, blocks can also be arbitrarily parameterized through multiplication, which allows the inclusion of trainable parameters in the definition of the observable.</p> <pre><code>from qadence import I, Z\n\na = VariationalParameter(\"a\")\nb = VariationalParameter(\"b\")\n\n# Magnetization with a trainable shift and scale\nobservable = a * I(0) + b * Z(0)\n\nmodel = QuantumModel(circuit, observable)\n</code></pre>"},{"location":"content/quantummodels/#quantum-neural-network-qnn","title":"Quantum Neural Network (QNN)","text":"<p>The <code>QNN</code> is a subclass of the <code>QuantumModel</code> geared towards quantum machine learning and parameter optimisation. See the quantum machine learning section section or the <code>QNN</code> API reference for more detailed information. There are three main differences in interface when compared with the <code>QuantumModel</code>:</p> <ul> <li>It is initialized with a list of the input parameter names, and then supports direct <code>torch.Tensor</code> inputs instead of the values dictionary shown above. The ordering of the input values should respect the order given in the input names.</li> <li>Passing an observable is mandatory.</li> <li>The <code>forward</code> method calls <code>model.expectation()</code>.</li> </ul> <pre><code>from torch import tensor\nfrom qadence import chain, kron, VariationalParameter, FeatureParameter\nfrom qadence import QNN, QuantumCircuit, PI, Z, RX, RY, CNOT\n\ntheta = FeatureParameter(\"theta\")\nphi = FeatureParameter(\"phi\")\n\nblock = chain(\n    kron(RX(0, phi), RX(1, phi)),\n    kron(RY(0, theta), RY(1, theta)),\n    CNOT(0, 1)\n)\n\ncircuit = QuantumCircuit(2, block)\nobservable = Z(0) + Z(1)\n\nmodel = QNN(circuit, observable, inputs = [\"phi\", \"theta\"])\n\n# \"phi\" = PI, PI/2, \"theta\" = 0.0, 1.0\nvalues = tensor([[PI, 0.0], [PI/2, 1.0]])\n\nex = model(values)\n</code></pre> <pre><code>ex = tensor([[-7.4988e-33],\n        [ 1.1102e-16]])\n</code></pre>"},{"location":"content/register/","title":"Quantum registers","text":"<p>In Qadence, quantum programs can be executed by specifying the layout of a register of resources as a lattice. Built-in <code>Register</code> types can be used or constructed for arbitrary topologies. Common register topologies are available and illustrated in the plot below.</p> 2025-01-08T18:01:10.466285 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"content/register/#building-and-drawing-registers","title":"Building and drawing registers","text":"<p>Built-in topologies are directly accessible in the <code>Register</code> methods:</p> <pre><code>from qadence import Register\n\nreg = Register.all_to_all(n_qubits = 4)\nreg_line = Register.line(n_qubits = 4)\nreg_circle = Register.circle(n_qubits = 4)\nreg_squre = Register.square(qubits_side = 2)\nreg_rect = Register.rectangular_lattice(qubits_row = 2, qubits_col = 2)\nreg_triang = Register.triangular_lattice(n_cells_row = 2, n_cells_col = 2)\nreg_honey = Register.honeycomb_lattice(n_cells_row = 2, n_cells_col = 2)\n</code></pre> <p>The <code>Register</code> class builds on top of the NetworkX <code>Graph</code>, and the graphs can be visualized with the <code>reg.draw()</code> method</p> <p>Qubit coordinates are saved as node properties in the underlying NetworkX graph, but can be accessed directly with the <code>coords</code> property.</p> <p><pre><code>reg = Register.square(2)\nprint(reg.coords)\n</code></pre> <pre><code>{0: (0.5, -0.5), 1: (0.5, 0.5), 2: (-0.5, 0.5), 3: (-0.5, -0.5)}\n</code></pre>  By default, the coords are scaled such that the minimum distance between any two qubits is 1, unless the register is created directly from specific coordinates as shown below. The <code>spacing</code> argument can be used to set the minimum spacing. The <code>rescale_coords</code> method can be used to create a new register by rescaling the coordinates of an already created register.</p> <pre><code>scaled_reg_1 = Register.square(2, spacing = 4.0)\nscaled_reg_2 = reg.rescale_coords(scaling = 4.0)\nprint(scaled_reg_1.coords)\nprint(scaled_reg_2.coords)\n</code></pre> <pre><code>{0: (2.0, -2.0), 1: (2.0, 2.0), 2: (-2.0, 2.0), 3: (-2.0, -2.0)}\n{0: (2.0, -2.0), 1: (2.0, 2.0), 2: (-2.0, 2.0), 3: (-2.0, -2.0)}\n</code></pre> <p>The distance between qubits can also be directly accessed with the <code>distances</code> and <code>edge_distances</code> properties.</p> <pre><code>print(reg.distances)\nprint(reg.edge_distances)\n</code></pre> <pre><code>Distance between all qubit pairs:\n{(0, 1): 1.0, (0, 2): 1.4142135623730951, (0, 3): 1.0, (1, 2): 1.0, (1, 3): 1.4142135623730951, (2, 3): 1.0}\nDistance between qubits connect by an edge in the graph\n{(0, 1): 1.0, (0, 3): 1.0, (1, 2): 1.0, (2, 3): 1.0}\n</code></pre> <p>By calling the <code>Register</code> directly, either the number of nodes or a specific graph can be given as input. If passing a custom graph directly, the node positions will not be defined automatically, and should be previously saved in the <code>\"pos\"</code> node property. If not, <code>reg.coords</code> will return empty tuples and all distances will be 0.</p> <pre><code>import networkx as nx\n\n# Same as Register.all_to_all(n_qubits = 2):\nreg = Register(2)\n\n# Register from a custom graph:\ngraph = nx.complete_graph(3)\n\n# Set node positions, in this case a simple line:\nfor i, node in enumerate(graph.nodes):\n    graph.nodes[node][\"pos\"] = (1.0 * i, 0.0)\n\nreg = Register(graph)\n\nprint(reg.distances)\n</code></pre> <pre><code>{(0, 1): 1.0, (0, 2): 2.0, (1, 2): 1.0}\n</code></pre> <p>Alternatively, arbitrarily shaped registers can also be constructed by providing the node coordinates. In this case, there will be no edges automatically created in the connectivity graph.</p> <pre><code>import numpy as np\nfrom qadence import Register, PI\n\nreg = Register.from_coordinates(\n    [(x, np.sin(x)) for x in np.linspace(0, 2*PI, 10)]\n)\n\nreg.draw(show=False)\n</code></pre> 2025-01-08T18:01:10.660073 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>Units for qubit coordinates</p> <p>In general, Qadence makes no assumption about the units for qubit coordinates and distances. However, if used in the context of a Hamiltonian coefficient, care should be taken by the user to guarantee the quantity \\(H.t\\) is dimensionless for exponentiation in the PyQTorch backend, where it is assumed that \\(\\hbar = 1\\). For registers passed to the Pulser backend, coordinates are in \\(\\mu \\textrm{m}\\).</p>"},{"location":"content/register/#connectivity-graphs","title":"Connectivity graphs","text":"<p>Register topology is often assumed in digital simulations to be an all-to-all qubit connectivity. When running on real devices that enable the digital-analog computing paradigm, qubit interactions must be specified either by specifying distances between qubits, or by defining edges in the register connectivity graph.</p> <p>The abstract graph nodes and edges are accessible for direct usage.</p> <pre><code>from qadence import Register\n\nreg = Register.rectangular_lattice(2,3)\n</code></pre> <pre><code>reg.nodes = NodeView((0, 1, 2, 3, 4, 5))\nreg.edges = EdgeView([(0, 2), (0, 1), (1, 3), (2, 4), (2, 3), (3, 5), (4, 5)])\n</code></pre> <p>There is also an <code>all_node_pairs</code> property for convenience:</p> <pre><code>print(reg.all_node_pairs)\n</code></pre> <pre><code>[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n</code></pre> <p>More details about the usage of <code>Register</code> types in the digital-analog paradigm can be found in the digital-analog basics section.</p>"},{"location":"content/serializ_and_prep/","title":"Serialization","text":"<p>Qadence offers convenience functions for serializing and deserializing any quantum program. This is useful for storing quantum programs and sending them for execution over the network via an API.</p> <p>Note</p> <p>Qadence currently uses a custom JSON serialization as interchange format. Support for QASM format for digital quantum programs is currently under consideration.</p> <ul> <li><code>serialize/deserialize</code>: serialize and deserialize a Qadence object into a dictionary</li> <li><code>save/load</code>: save and load a Qadence object to a file with one of the supported   formats. Currently, these are <code>.json</code> and the PyTorch-compatible <code>.pt</code> format.</li> </ul> <p>Let's start with serialization into a dictionary.</p> <pre><code>import torch\nfrom qadence import QuantumCircuit, QuantumModel, DiffMode\nfrom qadence import chain, hamiltonian_factory, feature_map, hea, Z\nfrom qadence.serialization import serialize, deserialize\n\nn_qubits = 4\n\nmy_block = chain(feature_map(n_qubits, param=\"x\"), hea(n_qubits, depth=2))\nobs = hamiltonian_factory(n_qubits, detuning=Z)\n\n# Use the block defined above to create a quantum circuit\n# serialize/deserialize it\nqc = QuantumCircuit(n_qubits, my_block)\nqc_dict = serialize(qc)\nqc_deserialized = deserialize(qc_dict)\nassert qc == qc_deserialized\n\n# Let's wrap it in a QuantumModel\n# and serialize it\nqm = QuantumModel(qc, obs, diff_mode=DiffMode.AD)\nqm_dict = serialize(qm)\nqm_deserialized = deserialize(qm_dict)\n\n# Check if the loaded QuantumModel returns the same expectation\nvalues = {\"x\": torch.rand(10)}\nassert torch.allclose(qm.expectation(values=values), qm_deserialized.expectation(values=values))\n</code></pre> <p>Finally, we can save the quantum circuit and the model with the two supported formats.</p> <pre><code>from qadence.serialization import serialize, deserialize, save, load, SerializationFormat\n\nqc_fname = \"circuit\"\nsave(qc, folder=\".\", file_name=qc_fname, format=SerializationFormat.PT)\nloaded_qc = load(f\"{qc_fname}.pt\")\nassert qc == loaded_qc\n\nqm_fname = \"model\"\nsave(qm, folder=\".\", file_name=qm_fname, format=SerializationFormat.JSON)\nmodel = load(f\"{qm_fname}.json\")\nassert isinstance(model, QuantumModel)\n</code></pre>"},{"location":"content/state_conventions/","title":"State Conventions","text":"<p>Here is an overview of the state conventions used in Qadence together with practical examples.</p>"},{"location":"content/state_conventions/#qubit-register-order","title":"Qubit register order","text":"<p>Qubit registers in quantum computing are often indexed in increasing or decreasing order from left to right. In Qadence, the convention is qubit indexation in increasing order. For example, a register of four qubits in bra-ket notation reads:</p> \\[|q_0, q_1, q_2, q_3\\rangle\\] <p>Furthermore, when displaying a quantum circuit, qubits are ordered from top to bottom.</p>"},{"location":"content/state_conventions/#basis-state-order","title":"Basis state order","text":"<p>Basis state ordering refers to how basis states are ordered when considering the conversion from bra-ket notation to the standard linear algebra basis. In Qadence, basis states are ordered in the following manner:</p> \\[ \\begin{align} |00\\rangle = [1, 0, 0, 0]^T\\\\ |01\\rangle = [0, 1, 0, 0]^T\\\\ |10\\rangle = [0, 0, 1, 0]^T\\\\ |11\\rangle = [0, 0, 0, 1]^T \\end{align} \\]"},{"location":"content/state_conventions/#endianness","title":"Endianness","text":"<p>Endianness refers to the storage convention for binary information (in bytes) in a classical memory register. In quantum computing, information is either stored in bits or in qubits. The most commonly used conventions are:</p> <ul> <li>A big-endian system stores the most significant bit of a binary word at the smallest memory address.</li> <li>A little-endian system stores the least significant bit of a binary word at the smallest memory address.</li> </ul> <p>Given the register convention in Qadence, the integer \\(2\\) written in binary big-endian as \\(10\\) can be encoded in a qubit register in both big-endian as \\(|10\\rangle\\) or little-endian as \\(|01\\rangle\\).</p> <p>The convention for Qadence is big-endian.</p>"},{"location":"content/state_conventions/#quantum-states","title":"Quantum states","text":"<p>In practical scenarios, conventions regarding register order, basis state order and endianness are very much intertwined, and identical results can be obtained by fixing or varying any of them. In Qadence, we assume that qubit ordering and basis state ordering is fixed, and allow an <code>endianness</code> argument that can be passed to control the expected result. Here are a few examples:</p> <p>A simple and direct way to exemplify the endianness convention is using convenience functions for state preparation.</p> <p>Bitstring convention as inputs</p> <p>When a bitstring is passed as input to a function for state preparation, it has to be understood in big-endian convention.</p> <pre><code>from qadence import Endianness, product_state\n\n# The state |10&gt;, the 3rd basis state.\nstate_big = product_state(\"10\", endianness=Endianness.BIG) # or just \"Big\"\n\n# The state |01&gt;, the 2nd basis state.\nstate_little = product_state(\"10\", endianness=Endianness.LITTLE) # or just \"Little\"\n</code></pre> <pre><code>State in big endian = tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]])\nState in little endian = tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> <p>Here, a bitword expressed as a Python string to encode the integer 2 in big-endian is used to create the respective basis state in both conventions. However, note that the same results can be obtained by fixing the endianness convention as big-endian (thus creating the state \\(|10\\rangle\\) in both cases), and changing the basis state ordering. A similar argument holds for fixing both endianness and basis state ordering and simply changing the qubit index order.</p> <p>Another example where endianness directly comes into play is when measuring a register. A big- or little-endian measurement will choose the first or the last qubit, respectively, as the most significant bit. Let's see this in an example:</p> <pre><code>from qadence import I, H, sample\n\n# Create superposition state: |00&gt; + |01&gt; (normalized)\nblock = I(0) @ H(1)  # Identity on qubit 0, Hadamard on qubit 1\n\n# Generate bitword samples following both conventions\n# Samples \"00\" and \"01\"\nresult_big = sample(block, endianness=Endianness.BIG)\n# Samples \"00\" and \"10\"\nresult_little = sample(block, endianness=Endianness.LITTLE)\n</code></pre> <pre><code>Sample in big endian = [OrderedCounter({'01': 59, '00': 41})]\nSample in little endian = [Counter({'10': 51, '00': 49})]\n</code></pre> <p>In Qadence, endianness can be flipped for many relevant objects:</p> <pre><code>from qadence import invert_endianness\n\n# Equivalent to sampling in little-endian.\nflip_big_sample = invert_endianness(result_big)\n\n# Equivalent to a state created in little-endian.\nflip_big_state = invert_endianness(state_big)\n</code></pre> <pre><code>Flipped sample = [Counter({'10': 59, '00': 41})]\nFlipped state = tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre>"},{"location":"content/state_conventions/#quantum-operations","title":"Quantum operations","text":"<p>When looking at the matricial form of quantum operations, the usage of the term endianness becomes slightly abusive. To exemplify, we may consider the <code>CNOT</code> operation with <code>control = 0</code> and <code>target = 1</code>. This operation is often described with two different matrices:</p> \\[ \\text{CNOT(0, 1)} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\qquad \\text{or} \\qquad \\text{CNOT(0, 1)} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] <p>The difference can be easily explained either by considering a different ordering of the qubit indices, or a different ordering of the basis states. In Qadence, both can be retrieved through the <code>endianness</code> argument:</p> <pre><code>from qadence import block_to_tensor, CNOT\n\nmatrix_big = block_to_tensor(CNOT(0, 1), endianness=Endianness.BIG)\nmatrix_little = block_to_tensor(CNOT(0, 1), endianness=Endianness.LITTLE)\n</code></pre> <pre><code>CNOT matrix in big endian =\n\ntensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]])\n\nCNOT matrix in little endian =\n\ntensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]]])\n</code></pre>"},{"location":"content/state_conventions/#backends","title":"Backends","text":"<p>An important part of having clear state conventions is that we need to make sure our results are consistent accross different computational backends, which may have their own conventions. In Qadence, this is taken care of automatically: by calling operations for different backends, the result is expected to be equivalent up to qubit ordering.</p> <pre><code>from qadence import BackendName, RX, run, sample, PI\n\n# RX(PI/4) on qubit 1\nn_qubits = 2\nop = RX(1, PI/4)\n</code></pre> <pre><code>Same sampling order in big endian:\n\nOn PyQTorch = [OrderedCounter({'00': 87, '01': 13})]\nOn Pulser = [Counter({'00': 84, '01': 16})]\n\nSame wavefunction order:\n\nOn PyQTorch = tensor([[0.9239+0.0000j, 0.0000-0.3827j, 0.0000+0.0000j, 0.0000+0.0000j]])\nOn Pulser = tensor([[0.9239+0.0000j, 0.0000-0.3826j, 0.0000+0.0000j, 0.0000+0.0000j]])\n</code></pre>"},{"location":"content/state_init/","title":"State initialization","text":"<p>Qadence offers convenience routines for preparing initial quantum states. These routines are divided into two approaches:</p> <ul> <li>As a dense matrix.</li> <li>From a suitable quantum circuit. This is available for every backend and it should be added in front of the desired quantum circuit to simulate.</li> </ul> <p>Let's illustrate the usage of the state preparation routine.</p> <pre><code>from qadence import random_state, product_state, is_normalized, StateGeneratorType\n\n# Random initial state.\n# the default `type` is StateGeneratorType.HaarMeasureFast\nstate = random_state(n_qubits=2, type=StateGeneratorType.RANDOM_ROTATIONS)\n\n# Check the normalization.\nassert is_normalized(state)\n\n# Product state from a given bitstring.\n# NB: Qadence follows the big endian convention.\nstate = product_state(\"01\")\n</code></pre> <pre><code>Random initial state generated with rotations:\n\nstate = [0.95906007+0.28320273j 0.        +0.j         0.        +0.j\n 0.        +0.j        ]\n\nProduct state corresponding to bitstring '01':\n\nstate = [0.+0.j 1.+0.j 0.+0.j 0.+0.j]\n</code></pre> <p>Now we see how to generate the product state corresponding to the one above with a suitable quantum circuit.</p> <p><pre><code>from qadence import product_block, tag, hea, QuantumCircuit\nfrom qadence.draw import display\n\nstate_prep_block = product_block(\"01\")\n# display(state_prep_block)\n\n# Let's now prepare a circuit.\nn_qubits = 4\n\nstate_prep_block = product_block(\"0001\")\ntag(state_prep_block, \"Prep block\")\n\ncircuit_block = tag(hea(n_qubits, depth = 2), \"Circuit block\")\n\nqc_with_state_prep = QuantumCircuit(n_qubits, state_prep_block, circuit_block)\n</code></pre> %3 cluster_88edb20c2d09423282a10c4dca66ab90 Circuit block cluster_d5fc1a366de34314b93e58878e0f5877 Prep block 8365e1d6af6e4527a056b0754602b5b2 0 04a90a58c4f5435fa92f357a791abc7d 8365e1d6af6e4527a056b0754602b5b2--04a90a58c4f5435fa92f357a791abc7d 3e026bb4e83f4fbe9a4652a009780935 1 0a92954537db4449a17b923f90a157b8 RX(theta\u2080) 04a90a58c4f5435fa92f357a791abc7d--0a92954537db4449a17b923f90a157b8 eeee0eca30b14a89ba19353179e64037 RY(theta\u2084) 0a92954537db4449a17b923f90a157b8--eeee0eca30b14a89ba19353179e64037 3e3c19293d724a95b2772fba828e7b62 RX(theta\u2088) eeee0eca30b14a89ba19353179e64037--3e3c19293d724a95b2772fba828e7b62 2460107fb237423d89af67c33890faf1 3e3c19293d724a95b2772fba828e7b62--2460107fb237423d89af67c33890faf1 573c61eb2317454e9ae25f5c688166a4 2460107fb237423d89af67c33890faf1--573c61eb2317454e9ae25f5c688166a4 282ab59406e74276a21b1d9043cdb635 RX(theta\u2081\u2082) 573c61eb2317454e9ae25f5c688166a4--282ab59406e74276a21b1d9043cdb635 9e1720acecd449c19400844ee1dae4bd RY(theta\u2081\u2086) 282ab59406e74276a21b1d9043cdb635--9e1720acecd449c19400844ee1dae4bd 31f1862e71a046318f0f39d03f190078 RX(theta\u2082\u2080) 9e1720acecd449c19400844ee1dae4bd--31f1862e71a046318f0f39d03f190078 58981d93abce4cc7a078da61ef456d70 31f1862e71a046318f0f39d03f190078--58981d93abce4cc7a078da61ef456d70 4a62e9e915ee429eb981c867aa0ac4f3 58981d93abce4cc7a078da61ef456d70--4a62e9e915ee429eb981c867aa0ac4f3 d5814ca912ac47a4bf54b37e64320aa1 4a62e9e915ee429eb981c867aa0ac4f3--d5814ca912ac47a4bf54b37e64320aa1 20c6a5b3f6e548b29c39131ae7b8175f 8742ab7fe4d547a48aa0aa22eb44d8be 3e026bb4e83f4fbe9a4652a009780935--8742ab7fe4d547a48aa0aa22eb44d8be 452930b78a6e421c8c24d159d221ac60 2 4c6f0a7605c045d4b1d14f73bbf5bd22 RX(theta\u2081) 8742ab7fe4d547a48aa0aa22eb44d8be--4c6f0a7605c045d4b1d14f73bbf5bd22 523745abdfd640c0b3a3570746811dfe RY(theta\u2085) 4c6f0a7605c045d4b1d14f73bbf5bd22--523745abdfd640c0b3a3570746811dfe 0dbf0aa3527f4c3a8997e123b92d4de0 RX(theta\u2089) 523745abdfd640c0b3a3570746811dfe--0dbf0aa3527f4c3a8997e123b92d4de0 c792e988497a41ebb60ca19375177b67 X 0dbf0aa3527f4c3a8997e123b92d4de0--c792e988497a41ebb60ca19375177b67 c792e988497a41ebb60ca19375177b67--2460107fb237423d89af67c33890faf1 092c53943c4d43ecb4ffcf3b58465488 c792e988497a41ebb60ca19375177b67--092c53943c4d43ecb4ffcf3b58465488 6f4aa08f2f82428296ea8fd7a7fc6501 RX(theta\u2081\u2083) 092c53943c4d43ecb4ffcf3b58465488--6f4aa08f2f82428296ea8fd7a7fc6501 8292c3a5b0fd424aabf353f2010835ec RY(theta\u2081\u2087) 6f4aa08f2f82428296ea8fd7a7fc6501--8292c3a5b0fd424aabf353f2010835ec 66a100647ceb4e34af1536c1d8a2106e RX(theta\u2082\u2081) 8292c3a5b0fd424aabf353f2010835ec--66a100647ceb4e34af1536c1d8a2106e 815b2b2f52f24062bcb8bfddc302fcca X 66a100647ceb4e34af1536c1d8a2106e--815b2b2f52f24062bcb8bfddc302fcca 815b2b2f52f24062bcb8bfddc302fcca--58981d93abce4cc7a078da61ef456d70 6851e8ac7bdf4af88d795b53cbaf25cd 815b2b2f52f24062bcb8bfddc302fcca--6851e8ac7bdf4af88d795b53cbaf25cd 6851e8ac7bdf4af88d795b53cbaf25cd--20c6a5b3f6e548b29c39131ae7b8175f ecd823085b654e87810f909f4ed18dc5 5456698631b44cd99f8a7ae987d50293 452930b78a6e421c8c24d159d221ac60--5456698631b44cd99f8a7ae987d50293 56019214a71045349087220adec6b25c 3 b08e1ec2fa4d4a2bbaf59b39b95ab0de RX(theta\u2082) 5456698631b44cd99f8a7ae987d50293--b08e1ec2fa4d4a2bbaf59b39b95ab0de 54889f73bc244d30800c9fb5cf702eda RY(theta\u2086) b08e1ec2fa4d4a2bbaf59b39b95ab0de--54889f73bc244d30800c9fb5cf702eda dca265a130ed4ac79f93e26f367f4dc9 RX(theta\u2081\u2080) 54889f73bc244d30800c9fb5cf702eda--dca265a130ed4ac79f93e26f367f4dc9 f59e2b501c7d4fba9c72e43db1e96753 dca265a130ed4ac79f93e26f367f4dc9--f59e2b501c7d4fba9c72e43db1e96753 9934396648784ff784e7e6d687dbd0d0 X f59e2b501c7d4fba9c72e43db1e96753--9934396648784ff784e7e6d687dbd0d0 9934396648784ff784e7e6d687dbd0d0--092c53943c4d43ecb4ffcf3b58465488 dcf30b64f58e43a8b463fd1719cf7b20 RX(theta\u2081\u2084) 9934396648784ff784e7e6d687dbd0d0--dcf30b64f58e43a8b463fd1719cf7b20 480d688e7e10456ab957fcd497672d18 RY(theta\u2081\u2088) dcf30b64f58e43a8b463fd1719cf7b20--480d688e7e10456ab957fcd497672d18 b008f68ea24c4d238642464bd6176321 RX(theta\u2082\u2082) 480d688e7e10456ab957fcd497672d18--b008f68ea24c4d238642464bd6176321 37c6264aba9942fd8ce9c954b8bb3437 b008f68ea24c4d238642464bd6176321--37c6264aba9942fd8ce9c954b8bb3437 12036dbeb778449cbb6a53efdd045e40 X 37c6264aba9942fd8ce9c954b8bb3437--12036dbeb778449cbb6a53efdd045e40 12036dbeb778449cbb6a53efdd045e40--6851e8ac7bdf4af88d795b53cbaf25cd 12036dbeb778449cbb6a53efdd045e40--ecd823085b654e87810f909f4ed18dc5 7cd58e8b790d4e44ba2764effe6fefea 9fa97f9a9adc42ee8d25097139a46c07 X 56019214a71045349087220adec6b25c--9fa97f9a9adc42ee8d25097139a46c07 8ca221f817574054ae504a62994076a4 RX(theta\u2083) 9fa97f9a9adc42ee8d25097139a46c07--8ca221f817574054ae504a62994076a4 117a6a98d20343d6855aca00b6191c55 RY(theta\u2087) 8ca221f817574054ae504a62994076a4--117a6a98d20343d6855aca00b6191c55 a9bf44ae986f4eb8a403c9aa5b2411a0 RX(theta\u2081\u2081) 117a6a98d20343d6855aca00b6191c55--a9bf44ae986f4eb8a403c9aa5b2411a0 4bb42067adbd4598a29dc0fa39f36d2e X a9bf44ae986f4eb8a403c9aa5b2411a0--4bb42067adbd4598a29dc0fa39f36d2e 4bb42067adbd4598a29dc0fa39f36d2e--f59e2b501c7d4fba9c72e43db1e96753 489385547e704d2f853fc778c10b1cd4 4bb42067adbd4598a29dc0fa39f36d2e--489385547e704d2f853fc778c10b1cd4 ea0f261c12db46bc88a973ffddc00fa6 RX(theta\u2081\u2085) 489385547e704d2f853fc778c10b1cd4--ea0f261c12db46bc88a973ffddc00fa6 a87e3b9fa10e4b09b385ad1780eafd44 RY(theta\u2081\u2089) ea0f261c12db46bc88a973ffddc00fa6--a87e3b9fa10e4b09b385ad1780eafd44 ea5a70867ae54cadbb3654786dc88829 RX(theta\u2082\u2083) a87e3b9fa10e4b09b385ad1780eafd44--ea5a70867ae54cadbb3654786dc88829 14a06f8f578f4acca4e0349f7676d9d5 X ea5a70867ae54cadbb3654786dc88829--14a06f8f578f4acca4e0349f7676d9d5 14a06f8f578f4acca4e0349f7676d9d5--37c6264aba9942fd8ce9c954b8bb3437 33e9eacafd734cbb87198c7294a682cc 14a06f8f578f4acca4e0349f7676d9d5--33e9eacafd734cbb87198c7294a682cc 33e9eacafd734cbb87198c7294a682cc--7cd58e8b790d4e44ba2764effe6fefea  Several standard quantum states can be conveniently initialized in Qadence, both in statevector form as well as in block form as shown in following.</p>"},{"location":"content/state_init/#state-vector-initialization","title":"State vector initialization","text":"<p>Qadence offers a number of constructor functions for state vector preparation.</p> <pre><code>from qadence import uniform_state, zero_state, one_state\n\nn_qubits = 3\nbatch_size = 2\n\nuniform_state = uniform_state(n_qubits, batch_size)\nzero_state = zero_state(n_qubits, batch_size)\none_state = one_state(n_qubits, batch_size)\n</code></pre> <pre><code>Uniform state = \n\ntensor([[0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,\n         0.3536+0.j],\n        [0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j, 0.3536+0.j,\n         0.3536+0.j]])\nZero state = \n\ntensor([[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\nOne state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre> <p>As already seen, product states can be easily created, even in batches:</p> <pre><code>from qadence import product_state, rand_product_state\n\n# From a bitsring \"100\"\nprod_state = product_state(\"100\", batch_size)\n\n# Or a random product state\nrand_state = rand_product_state(n_qubits, batch_size)\n</code></pre> <pre><code>Product state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]])\n\nRandom state = \n\ntensor([[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n        [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]])\n</code></pre> <p>Creating a GHZ state:</p> <pre><code>from qadence import ghz_state\n\nghz = ghz_state(n_qubits, batch_size)\n</code></pre> <pre><code>GHZ state = \n\ntensor([[0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j,\n         0.7071+0.j],\n        [0.7071+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.0000+0.j,\n         0.7071+0.j]])\n</code></pre> <p>Creating a random state uniformly sampled from a Haar measure:</p> <pre><code>from qadence import random_state\n\nrand_haar_state = random_state(n_qubits, batch_size)\n</code></pre> <pre><code>Random state from Haar = \n\ntensor([[-0.1979+0.1218j,  0.3503-0.3097j, -0.2027+0.2565j,  0.2727-0.3882j,\n          0.3138+0.4586j,  0.2025-0.1360j,  0.1061+0.0889j,  0.0225-0.0858j],\n        [ 0.5372+0.3381j,  0.3074-0.1143j, -0.2248+0.0703j, -0.0898-0.2938j,\n         -0.0896-0.1091j, -0.1593-0.4118j,  0.2188+0.1255j, -0.2026+0.1417j]])\n</code></pre> <p>Custom initial states can then be passed to either <code>run</code>, <code>sample</code> and <code>expectation</code> through the <code>state</code> argument</p> <pre><code>from qadence import random_state, product_state, CNOT, run\n\ninit_state = product_state(\"10\")\nfinal_state = run(CNOT(0, 1), state=init_state)\n</code></pre> <pre><code>Final state = tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]])\n</code></pre>"},{"location":"content/state_init/#density-matrices-conversion","title":"Density matrices conversion","text":"<p>It is also possible to obtain density matrices from statevectors. They can be passed as inputs to quantum programs performing density matrix based operations such as noisy simulations, when the backend allows such as PyQTorch.</p> <pre><code>from qadence import product_state, density_mat\n\ninit_state = product_state(\"10\")\ninit_density_matrix = density_mat(init_state)\n\nfinal_density_matrix = run(CNOT(0, 1), state=init_density_matrix)\n</code></pre> <pre><code>Initial = DensityMatrix([[[0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n                [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]]])\nFinal = DensityMatrix([[[0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 0.-0.j],\n                [0.-0.j, 0.-0.j, 0.-0.j, 1.-0.j]]])\n</code></pre>"},{"location":"content/state_init/#block-initialization","title":"Block initialization","text":"<p>Not all backends support custom statevector initialization, however previous utility functions have their counterparts to initialize the respective blocks:</p> <pre><code>from qadence import uniform_block, one_block\n\nn_qubits = 3\n\nuniform_block = uniform_block(n_qubits)\n\none_block = one_block(n_qubits)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 H(0)\n\u251c\u2500\u2500 H(1)\n\u2514\u2500\u2500 H(2)\nKronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 X(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> <p>Similarly, for product states:</p> <pre><code>from qadence import product_block, rand_product_block\n\nproduct_block = product_block(\"100\")\n\nrand_product_block = rand_product_block(n_qubits)\n</code></pre> <pre><code>KronBlock(0,1,2)\n\u251c\u2500\u2500 X(0)\n\u251c\u2500\u2500 I(1)\n\u2514\u2500\u2500 I(2)\nKronBlock(0,1,2)\n\u251c\u2500\u2500 I(0)\n\u251c\u2500\u2500 I(1)\n\u2514\u2500\u2500 X(2)\n</code></pre> <p>And GHZ states:</p> <pre><code>from qadence import ghz_block\n\nghz_block = ghz_block(n_qubits)\n</code></pre> <pre><code>ChainBlock(0,1,2)\n\u251c\u2500\u2500 H(0)\n\u2514\u2500\u2500 ChainBlock(0,1,2)\n    \u251c\u2500\u2500 CNOT(0, 1)\n    \u2514\u2500\u2500 CNOT(1, 2)\n</code></pre> <p>Initial state blocks can simply be chained at the start of a given circuit.</p>"},{"location":"content/state_init/#utility-functions","title":"Utility functions","text":"<p>Some state vector utility functions are also available. We can easily create the probability mass function of a given statevector using <code>torch.distributions.Categorical</code></p> <pre><code>from qadence import random_state, pmf\n\nn_qubits = 3\n\nstate = random_state(n_qubits)\ndistribution = pmf(state)\n</code></pre> <pre><code>Categorical(probs: torch.Size([1, 8]))\n</code></pre> <p>We can also check if a state is normalized:</p> <pre><code>from qadence import random_state, is_normalized\n\nstate = random_state(n_qubits)\nprint(is_normalized(state))\n</code></pre> <pre><code>True\n</code></pre> <p>Or normalize a state:</p> <pre><code>import torch\nfrom qadence import normalize, is_normalized\n\nstate = torch.tensor([[1, 1, 1, 1]], dtype = torch.cdouble)\nprint(normalize(state))\n</code></pre> <pre><code>tensor([[0.5000+0.j, 0.5000+0.j, 0.5000+0.j, 0.5000+0.j]])\n</code></pre>"},{"location":"content/time_dependent/","title":"Time-dependent generators","text":"<p>For use cases when the Hamiltonian of the system is time-dependent, Qadence provides a special parameter <code>TimeParameter(\"t\")</code> that denotes the explicit time dependence. Using this time parameter, one can define a parameterized block acting as the generator passed to <code>HamEvo</code> that encapsulates the required time dependence function.</p>"},{"location":"content/time_dependent/#noiseless-time-dependent-hamiltonian-evolution","title":"Noiseless time-dependent Hamiltonian evolution","text":"<pre><code>from qadence import X, Y, HamEvo, TimeParameter, FeatureParameter, run\nfrom pyqtorch.utils import SolverType\nimport torch\n\n# Simulation parameters\node_solver = SolverType.DP5_SE  # time-dependent Schrodinger equation solver method\nn_steps_hevo = 500  # integration time steps used by solver\n\n# Define block parameters\nt = TimeParameter(\"t\")\nomega_param = FeatureParameter(\"omega\")\n\n# Arbitrarily compose a time-dependent generator\ntd_generator = omega_param * (t * X(0) + t**2 * Y(1))\n\n# Create parameterized HamEvo block\nhamevo = HamEvo(td_generator, t)\n</code></pre> <p>Note that when using <code>HamEvo</code> with a time-dependent generator, the actual time parameter that was used to construct the generator must be passed for the second argument <code>parameter</code>.</p> <p>By default, the code above will initialize an internal parameter <code>FeatureParameter(\"duration\")</code> in the <code>HamEvo</code>. Alternatively, the <code>duration</code> argument can be used to rename this parameter, or to pass a fixed value directly. If no fixed value is passed, it must then be set in the <code>values</code> dictionary at runtime.</p> <p>Future improvements</p> <p>Currently it is only possible to pass a single value for the duration, and the only result obtained will be the one corresponding to the state at end of the integration. In the future we will change the interface to allow directly passing some array of save values to obtain expectation values or statevectors at intermediate steps during the evolution.</p> <pre><code>values = {\"omega\": torch.tensor(10.0), \"duration\": torch.tensor(1.0)}\n\nconfig = {\"ode_solver\": ode_solver, \"n_steps_hevo\": n_steps_hevo}\n\nout_state = run(hamevo, values = values, configuration = config)\n\nprint(out_state)\n</code></pre> <pre><code>tensor([[-0.2785+0.0000j, -0.0541+0.0000j,  0.0000-0.9414j,  0.0000-0.1827j]])\n</code></pre> <p>Note that Qadence makes no assumption on units. The unit of passed duration value \\(\\tau\\) must be aligned with the units of other parameters in the time-dependent generator so that the integral of generator \\(\\overset{\\tau}{\\underset{0}{\\int}}\\mathcal{\\hat{H}}(t){\\rm d}t\\) is dimensionless.</p>"},{"location":"content/time_dependent/#noisy-time-dependent-hamiltonian-evolution","title":"Noisy time-dependent Hamiltonian evolution","text":"<p>To perform noisy time-dependent Hamiltonian evolution, one needs to pass a list of noise operators to the <code>noise_operators</code> argument in <code>HamEvo</code>. They correspond to the jump operators used within the time-dependent Schrodinger equation solver method <code>SolverType.DP5_ME</code>:</p> <pre><code>from qadence import X, Y, HamEvo, TimeParameter, FeatureParameter, run\nfrom pyqtorch.utils import SolverType\nimport torch\n\n# Simulation parameters\node_solver = SolverType.DP5_ME  # time-dependent Schrodinger equation solver method\nn_steps_hevo = 500  # integration time steps used by solver\n\n# Define block parameters\nt = TimeParameter(\"t\")\nomega_param = FeatureParameter(\"omega\")\n\n# Arbitrarily compose a time-dependent generator\ntd_generator = omega_param * (t * X(0) + t**2 * Y(1))\n\n# Create parameterized HamEvo block\nnoise_operators = [X(i) for i in td_generator.qubit_support]\nhamevo = HamEvo(td_generator, t, noise_operators = noise_operators)\n\nvalues = {\"omega\": torch.tensor(10.0), \"duration\": torch.tensor(1.0)}\n\nconfig = {\"ode_solver\": ode_solver, \"n_steps_hevo\": n_steps_hevo}\n\nout_state = run(hamevo, values = values, configuration = config)\n\nprint(out_state)\n</code></pre>   DensityMatrix([[[0.2734+0.0000j, 0.0297+0.0000j, 0.0000-0.0227j, 0.0000-0.0025j],                 [0.0297+0.0000j, 0.1698+0.0000j, 0.0000-0.0025j, 0.0000-0.0141j],                 [0.0000+0.0227j, 0.0000+0.0025j, 0.3435+0.0000j, 0.0373+0.0000j],                 [0.0000+0.0025j, 0.0000+0.0141j, 0.0373+0.0000j, 0.2133+0.0000j]]])    <p>Noise operators definition</p> <p>Note it is not possible to define <code>noise_operators</code> with parametric operators. If you want to do so, we recommend obtaining the tensors via run and set <code>noise_operators</code> using <code>MatrixBlock</code>. Also, <code>noise_operators</code> should have the same or a subset of the qubit support of the <code>HamEvo</code> instance.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/","title":"Code of Conduct","text":""},{"location":"getting_started/CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"getting_started/CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"getting_started/CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"getting_started/CONTRIBUTING/","title":"How to contribute","text":"<p>We're grateful for your interest in participating in Qadence. Please follow our guidelines to ensure a smooth contribution process.</p>"},{"location":"getting_started/CONTRIBUTING/#reporting-an-issue-or-proposing-a-feature","title":"Reporting an issue or proposing a feature","text":"<p>Your course of action will depend on your objective, but generally, you should start by creating an issue. If you've discovered a bug or have a feature you'd like to see added to qadence, feel free to create an issue on qadence's GitHub issue tracker. Here are some steps to take:</p> <ol> <li>Quickly search the existing issues using relevant keywords to ensure your issue hasn't been addressed already.</li> <li> <p>If your issue is not listed, create a new one. Try to be as detailed and clear as possible in your description.</p> </li> <li> <p>If you're merely suggesting an improvement or reporting a bug, that's already excellent! We thank you for it. Your issue will be listed and, hopefully, addressed at some point.</p> </li> <li>However, if you're willing to be the one solving the issue, that would be even better! In such instances, you would proceed by preparing a Pull Request.</li> </ol>"},{"location":"getting_started/CONTRIBUTING/#submitting-a-pull-request","title":"Submitting a pull request","text":"<p>We're excited that you're eager to contribute to Qadence. To contribute, fork the <code>main</code> branch of qadence repository and once you are satisfied with your feature and all the tests pass create a Pull Request.</p> <p>Here's the process for making a contribution:</p> <p>Click the \"Fork\" button at the upper right corner of the repo page to create a new GitHub repo at <code>https://github.com/USERNAME/qadence</code>, where <code>USERNAME</code> is your GitHub ID. Then, <code>cd</code> into the directory where you want to place your new fork and clone it:</p> <pre><code>git clone https://github.com/USERNAME/qadence.git\n</code></pre> <p>Next, navigate to your new qadence fork directory and mark the main qadence repository as the <code>upstream</code>:</p> <pre><code>git remote add upstream https://github.com/pasqal-io/qadence.git\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#setting-up-your-development-environment","title":"Setting up your development environment","text":"<p>We recommended to use <code>hatch</code> for managing environments:</p> <p>To develop within qadence, use: <pre><code>pip install hatch\nhatch -v shell\n</code></pre></p> <p>To run qadence tests, use:</p> <pre><code>hatch -e tests run test\n</code></pre> <p>If you don't want to use <code>hatch</code>, you can use the environment manager of your choice (e.g. Conda) and execute the following:</p> <pre><code>pip install pytest\npip install -e .\npytest\n</code></pre>"},{"location":"getting_started/CONTRIBUTING/#useful-things-for-your-workflow-linting-and-testing","title":"Useful things for your workflow: linting and testing","text":"<p>Use <code>pre-commit</code> to lint your code and run the unit tests before pushing a new commit.</p> <p>Using <code>hatch</code>, it's simply:</p> <pre><code>hatch -e tests run pre-commit run --all-files\nhatch -e tests run test\n</code></pre> <p>Our CI/CD pipeline will also test if the documentation can be built correctly. To test it locally, please run:</p> <pre><code>hatch -e docs run mkdocs build --clean --strict\n</code></pre> <p>Without <code>hatch</code>, <code>pip</code> install those libraries first: \"mkdocs\", \"mkdocs-material\", \"mkdocstrings\", \"mkdocstrings-python\", \"mkdocs-section-index\", \"mkdocs-jupyter\", \"mkdocs-exclude\", \"markdown-exec\"</p> <p>And then:</p> <pre><code> mkdocs build --clean --strict\n</code></pre>"},{"location":"getting_started/LICENSE/","title":"Apache License","text":"<p>Version 2.0, January 2004</p> <p>http://www.apache.org/licenses/</p>"},{"location":"getting_started/LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"getting_started/LICENSE/#1-definitions","title":"1. Definitions:","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"getting_started/LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"getting_started/LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License.","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"getting_started/LICENSE/#4-redistribution","title":"4. Redistribution.","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ul> <li> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> </li> <li> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> </li> <li> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> </li> <li> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> </li> </ul> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"getting_started/LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions.","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"getting_started/LICENSE/#6-trademarks","title":"6. Trademarks.","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"getting_started/LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty.","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"getting_started/LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability.","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"getting_started/LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability.","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"getting_started/LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":""},{"location":"getting_started/LICENSE/#appendix-how-to-apply-the-apache-license-to-your-work","title":"APPENDIX: How to apply the Apache License to your work.","text":"<p>To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!)  The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives.</p> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<p>Qadence is fully tested on Linux/MacOS operating systems. For Windows users, we recommend using WSL2 to install a Linux distribution of choice.</p>"},{"location":"getting_started/installation/#installation","title":"Installation","text":"<p>Qadence can be installed from PyPI with <code>pip</code> as follows:</p> <pre><code>pip install qadence\n</code></pre> <p>By default, this will also install PyQTorch, a differentiable state vector simulator which serves as the main numerical backend for Qadence.</p> <p>It is possible to install additional backends and the circuit visualization library using the following extras:</p> <ul> <li><code>visualization</code>: to display quantum circuits.</li> <li><code>pulser</code>: the Pulser backend for composing, simulating and executing pulse sequences for neutral-atom quantum devices (in development).</li> </ul> <p>To install other backends or the visualization tool, please use:</p> <pre><code>pip install \"qadence[pulser, visualization]\"\n</code></pre> <p>Note</p> <p>In order to correctly install the <code>visualization</code> extra, the <code>graphviz</code> package needs to be installed in your system:</p> <pre><code># on Ubuntu\nsudo apt install graphviz\n\n# on MacOS\nbrew install graphviz\n\n# via conda\nconda install python-graphviz\n</code></pre>"},{"location":"getting_started/installation/#install-from-source","title":"Install from source","text":"<p>We recommend to use the <code>hatch</code> environment manager to install <code>qadence</code> from source:</p> <pre><code>python -m pip install hatch\n\n# get into a shell with all the dependencies\npython -m hatch shell\n\n# run a command within the virtual environment with all the dependencies\npython -m hatch run python my_script.py\n</code></pre> <p>Warning</p> <p><code>hatch</code> will not combine nicely with other environment managers such Conda. If you want to use Conda, install it from source using <code>pip</code>:</p> <pre><code># within the Conda environment\npython -m pip install -e .\n</code></pre>"},{"location":"getting_started/installation/#citation","title":"Citation","text":"<p>If you use Qadence for a publication, we kindly ask you to cite our work using the following BibTex entry:</p> <pre><code>@article{qadence2024pasqal,\n  title = {Qadence: a differentiable interface for digital-analog programs.},\n  author={Dominik Seitz and Niklas Heim and Jo\u00e3o P. Moutinho and Roland Guichard and Vytautas Abramavicius and Aleksander Wennersteen and Gert-Jan Both and Anton Quelle and Caroline de Groot and Gergana V. Velikova and Vincent E. Elfving and Mario Dagrada},\n  journal={arXiv:2401.09915},\n  url = {https://github.com/pasqal-io/qadence},\n  year = {2024}\n}\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section is undergoing changes and most of the information here will be reorganized.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p>"},{"location":"tutorials/advanced_tutorials/","title":"Advanced Tutorials","text":"<p>In this section, advanced programming concepts and implementations in Qadence are examplified.</p>"},{"location":"tutorials/advanced_tutorials/custom-models/","title":"Custom quantum models","text":"<p>In <code>qadence</code>, the <code>QuantumModel</code> is the central class point for executing <code>QuantumCircuit</code>s.  The idea of a <code>QuantumModel</code> is to decouple the backend execution from the management of circuit parameters and desired quantum computation output.</p> <p>In the following, we create a custom <code>QuantumModel</code> instance which introduces some additional optimizable parameters: *  an adjustable scaling factor in front of the observable to measured *  adjustable scale and shift factors to be applied to the model output before returning the result</p> <p>This can be easily done using PyTorch flexible model definition, and it will automatically work with the rest of <code>qadence</code> infrastructure.</p> <pre><code>import torch\nfrom qadence import QuantumModel, QuantumCircuit\n\n\nclass CustomQuantumModel(QuantumModel):\n\n    def __init__(self, circuit: QuantumCircuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\"):\n        super().__init__(circuit, observable=observable, backend=backend, diff_mode=diff_mode)\n\n        self.n_qubits = circuit.n_qubits\n\n        # define some additional parameters which will scale and shift (variationally) the\n        # output of the QuantumModel\n        # you can use all torch machinery for building those\n        self.scale_out = torch.nn.Parameter(torch.ones(1))\n        self.shift_out = torch.nn.Parameter(torch.ones(1))\n\n    # override the forward pass of the model\n    # the forward pass is the output of your QuantumModel and in this case\n    # it's the (scaled) expectation value of the total magnetization with\n    # a variable coefficient in front\n    def forward(self, values: dict[str, torch.Tensor]) -&gt; torch.Tensor:\n\n        # scale the observable\n        res = self.expectation(values)\n\n        # scale and shift the result before returning\n        return self.shift_out + res * self.scale_out\n</code></pre> <p>The custom model can be used like any other <code>QuantumModel</code>: <pre><code>from qadence import Parameter, RX, CNOT, QuantumCircuit\nfrom qadence import chain, kron, hamiltonian_factory, Z\nfrom sympy import acos\n\ndef quantum_circuit(n_qubits):\n\n    x = Parameter(\"x\", trainable=False)\n    fm = kron(RX(i, acos(x) * (i+1)) for i in range(n_qubits))\n\n    ansatz = kron(RX(i, f\"theta{i}\") for i in range(n_qubits))\n    ansatz = chain(ansatz, CNOT(0, n_qubits-1))\n\n    block = chain(fm, ansatz)\n    block.tag = \"circuit\"\n    return QuantumCircuit(n_qubits, block)\n\nn_qubits = 4\nbatch_size = 10\ncircuit = quantum_circuit(n_qubits)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)  # Total magnetization\n\nmodel = CustomQuantumModel(circuit, observable, backend=\"pyqtorch\")\n\nvalues = {\"x\": torch.rand(batch_size)}\nres = model(values)\nprint(\"Model output: \", res)\nassert len(res) == batch_size\n</code></pre> <pre><code>Model output:  tensor([[-1.0331],\n        [-1.3762],\n        [-0.0357],\n        [-1.5767],\n        [-1.5332],\n        [-1.0431],\n        [-0.2118],\n        [-1.0178],\n        [-1.5768],\n        [-1.4406]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> </p>"},{"location":"tutorials/advanced_tutorials/custom-models/#quantum-model-with-wavefunction-overlaps","title":"Quantum model with wavefunction overlaps","text":"<p><code>QuantumModel</code>'s can also use different quantum operations in their forward pass, such as wavefunction overlaps described here. Beware that the resulting overlap tensor has to be differentiable to apply gradient-based optimization. This is only applicable to the <code>\"EXACT\"</code> overlap method.</p> <p>Here we show how to use overlap calculation when fitting a parameterized quantum circuit to act as a standard Hadamard gate.</p> <pre><code>from qadence import RY, RX, H, Overlap\n\n# create a quantum model which acts as an Hadamard gate after training\nclass LearnHadamard(QuantumModel):\n    def __init__(\n        self,\n        train_circuit: QuantumCircuit,\n        target_circuit: QuantumCircuit,\n        backend=\"pyqtorch\",\n    ):\n        super().__init__(circuit=train_circuit, backend=backend)\n        self.overlap_fn = Overlap(train_circuit, target_circuit, backend=backend, method=\"exact\", diff_mode='ad')\n\n    def forward(self):\n        return self.overlap_fn()\n\n    # compute the wavefunction of the associated train circuit\n    def wavefunction(self):\n        return model.overlap_fn.run({})\n\n\ntrain_circuit = QuantumCircuit(1, chain(RX(0, \"phi\"), RY(0, \"theta\")))\ntarget_circuit = QuantumCircuit(1, H(0))\n\nmodel = LearnHadamard(train_circuit, target_circuit)\n\n# get the overlap between model and target circuit wavefunctions\nprint(model())\n</code></pre> <pre><code>tensor([[0.5985]], grad_fn=&lt;UnsqueezeBackward0&gt;)\n</code></pre> <p>This model can then be trained with the standard Qadence helper functions.</p> <pre><code>from qadence import run\nfrom qadence.ml_tools import Trainer, TrainConfig\nTrainer.set_use_grad(True)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n\ndef loss_fn(model: LearnHadamard, _unused) -&gt; tuple[torch.Tensor, dict]:\n    loss = criterion(torch.tensor([[1.0]]), model())\n    return loss, {}\n\nconfig = TrainConfig(max_iter=2500)\ntrainer = Trainer(\n    model, optimizer, config, loss_fn\n)\nmodel, optimizer = trainer.fit()\n\nwf_target = run(target_circuit)\nassert torch.allclose(wf_target, model.wavefunction(), atol=1e-2)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/advanced_tutorials/differentiability/","title":"Differentiability","text":"<p>Many application in quantum computing and quantum machine learning more specifically requires the differentiation of a quantum circuit with respect to its parameters.</p> <p>In Qadence, we perform quantum computations via the <code>QuantumModel</code> interface. The derivative of the outputs of quantum models with respect to feature and variational parameters in the quantum circuit can be implemented in Qadence with two different modes:</p> <ul> <li>Automatic differentiation (AD) mode <sup>1</sup>. This mode allows to differentiation both <code>run()</code> and <code>expectation()</code> methods of the <code>QuantumModel</code> and it is the fastest available differentiation method. Under the hood, it is based on the PyTorch autograd engine wrapped by the <code>DifferentiableBackend</code> class. This mode is not working on quantum devices.</li> <li>Generalized parameter shift rule (GPSR) mode. This is general implementation of the well known parameter  shift rule algorithm <sup>2</sup> which works for arbitrary quantum operations <sup>3</sup>. This mode is only applicable to  the <code>expectation()</code> method of <code>QuantumModel</code> but it is compatible with execution or quantum devices.</li> </ul>"},{"location":"tutorials/advanced_tutorials/differentiability/#automatic-differentiation","title":"Automatic differentiation","text":"<p>Automatic differentiation <sup>1</sup> is a procedure to derive a complex function defined as a sequence of elementary mathematical operations in the form of a computer program. Automatic differentiation is a cornerstone of modern machine learning and a crucial ingredient of its recent successes. In its so-called reverse mode, it follows this sequence of operations in reverse order by systematically applying the chain rule to recover the exact value of derivative. Reverse mode automatic differentiation is implemented in Qadence leveraging the PyTorch <code>autograd</code> engine.</p> <p>Only available via the PyQTorch or Horqrux backends</p> <p>Currently, automatic differentiation mode is only available when the <code>pyqtorch</code> or <code>horqrux</code> backends are selected.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#generalized-parameter-shift-rule","title":"Generalized parameter shift rule","text":"<p>The generalized parameter shift rule implementation in Qadence was introduced in <sup>3</sup>. Here the standard parameter shift rules, which only works for quantum operations whose generator has a single gap in its eigenvalue spectrum, was generalized to work with arbitrary generators of quantum operations.</p> <p>For this, we define the differentiable function as quantum expectation value</p> \\[ f(x) = \\left\\langle 0\\right|\\hat{U}^{\\dagger}(x)\\hat{C}\\hat{U}(x)\\left|0\\right\\rangle \\] <p>where \\(\\hat{U}(x)={\\rm exp}{\\left( -i\\frac{x}{2}\\hat{G}\\right)}\\) is the quantum evolution operator with generator \\(\\hat{G}\\) representing the structure of the underlying quantum circuit and \\(\\hat{C}\\) is the cost operator. Then using the eigenvalue spectrum \\(\\left\\{ \\lambda_n\\right\\}\\) of the generator \\(\\hat{G}\\) we calculate the full set of corresponding unique non-zero spectral gaps \\(\\left\\{ \\Delta_s\\right\\}\\) (differences between eigenvalues). It can be shown that the final expression of derivative of \\(f(x)\\) is then given by the following expression:</p> <p>\\(\\begin{equation} \\frac{{\\rm d}f\\left(x\\right)}{{\\rm d}x}=\\overset{S}{\\underset{s=1}{\\sum}}\\Delta_{s}R_{s}, \\end{equation}\\)</p> <p>where \\(S\\) is the number of unique non-zero spectral gaps and \\(R_s\\) are real quantities that are solutions of a system of linear equations</p> <p>\\(\\begin{equation} \\begin{cases} F_{1} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{1}\\Delta_{s}}{2}\\right)R_{s},\\\\ F_{2} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{2}\\Delta_{s}}{2}\\right)R_{s},\\\\  &amp; ...\\\\ F_{S} &amp; =4\\overset{S}{\\underset{s=1}{\\sum}}{\\rm sin}\\left(\\frac{\\delta_{M}\\Delta_{s}}{2}\\right)R_{s}. \\end{cases} \\end{equation}\\)</p> <p>Here \\(F_s=f(x+\\delta_s)-f(x-\\delta_s)\\) denotes the difference between values of functions evaluated at shifted arguments \\(x\\pm\\delta_s\\).</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#adjoint-differentiation","title":"Adjoint Differentiation","text":"<p>Qadence also offers a memory-efficient, non-device compatible alternative to automatic differentation, called 'Adjoint Differentiation' <sup>4</sup> and allows for precisely calculating the gradients of variational parameters in O(P) time and using O(1) state-vectors. Adjoint Differentation is currently only supported by the Torch Engine and allows for first-order derivatives only.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#usage","title":"Usage","text":""},{"location":"tutorials/advanced_tutorials/differentiability/#basics","title":"Basics","text":"<p>In Qadence, the differentiation modes can be selected via the <code>diff_mode</code> argument of the QuantumModel class. It either accepts a <code>DiffMode</code>(<code>DiffMode.GSPR</code>, <code>DiffMode.AD</code> or <code>DiffMode.ADJOINT</code>) or a string (<code>\"gpsr\"\"</code>, <code>\"ad\"</code> or <code>\"adjoint\"</code>). The code in the box below shows how to create <code>QuantumModel</code> instances with all available differentiation modes.</p> <pre><code>from qadence import (FeatureParameter, RX, Z, hea, chain,\n                    hamiltonian_factory, QuantumCircuit,\n                    QuantumModel, BackendName, DiffMode)\nimport torch\n\nn_qubits = 2\n\n# Define a symbolic parameter to differentiate with respect to\nx = FeatureParameter(\"x\")\n\nblock = chain(hea(n_qubits, 1), RX(0, x))\n\n# create quantum circuit\ncircuit = QuantumCircuit(n_qubits, block)\n\n# create total magnetization cost operator\nobs = hamiltonian_factory(n_qubits, detuning=Z)\n\n# create models with AD, ADJOINT and GPSR differentiation engines\nmodel_ad = QuantumModel(circuit, obs,\n                        backend=BackendName.PYQTORCH,\n                        diff_mode=DiffMode.AD)\nmodel_adjoint = QuantumModel(circuit, obs,\n                        backend=BackendName.PYQTORCH,\n                        diff_mode=DiffMode.ADJOINT)\nmodel_gpsr = QuantumModel(circuit, obs,\n                          backend=BackendName.PYQTORCH,\n                          diff_mode=DiffMode.GPSR)\n\n# Create concrete values for the parameter we want to differentiate with respect to\nxs = torch.linspace(0, 2*torch.pi, 100, requires_grad=True)\nvalues = {\"x\": xs}\n\n# calculate function f(x)\nexp_val_ad = model_ad.expectation(values)\nexp_val_adjoint = model_adjoint.expectation(values)\nexp_val_gpsr = model_gpsr.expectation(values)\n\n# calculate derivative df/dx using the PyTorch\n# autograd engine\ndexpval_x_ad = torch.autograd.grad(\n    exp_val_ad, values[\"x\"], torch.ones_like(exp_val_ad), create_graph=True\n)[0]\ndexpval_x_adjoint = torch.autograd.grad(\n    exp_val_adjoint, values[\"x\"], torch.ones_like(exp_val_ad), create_graph=True\n)[0]\ndexpval_x_gpsr = torch.autograd.grad(\n    exp_val_gpsr, values[\"x\"], torch.ones_like(exp_val_gpsr), create_graph=True\n)[0]\n</code></pre> <p>We can plot the resulting derivatives and see that in both cases they coincide.</p> <pre><code>import matplotlib.pyplot as plt\n\n# plot f(x) and df/dx derivatives calculated using AD ,ADJOINT and GPSR\n# differentiation engines\nfig, ax = plt.subplots()\nax.scatter(xs.detach().numpy(),\n           exp_val_ad.detach().numpy(),\n           label=\"f(x)\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_ad.detach().numpy(),\n           label=\"df/dx AD\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_adjoint.detach().numpy(),\n           label=\"df/dx ADJOINT\")\nax.scatter(xs.detach().numpy(),\n           dexpval_x_gpsr.detach().numpy(),\n           s=5,\n           label=\"df/dx GPSR\")\nplt.legend()\n</code></pre> 2025-01-08T18:01:28.540407 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/advanced_tutorials/differentiability/#low-level-control-on-the-shift-values","title":"Low-level control on the shift values","text":"<p>In order to get a finer control over the GPSR differentiation engine we can use the low-level Qadence API to define a <code>DifferentiableBackend</code>.</p> <pre><code>from qadence.engines.torch import DifferentiableBackend\nfrom qadence.backends.pyqtorch import Backend as PyQBackend\n\n# define differentiable quantum backend\nquantum_backend = PyQBackend()\nconv = quantum_backend.convert(circuit, obs)\npyq_circ, pyq_obs, embedding_fn, params = conv\ndiff_backend = DifferentiableBackend(quantum_backend, diff_mode=DiffMode.GPSR, shift_prefac=0.2)\n\n# calculate function f(x)\nexpval = diff_backend.expectation(pyq_circ, pyq_obs, embedding_fn(params, values))\n</code></pre> <p>Here we passed an additional argument <code>shift_prefac</code> to the <code>DifferentiableBackend</code> instance that governs the magnitude of shifts \\(\\delta\\equiv\\alpha\\delta^\\prime\\) shown in equation (2) above. In this relation \\(\\delta^\\prime\\) is set internally and \\(\\alpha\\) is the value passed by <code>shift_prefac</code> and the resulting shift value \\(\\delta\\) is then used in all the following GPSR calculations.</p> <p>Tuning parameter \\(\\alpha\\) is useful to improve results when the generator \\(\\hat{G}\\) or the quantum operation is a dense matrix, for example a complex <code>HamEvo</code> operation; if many entries of this matrix are sufficiently larger than 0 the operation is equivalent to a strongly interacting system. In such case parameter \\(\\alpha\\) should be gradually lowered in order to achieve exact derivative values.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#low-level-differentiation-of-qadence-circuits-using-jax","title":"Low-level differentiation of qadence circuits using JAX","text":"<p>For users interested in using the <code>JAX</code> engine instead, we show how to run and differentiate qadence programs using the <code>horqrux</code> backend under qadence examples.</p>"},{"location":"tutorials/advanced_tutorials/differentiability/#references","title":"References","text":"<ol> <li> <p>A. G. Baydin et al., Automatic Differentiation in Machine Learning: a Survey \u21a9\u21a9</p> </li> <li> <p>Schuld et al., Evaluating analytic gradients on quantum hardware (2018). \u21a9</p> </li> <li> <p>Kyriienko et al., General quantum circuit differentiation rules \u21a9\u21a9</p> </li> <li> <p>Tyson et al., Efficient calculation of gradients in classical simulations of variational quantum algorithms \u21a9</p> </li> </ol>"},{"location":"tutorials/advanced_tutorials/profiling-and-debugging/","title":"Profiling and debugging on CUDA devices","text":"<p>For this to work, you'll have to have the right to access perf counters on your machine (for example with sudo or inside a Docker container). See: https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/index.html.</p> <pre><code>$ pip install nvidia-pyindex\n$ pip install nvidia-dlprof[pytorch]\n</code></pre> <p>Make sure that your entrypoint is an executable script. That means that it must start with a she-bang on the top line, e.g. <pre><code>#!/bin/python\n</code></pre> and have execution rights <pre><code>$ chmod +x your_script.py\n</code></pre></p> <p>Lastly it's recommended to add <pre><code>import nvidia_dlprof_pytorch_nvtx\nnvidia_dlprof_pytorch_nvtx.init()\n</code></pre> To your script in the beginning enable extra annotations of PyTorch functions.</p> <p>You can then use dlprof to profile. <pre><code>dlprof --mode=pytorch your_script.py\n</code></pre></p> <pre><code>PYQ_LOG_LEVEL=info QADENCE_LOG_LEVEL=debug dlprof --mode=pytorch examples/backends/differentiable_backend.py\n</code></pre> <p>For example to achieve this through Docker we can start a session in the shell, also mounting our local Qadence version and PyQTorch (Both optional, but you probably need to mount your script at least) <pre><code>$ docker run --rm --gpus=1 --shm-size=1g --ulimit memlock=-1 \\\n --ulimit stack=67108864 -it -p8000:8000 -v./:/opt/qadence -v ../PyQ:/opt/pyqtorch pytorch:24.02-py3 bash\n</code></pre> (You may need to jump through extra hoops to make Docker access the GPUs if you have error messages like <code>docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].</code>)</p> <p>After this you should have a shell inside the container and you can <pre><code>root@2a85826c4e7b:/workspace# cd /opt/qadence/\nroot@2a85826c4e7b:/opt/qadence# pip3 install -e .\nroot@2a85826c4e7b:/opt/qadence# pip3 install -e ../pyqtorch/\nroot@2a85826c4e7b:/opt/qadence# pip3 install nvidia-dlprof[pytorch]\nroot@2a85826c4e7b:/opt/qadence# PYQ_LOG_LEVEL=debug QADENCE_LOG_LEVEL=debug dlprof --mode=pytorch --nsys_opts=\"-t cuda,nvtx,cublas,cusparse,cusparse-verbose,cublas-verbose --force-overwrite true\" examples/models/quantum_model.py\n</code></pre></p> <p>Where we have <code>--force-overwrite true</code> to always store the latest profiling result (hence you must rename the file) if you wish to keep several. We add <code>cublas,cusparse,cusparse-verbose,cublas-verbose</code> do get more details about the numerical backend pacakges being used.</p>"},{"location":"tutorials/advanced_tutorials/projectors/","title":"Projector blocks","text":"<p>This section introduces the <code>ProjectorBlock</code> as an implementation for the quantum mechanical projection operation onto the subspace spanned by \\(|a\\rangle\\): \\(\\mathbb{\\hat{P}}=|a\\rangle \\langle a|\\). It evaluates the outer product for bras and kets expressed as bitstrings for a given qubit support. They have to possess matching lengths.</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence.operations import Projector  # Projector as an operation.\n\n# Define a projector for |1&gt; onto the qubit labelled 0.\nprojector_block = Projector(ket=\"1\", bra=\"1\", qubit_support=0)\n\n# As any block, the matrix representation can be retrieved.\nprojector_matrix = block_to_tensor(projector_block)\n</code></pre> <pre><code>projector matrix = tensor([[[0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j]]])\n</code></pre> <p>Other standard operations are expressed as projectors in Qadence. For instance, the number operator is the projector onto the 1-subspace, \\(N=|1\\rangle\\langle 1|\\).</p> <p>In fact, projectors can be used to compose any arbitrary operator. For example, the <code>CNOT</code> can be defined as \\(\\textrm{CNOT}(i,j)=|0\\rangle\\langle 0|_i\\otimes \\mathbb{I}_j+|1\\rangle\\langle 1|_i\\otimes X_j\\) and we can compare its matrix representation with the native one in Qadence:</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence import kron, I, X, CNOT\n\n# Define a projector for |0&gt; onto the qubit labelled 0.\nprojector0 = Projector(ket=\"0\", bra=\"0\", qubit_support=0)\n\n# Define a projector for |1&gt; onto the qubit labelled 0.\nprojector1 = Projector(ket=\"1\", bra=\"1\", qubit_support=0)\n\n# Construct the projector controlled CNOT.\nprojector_cnot = kron(projector0, I(1)) + kron(projector1, X(1))\n\n# Get the underlying unitary.\nprojector_cnot_matrix = block_to_tensor(projector_cnot)\n\n# Qadence CNOT unitary.\nqadence_cnot_matrix = block_to_tensor(CNOT(0,1))\n</code></pre> <pre><code>projector cnot matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]])\nqadence cnot matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]]], grad_fn=&lt;AddBackward0&gt;)\n</code></pre> <p>Another example is the canonical SWAP unitary that can be defined as \\(SWAP=|00\\rangle\\langle 00|+|01\\rangle\\langle 10|+|10\\rangle\\langle 01|+|11\\rangle\\langle 11|\\). Indeed, it can be shown that their matricial representations are again identical:</p> <pre><code>from qadence.blocks import block_to_tensor\nfrom qadence import SWAP\n\n# Define all projectors.\nprojector00 = Projector(ket=\"00\", bra=\"00\", qubit_support=(0, 1))\nprojector01 = Projector(ket=\"01\", bra=\"10\", qubit_support=(0, 1))\nprojector10 = Projector(ket=\"10\", bra=\"01\", qubit_support=(0, 1))\nprojector11 = Projector(ket=\"11\", bra=\"11\", qubit_support=(0, 1))\n\n# Construct the SWAP gate.\nprojector_swap = projector00 + projector10 + projector01 + projector11\n\n# Get the underlying unitary.\nprojector_swap_matrix = block_to_tensor(projector_swap)\n\n# Qadence SWAP unitary.\nqadence_swap_matrix = block_to_tensor(SWAP(0,1))\n</code></pre> <pre><code>projector swap matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]])\nqadence swap matrix = tensor([[[1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n         [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n         [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j]]], grad_fn=&lt;UnsafeViewBackward0&gt;)\n</code></pre> <p>Warning</p> <p>Projectors are non-unitary operators, only supported by the PyQTorch backend.</p> <p>To examplify this point, let's run some non-unitary computation involving projectors.</p> <pre><code>from qadence import chain, run\nfrom qadence.operations import H, CNOT\n\n# Define a projector for |1&gt; onto the qubit labelled 1.\nprojector_block = Projector(ket=\"1\", bra=\"1\", qubit_support=1)\n\n# Some non-unitary computation.\nnon_unitary_block = chain(H(0), CNOT(0,1), projector_block)\n\n# Projected wavefunction becomes unnormalized\nprojected_wf = run(non_unitary_block)  # Run on PyQTorch.\n</code></pre> <pre><code>projected_wf = tensor([[0.0000+0.j, 0.0000+0.j, 0.0000+0.j, 0.7071+0.j]])\n</code></pre>"},{"location":"tutorials/development/architecture/","title":"Architecture and sharp bits","text":"<p>Qadence as a software library mixes functional and object-oriented programming. We do that by maintaining core objects and operating on them with functions.</p> <p>Furthermore, Qadence strives at keeping the lower level abstraction layers for automatic differentiation and quantum computation fully stateless while only the frontend layer which is the main user-facing interface is stateful.</p> <p>Code design philosopy</p> <p>Functional, stateless core with object-oriented, stateful user interface.</p>"},{"location":"tutorials/development/architecture/#abstraction-layers","title":"Abstraction layers","text":"<p>In Qadence there are 4 main objects spread across 3 different levels of abstraction:</p> <ul> <li> <p>Frontend layer: The user facing layer and encompasses two objects:</p> <ul> <li><code>QuantumCircuit</code>: A class representing an abstract quantum   circuit not tight not any particular framework. Parameters are represented symbolically using   <code>sympy</code> expressions.</li> <li><code>QuantumModel</code>: The models are higher-level abstraction   providing an interface for executing different kinds of common quantum computing models such   quantum neural networks (QNNs), quantum kernels etc.</li> </ul> </li> <li> <p>Differentiation layer: Intermediate layer has the purpose of integrating quantum   computation with a given automatic differentiation engine. It is meant to be purely stateless and   contains one object:</p> <ul> <li><code>DifferentiableBackend</code>:   An abstract class whose concrete implementation wraps a quantum backend and make it   automatically differentiable using different engines (e.g. PyTorch or Jax).   Note, that today only PyTorch is supported but there is plan to add also a Jax   differentiable backend which will require some changes in the base class implementation.</li> </ul> </li> <li> <p>Quantum layer: The lower-level layer which directly interfaces with quantum emulators   and processing units. It is meant to be purely stateless and it contains one base object which is   specialized for each supported backend:</p> <ul> <li><code>Backend</code>: An abstract class whose concrete implementation   enables the execution of quantum circuit with a variety of quantum backends (normally non   automatically differentiable by default) such as PyQTorch, or Pulser.</li> </ul> </li> </ul>"},{"location":"tutorials/development/architecture/#main-components","title":"Main components","text":""},{"location":"tutorials/development/architecture/#quantumcircuit","title":"<code>QuantumCircuit</code>","text":"<p>We consider <code>QuantumCircuit</code> to be an abstract object, i.e. it is not tied to any backend. However, it blocks are even more abstract. This is because we consider <code>QuantumCircuit</code>s \"real\", whereas the blocks are largely considered just syntax.</p> <p>Unitary <code>QuantumCircuits</code> (this encompasses digital, or gate-based, circuits as well as analog circuits) are constructed by [<code>PrimitiveBlocks</code>] using a syntax that allows you to execute them in sequence, dubbed <code>ChainBlock</code> in the code, or in parallel (i.e. at the same time) where applicable, dubbed <code>KronBlock</code> in the code. Notice that this differs from other packages by providing more control of the layout of the circuit than conventional packages like Qiskit, and from Yao where the blocks are the primary type.</p>"},{"location":"tutorials/development/architecture/#quantummodel","title":"<code>QuantumModel</code>","text":"<p><code>QuantumModel</code>s are meant to be the main entry point for quantum computations in <code>qadence</code>. In general, they take one or more quantum circuit as input and they wrap all the necessary boiler plate code to make the circuit executable and differentiable on the chosen backend.</p> <p>Models are meant to be specific for a certain kind of quantum problem or algorithm and you can easily create new ones starting from the base class <code>QuantumModel</code>, as explained in the custom model tutorial. Currently, Qadence offers a <code>QNN</code> model class which provides convenient methods to work with quantum neural networks with multi-dimensional inputs and outputs.</p>"},{"location":"tutorials/development/architecture/#differentiablebackend","title":"<code>DifferentiableBackend</code>","text":"<p>The differentiable backend is a thin wrapper which takes as input a <code>QuantumCircuit</code> instance and a chosen quantum backend and make the circuit execution routines (expectation value, overalap, etc.) differentiable. Qadence offers both a PyTorch and Jax differentiation engine.</p>"},{"location":"tutorials/development/architecture/#quantum-backend","title":"Quantum <code>Backend</code>","text":"<p>For execution the primary object is the <code>Backend</code>. Backends maintain the same user-facing interface, and internally connects to other libraries to execute circuits. Those other libraries can execute the code on QPUs and local or cloud-based emulators. The <code>Backends</code> use PyTorch tensors to represent data and leverages PyTorchs autograd to help compute derivatives of circuits.</p>"},{"location":"tutorials/development/architecture/#symbolic-parameters","title":"Symbolic parameters","text":"<p>To illustrate how parameters work in Qadence, let's consider the following simple block composed of just two rotations:</p> <pre><code>import sympy\nfrom qadence import Parameter, RX\n\nparam = Parameter(\"phi\", trainable=False)\nblock = RX(0, param) * RX(1, sympy.acos(param))\n</code></pre> <p>The rotation angles assigned to <code>RX</code> (and to any Qadence quantum operation) are defined as arbitrary expressions of <code>Parameter</code>'s. <code>Parameter</code> is a subclass of <code>sympy.Symbol</code>, thus fully interoperable with it.</p> <p>To assign values of the parameter <code>phi</code> in a quantum model, one should use a dictionary containing the a key with parameter name and the corresponding values values:</p> <pre><code>import torch\nfrom qadence import run\n\nvalues = {\"phi\": torch.rand(10)}\nwf = run(block, values=values)\n</code></pre> <p>This is the only interface for parameter assignment exposed to the user. Under the hood, parameters applied to every quantum operation are identified in different ways:</p> <ul> <li> <p>By default, with a stringified version of the <code>sympy</code> expression supplied to the quantum operation. Notice that multiple operations can have the same expression.</p> </li> <li> <p>In certain case, e.g. for constructing parameter shift rules, one must access a unique identifier of the parameter for each quantum operation. Therefore, Qadence also creates unique identifiers for each parametrized operation (see the <code>ParamMap</code> class).</p> </li> </ul> <p>By default, when one constructs a new backend, the parameter identifiers are the <code>sympy</code> expressions which are used when converting an abstract block into a native circuit for the chosen backend. However, one can use the unique identifiers as parameter names by setting the private flag <code>_use_gate_params</code> to <code>True</code> in the backend configuration <code>BackendConfiguration</code>. This is automatically set when PSR differentiation is selected (see next section for more details).</p> <p>You can see the logic for choosing the parameter identifier in <code>get_param_name</code>.</p>"},{"location":"tutorials/development/architecture/#differentiation-with-parameter-shift-rules-psr","title":"Differentiation with parameter shift rules (PSR)","text":"<p>In Qadence, parameter shift rules are applied by implementing a custom <code>torch.autograd.Function</code> class for PyTorch and the <code>custom_vjp</code> in the Jax Engine, respectively.</p> <p>A custom PyTorch <code>Function</code> looks like this:</p> <pre><code>import torch\nfrom torch.autograd import Function\n\nclass CustomFunction(Function):\n\n    # forward pass implementation giving the output of the module\n    @staticmethod\n    def forward(ctx, inputs: torch.Tensor, params: torch.Tensor):\n        ctx.save_for_backward(inputs, params)\n        ...\n\n    # backward pass implementation giving the derivative of the module\n    # with respect to the parameters. This must return the whole vector-jacobian\n    # product to integrate within the autograd engine\n    @staticmethod\n    def backward(ctx, grad_output: torch.Tensor):\n        inputs, params = ctx.saved_tensors\n        ...\n</code></pre> <p>The class <code>PSRExpectation</code> under <code>qadence.engines.torch.differentiable_expectation</code> implements parameter shift rules for all parameters using a custom function as the one above. There are a few implementation details to keep in mind if you want to modify the PSR code:</p> <ul> <li> <p>PyTorch <code>Function</code> only works with tensor arguments. Parameters in Qadence are passed around as   dictionaries with parameter names as keys and current parameter values (tensors)   as values. This works for both variational and feature parameters. However, the <code>Function</code> class   only work with PyTorch tensors as input, not dictionaries. Therefore, the forward pass of   <code>PSRExpectation</code> accepts one argument <code>param_keys</code> with the   parameter keys and a variadic positional argument <code>param_values</code> with the parameter values one by   one. The dictionary is reconstructed within the <code>forward()</code> pass body.</p> </li> <li> <p>Higher-order derivatives with PSR. Higher-order PSR derivatives can be tricky. Parameter shift   rules calls, under the hood, the <code>QuantumBackend</code> expectation value routine that usually yield a   non-differentiable output. Therefore, a second call to the backward pass would not work. However,   Qadence employs a very simple trick to make higher-order derivatives work: instead of using   directly the expectation value of the quantum backend, the PSR backward pass uses the PSR forward   pass itself as expectation value function (see the code below). In this way, multiple calls to the   backward pass are allowed since the <code>expectation_fn</code> routine is always differentiable by   definition. Notice that this implementation is simple but suboptimal since, in some corner cases,   higher-order derivates might include some repeated terms that, with this implementation, are   always recomputed.</p> </li> </ul> <pre><code># expectation value used in the PSR backward pass\ndef expectation_fn(params: dict[str, Tensor]) -&gt; Tensor:\n    return PSRExpectation.apply(\n        ctx.expectation_fn,\n        ctx.param_psrs,\n        params.keys(),\n        *params.values(),\n    )\n</code></pre> <ul> <li> <p>Operation parameters must be uniquely identified for PSR to work. Parameter shift rules work at the level of individual quantum operations. This means that, given a parameter <code>x</code>, one needs to sum the contributions from shifting the parameter values of all the operation where the parameter <code>x</code> appears. When constructing the PSR rules, one must access a unique parameter identifier for each operation even if the corresponding user-facing parameter is the same. Therefore, when PSR differentiation is selected, the flag <code>_use_gate_params</code> is automatically set to <code>True</code> in the backend configuration <code>BackendConfiguration</code> (see previous section).</p> </li> <li> <p>PSR must not be applied to observable parameters. In Qadence, Pauli observables can also be parametrized. However, the tunable parameters of observables are purely classical and should not be included in the differentiation with PSRs. However, the quantum expectation value depends on them, thus they still need to enter into the PSR evaluation. To solve this issue, the code sets the <code>requires_grad</code> attribute of all observable parameters to <code>False</code> when constructing the PSRs for the circuit as in the snippet below:</p> </li> </ul> <pre><code>for obs in observable:\n    for param_id, _ in uuid_to_eigen(obs).items():\n        param_to_psr[param_id] = lambda x: torch.tensor([0.0], requires_grad=False)\n</code></pre>"},{"location":"tutorials/development/draw/","title":"<code>qadence.draw</code> example plots","text":"<p>Mostly for quick, manual checking of correct plotting output.</p> <pre><code>from qadence import X, Y, kron\nfrom qadence.draw import display\n\nb = kron(X(0), Y(1))\n</code></pre> %3 dbc0a05a650344f0980da33ab7c2700c 0 4d3bdde00cd0409185e24892adbd58a8 X dbc0a05a650344f0980da33ab7c2700c--4d3bdde00cd0409185e24892adbd58a8 610dbcebe5e54ceaacc1ad0ee2e0ec41 1 f6b45abd609d4931b14a3c646d9dc823 4d3bdde00cd0409185e24892adbd58a8--f6b45abd609d4931b14a3c646d9dc823 87927b3d54f247948f93b9df0724e66f 633f4a7981e9438fbe174e821207fbf8 Y 610dbcebe5e54ceaacc1ad0ee2e0ec41--633f4a7981e9438fbe174e821207fbf8 633f4a7981e9438fbe174e821207fbf8--87927b3d54f247948f93b9df0724e66f <pre><code>from qadence import X, Y, chain\nfrom qadence.draw import display\n\nb = chain(X(0), Y(0))\n</code></pre> %3 88e577711cd54ea6a29a8513f771e381 0 a551f6d6a62f4310983b2e4b0fa49375 X 88e577711cd54ea6a29a8513f771e381--a551f6d6a62f4310983b2e4b0fa49375 50a1ef00550545a99376ec599b499e17 Y a551f6d6a62f4310983b2e4b0fa49375--50a1ef00550545a99376ec599b499e17 bac6fc7bc8a5488e946b53b39e8f1748 50a1ef00550545a99376ec599b499e17--bac6fc7bc8a5488e946b53b39e8f1748 <pre><code>from qadence import X, Y, chain\nfrom qadence.draw import display\n\nb = chain(X(0), Y(1))\n</code></pre> %3 d18f7a509cc84aa5b2cc4cfc13fcdc6c 0 7ab797b96c4542a4b81465bd4b09d460 X d18f7a509cc84aa5b2cc4cfc13fcdc6c--7ab797b96c4542a4b81465bd4b09d460 14d6c21f90424e049262dcb32d030c02 1 e0bb4987c2404dd8b79775dbe3569e24 7ab797b96c4542a4b81465bd4b09d460--e0bb4987c2404dd8b79775dbe3569e24 da01fc07bc6046aa93d03bd1e01262e0 e0bb4987c2404dd8b79775dbe3569e24--da01fc07bc6046aa93d03bd1e01262e0 e83e26ada4094029a36b1867c0de7087 1e26f8f2461041aeadc0dea30f37ef7e 14d6c21f90424e049262dcb32d030c02--1e26f8f2461041aeadc0dea30f37ef7e c39a0f0a9bf5489fb87dc1f5a284f4af Y 1e26f8f2461041aeadc0dea30f37ef7e--c39a0f0a9bf5489fb87dc1f5a284f4af c39a0f0a9bf5489fb87dc1f5a284f4af--e83e26ada4094029a36b1867c0de7087 <pre><code>from qadence import X, Y, add\nfrom qadence.draw import display\n\nb = add(X(0), Y(1), X(2))\n</code></pre> %3 cluster_abf1677ad92642a0bdfaef63124b664d 171c630acd3f4ce5a16adaf82986b0af 0 a8d9a9c84d4d496cacbe56bf95b063ca 171c630acd3f4ce5a16adaf82986b0af--a8d9a9c84d4d496cacbe56bf95b063ca c70a140aa32b43fd9c845051f8252533 1 9ff4ef1d19c442cbbd912d36ef1d74e0 a8d9a9c84d4d496cacbe56bf95b063ca--9ff4ef1d19c442cbbd912d36ef1d74e0 3645857142284aa4b96a5a73c37224a4 8c1041e591e947a1bec336c2fb583cdd AddBlock c70a140aa32b43fd9c845051f8252533--8c1041e591e947a1bec336c2fb583cdd a4926589fc7547bdb2af8fcea997f264 2 8c1041e591e947a1bec336c2fb583cdd--3645857142284aa4b96a5a73c37224a4 016fddefdc634207a317f9ac33dbd7e0 2cac47f4cfbe47029335b3dfa3589e81 a4926589fc7547bdb2af8fcea997f264--2cac47f4cfbe47029335b3dfa3589e81 2cac47f4cfbe47029335b3dfa3589e81--016fddefdc634207a317f9ac33dbd7e0 <pre><code>from qadence import CNOT, RX, HamEvo, X, Y, Z, chain, kron\n\nrx = kron(RX(3,0.5), RX(2, \"x\"))\nrx.tag = \"rx\"\ngen = chain(Z(i) for i in range(4))\n\n# `chain` puts things in sequence\nblock = chain(\n    kron(X(0), Y(1), rx),\n    CNOT(2,3),\n    HamEvo(gen, 10)\n)\n</code></pre> %3 cluster_bae7ca2e94564dc8bdda02665b55d242 cluster_ff900d4c789c4e1aa4867167e365ce91 rx c27fdbc4949a488fa16870be77e47146 0 7d19773b2ddb4b51b95d61427a72f748 X c27fdbc4949a488fa16870be77e47146--7d19773b2ddb4b51b95d61427a72f748 2bb2f7098e264264822ab055b00ac2fc 1 44f6700b27fb48ff9df10ec1cd85a722 7d19773b2ddb4b51b95d61427a72f748--44f6700b27fb48ff9df10ec1cd85a722 322839682c9241a8bf8831c3beac4a2a 44f6700b27fb48ff9df10ec1cd85a722--322839682c9241a8bf8831c3beac4a2a 184590d280f34fca802e465ecc98be27 322839682c9241a8bf8831c3beac4a2a--184590d280f34fca802e465ecc98be27 d0dc11c61546457bba449a34e64d0a3d e7757f782f8c4742a7c345fa58f5e720 Y 2bb2f7098e264264822ab055b00ac2fc--e7757f782f8c4742a7c345fa58f5e720 9329956f479a4f57a55fe5274b2a057c 2 4768b817d4dd4aef9560b2d9b0d11255 e7757f782f8c4742a7c345fa58f5e720--4768b817d4dd4aef9560b2d9b0d11255 1e37abf365ae41a989f308e7f7be1f3a HamEvo 4768b817d4dd4aef9560b2d9b0d11255--1e37abf365ae41a989f308e7f7be1f3a 1e37abf365ae41a989f308e7f7be1f3a--d0dc11c61546457bba449a34e64d0a3d 8008dbde159a4061b0d09772a4d83af6 9d688dfa47ba47aa8976d2fb5764df73 RX(x) 9329956f479a4f57a55fe5274b2a057c--9d688dfa47ba47aa8976d2fb5764df73 75571718789a4915ad46125373b734b4 3 5374f33d0e214d7384689c9d95f13cd7 9d688dfa47ba47aa8976d2fb5764df73--5374f33d0e214d7384689c9d95f13cd7 da7b0aed0b7b487eb407cb8ee5aaa7e0 t = 10 5374f33d0e214d7384689c9d95f13cd7--da7b0aed0b7b487eb407cb8ee5aaa7e0 da7b0aed0b7b487eb407cb8ee5aaa7e0--8008dbde159a4061b0d09772a4d83af6 038eb82c3aa94989b5aa2c9ae8aa6b58 804f4d55254a4c2b8d5dcaa0c381ab7d RX(0.5) 75571718789a4915ad46125373b734b4--804f4d55254a4c2b8d5dcaa0c381ab7d dec98977ca9d4637a622a35b0e6d6229 X 804f4d55254a4c2b8d5dcaa0c381ab7d--dec98977ca9d4637a622a35b0e6d6229 dec98977ca9d4637a622a35b0e6d6229--5374f33d0e214d7384689c9d95f13cd7 275af91c3e5343be83cedfcaa118fb65 dec98977ca9d4637a622a35b0e6d6229--275af91c3e5343be83cedfcaa118fb65 275af91c3e5343be83cedfcaa118fb65--038eb82c3aa94989b5aa2c9ae8aa6b58 <pre><code>from qadence import feature_map, hea, chain\n\nblock = chain(feature_map(4, reupload_scaling=\"Tower\"), hea(4,2))\n</code></pre> %3 cluster_1c2032767d164bdeb3d235f1ecc6fa3c HEA cluster_09605a12a11b41148e9d3569dd63516a Tower Fourier FM 3a58aab2dd7645de969a179e49f1f966 0 67219c7916354eae8fab4645cac522cb RX(1.0*phi) 3a58aab2dd7645de969a179e49f1f966--67219c7916354eae8fab4645cac522cb eb7b03aa88814f7faf9e2e6331ef64dc 1 7d6bc35b46b94ed5b02d56e12dac2d8e RX(theta\u2080) 67219c7916354eae8fab4645cac522cb--7d6bc35b46b94ed5b02d56e12dac2d8e a932d0d58bae4e6794f6c592d861d170 RY(theta\u2084) 7d6bc35b46b94ed5b02d56e12dac2d8e--a932d0d58bae4e6794f6c592d861d170 b6d237081f1f49e5a55c985f4a72e1fe RX(theta\u2088) a932d0d58bae4e6794f6c592d861d170--b6d237081f1f49e5a55c985f4a72e1fe 3d4cf5ccda9c4fa08c32557d2dcd64a1 b6d237081f1f49e5a55c985f4a72e1fe--3d4cf5ccda9c4fa08c32557d2dcd64a1 eb00433bcdce4d4e9a75e07cd08c1900 3d4cf5ccda9c4fa08c32557d2dcd64a1--eb00433bcdce4d4e9a75e07cd08c1900 859a4ac5545a45d39cf1b14bdd495cae RX(theta\u2081\u2082) eb00433bcdce4d4e9a75e07cd08c1900--859a4ac5545a45d39cf1b14bdd495cae a5a3df54eff34a7c8637c0872a1cf61b RY(theta\u2081\u2086) 859a4ac5545a45d39cf1b14bdd495cae--a5a3df54eff34a7c8637c0872a1cf61b 35c3ce120c914405aca08368b5d61b40 RX(theta\u2082\u2080) a5a3df54eff34a7c8637c0872a1cf61b--35c3ce120c914405aca08368b5d61b40 b38729df49a744ef8d6e9edc8aab6068 35c3ce120c914405aca08368b5d61b40--b38729df49a744ef8d6e9edc8aab6068 df62418ff2854d058d472fbe28a263fd b38729df49a744ef8d6e9edc8aab6068--df62418ff2854d058d472fbe28a263fd 84ee6cceb483441c8faa57e308cd2cc4 df62418ff2854d058d472fbe28a263fd--84ee6cceb483441c8faa57e308cd2cc4 9dd1636163604248bf9db0ec8d8db8ed 10de51d544a34be7bf4c7f3a72f7f74a RX(2.0*phi) eb7b03aa88814f7faf9e2e6331ef64dc--10de51d544a34be7bf4c7f3a72f7f74a 847b5921e41744a48262477baa0476d6 2 c7c62cfcf8494a98b3934046c91aa21c RX(theta\u2081) 10de51d544a34be7bf4c7f3a72f7f74a--c7c62cfcf8494a98b3934046c91aa21c bdcfdc1c1c9749ffb395f37175452172 RY(theta\u2085) c7c62cfcf8494a98b3934046c91aa21c--bdcfdc1c1c9749ffb395f37175452172 efa454bdb5f34160a8f87a352e69f9a2 RX(theta\u2089) bdcfdc1c1c9749ffb395f37175452172--efa454bdb5f34160a8f87a352e69f9a2 16b3814fb79745a6a3b4a5225060e5cc X efa454bdb5f34160a8f87a352e69f9a2--16b3814fb79745a6a3b4a5225060e5cc 16b3814fb79745a6a3b4a5225060e5cc--3d4cf5ccda9c4fa08c32557d2dcd64a1 477bc688ce2e4364868f959820dd4638 16b3814fb79745a6a3b4a5225060e5cc--477bc688ce2e4364868f959820dd4638 612400595a2741ddbe5cb1ee5dc5bcae RX(theta\u2081\u2083) 477bc688ce2e4364868f959820dd4638--612400595a2741ddbe5cb1ee5dc5bcae 8acea4e287a34a23801da63379db03b7 RY(theta\u2081\u2087) 612400595a2741ddbe5cb1ee5dc5bcae--8acea4e287a34a23801da63379db03b7 09bd940e818d405198d3f68b2e09bc7c RX(theta\u2082\u2081) 8acea4e287a34a23801da63379db03b7--09bd940e818d405198d3f68b2e09bc7c a036adf3434249d7bcd00950a9d344f4 X 09bd940e818d405198d3f68b2e09bc7c--a036adf3434249d7bcd00950a9d344f4 a036adf3434249d7bcd00950a9d344f4--b38729df49a744ef8d6e9edc8aab6068 edd0b8532f7242828411222817a113e6 a036adf3434249d7bcd00950a9d344f4--edd0b8532f7242828411222817a113e6 edd0b8532f7242828411222817a113e6--9dd1636163604248bf9db0ec8d8db8ed 8ff84828fd084e118ac7a047e28f1a97 6cf3621bd7df40b1a2d13d4685026075 RX(3.0*phi) 847b5921e41744a48262477baa0476d6--6cf3621bd7df40b1a2d13d4685026075 6d6fa32b1eec4b95b0b2952626b75326 3 f61c360df2c84debb2562a5409afcf4f RX(theta\u2082) 6cf3621bd7df40b1a2d13d4685026075--f61c360df2c84debb2562a5409afcf4f 8615ed3f8f1a49d4b4df4d77777fb7fb RY(theta\u2086) f61c360df2c84debb2562a5409afcf4f--8615ed3f8f1a49d4b4df4d77777fb7fb e4c79767864d42b1a1b32ff5a04d3586 RX(theta\u2081\u2080) 8615ed3f8f1a49d4b4df4d77777fb7fb--e4c79767864d42b1a1b32ff5a04d3586 afa14f175a2941609756f7007e9c82f9 e4c79767864d42b1a1b32ff5a04d3586--afa14f175a2941609756f7007e9c82f9 648ff42350624e3fbc981a89637fc23c X afa14f175a2941609756f7007e9c82f9--648ff42350624e3fbc981a89637fc23c 648ff42350624e3fbc981a89637fc23c--477bc688ce2e4364868f959820dd4638 725f77127cad423db51ee06a9e6eaed3 RX(theta\u2081\u2084) 648ff42350624e3fbc981a89637fc23c--725f77127cad423db51ee06a9e6eaed3 68b05f41558e47168e8b78992dcc5010 RY(theta\u2081\u2088) 725f77127cad423db51ee06a9e6eaed3--68b05f41558e47168e8b78992dcc5010 f682a613a3b1467a95648334d7bfbd8e RX(theta\u2082\u2082) 68b05f41558e47168e8b78992dcc5010--f682a613a3b1467a95648334d7bfbd8e 8e876e54f9a04898a29a48714d0206b0 f682a613a3b1467a95648334d7bfbd8e--8e876e54f9a04898a29a48714d0206b0 dc0b77dcd8134ab0aff4e462dc44089c X 8e876e54f9a04898a29a48714d0206b0--dc0b77dcd8134ab0aff4e462dc44089c dc0b77dcd8134ab0aff4e462dc44089c--edd0b8532f7242828411222817a113e6 dc0b77dcd8134ab0aff4e462dc44089c--8ff84828fd084e118ac7a047e28f1a97 cde594c6ab9943188cb3948bca8252af 06a69ca1eb1746c0891e852ba4a8883f RX(4.0*phi) 6d6fa32b1eec4b95b0b2952626b75326--06a69ca1eb1746c0891e852ba4a8883f 56de4459259f4874a391a01c771a8b7b RX(theta\u2083) 06a69ca1eb1746c0891e852ba4a8883f--56de4459259f4874a391a01c771a8b7b 4c345fbe459a4e97bdf8e1eeac0e330f RY(theta\u2087) 56de4459259f4874a391a01c771a8b7b--4c345fbe459a4e97bdf8e1eeac0e330f 4c70673e20b34c4ea4355e9649da94c9 RX(theta\u2081\u2081) 4c345fbe459a4e97bdf8e1eeac0e330f--4c70673e20b34c4ea4355e9649da94c9 99b9f766568943fcad40aca3d2097083 X 4c70673e20b34c4ea4355e9649da94c9--99b9f766568943fcad40aca3d2097083 99b9f766568943fcad40aca3d2097083--afa14f175a2941609756f7007e9c82f9 b4abd18a1ba64bbc89bd59af4f55b5f7 99b9f766568943fcad40aca3d2097083--b4abd18a1ba64bbc89bd59af4f55b5f7 2d39c55cc1e94ce9bec7efad7de78c4f RX(theta\u2081\u2085) b4abd18a1ba64bbc89bd59af4f55b5f7--2d39c55cc1e94ce9bec7efad7de78c4f 25e5c247be28434f8430e584912bee22 RY(theta\u2081\u2089) 2d39c55cc1e94ce9bec7efad7de78c4f--25e5c247be28434f8430e584912bee22 4ef87def19234135b9a0c9a6de19b47c RX(theta\u2082\u2083) 25e5c247be28434f8430e584912bee22--4ef87def19234135b9a0c9a6de19b47c 8f66e7189f2245afa7d3759fce12734b X 4ef87def19234135b9a0c9a6de19b47c--8f66e7189f2245afa7d3759fce12734b 8f66e7189f2245afa7d3759fce12734b--8e876e54f9a04898a29a48714d0206b0 0046357906c94ba0b51ae4e3a3272a06 8f66e7189f2245afa7d3759fce12734b--0046357906c94ba0b51ae4e3a3272a06 0046357906c94ba0b51ae4e3a3272a06--cde594c6ab9943188cb3948bca8252af <pre><code>from qadence import QuantumModel, QuantumCircuit, total_magnetization, hea\n\nmodel = QuantumModel(QuantumCircuit(3, hea(3,2)), total_magnetization(3))\n</code></pre> %3 cluster_fc283b0afa494409b095eb7fd42faec5 Obs. cluster_48ac74e75f0c4bfd8c6d43324434084b cluster_2ff62255c89f4450b7c0d760e1abcf77 HEA e6d165f093594ec18c691fbdb2b56f37 0 9fe6f848702f4124a5acefd92941d67c RX(theta\u2080) e6d165f093594ec18c691fbdb2b56f37--9fe6f848702f4124a5acefd92941d67c 66144446489e4e47be6cc6e60f2a2aeb 1 a4488784a72a4b6c9199a41e8d2ff9ba RY(theta\u2083) 9fe6f848702f4124a5acefd92941d67c--a4488784a72a4b6c9199a41e8d2ff9ba a1a97d2c10954cc2a38e137cbf5c9b81 RX(theta\u2086) a4488784a72a4b6c9199a41e8d2ff9ba--a1a97d2c10954cc2a38e137cbf5c9b81 2af9c60f913b47e180ebd67c20eb0efb a1a97d2c10954cc2a38e137cbf5c9b81--2af9c60f913b47e180ebd67c20eb0efb 87c8d03ad8144e7f8bef7652dbd7fbf8 2af9c60f913b47e180ebd67c20eb0efb--87c8d03ad8144e7f8bef7652dbd7fbf8 45c84584a51e4bf2845b9f751c7c6ee8 RX(theta\u2089) 87c8d03ad8144e7f8bef7652dbd7fbf8--45c84584a51e4bf2845b9f751c7c6ee8 8c8f474060944a60a132d9273e2a4ad7 RY(theta\u2081\u2082) 45c84584a51e4bf2845b9f751c7c6ee8--8c8f474060944a60a132d9273e2a4ad7 99bb00d4eedc4e0a9133cfa41b9bf940 RX(theta\u2081\u2085) 8c8f474060944a60a132d9273e2a4ad7--99bb00d4eedc4e0a9133cfa41b9bf940 aca25ee6692f41729b699c709f27bfc9 99bb00d4eedc4e0a9133cfa41b9bf940--aca25ee6692f41729b699c709f27bfc9 2be007a4c03e46299e729edc7c9b58fc aca25ee6692f41729b699c709f27bfc9--2be007a4c03e46299e729edc7c9b58fc 31089fb5455944c3862a7ff69ea887c2 2be007a4c03e46299e729edc7c9b58fc--31089fb5455944c3862a7ff69ea887c2 42122bb501414e3da388a29b1dcd5d7f 31089fb5455944c3862a7ff69ea887c2--42122bb501414e3da388a29b1dcd5d7f bba2d934bdf344f5842a79bd9418bc5a 8ed5a0c17cbb491287e915a9e4921204 RX(theta\u2081) 66144446489e4e47be6cc6e60f2a2aeb--8ed5a0c17cbb491287e915a9e4921204 c6cac8e61acc4f85bc4b52242ccf3b09 2 01cff552ef914015a6c0fcd6b51f564b RY(theta\u2084) 8ed5a0c17cbb491287e915a9e4921204--01cff552ef914015a6c0fcd6b51f564b 36b67b677d4743c896b5cdf0c1c7a089 RX(theta\u2087) 01cff552ef914015a6c0fcd6b51f564b--36b67b677d4743c896b5cdf0c1c7a089 56765e7df3b94fd085d3cd6c9dfab611 X 36b67b677d4743c896b5cdf0c1c7a089--56765e7df3b94fd085d3cd6c9dfab611 56765e7df3b94fd085d3cd6c9dfab611--2af9c60f913b47e180ebd67c20eb0efb d749745ccd234668bf3468c0c5593678 56765e7df3b94fd085d3cd6c9dfab611--d749745ccd234668bf3468c0c5593678 62ce6b934b7441cc91636f590aeca8d5 RX(theta\u2081\u2080) d749745ccd234668bf3468c0c5593678--62ce6b934b7441cc91636f590aeca8d5 05b7532d5b08425e92120c4ebc852746 RY(theta\u2081\u2083) 62ce6b934b7441cc91636f590aeca8d5--05b7532d5b08425e92120c4ebc852746 a677a155812a40e7b64c37e1cd980325 RX(theta\u2081\u2086) 05b7532d5b08425e92120c4ebc852746--a677a155812a40e7b64c37e1cd980325 bf61922d5e8246e6a4b16b3b69f9febe X a677a155812a40e7b64c37e1cd980325--bf61922d5e8246e6a4b16b3b69f9febe bf61922d5e8246e6a4b16b3b69f9febe--aca25ee6692f41729b699c709f27bfc9 8878bf35aac54951bb14a1b25c964cbb bf61922d5e8246e6a4b16b3b69f9febe--8878bf35aac54951bb14a1b25c964cbb af869fa4452042e684f971a997b100fa AddBlock 8878bf35aac54951bb14a1b25c964cbb--af869fa4452042e684f971a997b100fa af869fa4452042e684f971a997b100fa--bba2d934bdf344f5842a79bd9418bc5a 595c4427a2c545a394e1b7b9003f1528 ed9aeb86959449d293f13fce3783d319 RX(theta\u2082) c6cac8e61acc4f85bc4b52242ccf3b09--ed9aeb86959449d293f13fce3783d319 9a7a049e896e49999de8f04060252c75 RY(theta\u2085) ed9aeb86959449d293f13fce3783d319--9a7a049e896e49999de8f04060252c75 09e489e348c047cd896a7c479bf87544 RX(theta\u2088) 9a7a049e896e49999de8f04060252c75--09e489e348c047cd896a7c479bf87544 e2729aede62b45199c8df8f11eb53636 09e489e348c047cd896a7c479bf87544--e2729aede62b45199c8df8f11eb53636 ffe24720e4b0491f93a3d76ebd5270ba X e2729aede62b45199c8df8f11eb53636--ffe24720e4b0491f93a3d76ebd5270ba ffe24720e4b0491f93a3d76ebd5270ba--d749745ccd234668bf3468c0c5593678 9749740a58b14a4aae84868da00b29ba RX(theta\u2081\u2081) ffe24720e4b0491f93a3d76ebd5270ba--9749740a58b14a4aae84868da00b29ba 9a88255e07ce4af49f6e71b6782eb8ff RY(theta\u2081\u2084) 9749740a58b14a4aae84868da00b29ba--9a88255e07ce4af49f6e71b6782eb8ff 6622070f9344456491e54eee77594d16 RX(theta\u2081\u2087) 9a88255e07ce4af49f6e71b6782eb8ff--6622070f9344456491e54eee77594d16 1364de5f7fdf4931833dae9db11446f3 6622070f9344456491e54eee77594d16--1364de5f7fdf4931833dae9db11446f3 c8b7e0f9267e4603a85fe26a3b454c66 X 1364de5f7fdf4931833dae9db11446f3--c8b7e0f9267e4603a85fe26a3b454c66 c8b7e0f9267e4603a85fe26a3b454c66--8878bf35aac54951bb14a1b25c964cbb 2cbfa8d55eae471abdf790300dc1a5b1 c8b7e0f9267e4603a85fe26a3b454c66--2cbfa8d55eae471abdf790300dc1a5b1 2cbfa8d55eae471abdf790300dc1a5b1--595c4427a2c545a394e1b7b9003f1528 <pre><code>from qadence import *\n\nb = chain(SWAP(0,1), SWAP(0,3))\n</code></pre> %3 4147e01468aa4d8baa9cf68ee7379bf8 0 8829f903c3ea4564b5ef2321118069a7 4147e01468aa4d8baa9cf68ee7379bf8--8829f903c3ea4564b5ef2321118069a7 b2b0da1a2f094807bc0223d57c72c47b 1 186655bdcec5423d9f2d43cb5540f073 39801f3767c8426c904c862b2d27b428 8829f903c3ea4564b5ef2321118069a7--39801f3767c8426c904c862b2d27b428 86af36394bba4cd6a72b5cbfcc7d4aaa 186655bdcec5423d9f2d43cb5540f073--86af36394bba4cd6a72b5cbfcc7d4aaa 23d4f6005a9e44b9a189bd8a5830cb57 206972c18ae7496cafbad730086f0250 86af36394bba4cd6a72b5cbfcc7d4aaa--206972c18ae7496cafbad730086f0250 af6044b7bf384a7194e279f6f2e69cef 23d4f6005a9e44b9a189bd8a5830cb57--af6044b7bf384a7194e279f6f2e69cef 472a3253c5284da0b5b19d28e5ba2787 f039df29aa614d6c9835410f2a12eb5c b2b0da1a2f094807bc0223d57c72c47b--f039df29aa614d6c9835410f2a12eb5c 4156a45592ce40bd8a79e5969d1ae85a 2 f039df29aa614d6c9835410f2a12eb5c--186655bdcec5423d9f2d43cb5540f073 3777bb03b0c24ebda80a03b999238be5 39801f3767c8426c904c862b2d27b428--3777bb03b0c24ebda80a03b999238be5 09f4a64c8daf4620b92af0e69809cb15 3777bb03b0c24ebda80a03b999238be5--09f4a64c8daf4620b92af0e69809cb15 09f4a64c8daf4620b92af0e69809cb15--472a3253c5284da0b5b19d28e5ba2787 fe0f36cdcea04162b62f5f9d974c77bd c851197283374cb8a9805e4b44bb98e8 4156a45592ce40bd8a79e5969d1ae85a--c851197283374cb8a9805e4b44bb98e8 7d6197df92c24c4087561b06235b292a 3 20b0196308134f6290722c289bf6c518 c851197283374cb8a9805e4b44bb98e8--20b0196308134f6290722c289bf6c518 7d010dbe2a014e7f82fe5ae386e27f22 20b0196308134f6290722c289bf6c518--7d010dbe2a014e7f82fe5ae386e27f22 c752e8b8cbbc42e090d7b21afad30966 7d010dbe2a014e7f82fe5ae386e27f22--c752e8b8cbbc42e090d7b21afad30966 c752e8b8cbbc42e090d7b21afad30966--fe0f36cdcea04162b62f5f9d974c77bd 442c3747d782445ea3e2904f8626f0a9 9587d38bf16247c395b8d0618d189585 7d6197df92c24c4087561b06235b292a--9587d38bf16247c395b8d0618d189585 2457cb46d8034aa7813ca589c5a380c0 9587d38bf16247c395b8d0618d189585--2457cb46d8034aa7813ca589c5a380c0 e04ee714a6ef4a53830c8e8183b21924 2457cb46d8034aa7813ca589c5a380c0--e04ee714a6ef4a53830c8e8183b21924 e04ee714a6ef4a53830c8e8183b21924--23d4f6005a9e44b9a189bd8a5830cb57 206972c18ae7496cafbad730086f0250--442c3747d782445ea3e2904f8626f0a9 <pre><code>from qadence import *\n\nb = chain(CPHASE(0, 1, 0.5), CPHASE(0, 2, 0.5), CPHASE(0, 3, 0.5))\n</code></pre> %3 ea845a41068a4a94a9b89eafe88c577a 0 a30d88ae708f42f9b949573ba9e5a2dd ea845a41068a4a94a9b89eafe88c577a--a30d88ae708f42f9b949573ba9e5a2dd 2efe2434a7984b1fbd898802e011800f 1 376d4c4f7fc64a789ef0fe168a441311 a30d88ae708f42f9b949573ba9e5a2dd--376d4c4f7fc64a789ef0fe168a441311 071b6d482c2d4fa2b0ad2d21ec1d8682 376d4c4f7fc64a789ef0fe168a441311--071b6d482c2d4fa2b0ad2d21ec1d8682 c842645e23274d60b520e448dc593895 071b6d482c2d4fa2b0ad2d21ec1d8682--c842645e23274d60b520e448dc593895 d646e09fb95841899f0e3e4d175b375f 2e43fe7b04244948a914332f945d3529 PHASE(0.5) 2efe2434a7984b1fbd898802e011800f--2e43fe7b04244948a914332f945d3529 42a4870bae754ae7926f35c0fc5150d6 2 2e43fe7b04244948a914332f945d3529--a30d88ae708f42f9b949573ba9e5a2dd 2d1593ba2d7b4fc187a87541aa64dd86 2e43fe7b04244948a914332f945d3529--2d1593ba2d7b4fc187a87541aa64dd86 a9ef3d360dcc48789a73894cb0da2e5f 2d1593ba2d7b4fc187a87541aa64dd86--a9ef3d360dcc48789a73894cb0da2e5f a9ef3d360dcc48789a73894cb0da2e5f--d646e09fb95841899f0e3e4d175b375f bbc86f093ae24bfb91b8ead8b806c411 635d0ef10a1b4149b9201472b51d5a2e 42a4870bae754ae7926f35c0fc5150d6--635d0ef10a1b4149b9201472b51d5a2e 5241fcb6552745e489d513a5a0a669c0 3 480ef8420c674c70bf804828b31d3843 PHASE(0.5) 635d0ef10a1b4149b9201472b51d5a2e--480ef8420c674c70bf804828b31d3843 480ef8420c674c70bf804828b31d3843--376d4c4f7fc64a789ef0fe168a441311 94d417d5d8dc4729be6e393711fab2b4 480ef8420c674c70bf804828b31d3843--94d417d5d8dc4729be6e393711fab2b4 94d417d5d8dc4729be6e393711fab2b4--bbc86f093ae24bfb91b8ead8b806c411 99dbf0a3396546e6b2b9f7b942f4130c 3508b599e77b42e986e4bb9e946dc829 5241fcb6552745e489d513a5a0a669c0--3508b599e77b42e986e4bb9e946dc829 87abdaf1e9e5414289fe0df0652ed21d 3508b599e77b42e986e4bb9e946dc829--87abdaf1e9e5414289fe0df0652ed21d 4b98478324fe4548b7dfcb31c8c00286 PHASE(0.5) 87abdaf1e9e5414289fe0df0652ed21d--4b98478324fe4548b7dfcb31c8c00286 4b98478324fe4548b7dfcb31c8c00286--071b6d482c2d4fa2b0ad2d21ec1d8682 4b98478324fe4548b7dfcb31c8c00286--99dbf0a3396546e6b2b9f7b942f4130c"},{"location":"tutorials/development/draw/#developer-documentation","title":"Developer documentation","text":"<p>This section contains examples in pure graphviz that can be used to understand roughly what is done in the actual drawing backend.</p> <pre><code>import graphviz\n\nfont_name = \"Sans-Serif\"\nfont_size = \"8\"\n\ngraph_attr = {\n    \"rankdir\": \"LR\",  # LR = left to right, TB = top to bottom\n    \"nodesep\": \"0.1\",  # In inches, tells distance between nodes without edges\n    \"compound\": \"true\",  # Needed to draw properly edges in hamevo when content is hidden\n    \"splines\": \"false\",  # Needed to draw control gates vertical lines one over the other\n}  # These are the default values for graphs\n\nnode_attr = {\n    \"shape\": \"box\",  # 'box' for normal nodes, 'point' for control gates or 'plaintext' for starting nodes (the qubit label).\n    \"style\": \"rounded\",  # Unfortunately we can't specify the radius of the rounded, at least for this version\n    \"fontname\": font_name,\n    \"fontsize\": font_size,\n    \"width\": \"0.1\",  # In inches, it doesn't get tinier than the label font.\n    \"height\": \"0.1\"  # In inches, it doesn't get tinier than the label font.\n}  # These are the defaults values that can be overridden at node declaration.\n\ndefault_cluster_attr = {\n    \"fontname\": font_name,\n    \"fontsize\": font_size,\n    \"labelloc\": \"b\",  # location of cluster label. b as bottom, t as top\n    \"style\": \"rounded\"\n} # These are the defaults values that can be overridden at sub graph declaration\n\nhamevo_cluster_attr = {\n    \"label\": \"HamEvo(t=10)\"\n}\nhamevo_cluster_attr.update(default_cluster_attr)\n\nh = graphviz.Graph(graph_attr=graph_attr, node_attr=node_attr)\nh.node(\"Hello World!\")\nh\n</code></pre> <pre><code>\n</code></pre> <pre><code># Define graph\nh = graphviz.Graph(node_attr=node_attr, graph_attr=graph_attr)\n\n# Add start and end nodes\nfor i in range(4):\n    h.node(f's{i}', shape=\"plaintext\", label=f'{i}', group=f\"{i}\")\n    h.node(f'e{i}', style='invis', group=f\"{i}\")\n\n# Add nodes\nh.node('X', group=\"0\")\nh.node('Y', group=\"1\")\n\n# Add hamevo and its nodes\nhamevo = graphviz.Graph(name='cluster_hamevo', graph_attr=hamevo_cluster_attr)\nfor i in range(4):\n    hamevo.node(f'z{i}', shape=\"box\", style=\"invis\", label=f'{i}', group=f\"{i}\")\nh.subgraph(hamevo)\n\n# Add rx gates cluster and its nodes\ncluster_attr = {\"label\": \"RX gates\"}\ncluster_attr.update(default_cluster_attr)\ncluster = graphviz.Graph(name=\"cluster_0\", graph_attr=cluster_attr)\ncluster.node('RX(x)', group=\"2\")\ncluster.node('RX(0.5)', group=\"3\")\nh.subgraph(cluster)\n\nh.node('cnot0', label='', shape='point', width='0.1', group='0')\nh.node('cnot1', label='X', group='1')\nh.node('cnot2', label='', shape='point', width='0.1', group='2')\nh.node('cnot3', label='', shape='point', width='0.1', group='3')\n\n# Add edges\nh.edge('s0', 'X')\nh.edge('X', 'cnot0')\nh.edge('cnot0', 'z0', lhead='cluster_hamevo')\nh.edge('z0', 'e0', ltail='cluster_hamevo')\nh.edge('s1', 'Y')\nh.edge('Y', 'cnot1')\nh.edge('cnot1', 'z1', lhead='cluster_hamevo')\nh.edge('z1', 'e1', ltail='cluster_hamevo')\nh.edge('s2', 'RX(x)')\nh.edge('RX(x)', 'cnot2')\nh.edge('cnot2', 'z2', lhead='cluster_hamevo')\nh.edge('z2', 'e2', ltail='cluster_hamevo')\nh.edge('s3', 'RX(0.5)')\nh.edge('RX(0.5)', 'cnot3')\nh.edge('cnot3', 'z3', lhead='cluster_hamevo')\nh.edge('z3', 'e3', ltail='cluster_hamevo')\nh.edge('cnot1', 'cnot0', constraint='false')  # constraint: false is needed to draw vertical edges\nh.edge('cnot1', 'cnot2', constraint='false')  # constraint: false is needed to draw vertical edges\nh.edge('cnot1', 'cnot3', constraint='false')  # constraint: false is needed to draw vertical edges\nh\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/development/draw/#example-of-cluster-of-clusters","title":"Example of cluster of clusters","text":"<pre><code># Define graph\nh = graphviz.Graph(node_attr=node_attr, graph_attr=graph_attr)\n\n# Define start and end nodes\nfor i in range(4):\n    h.node(f's{i}', shape=\"plaintext\", label=f'{i}', group=f\"{i}\")\n    h.node(f'e{i}', style='invis', group=f\"{i}\")\n\n# Define outer cluster\ncluster_attr = {\"label\": \"Outer cluster\"}\ncluster_attr.update(default_cluster_attr)\nouter_cluster = graphviz.Graph(name=\"cluster_outer\", graph_attr=cluster_attr)\n\n# Define inner cluster 1 and its nodes\ncluster_attr = {\"label\": \"Inner cluster 1\"}\ncluster_attr.update(default_cluster_attr)\ninner1_cluster = graphviz.Graph(name=\"cluster_inner1\", graph_attr=cluster_attr)\ninner1_cluster.node(\"a0\", group=\"0\")\ninner1_cluster.node(\"a1\", group=\"1\")\nouter_cluster.subgraph(inner1_cluster)\n\n# Define inner cluster 2 and its nodes\ncluster_attr = {\"label\": \"Inner cluster 2\"}\ncluster_attr.update(default_cluster_attr)\ninner2_cluster = graphviz.Graph(name=\"cluster_inner2\", graph_attr=cluster_attr)\ninner2_cluster.node(\"a2\", group=\"2\")\ninner2_cluster.node(\"a3\", group=\"3\")\nouter_cluster.subgraph(inner2_cluster)\n\n# This has to be done here, after inner clusters definitions\nh.subgraph(outer_cluster)\n\n# Define more nodes\nfor i in range(4):\n    h.node(f\"b{i}\", group=f\"{i}\")\n\nfor i in range(4):\n    h.edge(f's{i}', f'a{i}')\n    h.edge(f'a{i}', f'b{i}')\n    h.edge(f'b{i}', f'e{i}')\n\nh\n</code></pre> <pre><code>\n</code></pre>"},{"location":"tutorials/digital_analog_qc/","title":"Digital-Analog Quantum Computation","text":"<p>Digital-analog quantum computation (DAQC) is a universal quantum computing paradigm<sup>1</sup>, based on two primary computations:</p> <ul> <li>Fast single-qubit operations (digital).</li> <li>Multi-partite entangling operations acting on all qubits (analog).</li> </ul> <p>A promising quantum computing platform for the implementation of the DAQC paradigm is neutral-atoms, where both these computations are realizable.</p>"},{"location":"tutorials/digital_analog_qc/#digital-analog-emulation","title":"Digital-analog emulation","text":"<p>Qadence simplifies the execution of DAQC programs on either emulated or real devices by providing a simplified interface for customizing interactions and interfacing with pulse-level programming in <code>Pulser</code><sup>3</sup>.</p>"},{"location":"tutorials/digital_analog_qc/#digital-analog-transformation","title":"Digital-analog transformation","text":"<p>Furthermore, the essence of digital-analog computation is the ability to represent any analog operation, i.e. any arbitrary Hamiltonian, using an auxiliary device-amenable Hamiltonian, such as the ubiquitous Ising model<sup>2</sup>. This is at the core of the DAQC implementation in Qadence.</p>"},{"location":"tutorials/digital_analog_qc/#execution-on-rydberg-atom-arrays-with-restriced-addressability","title":"Execution on Rydberg atom arrays with restriced addressability","text":"<p>Finally, Qadence offers some convenience constructors and interfaces to execute programs compatible with a DAQC flavor featuring only a restricted access to individual qubit addressability with always-on interaction. This regime is common in currently available neutral atom quantum computers.</p>"},{"location":"tutorials/digital_analog_qc/#references","title":"References","text":"<ol> <li> <p>Dodd et al., Universal quantum computation and simulation using any entangling Hamiltonian and local unitaries, PRA 65, 040301 (2002). \u21a9</p> </li> <li> <p>Pulser: An open-source package for the design of pulse sequences in programmable neutral-atom arrays \u21a9</p> </li> <li> <p>Parra-Rodriguez et al., Digital-Analog Quantum Computation, PRA 101, 022305 (2020). \u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/analog-basics/","title":"Basic operations on neutral-atoms","text":"<p>Warning</p> <p>The digital-analog emulation framework is under construction and more changes to the interface may still occur.</p> <p>Qadence includes primitives for the construction of programs implemented on a set of interacting qubits. The goal is to build digital-analog programs that better represent the reality of interacting qubit platforms, such as neutral-atoms, while maintaining a simplified interface for users coming from a digital quantum computing background that may not be as familiar with pulse-level programming.</p> <p>To build the intuition for the interface in Qadence, it is important to go over some of the underlying physics. We can write a general Hamiltonian for a set of \\(n\\) interacting qubits as</p> \\[ \\mathcal{H} = \\sum_{i=0}^{n-1}\\left(\\mathcal{H}^\\text{d}_{i}(t) + \\sum_{j&lt;i}\\mathcal{H}^\\text{int}_{ij}\\right), \\] <p>where the driving Hamiltonian \\(\\mathcal{H}^\\text{d}_{i}\\) describes the pulses used to control single-qubit rotations, and the interaction Hamiltonian \\(\\mathcal{H}^\\text{int}_{ij}\\) describes the natural interaction between qubits.</p>"},{"location":"tutorials/digital_analog_qc/analog-basics/#rydberg-atoms","title":"Rydberg atoms","text":"<p>For the purpose of digital-analog emulation of neutral-atom systems in Qadence, we now consider a simplified time-independent global driving Hamiltonian, written as</p> \\[ \\mathcal{H}^\\text{d}_{i} = \\frac{\\Omega}{2}\\left(\\cos(\\phi) X_i - \\sin(\\phi) Y_i \\right) - \\delta N_i \\] <p>where \\(\\Omega\\) is the Rabi frequency, \\(\\delta\\) is the detuning, \\(\\phi\\) is the phase, \\(X_i\\) and \\(Y_i\\) are the standard Pauli operators, and \\(N_i=\\frac{1}{2}(I_i-Z_i)\\) is the number operator. This Hamiltonian allows arbitrary global single-qubit rotations to be written, meaning that the values set for \\((\\Omega,\\phi,\\delta)\\) are the same accross the qubit support.</p> <p>For the interaction term, Rydberg atoms typically allow both an Ising and an XY mode of operation. For now, we focus on the Ising interaction, where the Hamiltonian is written as</p> \\[ \\mathcal{H}^\\text{int}_{ij} = \\frac{C_6}{r_{ij}^6}N_iN_j \\] <p>where \\(r_{ij}\\) is the distance between atoms \\(i\\) and \\(j\\), and \\(C_6\\) is a coefficient depending on the specific Rydberg level of the excited state used in the computational logic states. A typical value for rydberg level of 60 is \\(C_6\\approx 866~[\\text{rad} . \\mu \\text{m}^6 / \\text{ns}]\\).</p> <p>For a given register of atoms prepared in some spatial coordinates, the Hamiltonians described will generate the dynamics of some unitary operation as</p> \\[ U(t, \\Omega, \\delta, \\phi) = \\exp(-i\\mathcal{H}t) \\] <p>where we specify the final parameter \\(t\\), the duration of the operation.</p> <p>Qadence uses the following units for user-specified parameters:</p> <ul> <li>Rabi frequency and detuning \\(\\Omega\\), \\(\\delta\\): \\([\\text{rad}/\\mu \\text{s}]\\)</li> <li>Phase \\(\\phi\\): \\([\\text{rad}]\\)</li> <li>Duration \\(t\\): \\([\\text{ns}]\\)</li> <li>Atom coordinates: \\([\\mu \\text{m}]\\)</li> </ul>"},{"location":"tutorials/digital_analog_qc/analog-basics/#in-practice","title":"In practice","text":"<p>Given the Hamiltonian description in the previous section, we will now go over a few examples of the standard operations available in Qadence.</p>"},{"location":"tutorials/digital_analog_qc/analog-basics/#arbitrary-rotation","title":"Arbitrary rotation","text":"<p>To start, we will exemplify the a general rotation on a set of atoms. To create an arbitrary register of atoms, we refer the user to the register creation tutorial. Below, we create a line register of three qubits with a separation of \\(8~\\mu\\text{m}\\). This is a typical value used in combination with a standard experimental setup of neutral atoms such that the interaction term in the Hamiltonian can effectively be used for computations.</p> <pre><code>from qadence import Register\n\nreg = Register.line(3, spacing=8.0)  # Atom spacing in \u03bcm\n</code></pre> <p>Currently, the most general rotation operation uses the <code>AnalogRot</code> operation, which essentially implements \\(U(t, \\Omega, \\delta, \\phi)\\) defined above.</p> <pre><code>from qadence import AnalogRot, PI\n\nrot_op = AnalogRot(\n    duration = 500., # [ns]\n    omega = PI, # [rad/\u03bcs]\n    delta = PI, # [rad/\u03bcs]\n    phase = PI, # [rad]\n)\n</code></pre> <p>Note that in the code above a specific qubit support is not defined. By default this operation applies a global rotation on all qubits. We can define a circuit using the 3-qubit register and run it in the pyqtorch backend:</p> <pre><code>from qadence import BackendName, run\n\nwf = run(reg, rot_op, backend = BackendName.PYQTORCH)\n\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4248-0.2411j, -0.1687+0.3156j, -0.1696+0.2676j, -0.2040-0.2671j,\n         -0.1687+0.3156j,  0.0014-0.2721j, -0.2040-0.2671j,  0.3034-0.1130j]])\n</code></pre> Under the hood of AnalogRot      To be fully explicit about what goes on under the hood of `AnalogRot`, we can look at the example     code below.      <pre><code>from qadence import BackendName, HamEvo, X, Y, N, add, run, PI\nfrom qadence.analog.constants import C6_DICT\nfrom math import cos, sin\n\n# Following the 3-qubit register above\nn_qubits = 3\ndx = 8.0\n\n# Parameters used in the AnalogRot\nduration = 500.\nomega = PI\ndelta = PI\nphase = PI\n\n# Building the terms in the driving Hamiltonian\nh_x = (omega / 2) * cos(phase) * add(X(i) for i in range(n_qubits))\nh_y = (-1.0 * omega / 2) * sin(phase) * add(Y(i) for i in range(n_qubits))\nh_n = -1.0 * delta * add(N(i) for i in range(n_qubits))\n\n# Building the interaction Hamiltonian\n\n# Dictionary of coefficient values for each Rydberg level, which is 60 by default\nc_6 = C6_DICT[60]\n\nh_int = c_6 * (\n    1/(dx**6) * (N(0)@N(1)) +\n    1/(dx**6) * (N(1)@N(2)) +\n    1/((2*dx)**6) * (N(0)@N(2))\n)\n\nhamiltonian = h_x + h_y + h_n + h_int\n\n# Convert duration to \u00b5s due to the units of the Hamiltonian\nexplicit_rot = HamEvo(hamiltonian, duration / 1000)\n\nwf = run(n_qubits, explicit_rot, backend = BackendName.PYQTORCH)\n\n# We get the same final wavefunction\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4248-0.2411j, -0.1687+0.3156j, -0.1696+0.2676j, -0.2040-0.2671j,\n         -0.1687+0.3156j,  0.0014-0.2721j, -0.2040-0.2671j,  0.3034-0.1130j]])\n</code></pre> <p>When sending the <code>AnalogRot</code> operation to the pyqtorch backend, Qadence automatically builds the correct Hamiltonian and the corresponding <code>HamEvo</code> operation with the added qubit interactions, as shown explicitly in the minimized section above. However, this operation is also supported in the Pulser backend, where the correct pulses are automatically created.</p> <pre><code>wf = run(\n    reg,\n    rot_op,\n    backend = BackendName.PULSER,\n)\n\nprint(wf)\n</code></pre> <pre><code>tensor([[ 0.4253-0.2408j, -0.1688+0.3157j, -0.1698+0.2678j, -0.2044-0.2667j,\n         -0.1688+0.3157j,  0.0011-0.2721j, -0.2044-0.2667j,  0.3026-0.1137j]])\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#rx-ry-rz-rotations","title":"RX / RY / RZ rotations","text":"<p>The <code>AnalogRot</code> provides full control over the parameters of \\(\\mathcal{H}^\\text{d}\\), but users coming from a digital quantum computing background may be more familiar with the standard <code>RX</code>, <code>RY</code> and <code>RZ</code> rotations, also available in Qadence. For the emulated analog interface, Qadence provides alternative <code>AnalogRX</code>, <code>AnalogRY</code> and <code>AnalogRZ</code> operations which call <code>AnalogRot</code> under the hood to represent the rotations accross the respective axis.</p> <p>For a given angle of rotation \\(\\theta\\) provided to each of these operations, currently a set of hardcoded assumptions are made on the tunable Hamiltonian parameters:</p> \\[ \\begin{aligned} \\text{RX}:&amp; \\quad \\Omega = \\pi, \\quad \\delta = 0, \\quad \\phi = 0, \\quad t = (\\theta/\\Omega)\\times 10^3 \\\\ \\text{RY}:&amp; \\quad \\Omega = \\pi, \\quad \\delta = 0, \\quad \\phi = -\\pi/2, \\quad t = (\\theta/\\Omega)\\times 10^3 \\\\ \\text{RZ}:&amp; \\quad \\Omega = 0, \\quad \\delta = \\pi, \\quad \\phi = 0, \\quad t = (\\theta/\\delta)\\times 10^3 \\\\ \\end{aligned} \\] <p>Note that the \\(\\text{RZ}\\) operation as defined above includes a global phase compared to the standard \\(\\text{RZ}\\) rotation since it evolves \\(\\exp\\left(-i\\frac{\\theta}{2}\\frac{I-Z}{2}\\right)\\) instead of \\(\\exp\\left(-i\\frac{\\theta}{2}Z\\right)\\) given the detuning operator in \\(\\mathcal{H}^\\text{d}\\).</p> <p>Warning</p> <p>As shown above, the values of \\(\\Omega\\) and \\(\\delta\\) are currently hardcoded in these operators, and the effective angle of rotation is controlled by varying the duration of the evolution. Currently, the best way to overcome this is to use <code>AnalogRot</code> directly, but more general and convenient options will be provided soon in an improved interface.</p> <p>Below we exemplify the usage of <code>AnalogRX</code>:</p> <pre><code>from qadence import Register, BackendName\nfrom qadence import RX, AnalogRX, random_state, equivalent_state, kron, run, PI\n\nn_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\n# Rotation angle\ntheta = PI\n\n# Analog rotation using the Rydberg Hamiltonian\nrot_analog = AnalogRX(angle = theta)\n\n# Equivalent full-digital global rotation\nrot_digital = kron(RX(i, theta) for i in range(n_qubits))\n\n# Some random initial state\ninit_state = random_state(n_qubits)\n\n# Compare the final state using the full digital and the AnalogRX\nwf_analog_pyq = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\n\nwf_digital_pyq = run(\n    reg,\n    rot_digital,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_digital_pyq, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  False\n</code></pre> <p>As we can see, running a global <code>RX</code> or the <code>AnalogRX</code> does not result in equivalent states at the end, given that the digital <code>RX</code> operation does not include the interaction between the qubits. By setting <code>dx</code> very high in the code above the interaction will be less significant and the results will match.</p> <p>However, if we compare with the Pulser backend, we see that the results for <code>AnalogRX</code> are consistent with the expected results from a real device:</p> <pre><code>wf_analog_pulser = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PULSER,\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_analog_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#evolving-the-interaction-term","title":"Evolving the interaction term","text":"<p>Finally, besides applying specific qubit rotations, we can also choose to evolve only the interaction term \\(\\mathcal{H}^\\text{int}\\), equivalent to setting \\(\\Omega = \\delta = \\phi = 0\\). To do so, Qadence provides the function <code>AnalogInteraction</code> which does exactly this.</p> <pre><code>from qadence import Register, BackendName, random_state, equivalent_state, AnalogInteraction, run\n\nn_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\nduration = 1000.\nop = AnalogInteraction(duration = duration)\n\ninit_state = random_state(n_qubits)\n\nwf_pyq = run(reg, op, state = init_state, backend = BackendName.PYQTORCH)\nwf_pulser = run(reg, op, state = init_state, backend = BackendName.PULSER)\n\nbool_equiv = equivalent_state(wf_pyq, wf_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#device-specifications-in-qadence","title":"Device specifications in Qadence","text":"<p>As a way to control other specifications of the interacting Rydberg atoms, Qadence provides a <code>RydbergDevice</code> class, which is currently used for both the pyqtorch and the pulser backends. Below we initialize a Rydberg device showcasing all the possible options.</p> <pre><code>from qadence import RydbergDevice, DeviceType, Interaction, PI\n\ndevice_specs = RydbergDevice(\n    interaction=Interaction.NN, # Or Interaction.XY, supported only for pyqtorch\n    rydberg_level=60, # Integer value affecting the C_6 coefficient\n    coeff_xy=3700.00, # C_3 coefficient for the XY interaction\n    max_detuning=2 * PI * 4, # Max value for delta, currently only used in pulser\n    max_amp=2 * PI * 3, # Max value for omega, currently only used in pulser\n    pattern=None, # Semi-local addressing pattern, see the relevant tutorial\n    type=DeviceType.IDEALIZED, # Pulser device to which the qadence device is converted in that backend\n)\n</code></pre> <p>The values above are the defaults when simply running <code>device_specs = RydbergDevice()</code>. The convenience wrappers <code>IdealDevice()</code> or <code>RealisticDevice()</code> can also be used which simply change the <code>type</code> for the Pulser backend, but also allow an <code>AddressingPattern</code> passed in the <code>pattern</code> argument (see the relevant tutorial here).</p> <p>Warning</p> <p>Currently, the options above are not fully integrated in both backends and this class should mostly be used if a user wishes to experiment with a different <code>rydberg_level</code>, or to change the device type for the pulser backend.</p> <p>Planned features to add to the RydbergDevice include the definition of custom interaction functions, the control of other drive Hamiltonian parameters so that \\(\\Omega\\), \\(\\delta\\) and \\(\\phi\\) are not hardcoded when doing analog rotations, and the usage of the <code>max_detuning</code> and <code>max_amp</code> to control those respective parameters when training models in the pyqtorch backend.</p> <p>Finally, to change a given simulation, the device specifications are integrated in the Qadence <code>Register</code>. By default, all registers initialize an <code>IdealDevice()</code> under the hood. Below we run a quick test for a different rydberg level.</p> <pre><code>from qadence import Register, BackendName, random_state, equivalent_state, run\nfrom qadence import AnalogRX, RydbergDevice, PI\n\ndevice_specs = RydbergDevice(rydberg_level = 70)\n\nn_qubits_side = 2\nreg = Register.square(\n    n_qubits_side,\n    spacing = 8.0,\n    device_specs = device_specs\n)\n\nrot_analog = AnalogRX(angle = PI)\n\ninit_state = random_state(n_qubits = 4)\n\nwf_analog_pyq = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PYQTORCH\n)\n\nwf_analog_pulser = run(\n    reg,\n    rot_analog,\n    state = init_state,\n    backend = BackendName.PULSER\n)\n\nbool_equiv = equivalent_state(wf_analog_pyq, wf_analog_pulser, atol = 1e-03)\n\nprint(\"States equivalent: \", bool_equiv)\n</code></pre> <pre><code>States equivalent:  True\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-basics/#technical-details","title":"Technical details","text":"<p>Warning</p> <p>The details described here are relevant in the current version but will be lifted soon for the next version of the emulated analog interface.</p> <p>In the previous section we have exemplified the main ingredients of the current user-facing functionalities of the emulated analog interface, and in the next tutorial on Quantum Circuit Learning we will exmplify its usage in a simple QML example. Here we specify some extra details of this interface.</p> <p>In the block system, all analog rotation operators initialize a <code>ConstantAnalogRotation</code> block, while the <code>AnalogInteraction</code> operation initializes an <code>InteractionBlock</code>. As we have shown, by default, these blocks use a global qubit support, which can be passed explicitly by setting <code>qubit_support = QubitSupportType.GLOBAL</code>. However, composing blocks using <code>kron</code> with local qubit supports and different durations is not allowed.</p> <pre><code>from qadence import AnalogRX, AnalogRY, Register, kron\n\ndx = 8.0\nreg = Register.from_coordinates([(0, 0), (dx, 0)])\n\n# Does not work (the angle affects the duration, as seen above):\nrot_0 = AnalogRX(angle = 1.0, qubit_support = (0,))\nrot_1 = AnalogRY(angle = 2.0, qubit_support = (1,))\n\ntry:\n    block = kron(rot_0, rot_1)\nexcept ValueError as error:\n    print(\"Error:\", error)\n\n# Works:\nrot_0 = AnalogRX(angle = 1.0, qubit_support = (0,))\nrot_1 = AnalogRY(angle = 1.0, qubit_support = (1,))\n\nblock = kron(rot_0, rot_1)\n</code></pre> <pre><code>Error: Kron'ed blocks have to have same duration.\n</code></pre> <p>Using <code>chain</code> is only supported between analog blocks with global qubit support:</p> <pre><code>from qadence import chain\n\nrot_0 = AnalogRX(angle = 1.0, qubit_support = \"global\")\nrot_1 = AnalogRY(angle = 2.0, qubit_support = \"global\")\n\nblock = chain(rot_0, rot_1)\n</code></pre> <p>The restrictions above only apply to the analog blocks, and analog and digital blocks can currently be composed.</p> <pre><code>from qadence import RX\n\nrot_0 = AnalogRX(angle = 1.0, qubit_support = \"global\")\nrot_1 = AnalogRY(angle = 2.0, qubit_support = (0,))\nrot_digital = RX(1, 1.0)\n\nblock_0 = chain(rot_0, rot_digital)\nblock_1 = kron(rot_1, rot_digital)\n</code></pre>"},{"location":"tutorials/digital_analog_qc/analog-blocks-qcl/","title":"Fitting a function with analog blocks","text":"<p>Analog blocks can be parametrized in the usual Qadence manner. Like any other parameters, they can be optimized. The next snippet exemplifies the creation of an analog and parameterized ansatz to fit a simple function. First, define a register and feature map block. We again use a default spacing of \\(8~\\mu\\text{m}\\) as done in the basic tutorial.</p> <pre><code>from qadence import Register, FeatureParameter, chain\nfrom qadence import AnalogRX, AnalogRY, AnalogRZ, AnalogInteraction\nfrom sympy import acos\n\n# Line register\nn_qubits = 2\nregister = Register.line(n_qubits, spacing = 8.0)\n\n# The input feature x for the circuit to learn f(x)\nx = FeatureParameter(\"x\")\n\n# Feature map with a few global analog rotations\nfm = chain(\n    AnalogRX(x),\n    AnalogRY(2*x),\n    AnalogRZ(3*x),\n)\n</code></pre> <p>Next, we define the ansatz with parameterized rotations.</p> <pre><code>from qadence import hamiltonian_factory, Z\nfrom qadence import QuantumCircuit, QuantumModel, BackendName, DiffMode\nfrom qadence import VariationalParameter\n\nt_0 = 1000. * VariationalParameter(\"t_0\")\nt_1 = 1000. * VariationalParameter(\"t_1\")\nt_2 = 1000. * VariationalParameter(\"t_2\")\n\n# Creating the ansatz with parameterized rotations and wait time\nansatz = chain(\n    AnalogRX(\"tht_0\"),\n    AnalogRY(\"tht_1\"),\n    AnalogRZ(\"tht_2\"),\n    AnalogInteraction(t_0),\n    AnalogRX(\"tht_3\"),\n    AnalogRY(\"tht_4\"),\n    AnalogRZ(\"tht_5\"),\n    AnalogInteraction(t_1),\n    AnalogRX(\"tht_6\"),\n    AnalogRY(\"tht_7\"),\n    AnalogRZ(\"tht_8\"),\n    AnalogInteraction(t_2),\n)\n</code></pre> <p>We define the measured observable as the total magnetization, and build the <code>QuantumModel</code>.</p> <pre><code># Total magnetization observable\nobservable = hamiltonian_factory(n_qubits, detuning = Z)\n\n# Defining the circuit and observable\ncircuit = QuantumCircuit(register, fm, ansatz)\n\nmodel = QuantumModel(\n    circuit,\n    observable = observable,\n    backend = BackendName.PYQTORCH,\n    diff_mode = DiffMode.AD\n)\n</code></pre> <p>Now we can define the function to fit as well as our training and test data.</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Function to fit:\ndef f(x):\n    return x**2\n\nx_test = torch.linspace(-1.0, 1.0, steps=100)\ny_test = f(x_test)\n\nx_train = torch.linspace(-1.0, 1.0, steps=10)\ny_train = f(x_train)\n\n# Initial prediction from the model, to be visualized later\ny_pred_initial = model.expectation({\"x\": x_test}).detach()\n</code></pre> <p>Finally we define a simple loss function and training loop.</p> <pre><code>mse_loss = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(x_train, y_train):\n    out = model.expectation({\"x\": x_train})\n    loss = mse_loss(out.squeeze(), y_train)\n    return loss\n\nn_epochs = 200\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_fn(x_train, y_train)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>And with the model trained we can plot the final results.</p> <pre><code>y_pred_final = model.expectation({\"x\": x_test}).detach()\n\nplt.plot(x_test, y_pred_initial, label = \"Initial prediction\")\nplt.plot(x_test, y_pred_final, label = \"Final prediction\")\nplt.scatter(x_train, y_train, label = \"Training points\")\n</code></pre> 2025-01-08T18:01:51.072397 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/analog-qubo/","title":"Solve a QUBO problem","text":"<p>In this notebook, we solve a quadratic unconstrained binary optimization (QUBO) problem with Qadence. QUBOs are very popular combinatorial optimization problems with a wide range of applications. Here, we solve the problem using the QAOA <sup>1</sup> variational algorithm by embedding the QUBO problem weights onto a register as standard for neutral atom quantum devices.</p> <p>Additional background information on QUBOs can be found here, directly solved using the pulse-level interface Pulser.</p>"},{"location":"tutorials/digital_analog_qc/analog-qubo/#define-and-solve-qubo","title":"Define and solve QUBO","text":"Pre-requisite: optimal register coordinates for embedding the QUBO problem <p>A basic ingredient for solving a QUBO problem with a neutral atom device is to embed the problem onto the atomic register. In short, embedding algorithms cast the problem onto a graph mapped onto the register by optimally finding atomic coordinates. A discussion on the embedding algorithms is beyond the scope of this tutorial and a simplified version taken from here is added below.</p> <pre><code>import numpy as np\nimport numpy.typing as npt\nfrom scipy.optimize import minimize\nfrom scipy.spatial.distance import pdist, squareform\nfrom qadence import RydbergDevice\n\ndef qubo_register_coords(Q: np.ndarray, device: RydbergDevice) -&gt; list:\n    \"\"\"Compute coordinates for register.\"\"\"\n\n    def evaluate_mapping(new_coords, *args):\n        \"\"\"Cost function to minimize. Ideally, the pairwise\n        distances are conserved\"\"\"\n        Q, shape = args\n        new_coords = np.reshape(new_coords, shape)\n        interaction_coeff = device.coeff_ising\n        new_Q = squareform(interaction_coeff / pdist(new_coords) ** 6)\n        return np.linalg.norm(new_Q - Q)\n\n    shape = (len(Q), 2)\n    np.random.seed(0)\n    x0 = np.random.random(shape).flatten()\n    res = minimize(\n        evaluate_mapping,\n        x0,\n        args=(Q, shape),\n        method=\"Nelder-Mead\",\n        tol=1e-6,\n        options={\"maxiter\": 200000, \"maxfev\": None},\n    )\n    return [(x, y) for (x, y) in np.reshape(res.x, (len(Q), 2))]\n</code></pre> <p>With the embedding routine define above, we can translate a matrix defining a QUBO problem to a set of atom coordinates for the register. The QUBO problem is initially defined by a graph of weighted edges and a cost function to be optimized. The weighted edges are represented by a real-valued symmetric matrix <code>Q</code> which is used throughout the tutorial.</p> <pre><code>import torch\nfrom qadence import QuantumModel\n\nseed = 0\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# QUBO problem weights (real-value symmetric matrix)\nQ = np.array(\n    [\n        [-10.0, 19.7365809, 19.7365809, 5.42015853, 5.42015853],\n        [19.7365809, -10.0, 20.67626392, 0.17675796, 0.85604541],\n        [19.7365809, 20.67626392, -10.0, 0.85604541, 0.17675796],\n        [5.42015853, 0.17675796, 0.85604541, -10.0, 0.32306662],\n        [5.42015853, 0.85604541, 0.17675796, 0.32306662, -10.0],\n    ]\n)\n\n# Loss function to guide the optimization routine\ndef loss(model: QuantumModel, *args) -&gt; tuple[torch.Tensor, dict]:\n    to_arr_fn = lambda bitstring: np.array(list(bitstring), dtype=int)\n    cost_fn = lambda arr: arr.T @ Q @ arr\n    samples = model.sample({}, n_shots=1000)[0]\n    cost_fn = sum(samples[key] * cost_fn(to_arr_fn(key)) for key in samples)\n    return torch.tensor(cost_fn / sum(samples.values())), {}\n</code></pre> <p>The QAOA algorithm needs a variational quantum circuit with optimizable parameters. For that purpose, we use a fully analog circuit composed of two global rotations per layer on different axes of the Bloch sphere. The first rotation corresponds to the mixing Hamiltonian and the second one to the embedding Hamiltonian <sup>1</sup>. In this setting, the embedding is realized by the appropriate register coordinates and the resulting qubit interaction. Details on the analog blocks used here can be found in the analog basics tutorial.</p> Rydberg level <p>The Rydberg level is set to 70. We initialize the weighted register graph from the QUBO definition similarly to what is done in the original tutorial, and set the device specifications with the updated Rydberg level.</p> <pre><code>from qadence import QuantumCircuit, Register, RydbergDevice\nfrom qadence import chain, AnalogRX, AnalogRZ\n\n# Device specification and atomic register\ndevice = RydbergDevice(rydberg_level=70)\n\nreg = Register.from_coordinates(\n    qubo_register_coords(Q, device), device_specs=device\n)\n\n# Analog variational quantum circuit\nlayers = 2\nblock = chain(*[AnalogRX(f\"t{i}\") * AnalogRZ(f\"s{i}\") for i in range(layers)])\ncircuit = QuantumCircuit(reg, block)\n</code></pre> <p>By initializing the <code>QuantumModel</code> with this circuit we can check the initial counts where no clear solution can be found.</p> <pre><code>model = QuantumModel(circuit)\ninitial_counts = model.sample({}, n_shots=1000)[0]\n</code></pre> <pre><code>initial_counts = OrderedCounter({'00000': 101, '10000': 87, '01000': 75, '00110': 72, '00100': 70, '01010': 64, '01001': 62, '00101': 53, '00010': 51, '00011': 48, '01011': 46, '00001': 45, '10010': 45, '00111': 38, '10001': 34, '11000': 29, '10100': 18, '01100': 14, '01110': 11, '01111': 10, '10110': 7, '11001': 7, '11010': 6, '01101': 4, '10101': 2, '10011': 1})\n</code></pre> <p>Finally, we can proceed with the variational optimization. The cost function defined above is derived from bitstring computations and therefore non differentiable. We use Qadence ML facilities to run gradient-free optimizations using the <code>nevergrad</code> library.</p> <pre><code>from qadence.ml_tools import Trainer, TrainConfig, num_parameters\nimport nevergrad as ng\n\nTrainer.set_use_grad(False)\n\nconfig = TrainConfig(max_iter=100)\n\noptimizer = ng.optimizers.NGOpt(\n    budget=config.max_iter, parametrization=num_parameters(model)\n)\n\ntrainer = Trainer(model, optimizer, config, loss)\n\ntrainer.fit()\n\noptimal_counts = model.sample({}, n_shots=1000)[0]\n</code></pre> <p>Finally, let's plot the solution. The expected bitstrings are marked in red.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Known solutions to the QUBO problem.\nsolution_bitstrings = [\"01011\", \"00111\"]\n\ndef plot_distribution(C, ax, title):\n    C = dict(sorted(C.items(), key=lambda item: item[1], reverse=True))\n    color_dict = {key: \"r\" if key in solution_bitstrings else \"b\" for key in C}\n    ax.set_xlabel(\"bitstrings\")\n    ax.set_ylabel(\"counts\")\n    ax.set_xticks([i for i in range(len(C.keys()))], C.keys(), rotation=90)\n    ax.bar(C.keys(), C.values(), color=color_dict.values())\n    ax.set_title(title)\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\nplot_distribution(initial_counts, axs[0], \"Initial counts\")\nplot_distribution(optimal_counts, axs[1], \"Optimal counts\")\n</code></pre> 2025-01-08T18:01:57.965077 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/analog-qubo/#references","title":"References","text":"<ol> <li> <p>Edward Farhi, Jeffrey Goldstone, Sam Gutmann, A Quantum Approximate Optimization Algorithm, arXiv:1411.4028 (2014) \u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/","title":"<code>CNOT</code> with interacting qubits","text":"<p>Digital-analog quantum computing focuses on using single qubit digital gates combined with more complex and device-dependent analog interactions to represent quantum programs. This paradigm has been shown to be universal for quantum computation<sup>1</sup>. However, while this approach may have advantages when adapting quantum programs to real devices, known quantum algorithms are very often expressed in a fully digital paradigm. As such, it is also important to have concrete ways to transform from one paradigm to another.</p> <p>This tutorial will exemplify the DAQC transformation starting with the representation of a simple digital <code>CNOT</code> using the universality of the Ising Hamiltonian<sup>2</sup>.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#cnot-with-cphase","title":"<code>CNOT</code> with <code>CPHASE</code>","text":"<p>Let's look at a single example of how the digital-analog transformation can be used to perform a <code>CNOT</code> on two qubits inside a register of globally interacting qubits.</p> <p>First, note that the <code>CNOT</code> can be decomposed with two Hadamard and a <code>CPHASE</code> gate with \\(\\phi=\\pi\\):</p> <pre><code>import torch\nfrom qadence import chain, sample, product_state\n\nfrom qadence.draw import display\nfrom qadence import X, I, Z, H, N, CPHASE, CNOT, HamEvo, PI\n\nn_qubits = 2\n\n# CNOT gate\ncnot_gate = CNOT(0, 1)\n\n# CNOT decomposed\nphi = PI\ncnot_decomp = chain(H(1), CPHASE(0, 1, phi), H(1))\n\ninit_state = product_state(\"10\")\n</code></pre> <pre><code>sample from CNOT gate and 100 shots = [OrderedCounter({'11': 100})]\nsample from decomposed CNOT gate and 100 shots = [OrderedCounter({'11': 100})]\n</code></pre> <p>The <code>CPHASE</code> matrix is diagonal, and can be implemented by exponentiating an Ising-like Hamiltonian, or generator,</p> \\[\\text{CPHASE}(i,j,\\phi)=\\text{exp}\\left(-i\\phi \\mathcal{H}_\\text{CP}(i, j)\\right)\\] \\[\\begin{aligned} \\mathcal{H}_\\text{CP}&amp;=-\\frac{1}{4}(I_i-Z_i)(I_j-Z_j)\\\\ &amp;=-N_iN_j \\end{aligned}\\] <p>where the number operator \\(N_i = \\frac{1}{2}(I_i-Z_i)=\\hat{n}_i\\) is used, leading to an Ising-like interaction \\(\\hat{n}_i\\hat{n}_j\\) realisable in neutral-atom systems. Let's rebuild the <code>CNOT</code> using this evolution.</p> <pre><code>from qadence import kron, block_to_tensor\n\n# Hamiltonian for the CPHASE gate\nh_cphase = (-1.0) * kron(N(0), N(1))\n\n# Exponentiating and time-evolving the Hamiltonian until t=phi.\ncphase_evo = HamEvo(h_cphase, phi)\n\n# Check that we have the CPHASE gate:\ncphase_matrix = block_to_tensor(CPHASE(0, 1, phi))\ncphase_evo_matrix = block_to_tensor(cphase_evo)\n</code></pre> <pre><code>cphase_matrix == cphase_evo_matrix: True\n</code></pre> <p>Now that the <code>CPHASE</code> generator is checked, it can be applied to the <code>CNOT</code>:</p> <pre><code># CNOT with Hamiltonian Evolution\ncnot_evo = chain(\n    H(1),\n    cphase_evo,\n    H(1)\n)\n\n# Initialize state to check CNOTs sample outcomes.\ninit_state = product_state(\"10\")\n</code></pre> <pre><code>sample cnot_gate = [OrderedCounter({'11': 100})]\nsample cnot_evo = [OrderedCounter({'11': 100})]\n</code></pre> <p>Thus, a <code>CNOT</code> gate can be created by combining a few single-qubit gates together with a two-qubit Ising interaction between the control and the target qubit which is the essence of the Ising transform proposed in the seminal DAQC paper<sup>2</sup> for \\(ZZ\\) interactions. In Qadence, both \\(ZZ\\) and \\(NN\\) interactions are supported.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#cnot-in-an-interacting-system-of-three-qubits","title":"<code>CNOT</code> in an interacting system of three qubits","text":"<p>Consider a simple experimental setup with \\(n=3\\) interacting qubits laid out in a triangular grid. For the sake of simplicity, all qubits interact with each other with an \\(NN\\)-Ising interaction of constant strength \\(g_\\text{int}\\). The Hamiltonian for the system can be written by summing interaction terms over all pairs:</p> \\[\\mathcal{H}_\\text{sys}=\\sum_{i=0}^{n}\\sum_{j=0}^{i-1}g_\\text{int}N_iN_j,\\] <p>which in this case leads to only three interaction terms,</p> \\[\\mathcal{H}_\\text{sys}=g_\\text{int}(N_0N_1+N_1N_2+N_0N_2)\\] <p>This generator can be easily built in Qadence:</p> <pre><code>from qadence import add, kron\nn_qubits = 3\n\n# Interaction strength.\ng_int = 1.0\n\n# Build a list of interactions.\ninteraction_list = []\nfor i in range(n_qubits):\n    for j in range(i):\n        interaction_list.append(g_int * kron(N(i), N(j)))\n\nh_sys = add(*interaction_list)\n</code></pre> <pre><code>h_sys = AddBlock(0,1,2)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,1)\n\u2502       \u251c\u2500\u2500 N(1)\n\u2502       \u2514\u2500\u2500 N(0)\n\u251c\u2500\u2500 [mul: 1.000] \n\u2502   \u2514\u2500\u2500 KronBlock(0,2)\n\u2502       \u251c\u2500\u2500 N(2)\n\u2502       \u2514\u2500\u2500 N(0)\n\u2514\u2500\u2500 [mul: 1.000] \n    \u2514\u2500\u2500 KronBlock(1,2)\n        \u251c\u2500\u2500 N(2)\n        \u2514\u2500\u2500 N(1)\n</code></pre> <p>Now let's consider that the experimental system is fixed, and qubits can not be isolated one from another. The options are:</p> <ul> <li>Turn on or off the global system Hamiltonian.</li> <li>Perform local single-qubit rotations.</li> </ul> <p>To perform a fully digital <code>CNOT(0,1)</code>, the interacting control on qubit 0 and target on qubit 1 must be isolated from the third one to implement the gate directly. While this can be achieved for a three-qubit system, it becomes experimentally untractable when scaling the qubit count.</p> <p>However, this is not the case within the digital-analog paradigm. In fact, the two qubit Ising interaction required for the <code>CNOT</code> can be represented with a combination of the global system Hamiltonian and a specific set of single-qubit rotations. Full details about this transformation are to be found in the DAQC paper<sup>2</sup> but a more succint yet in-depth description takes place in the next section. It is conveniently available in Qadence by calling the <code>daqc_transform</code> function.</p> <p>In the most general sense, the <code>daqc_transform</code> function will return a circuit that represents the evolution of a target Hamiltonian \\(\\mathcal{H}_\\text{target}\\) (here the unitary of the gate) until a specified time \\(t_f\\) by using only the evolution of a build Hamiltonian \\(\\mathcal{H}_\\text{build}\\) (here \\(\\mathcal{H}_\\text{sys}\\)) together with local \\(X\\)-gates. In Qadence, <code>daqc_transform</code> is applicable for \\(\\mathcal{H}_\\text{target}\\) and \\(\\mathcal{H}_\\text{build}\\) composed only of \\(ZZ\\)- or \\(NN\\)-interactions. These generators are parsed by the <code>daqc_transform</code> function and the appropriate type is automatically determined together with the appropriate single-qubit detunings and global phases.</p> <p>Let's apply it for the <code>CNOT</code> implementation:</p> <pre><code>from qadence import daqc_transform, Strategy\n\n# Settings for the target CNOT operation\ni = 0  # Control qubit\nj = 1  # Target qubit\nk = 2  # The extra qubit\n\n# Define the target CNOT operation\n# by composing with identity on the extra qubit.\ncnot_target = kron(CNOT(i, j), I(k))\n\n# The two-qubit NN-Ising interaction term for the CPHASE\nh_int = (-1.0) * kron(N(i), N(j))\n\n# Transforming the two-qubit Ising interaction using only our system Hamiltonian\ntransformed_ising = daqc_transform(\n    n_qubits=3,        # Total number of qubits in the transformation\n    gen_target=h_int,  # The target Ising generator\n    t_f=PI,            # The target evolution time\n    gen_build=h_sys,   # The building block Ising generator to be used\n    strategy=Strategy.SDAQC,   # Currently only sDAQC is implemented\n    ignore_global_phases=False  # Global phases from mapping between Z and N\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_f25184f503de484ab7a6853c4660d1b4 cluster_1359ba9bc13b44c4afc8adf48837d06e cluster_326c2b571c1a4921960dcfa5e83ce6ee cluster_52b2780e2c994ccbb2499912fae49ef6 cluster_9a505eb789df4f0494c484233ebbc638 cluster_f9df323b0ca347edbc72207b16f58313 cluster_ac2871fe02a347258f1160e9380b61bd d5844530a1d04c1f9b3b58ef078c8d73 0 153e39b336d64077a31874d2adbdd6b8 HamEvo d5844530a1d04c1f9b3b58ef078c8d73--153e39b336d64077a31874d2adbdd6b8 ad49e3a3e85a42e4af97fda61c5058ca 1 4cab5b932f6b44f19080ca1238558794 HamEvo 153e39b336d64077a31874d2adbdd6b8--4cab5b932f6b44f19080ca1238558794 b923894e76644275901d963c9c5f90c5 HamEvo 4cab5b932f6b44f19080ca1238558794--b923894e76644275901d963c9c5f90c5 e01056836ff840d8862f2805f1fac961 X b923894e76644275901d963c9c5f90c5--e01056836ff840d8862f2805f1fac961 299b5884dff5410d93f180f27ee02e25 HamEvo e01056836ff840d8862f2805f1fac961--299b5884dff5410d93f180f27ee02e25 8be527909f3245948cf0bbd12de45ae4 HamEvo 299b5884dff5410d93f180f27ee02e25--8be527909f3245948cf0bbd12de45ae4 fe6397ba6c9a44288c56f7d1d5a0859b X 8be527909f3245948cf0bbd12de45ae4--fe6397ba6c9a44288c56f7d1d5a0859b 69966674c93747debafe08b56fc24f98 fe6397ba6c9a44288c56f7d1d5a0859b--69966674c93747debafe08b56fc24f98 05b1d76ea61241988c34ab3b9b62b12a HamEvo 69966674c93747debafe08b56fc24f98--05b1d76ea61241988c34ab3b9b62b12a 9aae7a9d242a4e45be4800b0f35070b6 HamEvo 05b1d76ea61241988c34ab3b9b62b12a--9aae7a9d242a4e45be4800b0f35070b6 69f344a7d29740f7bcc4c0bf05a77abb 9aae7a9d242a4e45be4800b0f35070b6--69f344a7d29740f7bcc4c0bf05a77abb 76f82cb2f5c1430e97aaa683427d6d08 69f344a7d29740f7bcc4c0bf05a77abb--76f82cb2f5c1430e97aaa683427d6d08 3fbee8cd832247179216f1f52187ba7e 150610eafec542789ce6dac393239fa7 t = -3.14 ad49e3a3e85a42e4af97fda61c5058ca--150610eafec542789ce6dac393239fa7 966b185577df49d4bf4e0807de681db8 2 c3f0369b76384f168df13ecf3562f84e t = 3.142 150610eafec542789ce6dac393239fa7--c3f0369b76384f168df13ecf3562f84e d44834804668415c9dc078768a5d0faa t = -3.14 c3f0369b76384f168df13ecf3562f84e--d44834804668415c9dc078768a5d0faa d95aba65522747f0ad35ee5e900df602 d44834804668415c9dc078768a5d0faa--d95aba65522747f0ad35ee5e900df602 df1439a7f4a94177a5608f7465778c56 t = 1.571 d95aba65522747f0ad35ee5e900df602--df1439a7f4a94177a5608f7465778c56 b8a851109479419eb424fb0ae751ce4a t = 1.571 df1439a7f4a94177a5608f7465778c56--b8a851109479419eb424fb0ae751ce4a 794b6b797af249e7b6b78ea6d16217f7 b8a851109479419eb424fb0ae751ce4a--794b6b797af249e7b6b78ea6d16217f7 90d7ca89110843b2948a1edf55806594 X 794b6b797af249e7b6b78ea6d16217f7--90d7ca89110843b2948a1edf55806594 23788d468906483eb554d9abcaa592f1 t = 1.571 90d7ca89110843b2948a1edf55806594--23788d468906483eb554d9abcaa592f1 c8e33462c8c447768b3d74067863f9cc t = 1.571 23788d468906483eb554d9abcaa592f1--c8e33462c8c447768b3d74067863f9cc 14a4242e9b514796a8fb7ae54cf37587 X c8e33462c8c447768b3d74067863f9cc--14a4242e9b514796a8fb7ae54cf37587 14a4242e9b514796a8fb7ae54cf37587--3fbee8cd832247179216f1f52187ba7e 941db546a4ea421ab12aeea4875f172f 8c813b17a55943e3bde3918f148d7961 966b185577df49d4bf4e0807de681db8--8c813b17a55943e3bde3918f148d7961 f77eba7d431d47a39a2c07b37775a07f 8c813b17a55943e3bde3918f148d7961--f77eba7d431d47a39a2c07b37775a07f c43aad4114884ddc9d8188799c445790 f77eba7d431d47a39a2c07b37775a07f--c43aad4114884ddc9d8188799c445790 8173ef64504746e391c127ff9489b16e X c43aad4114884ddc9d8188799c445790--8173ef64504746e391c127ff9489b16e 59e161f422c64bae8e150b3a0e09ed05 8173ef64504746e391c127ff9489b16e--59e161f422c64bae8e150b3a0e09ed05 6a9f3c0a4adc425282f97e2e5976789c 59e161f422c64bae8e150b3a0e09ed05--6a9f3c0a4adc425282f97e2e5976789c 21c018d12e0d43e28d4e01be74aea4e4 X 6a9f3c0a4adc425282f97e2e5976789c--21c018d12e0d43e28d4e01be74aea4e4 474fd4ae1d794dfa93117691f3264029 X 21c018d12e0d43e28d4e01be74aea4e4--474fd4ae1d794dfa93117691f3264029 ce28dbfa1c0e4e84a863aa23a4a6f5ee 474fd4ae1d794dfa93117691f3264029--ce28dbfa1c0e4e84a863aa23a4a6f5ee b1c4f89fae764d1793e452f3969c6637 ce28dbfa1c0e4e84a863aa23a4a6f5ee--b1c4f89fae764d1793e452f3969c6637 c7241f301f394a86b5e2fe8bfd9d87c8 X b1c4f89fae764d1793e452f3969c6637--c7241f301f394a86b5e2fe8bfd9d87c8 c7241f301f394a86b5e2fe8bfd9d87c8--941db546a4ea421ab12aeea4875f172f <p>The output circuit displays three groups of system Hamiltonian evolutions which account for global-phases and single-qubit detunings related to the mapping between the \\(Z\\) and \\(N\\) operators. Optionally, global phases can be ignored.</p> <p>In general, the mapping of a \\(n\\)-qubit Ising Hamiltonian to another will require at most \\(n(n-1)\\) evolutions. The transformed circuit performs these evolutions for specific times that are computed from the solution of a linear system of equations involving the set of interactions in the target and build Hamiltonians.</p> <p>In this case, the mapping is exact when using the step-wise DAQC strategy (<code>Strategy.SDAQC</code>) available in Qadence. In banged DAQC (<code>Strategy.BDAQC</code>) the mapping is approximate, but easier to implement on a physical device with always-on interactions such as neutral-atom systems.</p> <p>Just as before, the transformed Ising circuit can be checked to exactly recover the <code>CPHASE</code> gate:</p> <pre><code># CPHASE on (i, j), Identity on third qubit:\ncphase_matrix = block_to_tensor(kron(CPHASE(i, j, phi), I(k)))\n\n# CPHASE using the transformed circuit:\ncphase_evo_matrix = block_to_tensor(transformed_ising)\n\n# Check that it implements the CPHASE.\n# Will fail if global phases are ignored.\n</code></pre> <pre><code>cphase_matrix == cphase_evo_matrix : True\n</code></pre> <p>The <code>CNOT</code> gate can now finally be built:</p> <pre><code>from qadence import equivalent_state, run, sample\n\ncnot_daqc = chain(\n    H(j),\n    transformed_ising,\n    H(j)\n)\n\n# And finally apply the CNOT on a specific 3-qubit initial state:\ninit_state = product_state(\"101\")\n\n# Check we get an equivalent wavefunction\nwf_cnot = run(n_qubits, block=cnot_target, state=init_state)\nwf_daqc = run(n_qubits, block=cnot_daqc, state=init_state)\n\n# Visualize the CNOT bit-flip in samples.\n</code></pre> <pre><code>wf_cnot == wf_dacq : True\nsample cnot_target = [OrderedCounter({'111': 100})]\nsample cnot_dacq = [OrderedCounter({'111': 100})]\n</code></pre> <p>As one can see, a <code>CNOT</code> operation has been succesfully implemented on the desired target qubits by using only the global system as the building block Hamiltonian and single-qubit rotations. Decomposing a single digital gate into an Ising Hamiltonian serves as a proof of principle for the potential of this technique to represent universal quantum computation.</p>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#technical-details-on-the-daqc-transformation","title":"Technical details on the DAQC transformation","text":"<ul> <li>The mapping between target generator and final circuit is performed by solving a linear system of size \\(n(n-1)\\) where \\(n\\) is the number of qubits, so it can be computed efficiently (i.e., with a polynomial cost in the number of qubits).</li> <li>The linear system to be solved is actually not invertible for \\(n=4\\) qubits. This is very specific edge case requiring a workaround, that is currently not yet implemented.</li> <li>As mentioned, the final circuit has at most \\(n(n-1)\\) slices, so there is at most a quadratic overhead in circuit depth.</li> </ul> <p>Finally, and most important to its usage:</p> <ul> <li>The target Hamiltonian should be sufficiently represented in the building block Hamiltonian.</li> </ul> <p>To illustrate this point, consider the following target and build Hamiltonians:</p> <pre><code># Interaction between qubits 0 and 1\ngen_target = 1.0 * (Z(0) @ Z(1))\n\n# Fixed interaction between qubits 1 and 2, and customizable between 0 and 1\ndef gen_build(g_int):\n    return g_int * (Z(0) @ Z(1)) + 1.0 * (Z(1) @ Z(2))\n</code></pre> <p>And now we perform the DAQC transform by setting <code>g_int=1.0</code>, exactly matching the target Hamiltonian:</p> <pre><code>transformed_ising = daqc_transform(\n    n_qubits=3,\n    gen_target=gen_target,\n    t_f=1.0,\n    gen_build=gen_build(g_int=1.0),\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_a0ec4c1f23a24706b714a173b4405900 cluster_e59339ed3f30466dbc2cfb38da2ccb11 011d96e0ad5249b49749779a75a0d2c2 0 6a64b2aefd114cb09565bead0ea9b254 X 011d96e0ad5249b49749779a75a0d2c2--6a64b2aefd114cb09565bead0ea9b254 1e99d7a7b1ad485fb1f8be994754b12c 1 6a81566eb6ab4664a163dec12a48e416 HamEvo 6a64b2aefd114cb09565bead0ea9b254--6a81566eb6ab4664a163dec12a48e416 3794ff96ca1843c787b27b1abe3eee1b X 6a81566eb6ab4664a163dec12a48e416--3794ff96ca1843c787b27b1abe3eee1b 34dda5888b1c4760b1a305fde7419861 3794ff96ca1843c787b27b1abe3eee1b--34dda5888b1c4760b1a305fde7419861 f20cbbf417f74644b33bfd46614a7905 HamEvo 34dda5888b1c4760b1a305fde7419861--f20cbbf417f74644b33bfd46614a7905 f04ab5dcef1747779e607404c78c9627 f20cbbf417f74644b33bfd46614a7905--f04ab5dcef1747779e607404c78c9627 beea44b559ea4cf68dad74e77cd91e06 f04ab5dcef1747779e607404c78c9627--beea44b559ea4cf68dad74e77cd91e06 370679d5c71848428368d38d7221dde4 17619ae585d94b6a8b1e7c1748ba6b8f 1e99d7a7b1ad485fb1f8be994754b12c--17619ae585d94b6a8b1e7c1748ba6b8f 13480598af2d4b37bec4e2a228cffda0 2 54b51477ab544ad39c02b6a56d32b70a t = -0.50 17619ae585d94b6a8b1e7c1748ba6b8f--54b51477ab544ad39c02b6a56d32b70a c913f9e938134ce89bfd642df8d8ffea 54b51477ab544ad39c02b6a56d32b70a--c913f9e938134ce89bfd642df8d8ffea 41fc75aaf8164d999e9442fb6b600538 X c913f9e938134ce89bfd642df8d8ffea--41fc75aaf8164d999e9442fb6b600538 d6136baa648543e58697abc258a1ad2b t = -0.50 41fc75aaf8164d999e9442fb6b600538--d6136baa648543e58697abc258a1ad2b 66e98c27eaa549f8ae1ae020ba78447e X d6136baa648543e58697abc258a1ad2b--66e98c27eaa549f8ae1ae020ba78447e 66e98c27eaa549f8ae1ae020ba78447e--370679d5c71848428368d38d7221dde4 b5ca557a42f14a3ca150946c57c582e6 c5c34811630944e2a95e57c25c4e9d66 X 13480598af2d4b37bec4e2a228cffda0--c5c34811630944e2a95e57c25c4e9d66 a39109cc888e4f30ab4f33f83bc90775 c5c34811630944e2a95e57c25c4e9d66--a39109cc888e4f30ab4f33f83bc90775 2bbf52fd573646b6b9f1d078e4d47bd1 X a39109cc888e4f30ab4f33f83bc90775--2bbf52fd573646b6b9f1d078e4d47bd1 558e10ddb0fc4b0e895cfd477cc95ce5 X 2bbf52fd573646b6b9f1d078e4d47bd1--558e10ddb0fc4b0e895cfd477cc95ce5 7d7e1ff35bc247b7b076a3d99308c8d9 558e10ddb0fc4b0e895cfd477cc95ce5--7d7e1ff35bc247b7b076a3d99308c8d9 f1cbebd120c241a2a0ba729e5e8a5c68 X 7d7e1ff35bc247b7b076a3d99308c8d9--f1cbebd120c241a2a0ba729e5e8a5c68 f1cbebd120c241a2a0ba729e5e8a5c68--b5ca557a42f14a3ca150946c57c582e6 <p>Now, if the interaction between qubits 0 and 1 is weakened in the build Hamiltonian:</p> <pre><code>transformed_ising = daqc_transform(\n    n_qubits=3,\n    gen_target=gen_target,\n    t_f=1.0,\n    gen_build=gen_build(g_int=0.001),\n)\n\n# display(transformed_ising)\n</code></pre> %3 cluster_080c80bd41fb4482b433f526b71190b4 cluster_f7ba2a11109d47cb8299da71e81f9401 fc4e1d415e1c4833addf4ead25b9a732 0 797b9f41f37940b0a78205f6b99595a5 X fc4e1d415e1c4833addf4ead25b9a732--797b9f41f37940b0a78205f6b99595a5 3c307c48227342fa84c77df0d190e7c6 1 24f216f551514388a824f021296a1180 HamEvo 797b9f41f37940b0a78205f6b99595a5--24f216f551514388a824f021296a1180 96183b7c86f54b6382bf4526e22ad599 X 24f216f551514388a824f021296a1180--96183b7c86f54b6382bf4526e22ad599 f77885ae72e7490d8380c38a12b63829 96183b7c86f54b6382bf4526e22ad599--f77885ae72e7490d8380c38a12b63829 81968d1a282a4ae3a315f783033003fd HamEvo f77885ae72e7490d8380c38a12b63829--81968d1a282a4ae3a315f783033003fd 55609d906b214b02841395570b8b7550 81968d1a282a4ae3a315f783033003fd--55609d906b214b02841395570b8b7550 46069258712e4e0987b175c53f41c43f 55609d906b214b02841395570b8b7550--46069258712e4e0987b175c53f41c43f 37ef6ca1f292463e8554f812ae56c9a7 4bd44b04592b4981a2ce5c0c585dadc1 3c307c48227342fa84c77df0d190e7c6--4bd44b04592b4981a2ce5c0c585dadc1 ef40df51711346c0b4cc632e776a03eb 2 e3737c9b4a14471493d52bad32e503e3 t = -500. 4bd44b04592b4981a2ce5c0c585dadc1--e3737c9b4a14471493d52bad32e503e3 df0ec47224e64255b0f55fde04bb4e2a e3737c9b4a14471493d52bad32e503e3--df0ec47224e64255b0f55fde04bb4e2a 222a85f2a174471ab06e483bf797ab23 X df0ec47224e64255b0f55fde04bb4e2a--222a85f2a174471ab06e483bf797ab23 48db01ff0303475db9a4b83e4939c882 t = -500. 222a85f2a174471ab06e483bf797ab23--48db01ff0303475db9a4b83e4939c882 de84704b8da74f11a234fb1a77c7b262 X 48db01ff0303475db9a4b83e4939c882--de84704b8da74f11a234fb1a77c7b262 de84704b8da74f11a234fb1a77c7b262--37ef6ca1f292463e8554f812ae56c9a7 c54985cc0370487b9b595c665100248f bca1523bb902442392a2549cb8084b86 X ef40df51711346c0b4cc632e776a03eb--bca1523bb902442392a2549cb8084b86 c24fe8eadbdc4e8fbeb1140908d66eda bca1523bb902442392a2549cb8084b86--c24fe8eadbdc4e8fbeb1140908d66eda 0c8b9a27a4cd435da8ce9ae34bda1b1a X c24fe8eadbdc4e8fbeb1140908d66eda--0c8b9a27a4cd435da8ce9ae34bda1b1a 1712c43328d34df6829265d62f528c7c X 0c8b9a27a4cd435da8ce9ae34bda1b1a--1712c43328d34df6829265d62f528c7c 19d9ac05af2f4d41a227fab4254ae319 1712c43328d34df6829265d62f528c7c--19d9ac05af2f4d41a227fab4254ae319 a26eff8b39c84e2a8607d791c3eb215d X 19d9ac05af2f4d41a227fab4254ae319--a26eff8b39c84e2a8607d791c3eb215d a26eff8b39c84e2a8607d791c3eb215d--c54985cc0370487b9b595c665100248f <p>The times slices using the build Hamiltonian need now to evolve for much longer to represent the same interaction since it is not sufficiently represented in the building block Hamiltonian.</p> <p>In the limit where that interaction is not present, the transform will not work:</p> <pre><code>try:\n    transformed_ising = daqc_transform(\n        n_qubits=3,\n        gen_target=gen_target,\n        t_f=1.0,\n        gen_build=gen_build(g_int = 0.0),\n    )\nexcept ValueError as error:\n    print(\"Error:\", error)\n</code></pre> <pre><code>Error: Incompatible interactions between target and build Hamiltonians.\n</code></pre>"},{"location":"tutorials/digital_analog_qc/daqc-cnot/#references","title":"References","text":"<ol> <li> <p>Dodd et al., Universal quantum computation and simulation using any entangling Hamiltonian and local unitaries, PRA 65, 040301 (2002). \u21a9</p> </li> <li> <p>Parra-Rodriguez et al., Digital-Analog Quantum Computation, PRA 101, 022305 (2020). \u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/","title":"Fitting a function with a Hamiltonian ansatz","text":"<p>In the analog QCL tutorial we used analog blocks to learn a function of interest. The analog blocks are a direct abstraction of device execution with global addressing. However, we may want to directly program an Hamiltonian-level ansatz to have a finer control on our model. In Qadence this can easily be done through digital-analog programs. In this tutorial we will solve a simple QCL problem with this approach.</p>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#setting-up-the-problem","title":"Setting up the problem","text":"<p>The example problem considered is to fit a function of interest in a specified range. Below we define and plot the function \\(f(x)=x^5\\).</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Function to fit:\ndef f(x):\n    return x**5\n\nxmin = -1.0\nxmax = 1.0\nn_test = 100\n\nx_test = torch.linspace(xmin, xmax, steps = n_test)\ny_test = f(x_test)\n\nplt.plot(x_test, y_test)\nplt.xlim((-1.1, 1.1))\nplt.ylim((-1.1, 1.1))\n</code></pre> 2025-01-08T18:01:58.375411 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#digital-analog-ansatz","title":"Digital-Analog Ansatz","text":"<p>We start by defining the register of qubits. The topology we use now will define the interactions in the entangling Hamiltonian. As an example, we can define a rectangular lattice with 6 qubits.</p> <pre><code>from qadence import Register\n\nreg = Register.rectangular_lattice(\n    qubits_row = 3,\n    qubits_col = 2,\n)\n</code></pre> <p>Inspired by the Ising interaction mode of Rydberg atoms, we can now define an interaction Hamiltonian as \\(\\mathcal{H}_{ij}=\\frac{1}{r_{ij}^6}N_iN_j\\), where \\(N_i=(1/2)(I_i-Z_i)\\) is the number operator and and \\(r_{ij}\\) is the distance between qubits \\(i\\) and \\(j\\). We can easily instatiate this interaction Hamiltonian from the register information:</p> <pre><code>from qadence import N, add\n\ndef h_ij(i: int, j: int):\n    return N(i)@N(j)\n\nh_int = add(h_ij(*edge)/r**6 for edge, r in reg.edge_distances.items())\n</code></pre> <p>To build the digital-analog ansatz we can make use of the standard <code>hea</code> function by specifying we want to use the <code>Strategy.SDAQC</code> and passing the Hamiltonian we created as the entangler, as see in the QML constructors tutorial. The entangling operation will be replaced by the evolution of this Hamiltonian <code>HamEvo(h_int, t)</code>, where the time parameter <code>t</code> is considered to be a variational parameter at each layer.</p> <pre><code>from qadence import hea, Strategy, RX, RY\n\ndepth = 2\n\nda_ansatz = hea(\n    n_qubits = reg.n_qubits,\n    depth = depth,\n    operations = [RX, RY, RX],\n    entangler = h_int,\n    strategy = Strategy.SDAQC,\n)\n\nprint(html_string(da_ansatz))\n</code></pre> %3 cluster_89b1d8ab624941bb8119bdf2c5817539 cluster_825b55d9c8f34fc0bbcbf8944b205116 5fe21d985e5d43b2b84d03f01887a120 0 50e21ff8267449c5bac22330997baf61 RX(theta\u2080) 5fe21d985e5d43b2b84d03f01887a120--50e21ff8267449c5bac22330997baf61 daa25264c65844798cc91db6e243ef6d 1 806da9483f3c4f9e89356c246cf5f1c1 RY(theta\u2086) 50e21ff8267449c5bac22330997baf61--806da9483f3c4f9e89356c246cf5f1c1 a24d8d1b2fc5443e8fe0a92d5c1a441d RX(theta\u2081\u2082) 806da9483f3c4f9e89356c246cf5f1c1--a24d8d1b2fc5443e8fe0a92d5c1a441d c3e501a12cee41668c29791c3f28502a a24d8d1b2fc5443e8fe0a92d5c1a441d--c3e501a12cee41668c29791c3f28502a 7335deb07a454ae69607f364465035fb RX(theta\u2081\u2088) c3e501a12cee41668c29791c3f28502a--7335deb07a454ae69607f364465035fb 75ca3aa8fd2f477a86f3dd8c0d9e6512 RY(theta\u2082\u2084) 7335deb07a454ae69607f364465035fb--75ca3aa8fd2f477a86f3dd8c0d9e6512 290716f0989a43158838e12be07f2f9e RX(theta\u2083\u2080) 75ca3aa8fd2f477a86f3dd8c0d9e6512--290716f0989a43158838e12be07f2f9e 2b856880c9824c8da4e0397cf9c55115 290716f0989a43158838e12be07f2f9e--2b856880c9824c8da4e0397cf9c55115 0c04bbc96ded4eb68e2192e5cf11278f 2b856880c9824c8da4e0397cf9c55115--0c04bbc96ded4eb68e2192e5cf11278f 8e28a7303b1b41b08237bce772bdeb84 dd8a16a5cdbf4edb89033fc73462d7ae RX(theta\u2081) daa25264c65844798cc91db6e243ef6d--dd8a16a5cdbf4edb89033fc73462d7ae 12e02e554ecf40aba69f83965d337840 2 e4730f7b326046e3b7f1b27cfb989cc2 RY(theta\u2087) dd8a16a5cdbf4edb89033fc73462d7ae--e4730f7b326046e3b7f1b27cfb989cc2 3050403a391a4c00a2bed0a72d273cb0 RX(theta\u2081\u2083) e4730f7b326046e3b7f1b27cfb989cc2--3050403a391a4c00a2bed0a72d273cb0 d1917e9e78c3470caccfb7bafbf2a740 3050403a391a4c00a2bed0a72d273cb0--d1917e9e78c3470caccfb7bafbf2a740 36baefb09a3240e9bebdb011f1bb50d0 RX(theta\u2081\u2089) d1917e9e78c3470caccfb7bafbf2a740--36baefb09a3240e9bebdb011f1bb50d0 6514b86eade847e09791e8f55ad16403 RY(theta\u2082\u2085) 36baefb09a3240e9bebdb011f1bb50d0--6514b86eade847e09791e8f55ad16403 d13e8b02579a4b97a6f9ce4fad51125b RX(theta\u2083\u2081) 6514b86eade847e09791e8f55ad16403--d13e8b02579a4b97a6f9ce4fad51125b 7280147ff9c14f41ab5065be0d8e46dd d13e8b02579a4b97a6f9ce4fad51125b--7280147ff9c14f41ab5065be0d8e46dd 7280147ff9c14f41ab5065be0d8e46dd--8e28a7303b1b41b08237bce772bdeb84 ff207b1b9e704774b02e3ff9d86cd679 84bbfa559625414ca5afbf0d8a0719ca RX(theta\u2082) 12e02e554ecf40aba69f83965d337840--84bbfa559625414ca5afbf0d8a0719ca 1339ca0908044dc5bc972f17a8183171 3 d24903f37e404b32bc1b2ec79bf31dc8 RY(theta\u2088) 84bbfa559625414ca5afbf0d8a0719ca--d24903f37e404b32bc1b2ec79bf31dc8 0bc7245b80e84835a02c82f569fa1bbe RX(theta\u2081\u2084) d24903f37e404b32bc1b2ec79bf31dc8--0bc7245b80e84835a02c82f569fa1bbe b69a1428f4f44d769043189694ca81f3 HamEvo 0bc7245b80e84835a02c82f569fa1bbe--b69a1428f4f44d769043189694ca81f3 28b180a440614fe5aa329a0e057f2b9d RX(theta\u2082\u2080) b69a1428f4f44d769043189694ca81f3--28b180a440614fe5aa329a0e057f2b9d 1aeed4c725904bfd80dd0c1bbfd62101 RY(theta\u2082\u2086) 28b180a440614fe5aa329a0e057f2b9d--1aeed4c725904bfd80dd0c1bbfd62101 767e9ae74aa142438b84e4bd8f9edf40 RX(theta\u2083\u2082) 1aeed4c725904bfd80dd0c1bbfd62101--767e9ae74aa142438b84e4bd8f9edf40 743d34089b4843c495138366da9a3bb1 HamEvo 767e9ae74aa142438b84e4bd8f9edf40--743d34089b4843c495138366da9a3bb1 743d34089b4843c495138366da9a3bb1--ff207b1b9e704774b02e3ff9d86cd679 9b8dd79227af45cba2d9fbc77d4f5e10 05430b33b1144d56983f26a4bd59d77a RX(theta\u2083) 1339ca0908044dc5bc972f17a8183171--05430b33b1144d56983f26a4bd59d77a 088283896c784071a139fec6ab7edea7 4 4ce4a805a7984feab616bf9832ea1fb4 RY(theta\u2089) 05430b33b1144d56983f26a4bd59d77a--4ce4a805a7984feab616bf9832ea1fb4 ca2521a4265648d6a13ba812606e15eb RX(theta\u2081\u2085) 4ce4a805a7984feab616bf9832ea1fb4--ca2521a4265648d6a13ba812606e15eb ddb610be97c349ae9427b5eee6128777 t = theta_t\u2080 ca2521a4265648d6a13ba812606e15eb--ddb610be97c349ae9427b5eee6128777 977af27cea174156bea83c517d2f2ee6 RX(theta\u2082\u2081) ddb610be97c349ae9427b5eee6128777--977af27cea174156bea83c517d2f2ee6 07627b42098845809d67d8ed6a56ad7c RY(theta\u2082\u2087) 977af27cea174156bea83c517d2f2ee6--07627b42098845809d67d8ed6a56ad7c d23d2412890642a8b771088a09ea1437 RX(theta\u2083\u2083) 07627b42098845809d67d8ed6a56ad7c--d23d2412890642a8b771088a09ea1437 6ad87238dccb424984002c133da74b2b t = theta_t\u2081 d23d2412890642a8b771088a09ea1437--6ad87238dccb424984002c133da74b2b 6ad87238dccb424984002c133da74b2b--9b8dd79227af45cba2d9fbc77d4f5e10 ab82030d044f4552ae6648fa20d44ae1 f0aba015176f4424967c292758c3d958 RX(theta\u2084) 088283896c784071a139fec6ab7edea7--f0aba015176f4424967c292758c3d958 03ee4ddcedc045d6a2dadb8c5af69a87 5 c8b5a029979546aba07ceb5f892e48b7 RY(theta\u2081\u2080) f0aba015176f4424967c292758c3d958--c8b5a029979546aba07ceb5f892e48b7 0445a0df2d4a40839b1659c25650fae2 RX(theta\u2081\u2086) c8b5a029979546aba07ceb5f892e48b7--0445a0df2d4a40839b1659c25650fae2 1740ff51632742ce8aaf7325f7c4dd33 0445a0df2d4a40839b1659c25650fae2--1740ff51632742ce8aaf7325f7c4dd33 01cb883b214a4d0892a8b49ca3544cf7 RX(theta\u2082\u2082) 1740ff51632742ce8aaf7325f7c4dd33--01cb883b214a4d0892a8b49ca3544cf7 423cba5828f44278adc38e7ed048a0e2 RY(theta\u2082\u2088) 01cb883b214a4d0892a8b49ca3544cf7--423cba5828f44278adc38e7ed048a0e2 ef3a97060432454d8cc431f52c18173a RX(theta\u2083\u2084) 423cba5828f44278adc38e7ed048a0e2--ef3a97060432454d8cc431f52c18173a 3bf7956bc7e647d0ad0b95c43d2233fe ef3a97060432454d8cc431f52c18173a--3bf7956bc7e647d0ad0b95c43d2233fe 3bf7956bc7e647d0ad0b95c43d2233fe--ab82030d044f4552ae6648fa20d44ae1 60fef0825b88492383207df0e764ae0f 5f15b15ff2b9412cae2f5d28400065d1 RX(theta\u2085) 03ee4ddcedc045d6a2dadb8c5af69a87--5f15b15ff2b9412cae2f5d28400065d1 0acc24a6b0fc48a4b3f7c53ff7a0d481 RY(theta\u2081\u2081) 5f15b15ff2b9412cae2f5d28400065d1--0acc24a6b0fc48a4b3f7c53ff7a0d481 cb15f4c0560f4d9386da7f9ed1f75601 RX(theta\u2081\u2087) 0acc24a6b0fc48a4b3f7c53ff7a0d481--cb15f4c0560f4d9386da7f9ed1f75601 d731aa49b5cc4578b2ddef1190a8a016 cb15f4c0560f4d9386da7f9ed1f75601--d731aa49b5cc4578b2ddef1190a8a016 00206eae48d74537a90946658cc0936f RX(theta\u2082\u2083) d731aa49b5cc4578b2ddef1190a8a016--00206eae48d74537a90946658cc0936f 3d3a160c7a2540499de29c49dfced7fc RY(theta\u2082\u2089) 00206eae48d74537a90946658cc0936f--3d3a160c7a2540499de29c49dfced7fc 3484b069814c4cc5ba871ff6d885f5a6 RX(theta\u2083\u2085) 3d3a160c7a2540499de29c49dfced7fc--3484b069814c4cc5ba871ff6d885f5a6 88444683b36a43ee9a11c5f178946dd2 3484b069814c4cc5ba871ff6d885f5a6--88444683b36a43ee9a11c5f178946dd2 88444683b36a43ee9a11c5f178946dd2--60fef0825b88492383207df0e764ae0f"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#creating-the-quantummodel","title":"Creating the QuantumModel","text":"<p>The rest of the procedure is the same as any other Qadence workflow. We start by defining a feature map for input encoding and an observable for output decoding.</p> <pre><code>from qadence import feature_map, BasisSet, ReuploadScaling\nfrom qadence import Z, I\n\nfm = feature_map(\n    n_qubits = reg.n_qubits,\n    param = \"x\",\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.TOWER,\n)\n\n# Total magnetization\nobservable = add(Z(i) for i in range(reg.n_qubits))\n</code></pre> <p>And we have all the ingredients to initialize the <code>QuantumModel</code>:</p> <pre><code>from qadence import QuantumCircuit, QuantumModel\n\ncircuit = QuantumCircuit(reg, fm, da_ansatz)\n\nmodel = QuantumModel(circuit, observable = observable)\n</code></pre>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#training-the-model","title":"Training the model","text":"<p>We can now train the model. We use a set of 20 equally spaced training points.</p> <pre><code># Chebyshev FM does not accept x = -1, 1\nxmin = -0.99\nxmax = 0.99\nn_train = 20\n\nx_train = torch.linspace(xmin, xmax, steps = n_train)\ny_train = f(x_train)\n\n# Initial model prediction\ny_pred_initial = model.expectation({\"x\": x_test}).detach()\n</code></pre> <p>And we use a simple custom training loop.</p> <pre><code>criterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nn_epochs = 200\n\ndef loss_fn(x_train, y_train):\n    out = model.expectation({\"x\": x_train})\n    loss = criterion(out.squeeze(), y_train)\n    return loss\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_fn(x_train, y_train)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/digital_analog_qc/digital-analog-qcl/#results","title":"Results","text":"<p>Finally we can plot the resulting trained model.</p> <pre><code>y_pred_final = model.expectation({\"x\": x_test}).detach()\n\nplt.plot(x_test, y_pred_initial, label = \"Initial prediction\")\nplt.plot(x_test, y_pred_final, label = \"Final prediction\")\nplt.scatter(x_train, y_train, label = \"Training points\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.xlim((-1.1, 1.1))\nplt.ylim((-1.1, 1.1))\n</code></pre> 2025-01-08T18:02:06.867555 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/","title":"Pulse-level programming with Pulser","text":"<p>Qadence offers a direct interface with Pulser<sup>1</sup>, an open-source pulse-level interface written in Python and specifically designed for programming neutral atom quantum computers.</p> <p>Using directly Pulser requires advanced knowledge on pulse-level programming and on how neutral atom devices work. Qadence abstracts this complexity out by using the familiar block-based interface for building pulse sequences in Pulser while leaving the possibility to directly manipulate them if required by, for instance, optimal pulse shaping.</p> <p>Note</p> <p>The Pulser backend is still experimental and the interface might change in the future. Please note that it does not support <code>DiffMode.AD</code>.</p> <p>Note</p> <p>With the Pulser backend, <code>qadence</code> simulations can be executed on the cloud emulators available on the PASQAL cloud platform. In order to do so, make to have valid credentials for the PASQAL cloud platform and use the following configuration for the Pulser backend:</p> <pre><code>config = {\n    \"cloud_configuration\": {\n        \"username\": \"&lt;changeme&gt;\",\n        \"password\": \"&lt;changeme&gt;\",\n        \"project_id\": \"&lt;changeme&gt;\",  # the project should have access to emulators\n        \"platform\": \"EMU_FREE\"  # choose between `EMU_TN` and `EMU_FREE`\n    }\n}\n</code></pre> <p>For inquiries and more details on the cloud credentials, please contact info@pasqal.com.</p>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#default-qubit-interaction","title":"Default qubit interaction","text":"<p>When simulating pulse sequences written using Pulser, the underlying constructed Hamiltonian is equivalent to a digital-analog quantum computing program (see digital-analog emulation for more details) with the following interaction term:</p> \\[ \\mathcal{H}_{\\textrm{int}} = \\sum_{i&lt;j} \\frac{C_6}{|R_i - R_j|^6} \\hat{n}_i \\hat{n}_j \\] <p>where \\(C_6\\) is an interaction strength coefficient dependent on the principal quantum number of chosen the neutral atom system, \\(R_i\\) are atomic positions in Cartesian coordinates and \\(\\hat{n} = \\frac{1-\\sigma^z_i}{2}\\) the number operator.</p> <p>Note</p> <p>The Ising interaction is always-on for all computations performed with the Pulser backend. It cannot be switched off.</p>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#available-quantum-operations","title":"Available quantum operations","text":"<p>Currently, the Pulser backend supports the following operations:</p> gate description trainable parameter <code>RX</code>, <code>RY</code> Single qubit rotations. Notice that the interaction is on and this affects the resulting gate fidelity. rotation angle <code>AnalogRX</code>, <code>AnalogRY</code>, <code>AnalogRZ</code> Span a single qubit rotation among the entire register. rotation angle <code>entangle</code> Fully entangle the register. interaction time <code>AnalogInteraction</code> An idle block to to free-evolve for a duration according to the interaction. free evolution time"},{"location":"tutorials/digital_analog_qc/pulser-basic/#sequence-the-bell-state-on-a-two-qubit-register","title":"Sequence the Bell state on a two qubit register","text":"<p>The next example illustrates how to create a pulse sequence to prepare a Bell state. This is a sequence of an entanglement operation, represented as an <code>entangle</code> gate (using <code>CZ</code> interactions) in the \\(X\\)-basis and a \\(Y\\) rotation for readout in the \\(Z\\)-basis:</p> <pre><code>from qadence import chain, entangle, RY\n\nbell_state = chain(\n   entangle(\"t\", qubit_support=(0,1)),\n   RY(0, \"y\"),\n)\n</code></pre> <pre><code>bell_state = ChainBlock(0,1)\n\u251c\u2500\u2500 AnalogEntanglement(t=0.25022735767019744, support=(0, 1))\n\u2514\u2500\u2500 RY(0) [params: ['y']]\n</code></pre> <p>Next, a <code>Register</code> with two qubits is combined with the resulting <code>ChainBlock</code> to form a circuit. Then, the <code>QuantumModel</code> converts the circuit into a proper parametrized pulse sequence with the Pulser backend. Supplying the parameter values allows to sample the pulse sequence outcome:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom qadence import Register, QuantumCircuit, QuantumModel, PI\n\nregister = Register.line(2, spacing = 8.0)  # Two qubits with a distance of 8\u00b5m\ncircuit = QuantumCircuit(register, bell_state)\nmodel = QuantumModel(circuit, backend=\"pulser\", diff_mode=\"gpsr\")\n\nparams = {\n    \"t\": torch.tensor([1000]),  # ns\n    \"y\": torch.tensor([3*PI/2]),\n}\n\n# Return the final state vector\nfinal_vector = model.run(params)\n\n# Sample from the result state vector\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> <pre><code>final_vector = tensor([[-0.7114-0.0169j, -0.0338+0.0155j,  0.0110-0.0457j,  0.6631-0.2245j]])\nsample = Counter({'11': 26, '00': 23, '10': 1})\n</code></pre> <p>Plot the distribution:</p> <p><pre><code>\n</code></pre> 2025-01-08T18:02:07.076525 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/  One can visualise the pulse sequence with different parameters using the <code>assign_paramters</code> method.</p> <pre><code>model.assign_parameters(params).draw(show=False)\n</code></pre> 2025-01-08T18:02:07.197131 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#change-device-specifications","title":"Change device specifications","text":"<p>At variance with other backends, Pulser provides the concept of <code>Device</code>. A <code>Device</code> instance encapsulates all the properties for the definition of a real neutral atoms processor, including but not limited to the maximum laser amplitude for pulses, the maximum distance between two qubits and the maximum duration of the pulse. For more information, please check this tutorial.</p> <p>Qadence offers a simplified interface with only two devices which are detailed here:</p> <ul> <li><code>IDEALIZED</code> (default): ideal device which should be used only for testing purposes. It does not restrict the simulation of pulse sequences.</li> <li><code>REALISTIC</code>: device specification close to real neutral atom quantum processors.</li> </ul> <p>Note</p> <p>If you want to perform simulations closer to the specifications of real neutral atom machines, always select the <code>REALISTIC</code> device.</p> <p>One can use the <code>Configuration</code> of the Pulser backend to select the appropriate device:</p> <pre><code>from qadence import BackendName, DiffMode\nfrom qadence import RealisticDevice\n\n# Choose a realistic device\nregister = Register.line(2, spacing = 8.0, device_specs = RealisticDevice())\n\ncircuit = QuantumCircuit(register, bell_state)\n\nmodel = QuantumModel(\n    circuit,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR,\n)\n\nparams = {\n    \"t\": torch.tensor([1000]),  # ns\n    \"y\": torch.tensor([3*PI/2]),\n}\n\n# Sample from the result state vector\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> <pre><code>sample = Counter({'00': 27, '11': 22, '10': 1})\n</code></pre>"},{"location":"tutorials/digital_analog_qc/pulser-basic/#create-a-custom-gate","title":"Create a custom gate","text":"<p>A major advantage of the block-based interface in Qadence is the ease to compose complex operations from a restricted set of primitive ones. In the following, a custom entanglement operation is used as an example.</p> <p>The operation consists of moving all the qubits to the \\(X\\)-basis. This is realized when the atomic interaction performs a controlled-\\(Z\\) operation during the free evolution. As seen before, this is implemented with the <code>AnalogInteraction</code> and <code>AnalogRY</code> blocks together with appropriate parameters.</p> <pre><code>from qadence import AnalogRY, chain, AnalogInteraction\n\n# Custom entanglement operation.\ndef my_entanglement(duration):\n    return chain(\n        AnalogRY(-PI / 2),\n        AnalogInteraction(duration)\n    )\n\nprotocol = chain(\n   my_entanglement(\"t\"),\n   RY(0, \"y\"),\n)\n\nregister = Register.line(2, spacing = 8.0)\ncircuit = QuantumCircuit(register, protocol)\nmodel = QuantumModel(circuit, backend=BackendName.PULSER, diff_mode=DiffMode.GPSR)\n\nparams = {\n    \"t\": torch.tensor([500]),  # ns\n    \"y\": torch.tensor([PI / 2]),\n}\n\nsample = model.sample(params, n_shots=50)[0]\n</code></pre> 2025-01-08T18:02:07.578036 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#digital-analog-qnn-circuit","title":"Digital-analog QNN circuit","text":"<p>Finally, let's put all together by constructing a digital-analog version of a quantum neural network circuit with feature map and variational ansatz.</p> <pre><code>from qadence import kron, feature_map, BasisSet\nfrom qadence.operations import RX, RY, AnalogRX\n\nhea_one_layer = chain(\n    kron(RY(0, \"th00\"), RY(1, \"th01\")),\n    kron(RX(0, \"th10\"), RX(1, \"th11\")),\n    kron(RY(0, \"th20\"), RY(1, \"th21\")),\n    entangle(\"t\", qubit_support=(0,1)),\n)\n\nprotocol = chain(\n    feature_map(1, param=\"x\", fm_type=BasisSet.FOURIER),\n    hea_one_layer,\n    AnalogRX(PI/4)\n)\n\nregister = Register.line(2, spacing=8.0)\ncircuit = QuantumCircuit(register, protocol)\nmodel = QuantumModel(circuit, backend=BackendName.PULSER, diff_mode=DiffMode.GPSR)\n\nparams = {\n    \"x\": torch.tensor([0.8]), # rad\n    \"t\": torch.tensor([900]), # ns\n    \"th00\":  torch.rand(1), # rad\n    \"th01\":  torch.rand(1), # rad\n    \"th10\":  torch.rand(1), # rad\n    \"th11\":  torch.rand(1), # rad\n    \"th20\":  torch.rand(1), # rad\n    \"th21\":  torch.rand(1), # rad\n}\n\nmodel.assign_parameters(params).draw(draw_phase_area=True, show=False)\n</code></pre> 2025-01-08T18:02:07.739598 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/digital_analog_qc/pulser-basic/#references","title":"References","text":"<ol> <li> <p>Pulser: An open-source package for the design of pulse sequences in programmable neutral-atom arrays \u21a9</p> </li> </ol>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/","title":"Restricted local addressability","text":""},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#physics-behind-semi-local-addressing-patterns","title":"Physics behind semi-local addressing patterns","text":"<p>Recall that in Qadence the general neutral-atom Hamiltonian for a set of \\(n\\) interacting qubits is given by expression</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm drive} + \\mathcal{H}_{\\rm int} = \\sum_{i=0}^{n-1}\\left(\\mathcal{H}^\\text{d}_{i}(t) + \\sum_{j&lt;i}\\mathcal{H}^\\text{int}_{ij}\\right) \\] <p>as is described in detail in the analog interface basics documentation.</p> <p>The driving Hamiltonian term in priciple can model any local single-qubit rotation by addressing each qubit individually. However, some neutral-atom devices offer restricted local addressability using devices called spatial light modulators (SLMs).</p> <p>We refer to this regime as semi-local addressability. In this regime, the individual qubit addressing is restricted to a pattern of targeted qubits which is kept fixed during the execution of the quantum circuit. More formally, the addressing pattern appears as an additional term in the neutral-atom Hamiltonian:</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm drive} + \\mathcal{H}_{\\rm int} + \\mathcal{H}_{\\rm local} \\] <p>where \\(\\mathcal{H}_{\\rm pattern}\\) is given by</p> \\[ \\mathcal{H}_{\\rm local} = \\sum_{i=0}^{n-1}\\left(-\\Delta w_i^{\\rm det} \\hat{n}_i + \\Gamma w_i^{\\rm drive} \\hat{\\sigma}^x_i\\right). \\] <p>Here \\(\\Delta\\) specifies the maximal negative detuning that each qubit in the register can be exposed to. The weight \\(w_i^{\\rm det}\\in [0, 1]\\) determines the actual value of detuning that \\(i\\)-th qubit feels and this way the detuning pattern is emulated. Similarly, for the amplitude pattern \\(\\Gamma\\) determines the maximal additional positive drive that acts on qubits. In this case the corresponding weights \\(w_i^{\\rm drive}\\) can vary in the interval \\([0, 1]\\).</p> <p>Using the detuning and amplitude patterns described above one can modify the behavior of a selected set of qubits, thus achieving semi-local addressing.</p> <p>Qadence implements semi-local addressing in two different flavors of increasing complexity: either as a circuit constructor or directly as a pattern added to the general evolution Hamiltonian described by the circuit.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#using-circuit-constructors","title":"Using circuit constructors","text":"<p>The <code>rydberg_hea</code> constructor routine allows to build a circuit instance implementing a basic version of the Hamiltonian evolution described above where both \\(\\Delta\\) and \\(\\tilde{\\Omega}\\) coefficients are considered constants. Furthemore, no global drive and detuning are explicitly added to the Hamiltonian. Therefore, the final Hamiltonian generator of the circuit reads as follows:</p> \\[ \\mathcal{H} = \\mathcal{H}_{\\rm local}(w^{\\rm drive}, w^{\\rm det}) + \\mathcal{H}_{\\textrm{int}} \\] <p>This implementation does not perform any checks on the weights normalization, thus making it not realistic. This implies that global drive and detuning can be retrieved by appropriately choosing the weights.</p> <p>You can easily create a Rydberg hardware efficient ansatz implementing multiple layers of the evolution generated by the local addressing Hamiltonian:</p> \\[ \\mathcal{H}_{\\rm evo} = \\sum_j \\mathcal{H}_{\\textrm{local}}(w_{j}^{\\rm drive}, w_{j}^{\\rm det}) \\] <p>Notice that in real-device implementation, one layer only is usually achievable.</p> <pre><code>import qadence as qd\nfrom qadence import rydberg_hea, rydberg_hea_layer\n\nn_qubits = 4\nn_layers = 2\nregister = qd.Register.line(n_qubits)\n\n# ansatz constructor\n# the evolution time is parametrized for each layer of the evolution\nansatz = rydberg_hea(\n    register,\n    n_layers=n_layers,  # number of subsequent layers of Hamiltonian evolution\n    addressable_detuning=True,  # make the local detuning weights w_i^{det} as variational parameters\n    addressable_drive=True, # make the local drive weights w_i^{drv} as variational parameters\n    tunable_phase=True, # make the phase \\phi as a variational parameter\n)\n\n# alternatively, a single ansatz layer can also be created for\n# better flexibility\n\n# these can be variational parameters\ntevo_drive = 1.0  # evolution time for the locally addressed drive term\ntevo_det = 1.0 # evolution time for the locally addressed detuning term\ntevo_int = 1.0  # evolution time for the interaction term\n\n# these can be list of variational parameters\nweights_drive = [0.0, 0.25, 0.5, 0.25]\nweights_det = [0.0, 0.0, 0.5, 0.5]\n\nansatz_layer = rydberg_hea_layer(\n    register,\n    tevo_det,\n    tevo_drive,\n    tevo_int,\n    detunings=weights_det,\n    drives=weights_drive,\n)\n</code></pre> <pre><code>\n</code></pre> <p>This circuit constructor is meant to be used with fully differentiable backends such as <code>pyqtorch</code> and mainly for quick experimentation with neutral atom compatible ansatze.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#using-addressing-patterns","title":"Using addressing patterns","text":"<p>In Qadence semi-local addressing patterns can be created by either specifying fixed values for the weights of the qubits being addressed or defining them as trainable parameters that can be optimized later in some training loop. Semi-local addressing patterns can be defined with the <code>AddressingPattern</code> dataclass.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#fixed-weights","title":"Fixed weights","text":"<p>With fixed weights, detuning/amplitude addressing patterns can be defined in the following way:</p> <pre><code>import torch\nfrom qadence.analog import AddressingPattern\n\nn_qubits = 3\n\nw_det = {0: 0.9, 1: 0.5, 2: 1.0}\nw_amp = {0: 0.1, 1: 0.4, 2: 0.8}\ndet = 9.0\namp = 6.5\npattern = AddressingPattern(\n    n_qubits=n_qubits,\n    det=det,\n    amp=amp,\n    weights_det=w_det,\n    weights_amp=w_amp,\n)\n</code></pre> <p>If only detuning or amplitude pattern is needed - the corresponding weights for all qubits can be set to 0.</p> <p>The created addressing pattern can now be passed as an argument to any Qadence device class, or to the <code>IdealDevice</code> or <code>RealisticDevice</code> to make use of the pre-defined options in those devices,</p> <pre><code>import torch\nfrom qadence import (\n    AnalogRX,\n    AnalogRY,\n    BackendName,\n    DiffMode,\n    Parameter,\n    QuantumCircuit,\n    QuantumModel,\n    Register,\n    chain,\n    total_magnetization,\n    IdealDevice,\n    PI\n)\n\n# define register and circuit\nspacing = 8.0\nx = Parameter(\"x\")\nblock = chain(AnalogRX(3 * x), AnalogRY(0.5 * x))\n\ndevice_specs = IdealDevice(pattern = pattern)\n\nreg = Register.line(\n    n_qubits,\n    spacing=spacing,\n    device_specs=device_specs,\n)\n\ncirc = QuantumCircuit(reg, block)\n\nobs = total_magnetization(n_qubits)\n\nmodel_pyq = QuantumModel(\n    circuit=circ, observable=obs, backend=BackendName.PYQTORCH, diff_mode=DiffMode.AD\n)\n\n# calculate expectation value of the circuit for random input value\nvalue = {\"x\": 1.0 + torch.rand(1)}\nexpval_pyq = model_pyq.expectation(values = value)\n</code></pre>   Expectation value on PyQ:  tensor([2.1555])     <p>The same configuration can also be seamlessly used to create a model with the Pulser backend.</p> <pre><code>model_pulser = QuantumModel(\n    circuit=circ,\n    observable=obs,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR\n)\n\n# calculate expectation value of the circuit for same random input value\nexpval_pulser = model_pulser.expectation(values = value)\n</code></pre>   Expectation value on Pulser:  tensor([2.1534])     <p>Note that by default the addressing pattern terms are added to every analog operation in the circuit. However, it is possible to turn the addressing pattern off for specific operations by passing <code>add_pattern=False</code> in the operation. For example <code>AnalogRX(pi)</code> will get the extra addressing pattern term, but <code>AnalogRX(pi, add_pattern=False)</code> will not. This is currently only implemented for the PyQTorch backend. If an addressing pattern is specified for the Pulser backend, it will be added to all the blocks.</p>"},{"location":"tutorials/digital_analog_qc/semi-local-addressing/#trainable-weights","title":"Trainable weights","text":"<p>Note</p> <p>Trainable parameters currently are supported only by <code>pyqtorch</code> backend.</p> <p>Since both the maximum detuning/amplitude value of the addressing pattern and the corresponding weights can be user specified, they can be variationally used in some QML setting. This can be achieved by defining pattern weights as trainable <code>Parameter</code> instances or strings specifying weight names.</p> <pre><code>n_qubits = 3\nreg = Register.line(n_qubits, spacing=8.0)\n\n# some random target function value\nf_value = torch.rand(1)\n\n# define trainable addressing pattern\nw_amp = {i: f\"w_amp{i}\" for i in range(n_qubits)}\nw_det = {i: f\"w_det{i}\" for i in range(n_qubits)}\namp = \"max_amp\"\ndet = \"max_det\"\n\npattern = AddressingPattern(\n    n_qubits=n_qubits,\n    det=det,\n    amp=amp,\n    weights_det=w_det,\n    weights_amp=w_amp,\n)\n\n# some fixed analog operation\nblock = AnalogRX(PI)\n\ndevice_specs = IdealDevice(pattern = pattern)\n\nreg = Register.line(\n    n_qubits,\n    spacing=spacing,\n    device_specs=device_specs,\n)\n\ncirc = QuantumCircuit(reg, block)\n\n# define quantum model\nobs = total_magnetization(n_qubits)\nmodel = QuantumModel(circuit=circ, observable=obs, backend=BackendName.PYQTORCH)\n\n# prepare for training\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nloss_criterion = torch.nn.MSELoss()\nn_epochs = 200\nloss_save = []\n\n# train model\nfor _ in range(n_epochs):\n    optimizer.zero_grad()\n    out = model.expectation()\n    loss = loss_criterion(f_value, out)\n    loss.backward()\n    optimizer.step()\n    loss_save.append(loss.item())\n\n# get final results\nf_value_model = model.expectation().detach()\n\nassert torch.isclose(f_value, f_value_model, atol=0.01)\n</code></pre>   The target function value:  tensor([0.5492]) The trained function value:  tensor([[0.5492]])    <p>Here, the expectation value of the circuit is fitted by varying the parameters of the addressing pattern.</p>"},{"location":"tutorials/qml/","title":"Variational quantum algorithms","text":"<p>Variational algorithms on noisy devices and quantum machine learning (QML)[^1] in particular are one of the main target applications for Qadence. For this purpose, the library offers both flexible symbolic expressions for the quantum circuit parameters via <code>sympy</code> (see here for more details) and native automatic differentiation via integration with PyTorch deep learning framework.</p> <p>Furthermore, Qadence offers a wide range of utilities for helping building and researching quantum machine learning algorithms, including:</p> <ul> <li>a set of constructors for circuits commonly used in quantum machine learning such as feature maps and ansatze</li> <li>a set of tools for training and optimizing quantum neural networks and loading classical data into a QML algorithm</li> </ul>"},{"location":"tutorials/qml/#some-simple-examples","title":"Some simple examples","text":"<p>Qadence symbolic parameter interface allows to create arbitrary feature maps to encode classical data into quantum circuits with an arbitrary non-linear function embedding for the input values:</p> <pre><code>import qadence as qd\nfrom qadence.operations import *\nimport torch\nfrom sympy import acos\n\nn_qubits = 4\n\n# Example feature map, also directly available with the `feature_map` function\nfp = qd.FeatureParameter(\"phi\")\nfm = qd.kron(RX(i, acos(fp)) for i in range(n_qubits))\n\n# the key in the dictionary must correspond to\n# the name of the assigned to the feature parameter\ninputs = {\"phi\": torch.rand(3)}\nsamples = qd.sample(fm, values=inputs)\n</code></pre> <pre><code>samples = OrderedCounter({'0000': 99, '0010': 1})\n</code></pre> <p>The <code>constructors.feature_map</code> module provides convenience functions to build commonly used feature maps where the input parameter is encoded in the single-qubit gates rotation angle. This function will be further demonstrated in the QML constructors tutorial.</p> <p>Furthermore, Qadence is natively integrated with PyTorch automatic differentiation engine thus Qadence quantum models can be used seamlessly in a PyTorch workflow.</p> <p>Let's create a quantum neural network model using the feature map just defined, a digital-analog variational ansatz (also explained here) and a simple observable \\(X(0) \\otimes X(1)\\). We use the convenience <code>QNN</code> quantum model abstraction.</p> <pre><code>ansatz = qd.hea(n_qubits, strategy=\"sDAQC\")\ncircuit = qd.QuantumCircuit(n_qubits, fm, ansatz)\nobservable = qd.kron(X(0), X(1))\n\nmodel = qd.QNN(circuit, observable)\n\n# NOTE: the `QNN` is a torch.nn.Module\nassert isinstance(model, torch.nn.Module)\n</code></pre> <pre><code>True\n</code></pre> <p>Differentiation works the same way as any other PyTorch module:</p> <pre><code>values = {\"phi\": torch.rand(10, requires_grad=True)}\n\n# the forward pass of the quantum model returns the expectation\n# value of the input observable\nout = model(values)\n\n# you can compute the gradient with respect to inputs using\n# PyTorch autograd differentiation engine\ndout = torch.autograd.grad(out, values[\"phi\"], torch.ones_like(out), create_graph=True)[0]\nprint(f\"First-order derivative w.r.t. the feature parameter: \\n{dout}\")\n\n# you can also call directly a backward pass to compute derivatives with respect\n# to the variational parameters and use it for implementing variational\n# optimization\nout.sum().backward()\n</code></pre> <pre><code>Quantum model output: \ntensor([[ 0.0231],\n        [-0.0133],\n        [ 0.3699],\n        [ 0.4348],\n        [ 0.3164],\n        [ 0.2681],\n        [-0.0471],\n        [ 0.1624],\n        [ 0.2670],\n        [ 0.3393]], grad_fn=&lt;CatBackward0&gt;)\n\nFirst-order derivative w.r.t. the feature parameter: \ntensor([ 0.6922,  0.6359,  0.5828, -0.0646, -2.0352,  0.7593,  0.5677,  0.7926,\n         0.7603,  0.6604], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>To run QML on real devices, Qadence offers generalized parameter shift rules (GPSR) <sup>1</sup> for arbitrary quantum operations which can be selected when constructing the <code>QNN</code> model:</p> <pre><code>model = qd.QNN(circuit, observable, diff_mode=\"gpsr\")\nout = model(values)\n\ndout = torch.autograd.grad(out, values[\"phi\"], torch.ones_like(out), create_graph=True)[0]\nprint(f\"First-order derivative w.r.t. the feature parameter: \\n{dout}\")\n</code></pre> <pre><code>First-order derivative w.r.t. the feature parameter: \ntensor([ 0.6922,  0.6359,  0.5828, -0.0646, -2.0352,  0.7593,  0.5677,  0.7926,\n         0.7603,  0.6604], grad_fn=&lt;MulBackward0&gt;)\n</code></pre> <p>See here for more details on how the parameter shift rules implementation works in Qadence.</p>"},{"location":"tutorials/qml/#references","title":"References","text":"<p>[^1] Schuld, Petruccione, Machine learning on Quantum Computers, Springer Nature (2021)</p> <ol> <li> <p>Kyriienko et al., General quantum circuit differentiation rules \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/config_qnn/","title":"Configuring a QNN","text":"<p>In <code>qadence</code>, the <code>QNN</code> is a variational quantum model that can potentially take multi-dimensional input.</p> <p>The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>The circuit has two parts, the feature map and the ansatz. The feature map is responsible for encoding the input data into the quantum state, while the ansatz is responsible for the variational part of the model. In addition, a third part of the QNN is the observables, which is (a list of) operators that are measured at the end of the circuit.</p> <p>In QML Constructors we have seen how to construct the feature map and the ansatz. In this tutorial, we will see how to do the same using configs.</p> <p>One convenient way to construct these three parts of the model is to use the config classes, namely, <code>ObservableConfig</code>, <code>FeatureMapConfig</code>, <code>AnsatzConfig</code>. These classes allow you to specify the type of circuit and the parameters of the circuit in a structured way.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-observable","title":"Defining the Observable","text":"<p>The model output is the expectation value of the defined observable(s). We use the <code>ObservableConfig</code> class to specify the observable.</p> <p>We can specify any Hamiltonian that we want to measure at the end of the circuit. Let us say we want to measure the \\(Z\\) operator.</p> <pre><code>from qadence import observable_from_config, ObservableConfig, Z\n\nobservable_config = ObservableConfig(\n    detuning=Z,\n    scale=3.0,\n    shift=-1.0,\n)\n\nobservable = observable_from_config(register=4, config=observable_config)\n</code></pre> %3 cluster_dab9dcd7f443471c88f9a2d760df01ac dc8b1f4811d946b29afac44aa03a541c 0 4b211e98187d42db993351b0a309dbb4 dc8b1f4811d946b29afac44aa03a541c--4b211e98187d42db993351b0a309dbb4 e6b198d8068845bdbdb3e82e3206d073 1 207bd17268fa4b77b375e7aac58f115c 4b211e98187d42db993351b0a309dbb4--207bd17268fa4b77b375e7aac58f115c 9e30825c95664caf8be66d37ad5b394a f995ee2c28fa473680d166641c15cb79 AddBlock e6b198d8068845bdbdb3e82e3206d073--f995ee2c28fa473680d166641c15cb79 dd30121e1987467e98d539581ce04698 2 f995ee2c28fa473680d166641c15cb79--9e30825c95664caf8be66d37ad5b394a 50dcdbaf729443c4b9b63acde2b512c5 dc517641c23e450fbc501974dd751abc dd30121e1987467e98d539581ce04698--dc517641c23e450fbc501974dd751abc 50b744749933476a84f49aa39e4bd4f4 3 dc517641c23e450fbc501974dd751abc--50dcdbaf729443c4b9b63acde2b512c5 bb4e2c09816744c2a4879efee3fa1931 8044c306810041258e0ab4a5b4017ec3 50b744749933476a84f49aa39e4bd4f4--8044c306810041258e0ab4a5b4017ec3 8044c306810041258e0ab4a5b4017ec3--bb4e2c09816744c2a4879efee3fa1931 <p>We have specified the observable Hamiltonian to be one with \\(Z\\)-detuning. The result is linearly scaled by 3.0 and shifted by -1.0. These parameters can optionally also be FeatureParameter or VariationalParameter</p> <p>One can also specify the observable as a list of observables, in which case the QNN will output a list of values.</p> <p>For full details on the <code>ObservableConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-feature-map","title":"Defining the Feature Map","text":"<p>Let us say we want to build a 4-qubit QNN that takes two inputs, namely, the \\(x\\) and the \\(y\\) coordinates of a point in the plane. We can use the <code>FeatureMapConfig</code> class to specify the feature map.</p> <pre><code>from qadence import BasisSet, chain, create_fm_blocks, FeatureMapConfig, ReuploadScaling\n\nfm_config = FeatureMapConfig(\n    num_features=2,\n    inputs = [\"x\", \"y\"],\n    basis_set=BasisSet.CHEBYSHEV,\n    reupload_scaling=ReuploadScaling.TOWER,\n    feature_range={\n        \"x\": (-1.0, 1.0),\n        \"y\": (0.0, 1.0),\n    },\n)\n\nfm_blocks = create_fm_blocks(register=4, config=fm_config)\nfeature_map = chain(*fm_blocks)\n</code></pre> %3 cluster_86db9a64edef49e0a5ba7a5e74c4577c Tower Chebyshev FM cluster_954640dec9bc4a26853e22e3a4063cd2 Tower Chebyshev FM 546952ad090849f389c933b743a748e0 0 cbf4e336cc0a4e4ab402033a9ece15db RX(1.0*acos(x)) 546952ad090849f389c933b743a748e0--cbf4e336cc0a4e4ab402033a9ece15db e8c67537afe6409f8c66e461d2aeb46c 1 59f6cbee41fc44b0a43080dee42b3b54 cbf4e336cc0a4e4ab402033a9ece15db--59f6cbee41fc44b0a43080dee42b3b54 d28ac868a90a40f6a2a3e21b02464c9a 0122aaec1543472db94c2b6d154b49e7 RX(2.0*acos(x)) e8c67537afe6409f8c66e461d2aeb46c--0122aaec1543472db94c2b6d154b49e7 71c21f17f32246498bb3e970b616a5c9 2 0122aaec1543472db94c2b6d154b49e7--d28ac868a90a40f6a2a3e21b02464c9a 9e75db0932c34da7b18079860ee11147 d954c22a7cb64e25bbd1916ade2d9446 RX(1.0*acos(2.0*y - 1.0)) 71c21f17f32246498bb3e970b616a5c9--d954c22a7cb64e25bbd1916ade2d9446 a23cc9f3b0804c9aa5d463ce8246bc3d 3 d954c22a7cb64e25bbd1916ade2d9446--9e75db0932c34da7b18079860ee11147 90fe3b6729c84090ae8ad9454d5654b3 e755aa26de7144c18c8b2be051a6928b RX(2.0*acos(2.0*y - 1.0)) a23cc9f3b0804c9aa5d463ce8246bc3d--e755aa26de7144c18c8b2be051a6928b e755aa26de7144c18c8b2be051a6928b--90fe3b6729c84090ae8ad9454d5654b3 <p>We have specified that the feature map should take two features, and have named the <code>FeatureParameter</code> \"x\" and \"y\" respectively. Both these parameters are encoded using the Chebyshev basis set, and the reupload scaling is set to <code>ReuploadScaling.TOWER</code>. One can optionally add the basis and the reupload scaling for each parameter separately.</p> <p>The <code>feature_range</code> parameter is a dictionary that specifies the range of values that each feature comes from. This is useful for scaling the input data to the range that the encoding function can handle. In default case, this range is mapped to the target range of the Chebyshev basis set which is \\([-1, 1]\\). One can also specify the target range for each feature separately.</p> <p>For full details on the <code>FeatureMapConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-ansatz","title":"Defining the Ansatz","text":"<p>The next part of the QNN is the ansatz. We use <code>AnsatzConfig</code> class to specify the type of ansatz.</p> <p>Let us say, we want to follow this feature map with 2 layers of hardware efficient ansatz.</p> <pre><code>from qadence import AnsatzConfig, AnsatzType, create_ansatz, Strategy\n\nansatz_config = AnsatzConfig(\n    depth=2,\n    ansatz_type=AnsatzType.HEA,\n    ansatz_strategy=Strategy.DIGITAL,\n)\n\nansatz = create_ansatz(register=4, config=ansatz_config)\n</code></pre> %3 3d99f06aebb8466fa717d560250ae6ce 0 1734e24ca45541feb926612f4457e219 RX(theta\u2080) 3d99f06aebb8466fa717d560250ae6ce--1734e24ca45541feb926612f4457e219 88eed6a60ff7485d9706dce1c866c6ca 1 bf34d2a368f240d4891c10244a0e1db3 RY(theta\u2084) 1734e24ca45541feb926612f4457e219--bf34d2a368f240d4891c10244a0e1db3 8b9d2c1bf0454a3e912a7b57b481eaa4 RX(theta\u2088) bf34d2a368f240d4891c10244a0e1db3--8b9d2c1bf0454a3e912a7b57b481eaa4 141b9c62dde148e6bf02117f3f705a3a 8b9d2c1bf0454a3e912a7b57b481eaa4--141b9c62dde148e6bf02117f3f705a3a f3d5f595c2a942eaaaef1935d76c55e3 141b9c62dde148e6bf02117f3f705a3a--f3d5f595c2a942eaaaef1935d76c55e3 23542abe87424e458325abf82550721f RX(theta\u2081\u2082) f3d5f595c2a942eaaaef1935d76c55e3--23542abe87424e458325abf82550721f 7d4fe90a6b614c92bdbe4945c129d54a RY(theta\u2081\u2086) 23542abe87424e458325abf82550721f--7d4fe90a6b614c92bdbe4945c129d54a b6e9e9f119da450f98a70aac54633ecf RX(theta\u2082\u2080) 7d4fe90a6b614c92bdbe4945c129d54a--b6e9e9f119da450f98a70aac54633ecf 856d1b25409441409592bfe111673bd3 b6e9e9f119da450f98a70aac54633ecf--856d1b25409441409592bfe111673bd3 6f0ef3b1d84a44339eb84ddedf8c2040 856d1b25409441409592bfe111673bd3--6f0ef3b1d84a44339eb84ddedf8c2040 82451ed1afa74cc3b81b66d0f584a189 6f0ef3b1d84a44339eb84ddedf8c2040--82451ed1afa74cc3b81b66d0f584a189 b55c9c46faa04d77add91c85b4c1b5b9 5ebeaff21dac409bb47a1aaf4272cd16 RX(theta\u2081) 88eed6a60ff7485d9706dce1c866c6ca--5ebeaff21dac409bb47a1aaf4272cd16 b06b638592f747ad89e79696bafcc161 2 837d8d0b51ff4f1fb77efe482be4271c RY(theta\u2085) 5ebeaff21dac409bb47a1aaf4272cd16--837d8d0b51ff4f1fb77efe482be4271c 7688d196df2f48138630b46dc538b138 RX(theta\u2089) 837d8d0b51ff4f1fb77efe482be4271c--7688d196df2f48138630b46dc538b138 2261ac89a6c9492cb9ada4fc88fe61a6 X 7688d196df2f48138630b46dc538b138--2261ac89a6c9492cb9ada4fc88fe61a6 2261ac89a6c9492cb9ada4fc88fe61a6--141b9c62dde148e6bf02117f3f705a3a 9b46df0c7ba7475097f516300cd1ada8 2261ac89a6c9492cb9ada4fc88fe61a6--9b46df0c7ba7475097f516300cd1ada8 a6dc889590b243308c8acefa76c18912 RX(theta\u2081\u2083) 9b46df0c7ba7475097f516300cd1ada8--a6dc889590b243308c8acefa76c18912 1a616cd7e0004eaa96f520e876bfa77e RY(theta\u2081\u2087) a6dc889590b243308c8acefa76c18912--1a616cd7e0004eaa96f520e876bfa77e 1ddd9f3c0909463ab68bce306d0cd2ef RX(theta\u2082\u2081) 1a616cd7e0004eaa96f520e876bfa77e--1ddd9f3c0909463ab68bce306d0cd2ef 9b31d2446ac0493181c7dbad8d5d1268 X 1ddd9f3c0909463ab68bce306d0cd2ef--9b31d2446ac0493181c7dbad8d5d1268 9b31d2446ac0493181c7dbad8d5d1268--856d1b25409441409592bfe111673bd3 d9081bb481c54110bd9a114d4b06b445 9b31d2446ac0493181c7dbad8d5d1268--d9081bb481c54110bd9a114d4b06b445 d9081bb481c54110bd9a114d4b06b445--b55c9c46faa04d77add91c85b4c1b5b9 85a7103674004e17b0ad5bb37e6e35d7 406927909c0d4075968464360e7a452e RX(theta\u2082) b06b638592f747ad89e79696bafcc161--406927909c0d4075968464360e7a452e 469f4a67649040bc81761cbaa37761d5 3 64e06ac0abad4dc08d00866d58774cf1 RY(theta\u2086) 406927909c0d4075968464360e7a452e--64e06ac0abad4dc08d00866d58774cf1 8a11be122fba44c6b47903ea0da72c94 RX(theta\u2081\u2080) 64e06ac0abad4dc08d00866d58774cf1--8a11be122fba44c6b47903ea0da72c94 d212f3d712ee40ff9c5dd1018247905b 8a11be122fba44c6b47903ea0da72c94--d212f3d712ee40ff9c5dd1018247905b af2db88087da4b268b4dca60f89a6898 X d212f3d712ee40ff9c5dd1018247905b--af2db88087da4b268b4dca60f89a6898 af2db88087da4b268b4dca60f89a6898--9b46df0c7ba7475097f516300cd1ada8 3e89f58654884c34857f693265794dcd RX(theta\u2081\u2084) af2db88087da4b268b4dca60f89a6898--3e89f58654884c34857f693265794dcd 1bc8acc3727b4990b4680b5592e426bd RY(theta\u2081\u2088) 3e89f58654884c34857f693265794dcd--1bc8acc3727b4990b4680b5592e426bd 68dd45b6524748f7bc15df05cab88c75 RX(theta\u2082\u2082) 1bc8acc3727b4990b4680b5592e426bd--68dd45b6524748f7bc15df05cab88c75 b9713a9f39ab4b769a5b03520cbd3511 68dd45b6524748f7bc15df05cab88c75--b9713a9f39ab4b769a5b03520cbd3511 0838caeb24bf434584365475b8efe680 X b9713a9f39ab4b769a5b03520cbd3511--0838caeb24bf434584365475b8efe680 0838caeb24bf434584365475b8efe680--d9081bb481c54110bd9a114d4b06b445 0838caeb24bf434584365475b8efe680--85a7103674004e17b0ad5bb37e6e35d7 55708194e2984cee90ba570a09a1c08d 396bd1c374b749bd8c0dd33707eb9a23 RX(theta\u2083) 469f4a67649040bc81761cbaa37761d5--396bd1c374b749bd8c0dd33707eb9a23 2ded23e72e7140f9b25a16d8e553b4ae RY(theta\u2087) 396bd1c374b749bd8c0dd33707eb9a23--2ded23e72e7140f9b25a16d8e553b4ae 0bd81140c063485e807481ecd8e40374 RX(theta\u2081\u2081) 2ded23e72e7140f9b25a16d8e553b4ae--0bd81140c063485e807481ecd8e40374 9a773419790b4fb48fdbe28e96b51604 X 0bd81140c063485e807481ecd8e40374--9a773419790b4fb48fdbe28e96b51604 9a773419790b4fb48fdbe28e96b51604--d212f3d712ee40ff9c5dd1018247905b 9d52bad1fd9845feaaac43b42eaa31f0 9a773419790b4fb48fdbe28e96b51604--9d52bad1fd9845feaaac43b42eaa31f0 258547f386c94be793be047c8ea68f1d RX(theta\u2081\u2085) 9d52bad1fd9845feaaac43b42eaa31f0--258547f386c94be793be047c8ea68f1d 7f36cc266909468bbc53f21c48b83f5f RY(theta\u2081\u2089) 258547f386c94be793be047c8ea68f1d--7f36cc266909468bbc53f21c48b83f5f 6ead6ecea45e4c0d9a1af451a832ef4d RX(theta\u2082\u2083) 7f36cc266909468bbc53f21c48b83f5f--6ead6ecea45e4c0d9a1af451a832ef4d 35efbbea752644f2aee9c2938117326b X 6ead6ecea45e4c0d9a1af451a832ef4d--35efbbea752644f2aee9c2938117326b 35efbbea752644f2aee9c2938117326b--b9713a9f39ab4b769a5b03520cbd3511 4dbf7aa142da4847b586b7d93473fb2a 35efbbea752644f2aee9c2938117326b--4dbf7aa142da4847b586b7d93473fb2a 4dbf7aa142da4847b586b7d93473fb2a--55708194e2984cee90ba570a09a1c08d <p>We have specified that the ansatz should have a depth of 2, and the ansatz type is \"hea\" (Hardware Efficient Ansatz). The ansatz strategy is set to \"digital\", which means digital gates are being used. One could alternatively use \"analog\" or \"rydberg\" as the ansatz strategy.</p> <p>For full details on the <code>AnsatzConfig</code> class, see the API documentation.</p>"},{"location":"tutorials/qml/config_qnn/#defining-the-qnn-from-the-configs","title":"Defining the QNN from the Configs","text":"<p>To build the QNN, we can now use the <code>QNN</code> class as a <code>QuantumModel</code> subtype. In addition to the feature map, ansatz and the observable configs, we can also specify options such as the <code>backend</code>, <code>diff_mode</code>, etc. For full details on the <code>QNN</code> class, see the API documentation or the documentation on the config constructor here.</p> <pre><code>from qadence import BackendName, DiffMode, QNN\n\nqnn = QNN.from_configs(\n    register=4,\n    obs_config=observable_config,\n    fm_config=fm_config,\n    ansatz_config=ansatz_config,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.AD,\n)\n</code></pre> %3 cluster_2ae0c25c5fb649ffa9b1940ea8acf5c4 Obs. cluster_dba5fe2a99bc4cef8466a1674fbc1c36 cluster_11ecd2d0998547f2b322b6d573f5cd46 Tower Chebyshev FM cluster_52cf9fcf61b1438e9edde97ad8edec59 Tower Chebyshev FM cluster_9e71fd3da567471cb69148cb2f889eb5 HEA 96ad845ffec54681abe90a7f296eeb19 0 062dfc2e5f73465484e5506cd406f56b RX(1.0*acos(x)) 96ad845ffec54681abe90a7f296eeb19--062dfc2e5f73465484e5506cd406f56b 878a0656d0f648d691728d20fadac2d3 1 0a05c1a3505b48a8b3e6f2a41e176685 RX(theta\u2080) 062dfc2e5f73465484e5506cd406f56b--0a05c1a3505b48a8b3e6f2a41e176685 671d473fb20e4b3bae5eb8e19c87f4aa RY(theta\u2084) 0a05c1a3505b48a8b3e6f2a41e176685--671d473fb20e4b3bae5eb8e19c87f4aa 829cce0c3545494b9c17bfe84cb15954 RX(theta\u2088) 671d473fb20e4b3bae5eb8e19c87f4aa--829cce0c3545494b9c17bfe84cb15954 761ff8ae43ce4c92a6d9f07c38b47476 829cce0c3545494b9c17bfe84cb15954--761ff8ae43ce4c92a6d9f07c38b47476 9f26b74b324347e98840ac647614350d 761ff8ae43ce4c92a6d9f07c38b47476--9f26b74b324347e98840ac647614350d 5a137f26bc024615aff618b5d02456c3 RX(theta\u2081\u2082) 9f26b74b324347e98840ac647614350d--5a137f26bc024615aff618b5d02456c3 b94f8e69096e40cbbdc809810893ca2d RY(theta\u2081\u2086) 5a137f26bc024615aff618b5d02456c3--b94f8e69096e40cbbdc809810893ca2d 531e12201a4a4474a080ae9ff2592d25 RX(theta\u2082\u2080) b94f8e69096e40cbbdc809810893ca2d--531e12201a4a4474a080ae9ff2592d25 0217b02ec06244abafdd5a3c5d085e7a 531e12201a4a4474a080ae9ff2592d25--0217b02ec06244abafdd5a3c5d085e7a fddb9358ea9a4edea1b90cca3a95e30a 0217b02ec06244abafdd5a3c5d085e7a--fddb9358ea9a4edea1b90cca3a95e30a 72cf8f56476c4c7a9bf67e4d70f9c3ef fddb9358ea9a4edea1b90cca3a95e30a--72cf8f56476c4c7a9bf67e4d70f9c3ef bc6b03faeac74e3d9c5ec0737956d6bc 72cf8f56476c4c7a9bf67e4d70f9c3ef--bc6b03faeac74e3d9c5ec0737956d6bc 04e68e2a54cd43b88be77467fe37b48a b3c6db2a8368400590407155d685dcdb RX(2.0*acos(x)) 878a0656d0f648d691728d20fadac2d3--b3c6db2a8368400590407155d685dcdb c190d2c7c74b47b8bc20783ff5c66ac3 2 a7560d9b163843ccb4e03a62d0f88f47 RX(theta\u2081) b3c6db2a8368400590407155d685dcdb--a7560d9b163843ccb4e03a62d0f88f47 2c5181f09be744ac9c015c08ae0a9e5d RY(theta\u2085) a7560d9b163843ccb4e03a62d0f88f47--2c5181f09be744ac9c015c08ae0a9e5d 7ebc8ba4aa154d7d895dfda02589de6f RX(theta\u2089) 2c5181f09be744ac9c015c08ae0a9e5d--7ebc8ba4aa154d7d895dfda02589de6f e907e95c82ae4d19929b3a56d7468c3e X 7ebc8ba4aa154d7d895dfda02589de6f--e907e95c82ae4d19929b3a56d7468c3e e907e95c82ae4d19929b3a56d7468c3e--761ff8ae43ce4c92a6d9f07c38b47476 127ff6836eb64f69a2b40ed871712a1f e907e95c82ae4d19929b3a56d7468c3e--127ff6836eb64f69a2b40ed871712a1f 1cd6e9c47d934b399c2a97a095f95ca3 RX(theta\u2081\u2083) 127ff6836eb64f69a2b40ed871712a1f--1cd6e9c47d934b399c2a97a095f95ca3 832e788024a94fb6acd7cbea3fb6b18f RY(theta\u2081\u2087) 1cd6e9c47d934b399c2a97a095f95ca3--832e788024a94fb6acd7cbea3fb6b18f 2c31d69c2da8449e9a2e11758f14a7a3 RX(theta\u2082\u2081) 832e788024a94fb6acd7cbea3fb6b18f--2c31d69c2da8449e9a2e11758f14a7a3 7a7dda2cde9f4cfebdafe32d9ac15be5 X 2c31d69c2da8449e9a2e11758f14a7a3--7a7dda2cde9f4cfebdafe32d9ac15be5 7a7dda2cde9f4cfebdafe32d9ac15be5--0217b02ec06244abafdd5a3c5d085e7a 9ed51fe90de241f8bdd1f1b3dc65b0f2 7a7dda2cde9f4cfebdafe32d9ac15be5--9ed51fe90de241f8bdd1f1b3dc65b0f2 95d9163657554f438c13b5806596b1bc AddBlock 9ed51fe90de241f8bdd1f1b3dc65b0f2--95d9163657554f438c13b5806596b1bc 95d9163657554f438c13b5806596b1bc--04e68e2a54cd43b88be77467fe37b48a a0461be0e1cc499da4b762d586a21b24 d62fca460d9f44f0a932643654ebe0c2 RX(1.0*acos(2.0*y - 1.0)) c190d2c7c74b47b8bc20783ff5c66ac3--d62fca460d9f44f0a932643654ebe0c2 01270aa2b2994aa5b52f85593871b24a 3 e4ee6d3f492c4ad99b72a10e96027c7a RX(theta\u2082) d62fca460d9f44f0a932643654ebe0c2--e4ee6d3f492c4ad99b72a10e96027c7a 98fd911ac9bc4e4d905ce20ce99f5ef2 RY(theta\u2086) e4ee6d3f492c4ad99b72a10e96027c7a--98fd911ac9bc4e4d905ce20ce99f5ef2 dc2adaaaeb064596939d9dabe57f4b11 RX(theta\u2081\u2080) 98fd911ac9bc4e4d905ce20ce99f5ef2--dc2adaaaeb064596939d9dabe57f4b11 5b3ee4b1f0bb4ffd88065759c8ab0a2f dc2adaaaeb064596939d9dabe57f4b11--5b3ee4b1f0bb4ffd88065759c8ab0a2f 07378070713c4feab26a9f337c1b79d1 X 5b3ee4b1f0bb4ffd88065759c8ab0a2f--07378070713c4feab26a9f337c1b79d1 07378070713c4feab26a9f337c1b79d1--127ff6836eb64f69a2b40ed871712a1f 6244c6da2bfd4c44b038f39820db1944 RX(theta\u2081\u2084) 07378070713c4feab26a9f337c1b79d1--6244c6da2bfd4c44b038f39820db1944 84e623e23f4c4799991557bcd61f27e5 RY(theta\u2081\u2088) 6244c6da2bfd4c44b038f39820db1944--84e623e23f4c4799991557bcd61f27e5 a38f3fcd775743d0aa9f432dcfb13df4 RX(theta\u2082\u2082) 84e623e23f4c4799991557bcd61f27e5--a38f3fcd775743d0aa9f432dcfb13df4 689a7454516540eba0b175ab0f82b1ea a38f3fcd775743d0aa9f432dcfb13df4--689a7454516540eba0b175ab0f82b1ea eca17e29323b41d4a006a7ae0e36a4db X 689a7454516540eba0b175ab0f82b1ea--eca17e29323b41d4a006a7ae0e36a4db eca17e29323b41d4a006a7ae0e36a4db--9ed51fe90de241f8bdd1f1b3dc65b0f2 e394118c7ed74e7f938c5ba46ac5daaf eca17e29323b41d4a006a7ae0e36a4db--e394118c7ed74e7f938c5ba46ac5daaf e394118c7ed74e7f938c5ba46ac5daaf--a0461be0e1cc499da4b762d586a21b24 364ff7b1e1494fe18cc2d6e5967b5cda a160ffb5df7c428f86fb9f44c1e4649d RX(2.0*acos(2.0*y - 1.0)) 01270aa2b2994aa5b52f85593871b24a--a160ffb5df7c428f86fb9f44c1e4649d 218dfa10b88d48ee8a12817870ad3b21 RX(theta\u2083) a160ffb5df7c428f86fb9f44c1e4649d--218dfa10b88d48ee8a12817870ad3b21 de5a519bbdf248acbb451dc8ca0b75a4 RY(theta\u2087) 218dfa10b88d48ee8a12817870ad3b21--de5a519bbdf248acbb451dc8ca0b75a4 455b7067e3cc431fb4d4d397578678bf RX(theta\u2081\u2081) de5a519bbdf248acbb451dc8ca0b75a4--455b7067e3cc431fb4d4d397578678bf 09594b7bc784416384b3b933def751df X 455b7067e3cc431fb4d4d397578678bf--09594b7bc784416384b3b933def751df 09594b7bc784416384b3b933def751df--5b3ee4b1f0bb4ffd88065759c8ab0a2f 99b31a8ac3ab426c95a6cb8504e0dd88 09594b7bc784416384b3b933def751df--99b31a8ac3ab426c95a6cb8504e0dd88 959087d27b3c4321bcd05ba96c5d22f3 RX(theta\u2081\u2085) 99b31a8ac3ab426c95a6cb8504e0dd88--959087d27b3c4321bcd05ba96c5d22f3 26872e37c4374f80ac6be270e347437f RY(theta\u2081\u2089) 959087d27b3c4321bcd05ba96c5d22f3--26872e37c4374f80ac6be270e347437f 4b35d136a72b45f1add453e4fc13707d RX(theta\u2082\u2083) 26872e37c4374f80ac6be270e347437f--4b35d136a72b45f1add453e4fc13707d db13b598629744b9984c15fee21d6e31 X 4b35d136a72b45f1add453e4fc13707d--db13b598629744b9984c15fee21d6e31 db13b598629744b9984c15fee21d6e31--689a7454516540eba0b175ab0f82b1ea a208f5c55887419089584be67d6557e3 db13b598629744b9984c15fee21d6e31--a208f5c55887419089584be67d6557e3 232aa790f07c46298b4e387e89ae5712 a208f5c55887419089584be67d6557e3--232aa790f07c46298b4e387e89ae5712 232aa790f07c46298b4e387e89ae5712--364ff7b1e1494fe18cc2d6e5967b5cda"},{"location":"tutorials/qml/dqc_1d/","title":"Solving a 1D ODE","text":"<p>In this tutorial we will show how to use Qadence to solve a basic 1D Ordinary Differential Equation (ODE) with a QNN using Differentiable Quantum Circuits (DQC) <sup>1</sup>.</p> <p>Consider the following non-linear ODE and boundary condition:</p> \\[ \\frac{df}{dx}= 5\\times(4x^3+x^2-2x-\\frac12), \\qquad f(0)=0 \\] <p>It admits an exact solution:</p> \\[ f(x)=5\\times(x^4+\\frac13x^3-x^2-\\frac12x) \\] <p>Our goal will be to find this solution for \\(x\\in[-1, 1]\\).</p> <pre><code>import torch\n\ndef dfdx_equation(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Derivative as per the equation.\"\"\"\n    return 5*(4*x**3 + x**2 - 2*x - 0.5)\n</code></pre> <p>For the purpose of this tutorial, we will compute the derivative of the circuit using <code>torch.autograd</code>. The point of the DQC algorithm is to use differentiable circuits with parameter shift rules. In Qadence, PSR is implemented directly as custom overrides of the derivative function in the autograd engine, and thus we can later change the derivative method for the model itself if we wish.</p> <pre><code>def calc_deriv(outputs: torch.Tensor, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute a derivative of model that learns f(x), computes df/dx using torch.autograd.\"\"\"\n    grad = torch.autograd.grad(\n        outputs=outputs,\n        inputs=inputs,\n        grad_outputs = torch.ones_like(inputs),\n        create_graph = True,\n        retain_graph = True,\n    )[0]\n    return grad\n</code></pre>"},{"location":"tutorials/qml/dqc_1d/#defining-the-loss-function","title":"Defining the loss function","text":"<p>The essential part of solving this problem is to define the right loss function to represent our goal. In this case, we want to define a model that has the capacity to learn the target solution, and we want to minimize: - The derivative of this model in comparison with the exact derivative in the equation; - The output of the model at the boundary in comparison with the value for the boundary condition;</p> <p>We can write it like so:</p> <pre><code># Mean-squared error as the comparison criterion\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, inputs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Loss function encoding the problem to solve.\"\"\"\n    # Equation loss\n    model_output = model(inputs)\n    deriv_model = calc_deriv(model_output, inputs)\n    deriv_exact = dfdx_equation(inputs)\n    ode_loss = criterion(deriv_model, deriv_exact)\n\n    # Boundary loss, f(0) = 0\n    boundary_model = model(torch.tensor([[0.0]]))\n    boundary_exact = torch.tensor([[0.0]])\n    boundary_loss = criterion(boundary_model, boundary_exact)\n\n    return ode_loss + boundary_loss\n</code></pre> <p>Different loss criterions could be considered, and we could also play with the balance between the sum of the two loss terms. For now, let's proceed with the definition above.</p> <p>Note that so far we have not used any quantum specific assumption, and we could in principle use the same loss function with a classical neural network.</p>"},{"location":"tutorials/qml/dqc_1d/#defining-a-qnn-with-qadence","title":"Defining a QNN with Qadence","text":"<p>Now, we can finally use Qadence to write a QNN. We will use a feature map to encode the input values, a trainable ansatz circuit, and an observable to measure as the output.</p> <pre><code>from qadence import feature_map, hea, chain\nfrom qadence import QNN, QuantumCircuit, Z\nfrom qadence.types import BasisSet, ReuploadScaling\n\nn_qubits = 3\ndepth = 3\n\n# Feature map\nfm = feature_map(\n    n_qubits = n_qubits,\n    param = \"x\",\n    fm_type = BasisSet.CHEBYSHEV,\n    reupload_scaling = ReuploadScaling.TOWER,\n)\n\n# Ansatz\nansatz = hea(n_qubits = n_qubits, depth = depth)\n\n# Observable\nobservable = Z(0)\n\ncircuit = QuantumCircuit(n_qubits, chain(fm, ansatz))\nmodel = QNN(circuit = circuit, observable = observable, inputs = [\"x\"])\n</code></pre> <p>We used a Chebyshev feature map with a tower-like scaling of the input reupload, and a standard hardware-efficient ansatz. You can check the qml constructors tutorial to see how you can customize these components. In the observable, for now we consider the simple case of measuring the magnetization of the first qubit.</p> <pre><code>from qadence.draw import display\n\n# display(circuit)\n</code></pre> %3 cluster_b294fee525064108bbd7ef8407c570f7 HEA cluster_2bc66ae633584aacac155830db4ce2ca Tower Chebyshev FM 180ea70846654630b9412603f4222a93 0 3d7777e68da944dbb6b45d3dc9e00a1b RX(1.0*acos(x)) 180ea70846654630b9412603f4222a93--3d7777e68da944dbb6b45d3dc9e00a1b 6863a4f442d84d9086f25756f89f6a58 1 7880595be9604bd7b9e81c36fb6d3c0d RX(theta\u2080) 3d7777e68da944dbb6b45d3dc9e00a1b--7880595be9604bd7b9e81c36fb6d3c0d df9c1675bc9644abbe80918c644fbd78 RY(theta\u2083) 7880595be9604bd7b9e81c36fb6d3c0d--df9c1675bc9644abbe80918c644fbd78 ebe95a7ab8744ec19e49ace928a3de3d RX(theta\u2086) df9c1675bc9644abbe80918c644fbd78--ebe95a7ab8744ec19e49ace928a3de3d 1ea470a05f5e441c9f1b57324ad19edb ebe95a7ab8744ec19e49ace928a3de3d--1ea470a05f5e441c9f1b57324ad19edb a23cc657efa34696bbff5b9e7ae75cf9 1ea470a05f5e441c9f1b57324ad19edb--a23cc657efa34696bbff5b9e7ae75cf9 24a7a7daf3c345c2abf0fb41e3d0fedd RX(theta\u2089) a23cc657efa34696bbff5b9e7ae75cf9--24a7a7daf3c345c2abf0fb41e3d0fedd 7b76d06613f54a46b4f68f6cf42fca56 RY(theta\u2081\u2082) 24a7a7daf3c345c2abf0fb41e3d0fedd--7b76d06613f54a46b4f68f6cf42fca56 285c4dbc7deb4c939c7c5d78a54bca02 RX(theta\u2081\u2085) 7b76d06613f54a46b4f68f6cf42fca56--285c4dbc7deb4c939c7c5d78a54bca02 cd6939731a7641eaa6433814b462c2ae 285c4dbc7deb4c939c7c5d78a54bca02--cd6939731a7641eaa6433814b462c2ae c882637315b6451083d60035298ba797 cd6939731a7641eaa6433814b462c2ae--c882637315b6451083d60035298ba797 a77287cdb1d34897beadbb2cd3e3b700 RX(theta\u2081\u2088) c882637315b6451083d60035298ba797--a77287cdb1d34897beadbb2cd3e3b700 be26f3509ce24fa6b5f1c60dc7113b7e RY(theta\u2082\u2081) a77287cdb1d34897beadbb2cd3e3b700--be26f3509ce24fa6b5f1c60dc7113b7e 7d124f85d4a740e289e8d5ca17a84c11 RX(theta\u2082\u2084) be26f3509ce24fa6b5f1c60dc7113b7e--7d124f85d4a740e289e8d5ca17a84c11 ebd6878be1fd4905a80afc7bde8f23c2 7d124f85d4a740e289e8d5ca17a84c11--ebd6878be1fd4905a80afc7bde8f23c2 82b8e6e6cc6f4e1baba4ee9e7d699c58 ebd6878be1fd4905a80afc7bde8f23c2--82b8e6e6cc6f4e1baba4ee9e7d699c58 5d802807e6f74eefa67caaff48acca15 82b8e6e6cc6f4e1baba4ee9e7d699c58--5d802807e6f74eefa67caaff48acca15 d8d075536f5c4f619ff7dad5c3afbff6 cc36919d88184e198269e9f8979650ca RX(2.0*acos(x)) 6863a4f442d84d9086f25756f89f6a58--cc36919d88184e198269e9f8979650ca 16ca4d416cee4ef7802487b60122bc37 2 acda98cffca74aa0b2fd1fcfd2cef94a RX(theta\u2081) cc36919d88184e198269e9f8979650ca--acda98cffca74aa0b2fd1fcfd2cef94a 6a27e66c683a46f1a973723648144471 RY(theta\u2084) acda98cffca74aa0b2fd1fcfd2cef94a--6a27e66c683a46f1a973723648144471 a3e717bdc38643f0aeac03c07ebcf895 RX(theta\u2087) 6a27e66c683a46f1a973723648144471--a3e717bdc38643f0aeac03c07ebcf895 2f94651a310e4aeab9fc6d1b80cd9c01 X a3e717bdc38643f0aeac03c07ebcf895--2f94651a310e4aeab9fc6d1b80cd9c01 2f94651a310e4aeab9fc6d1b80cd9c01--1ea470a05f5e441c9f1b57324ad19edb e884a7ac9dbc483090ce58d0866b96cb 2f94651a310e4aeab9fc6d1b80cd9c01--e884a7ac9dbc483090ce58d0866b96cb c46c7c2c1e06410aa58cb625e1cb737f RX(theta\u2081\u2080) e884a7ac9dbc483090ce58d0866b96cb--c46c7c2c1e06410aa58cb625e1cb737f 369602ea69fd4b63aacf4c25a8f9b4fc RY(theta\u2081\u2083) c46c7c2c1e06410aa58cb625e1cb737f--369602ea69fd4b63aacf4c25a8f9b4fc c9e6e0e448e949ebb4f1a7d8f42a6870 RX(theta\u2081\u2086) 369602ea69fd4b63aacf4c25a8f9b4fc--c9e6e0e448e949ebb4f1a7d8f42a6870 8e5ab49538dc4b729c16d0a48a8a5791 X c9e6e0e448e949ebb4f1a7d8f42a6870--8e5ab49538dc4b729c16d0a48a8a5791 8e5ab49538dc4b729c16d0a48a8a5791--cd6939731a7641eaa6433814b462c2ae 3838999ffe9643e48171846f416c41fa 8e5ab49538dc4b729c16d0a48a8a5791--3838999ffe9643e48171846f416c41fa 360ae2aad24f4b698c0cdef28c98e759 RX(theta\u2081\u2089) 3838999ffe9643e48171846f416c41fa--360ae2aad24f4b698c0cdef28c98e759 0f7216e22c78400d8a9aedd4abfa91a9 RY(theta\u2082\u2082) 360ae2aad24f4b698c0cdef28c98e759--0f7216e22c78400d8a9aedd4abfa91a9 cdb5ffaa981d4f81805c96799ba0be3d RX(theta\u2082\u2085) 0f7216e22c78400d8a9aedd4abfa91a9--cdb5ffaa981d4f81805c96799ba0be3d 34967b3dcb71498797cd0498b6f51815 X cdb5ffaa981d4f81805c96799ba0be3d--34967b3dcb71498797cd0498b6f51815 34967b3dcb71498797cd0498b6f51815--ebd6878be1fd4905a80afc7bde8f23c2 610c250f139442ed992d50c3919b8098 34967b3dcb71498797cd0498b6f51815--610c250f139442ed992d50c3919b8098 610c250f139442ed992d50c3919b8098--d8d075536f5c4f619ff7dad5c3afbff6 a71135a116ef43e984dcb3c429f330f7 daf580fbf0b7424d87ce81dd9dfc1dee RX(3.0*acos(x)) 16ca4d416cee4ef7802487b60122bc37--daf580fbf0b7424d87ce81dd9dfc1dee ad71b35aaa9a4c179621e11423749d95 RX(theta\u2082) daf580fbf0b7424d87ce81dd9dfc1dee--ad71b35aaa9a4c179621e11423749d95 034c046b8f5a4cd6973bc054a75daa02 RY(theta\u2085) ad71b35aaa9a4c179621e11423749d95--034c046b8f5a4cd6973bc054a75daa02 92b6eb93ee174fdabb4a56035d233480 RX(theta\u2088) 034c046b8f5a4cd6973bc054a75daa02--92b6eb93ee174fdabb4a56035d233480 e84ba259f14b4315898114d42f85b440 92b6eb93ee174fdabb4a56035d233480--e84ba259f14b4315898114d42f85b440 ec217ed288344d89ad2f22e6b2e5a928 X e84ba259f14b4315898114d42f85b440--ec217ed288344d89ad2f22e6b2e5a928 ec217ed288344d89ad2f22e6b2e5a928--e884a7ac9dbc483090ce58d0866b96cb f1cacfd27e9f4a83bc98f1a2ca0f00c8 RX(theta\u2081\u2081) ec217ed288344d89ad2f22e6b2e5a928--f1cacfd27e9f4a83bc98f1a2ca0f00c8 25c9299a49fd43d58c2eabce4d5dad03 RY(theta\u2081\u2084) f1cacfd27e9f4a83bc98f1a2ca0f00c8--25c9299a49fd43d58c2eabce4d5dad03 33c8713a790340e68b48a481b159db2b RX(theta\u2081\u2087) 25c9299a49fd43d58c2eabce4d5dad03--33c8713a790340e68b48a481b159db2b 715c71fe1e764a2db51987265b8f634d 33c8713a790340e68b48a481b159db2b--715c71fe1e764a2db51987265b8f634d b9e555bec5c6456782d443e2581780a2 X 715c71fe1e764a2db51987265b8f634d--b9e555bec5c6456782d443e2581780a2 b9e555bec5c6456782d443e2581780a2--3838999ffe9643e48171846f416c41fa e0c675909ea3440aa3fd87dfbd701a8b RX(theta\u2082\u2080) b9e555bec5c6456782d443e2581780a2--e0c675909ea3440aa3fd87dfbd701a8b a66dadd7536e4045a5541645a3139805 RY(theta\u2082\u2083) e0c675909ea3440aa3fd87dfbd701a8b--a66dadd7536e4045a5541645a3139805 4391cf54c67c485a9a15abd613cb4fd8 RX(theta\u2082\u2086) a66dadd7536e4045a5541645a3139805--4391cf54c67c485a9a15abd613cb4fd8 546fd9b9e0164471aef048d7d9f51ec6 4391cf54c67c485a9a15abd613cb4fd8--546fd9b9e0164471aef048d7d9f51ec6 a5e2b85e94a14cc9b766ecdcae680134 X 546fd9b9e0164471aef048d7d9f51ec6--a5e2b85e94a14cc9b766ecdcae680134 a5e2b85e94a14cc9b766ecdcae680134--610c250f139442ed992d50c3919b8098 a5e2b85e94a14cc9b766ecdcae680134--a71135a116ef43e984dcb3c429f330f7"},{"location":"tutorials/qml/dqc_1d/#training-the-model","title":"Training the model","text":"<p>Now that the model is defined we can proceed with the training. the <code>QNN</code> class can be used like any other <code>torch.nn.Module</code>. Here we write a simple training loop, but you can also look at the ml tools tutorial to use the convenience training functions that Qadence provides.</p> <p>To train the model, we will select a random set of collocation points uniformly distributed within \\(-1.0&lt; x &lt;1.0\\) and compute the loss function for those points.</p> <pre><code>n_epochs = 200\nn_points = 10\n\nxmin = -0.99\nxmax = 0.99\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Training data. We unsqueeze essentially making each batch have a single x value.\n    x_train = (xmin + (xmax-xmin)*torch.rand(n_points, requires_grad = True)).unsqueeze(1)\n\n    loss = loss_fn(inputs = x_train, model = model)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>Note the values of \\(x\\) are only picked from \\(x\\in[-0.99, 0.99]\\) since we are using a Chebyshev feature map, and derivative of \\(\\text{acos}(x)\\) diverges at \\(-1\\) and \\(1\\).</p>"},{"location":"tutorials/qml/dqc_1d/#plotting-the-results","title":"Plotting the results","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef f_exact(x: torch.Tensor) -&gt; torch.Tensor:\n    return 5*(x**4 + (1/3)*x**3 - x**2 - 0.5*x)\n\nx_test = torch.arange(xmin, xmax, step = 0.01).unsqueeze(1)\n\nresult_exact = f_exact(x_test).flatten()\n\nresult_model = model(x_test).flatten().detach()\n\nplt.plot(x_test, result_exact, label = \"Exact solution\")\nplt.plot(x_test, result_model, label = \" Trained model\")\n</code></pre> 2025-01-08T18:02:33.109561 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>Clearly, the result is not optimal.</p>"},{"location":"tutorials/qml/dqc_1d/#improving-the-solution","title":"Improving the solution","text":"<p>One point to consider when defining the QNN is the possible output range, which is bounded by the spectrum of the chosen observable. For the magnetization of a single qubit, this means that the output is bounded between -1 and 1, which we can clearly see in the plot.</p> <p>One option would be to define the observable as the total magnetization over all qubits, which would allow a range of -3 to 3.</p> <pre><code>from qadence import add\n\nobservable = add(Z(i) for i in range(n_qubits))\n\nmodel = QNN(circuit = circuit, observable = observable, inputs = [\"x\"])\n\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n\nfor epoch in range(n_epochs):\n    optimizer.zero_grad()\n\n    # Training data\n    x_train = (xmin + (xmax-xmin)*torch.rand(n_points, requires_grad = True)).unsqueeze(1)\n\n    loss = loss_fn(inputs = x_train, model = model)\n    loss.backward()\n    optimizer.step()\n</code></pre> <p>And we again plot the result:</p> <pre><code>x_test = torch.arange(xmin, xmax, step = 0.01).unsqueeze(1)\n\nresult_exact = f_exact(x_test).flatten()\n\nresult_model = model(x_test).flatten().detach()\n\nplt.plot(x_test, result_exact, label = \"Exact solution\")\nplt.plot(x_test, result_model, label = \"Trained model\")\n</code></pre> 2025-01-08T18:02:40.272002 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/dqc_1d/#references","title":"References","text":"<ol> <li> <p>Kyriienko et al., Solving nonlinear differential equations with differentiable quantum circuits. \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/qaoa/","title":"Solving MaxCut with QAOA","text":"<p>This tutorial shows how to solve the maximum cut (MaxCut) combinatorial optimization problem on a graph using the Quantum Approximate Optimization Algorithm (QAOA), first introduced by Farhi et al. in 2014 <sup>1</sup>.</p> <p>Given an arbitrary graph, the MaxCut problem consists in finding a graph cut which partitions the nodes into two disjoint sets, such that the number of edges in the cut is maximized. This is a very common combinatorial optimization problem known to be computationally hard (NP-hard).</p> <p>The graph used for this tutorial is an unweighted graph randomly generated using the <code>networkx</code> library with a certain probability \\(p\\) of having an edge between two arbitrary nodes (known as Erd\u0151s\u2013R\u00e9nyi graph).</p> <pre><code>import numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport random\n\n# ensure reproducibility\nseed = 42\nnp.random.seed(seed)\nrandom.seed(seed)\n\n# Create random graph\nn_nodes = 4\nedge_prob = 0.8\ngraph = nx.gnp_random_graph(n_nodes, edge_prob)\n\nnx.draw(graph)\n</code></pre> 2025-01-08T18:02:40.374133 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/ <p>The goal of the MaxCut algorithm is to maximize the following cost function:</p> \\[\\mathcal{C}(p) = \\sum_{\\alpha}^m \\mathcal{C}_{\\alpha}(p)\\] <p>where \\(p\\) is a given cut of the graph, \\(\\alpha\\) is an index over the edges and \\(\\mathcal{C}_{\\alpha}(p)\\) is written such that if the nodes connected by the \\(\\alpha\\) edge are in the same set, it returns \\(0\\), otherwise it returns \\(1\\). We will represent a cut \\(p\\) as a bitstring of length \\(N\\), where \\(N\\) is the number of nodes, and where the bit in position \\(i\\) shows to which partition node \\(i\\) belongs. We assign value 0 to one of the partitions defined by the cut and 1 to the other. Since this choice is arbitrary, every cut is represented by two bitstrings, e.g. \"0011\" and \"1100\" are equivalent.</p> <p>Since in this tutorial we are only dealing with small graphs, we can find the maximum cut by brute force to make sure QAOA works as intended. <pre><code># Function to calculate the cost associated with a cut\ndef calculate_cost(cut: str, graph: nx.graph) -&gt; float:\n    \"\"\"Returns the cost of a given cut (represented by a bitstring)\"\"\"\n    cost = 0\n    for edge in graph.edges():\n        (i, j) = edge\n        if cut[i] != cut[j]:\n            cost += 1\n    return cost\n\n\n# Function to get a binary representation of an int\nget_binary = lambda x, n: format(x, \"b\").zfill(n)\n\n# List of all possible cuts\nall_possible_cuts = [bin(k)[2:].rjust(n_nodes, \"0\") for k in range(2**n_nodes)]\n\n# List with the costs associated to each cut\nall_costs = [calculate_cost(cut, graph) for cut in all_possible_cuts]\n\n# Get the maximum cost\nmaxcost = max(all_costs)\n\n# Get all cuts that correspond to the maximum cost\nmaxcuts = [get_binary(i, n_nodes) for i, j in enumerate(all_costs) if j == maxcost]\nprint(f\"The maximum cut is represented by the bitstrings {maxcuts}, with a cost of {maxcost}\")\n</code></pre> <pre><code>The maximum cut is represented by the bitstrings ['0011', '0101', '0110', '1001', '1010', '1100'], with a cost of 4\n</code></pre> </p>"},{"location":"tutorials/qml/qaoa/#the-qaoa-quantum-circuit","title":"The QAOA quantum circuit","text":"<p>The Max-Cut problem can be solved by using the QAOA algorithm. QAOA belongs to the class of Variational Quantum Algorithms (VQAs), which means that its quantum circuit contains a certain number of parametrized quantum gates that need to be optimized with a classical optimizer. The QAOA circuit is composed of two operators:</p> <ul> <li>The cost operator \\(U_c\\): a circuit generated by the cost Hamiltonian which encodes the cost function described above into a quantum circuit. The solution to the optimization problem is encoded in the ground state of the cost Hamiltonian \\(H_c\\). The cost operator  is simply the evolution of the cost Hamiltonian parametrized by a variational parameter \\(\\gamma\\) so that \\(U_c = e^{i\\gamma H_c}.\\)</li> <li>The mixing operator \\(U_b\\): a simple set of single-qubit rotations with adjustable   angles which are tuned during the classical optimization loop to minimize the cost</li> </ul> <p>The cost Hamiltonian of the MaxCut problem can be written as:</p> \\[H_c = \\frac12 \\sum_{\\langle i,j\\rangle} (\\mathbb{1} - Z_iZ_j)\\] <p>where \\(\\langle i,j\\rangle\\) represents the edge between nodes \\(i\\) and \\(j\\). The solution of the MaxCut problem is encoded in the ground state of the above Hamiltonian.</p> <p>The QAOA quantum circuit consists of a number of layers, each layer containing a cost and a mixing operator. Below, the QAOA quantum circuit is defined using <code>qadence</code> operations. First, a layer of Hadamard gates is applied to all qubits to prepare the initial state \\(|+\\rangle ^{\\otimes n}\\). The cost operator of each layer can be built \"manually\", implementing the \\(e^{iZZ\\gamma}\\) terms with CNOTs and a \\(\\rm{RZ}(2\\gamma)\\) rotation, or it can also be automatically decomposed into digital single and two-qubits operations via the <code>.digital_decomposition()</code> method. The decomposition is exact since the Hamiltonian generator is diagonal.</p> <pre><code>from qadence import tag, kron, chain, RX, RZ, Z, H, CNOT, I, add\nfrom qadence import HamEvo, QuantumCircuit, Parameter\n\nn_qubits = graph.number_of_nodes()\nn_edges = graph.number_of_edges()\nn_layers = 6\n\n# Generate the cost Hamiltonian\nzz_ops = add(Z(edge[0]) @ Z(edge[1]) for edge in graph.edges)\ncost_ham = 0.5 * (n_edges * kron(I(i) for i in range(n_qubits)) - zz_ops)\n\n\n# QAOA circuit\ndef build_qaoa_circuit(n_qubits, n_layers, graph):\n    layers = []\n    # Layer of Hadamards\n    initial_layer = kron(H(i) for i in range(n_qubits))\n    layers.append(initial_layer)\n    for layer in range(n_layers):\n\n        # cost layer with digital decomposition\n        # cost_layer = HamEvo(cost_ham, f\"g{layer}\").digital_decomposition(approximation=\"basic\")\n        cost_layer = []\n        for edge in graph.edges():\n            (q0, q1) = edge\n            zz_term = chain(\n                CNOT(q0, q1),\n                RZ(q1, Parameter(f\"g{layer}\")),\n                CNOT(q0, q1),\n            )\n            cost_layer.append(zz_term)\n        cost_layer = chain(*cost_layer)\n        cost_layer = tag(cost_layer, \"cost\")\n\n        # mixing layer with single qubit rotations\n        mixing_layer = kron(RX(i, f\"b{layer}\") for i in range(n_qubits))\n        mixing_layer = tag(mixing_layer, \"mixing\")\n\n        # putting all together in a single ChainBlock\n        layers.append(chain(cost_layer, mixing_layer))\n\n    final_b = chain(*layers)\n    return QuantumCircuit(n_qubits, final_b)\n\n\ncircuit = build_qaoa_circuit(n_qubits, n_layers, graph)\n\n# Print a single layer of the circuit\n</code></pre> %3 cluster_d1dddc5b361f4a90ae386f9d0ff7e2f7 mixing cluster_bfa4e2e4cd0a413eaf6d56c9029a7b10 cost fb85874d57714caaae40342f43781887 0 dfd11c6f83ed48af96e35913bbed21e4 H fb85874d57714caaae40342f43781887--dfd11c6f83ed48af96e35913bbed21e4 d6c32e4abef941078dc2376bf8f8cf1d 1 6bd4764b82d048c1a8dd05648a3105d0 dfd11c6f83ed48af96e35913bbed21e4--6bd4764b82d048c1a8dd05648a3105d0 51bcbfcc31f64b7ba7e845e43f4ccb97 6bd4764b82d048c1a8dd05648a3105d0--51bcbfcc31f64b7ba7e845e43f4ccb97 a477119d58e8470991067e68d22ef87c 51bcbfcc31f64b7ba7e845e43f4ccb97--a477119d58e8470991067e68d22ef87c ffc23220e7914254b082787e5dd64066 a477119d58e8470991067e68d22ef87c--ffc23220e7914254b082787e5dd64066 0d92803beb9643e3b7ef7431b0455ffc ffc23220e7914254b082787e5dd64066--0d92803beb9643e3b7ef7431b0455ffc 0044241a82f24f08a6e6bcfd03d7659c 0d92803beb9643e3b7ef7431b0455ffc--0044241a82f24f08a6e6bcfd03d7659c ad9d45b352bf4d17b65e15b98696867f 0044241a82f24f08a6e6bcfd03d7659c--ad9d45b352bf4d17b65e15b98696867f 3a21a3a7b7cd41ccabe87983364cde3c ad9d45b352bf4d17b65e15b98696867f--3a21a3a7b7cd41ccabe87983364cde3c 61d8503497584e5ea19302ff8188fde5 3a21a3a7b7cd41ccabe87983364cde3c--61d8503497584e5ea19302ff8188fde5 8d790c410c6e4c3eb7ce2755f3668d9a 61d8503497584e5ea19302ff8188fde5--8d790c410c6e4c3eb7ce2755f3668d9a 61048834955243529c2b612c679eefc3 8d790c410c6e4c3eb7ce2755f3668d9a--61048834955243529c2b612c679eefc3 d5eb5cab4c8242e1a924158d4b3e4fb3 61048834955243529c2b612c679eefc3--d5eb5cab4c8242e1a924158d4b3e4fb3 26cb4db01cba465cb288f9071db708c8 d5eb5cab4c8242e1a924158d4b3e4fb3--26cb4db01cba465cb288f9071db708c8 5a83d8884a9c4e768910d107047f88ad 26cb4db01cba465cb288f9071db708c8--5a83d8884a9c4e768910d107047f88ad 52dd79e2bce14789b4894dad1948cf2b 5a83d8884a9c4e768910d107047f88ad--52dd79e2bce14789b4894dad1948cf2b 31b2b523ceaf468ab33f6266c7f46c1a 52dd79e2bce14789b4894dad1948cf2b--31b2b523ceaf468ab33f6266c7f46c1a e8710b5769f24bc381fee20c98d25454 31b2b523ceaf468ab33f6266c7f46c1a--e8710b5769f24bc381fee20c98d25454 14ca4700c9b94c44abaf1f680b6fb251 e8710b5769f24bc381fee20c98d25454--14ca4700c9b94c44abaf1f680b6fb251 f941dcd023094d42a1bb055ec69a0082 RX(b0) 14ca4700c9b94c44abaf1f680b6fb251--f941dcd023094d42a1bb055ec69a0082 ec45cb003bae470f90d98dde52fe537e f941dcd023094d42a1bb055ec69a0082--ec45cb003bae470f90d98dde52fe537e fd6a7d4cbed7438b990055249a6c2d3c 98e0534a2a8145f782a9fd96da463c4d H d6c32e4abef941078dc2376bf8f8cf1d--98e0534a2a8145f782a9fd96da463c4d 37de1221f59c4844acb11023b13ca732 2 de0acc0806214014b566a78840ae2eb7 X 98e0534a2a8145f782a9fd96da463c4d--de0acc0806214014b566a78840ae2eb7 de0acc0806214014b566a78840ae2eb7--6bd4764b82d048c1a8dd05648a3105d0 ec60b8c465fe4cac921ef6ed7a086251 RZ(g0) de0acc0806214014b566a78840ae2eb7--ec60b8c465fe4cac921ef6ed7a086251 5ac530d6dfe241fb81fbdfe21f5c155d X ec60b8c465fe4cac921ef6ed7a086251--5ac530d6dfe241fb81fbdfe21f5c155d 5ac530d6dfe241fb81fbdfe21f5c155d--a477119d58e8470991067e68d22ef87c 23415c4d8fcd4b428976c7c7ca881a05 5ac530d6dfe241fb81fbdfe21f5c155d--23415c4d8fcd4b428976c7c7ca881a05 76d70c1943df4b74a93e0c3074c51965 23415c4d8fcd4b428976c7c7ca881a05--76d70c1943df4b74a93e0c3074c51965 3051a67171fd4aab9e27ca520452fe54 76d70c1943df4b74a93e0c3074c51965--3051a67171fd4aab9e27ca520452fe54 7023127519644116b9a72ac5bcacac45 3051a67171fd4aab9e27ca520452fe54--7023127519644116b9a72ac5bcacac45 e660eaaf16c348939f32d9d39858db0e 7023127519644116b9a72ac5bcacac45--e660eaaf16c348939f32d9d39858db0e 31a8645df6044005b8370be8dcdca3bb e660eaaf16c348939f32d9d39858db0e--31a8645df6044005b8370be8dcdca3bb 29199d6062484e4898abbf7e8b962d01 31a8645df6044005b8370be8dcdca3bb--29199d6062484e4898abbf7e8b962d01 b50f23a023be4264aafe3e36230519c7 29199d6062484e4898abbf7e8b962d01--b50f23a023be4264aafe3e36230519c7 598eaff68c0c4a8fbaa4beb084f3690c b50f23a023be4264aafe3e36230519c7--598eaff68c0c4a8fbaa4beb084f3690c 688f4c7b4f5148aebc6603c6eeaa3bf8 598eaff68c0c4a8fbaa4beb084f3690c--688f4c7b4f5148aebc6603c6eeaa3bf8 58b45214649f45d1a3ec6cc3c7dc5632 688f4c7b4f5148aebc6603c6eeaa3bf8--58b45214649f45d1a3ec6cc3c7dc5632 4e396eb0ff89443a8e250073c8349d60 58b45214649f45d1a3ec6cc3c7dc5632--4e396eb0ff89443a8e250073c8349d60 e0f0fd2bacf847caba671e048d566c6a 4e396eb0ff89443a8e250073c8349d60--e0f0fd2bacf847caba671e048d566c6a 17ccfc3fcd144129b3b696609d05ce81 e0f0fd2bacf847caba671e048d566c6a--17ccfc3fcd144129b3b696609d05ce81 331c4b0b4ed6440793d973c8bbdc2028 17ccfc3fcd144129b3b696609d05ce81--331c4b0b4ed6440793d973c8bbdc2028 36bbfe72f5394d41a49ab1392b6289cd RX(b0) 331c4b0b4ed6440793d973c8bbdc2028--36bbfe72f5394d41a49ab1392b6289cd 36bbfe72f5394d41a49ab1392b6289cd--fd6a7d4cbed7438b990055249a6c2d3c fbd1886237b741739b7b62725a06636c 5ceb82511ed14d03928d266dd87a6ec3 H 37de1221f59c4844acb11023b13ca732--5ceb82511ed14d03928d266dd87a6ec3 90e7351282bd4c95916cf0e9f894e297 3 4d8755da25244e5f9871941a945d44c5 5ceb82511ed14d03928d266dd87a6ec3--4d8755da25244e5f9871941a945d44c5 5125947faca14b42944a5d8a3cd009d6 4d8755da25244e5f9871941a945d44c5--5125947faca14b42944a5d8a3cd009d6 15662d81b2be484c8369f3a5eb80b714 5125947faca14b42944a5d8a3cd009d6--15662d81b2be484c8369f3a5eb80b714 4d8f948ccf1e4f82943724ea54445979 X 15662d81b2be484c8369f3a5eb80b714--4d8f948ccf1e4f82943724ea54445979 4d8f948ccf1e4f82943724ea54445979--ffc23220e7914254b082787e5dd64066 eb1d82221b684205a1848cc52756e0e9 RZ(g0) 4d8f948ccf1e4f82943724ea54445979--eb1d82221b684205a1848cc52756e0e9 07267370217a488b8148efc1267ff13c X eb1d82221b684205a1848cc52756e0e9--07267370217a488b8148efc1267ff13c 07267370217a488b8148efc1267ff13c--0044241a82f24f08a6e6bcfd03d7659c 5d5546c940ab44b8bb0fd10bb518608e 07267370217a488b8148efc1267ff13c--5d5546c940ab44b8bb0fd10bb518608e ab34bc95e9b6429eabd2c72c916d6d90 5d5546c940ab44b8bb0fd10bb518608e--ab34bc95e9b6429eabd2c72c916d6d90 031771fc747a4e84bde04c1cdd10f201 ab34bc95e9b6429eabd2c72c916d6d90--031771fc747a4e84bde04c1cdd10f201 393553d19fa141f2ae332dbdfc888b19 X 031771fc747a4e84bde04c1cdd10f201--393553d19fa141f2ae332dbdfc888b19 393553d19fa141f2ae332dbdfc888b19--29199d6062484e4898abbf7e8b962d01 c04bf5075f894e5a8a692d1c6d26c1a4 RZ(g0) 393553d19fa141f2ae332dbdfc888b19--c04bf5075f894e5a8a692d1c6d26c1a4 2c34b92539a94acdbccf55c38d1daaff X c04bf5075f894e5a8a692d1c6d26c1a4--2c34b92539a94acdbccf55c38d1daaff 2c34b92539a94acdbccf55c38d1daaff--598eaff68c0c4a8fbaa4beb084f3690c fd89fc36095644f0a9063e717832cea0 2c34b92539a94acdbccf55c38d1daaff--fd89fc36095644f0a9063e717832cea0 b3abc391bc014fa594b24c135c4074e5 fd89fc36095644f0a9063e717832cea0--b3abc391bc014fa594b24c135c4074e5 07f6160b52be4c58a5c5bd6e4b53ee5c b3abc391bc014fa594b24c135c4074e5--07f6160b52be4c58a5c5bd6e4b53ee5c f5e737fccd8440c0a221c3fd8af08894 07f6160b52be4c58a5c5bd6e4b53ee5c--f5e737fccd8440c0a221c3fd8af08894 8780ec934ea04d6b84fecba6445ed563 f5e737fccd8440c0a221c3fd8af08894--8780ec934ea04d6b84fecba6445ed563 f3409bbffcbc4ef08adf52a0016179c1 8780ec934ea04d6b84fecba6445ed563--f3409bbffcbc4ef08adf52a0016179c1 5c826c313c11470181069a0fea8b65c6 RX(b0) f3409bbffcbc4ef08adf52a0016179c1--5c826c313c11470181069a0fea8b65c6 5c826c313c11470181069a0fea8b65c6--fbd1886237b741739b7b62725a06636c 42192d76dfb24efba156a7d1b25552ed b7b2c910fbb645bf8162302cdbcdd20b H 90e7351282bd4c95916cf0e9f894e297--b7b2c910fbb645bf8162302cdbcdd20b daf1e7c7a8574c6aa6cc1725a92f58b7 b7b2c910fbb645bf8162302cdbcdd20b--daf1e7c7a8574c6aa6cc1725a92f58b7 2fff1b753b714ebe9325e76dc1b943c2 daf1e7c7a8574c6aa6cc1725a92f58b7--2fff1b753b714ebe9325e76dc1b943c2 382f7fc71b8247ccbf805df422c3b000 2fff1b753b714ebe9325e76dc1b943c2--382f7fc71b8247ccbf805df422c3b000 c9e53f0d9c224406b8ca80d5c9ac102d 382f7fc71b8247ccbf805df422c3b000--c9e53f0d9c224406b8ca80d5c9ac102d 422c2e8cb52a4504b2d151c368446c31 c9e53f0d9c224406b8ca80d5c9ac102d--422c2e8cb52a4504b2d151c368446c31 06328a0a0c554d23a9c6ed911d7eb147 422c2e8cb52a4504b2d151c368446c31--06328a0a0c554d23a9c6ed911d7eb147 1f94406c52b74e49a5af3f07b50bf015 X 06328a0a0c554d23a9c6ed911d7eb147--1f94406c52b74e49a5af3f07b50bf015 1f94406c52b74e49a5af3f07b50bf015--ad9d45b352bf4d17b65e15b98696867f 653be5026b0f4461995019dd498aae29 RZ(g0) 1f94406c52b74e49a5af3f07b50bf015--653be5026b0f4461995019dd498aae29 c5601ca8b26a48dfafd5b78fc90d2c29 X 653be5026b0f4461995019dd498aae29--c5601ca8b26a48dfafd5b78fc90d2c29 c5601ca8b26a48dfafd5b78fc90d2c29--61d8503497584e5ea19302ff8188fde5 00914024600c4cf09c222d0b94fabb49 c5601ca8b26a48dfafd5b78fc90d2c29--00914024600c4cf09c222d0b94fabb49 eca5629a90d645b78d43aac89303aaa0 00914024600c4cf09c222d0b94fabb49--eca5629a90d645b78d43aac89303aaa0 b1a5277f36dc41c99fd3b86806eafa2c eca5629a90d645b78d43aac89303aaa0--b1a5277f36dc41c99fd3b86806eafa2c 26acb97d25f94754baa38dab2b67e4d1 X b1a5277f36dc41c99fd3b86806eafa2c--26acb97d25f94754baa38dab2b67e4d1 26acb97d25f94754baa38dab2b67e4d1--688f4c7b4f5148aebc6603c6eeaa3bf8 f74aa89a92744fa78b2b613ed65c65a7 RZ(g0) 26acb97d25f94754baa38dab2b67e4d1--f74aa89a92744fa78b2b613ed65c65a7 7f4629ed71ff4e74bfe02cf9370fb105 X f74aa89a92744fa78b2b613ed65c65a7--7f4629ed71ff4e74bfe02cf9370fb105 7f4629ed71ff4e74bfe02cf9370fb105--4e396eb0ff89443a8e250073c8349d60 c6e03714bb8a42e89ed54a1e072e0940 X 7f4629ed71ff4e74bfe02cf9370fb105--c6e03714bb8a42e89ed54a1e072e0940 c6e03714bb8a42e89ed54a1e072e0940--f5e737fccd8440c0a221c3fd8af08894 c8a8a9ffd8404447aa864e2892c5bc2c RZ(g0) c6e03714bb8a42e89ed54a1e072e0940--c8a8a9ffd8404447aa864e2892c5bc2c 5d19c4c0ff98448ba2b2b09dd743c74e X c8a8a9ffd8404447aa864e2892c5bc2c--5d19c4c0ff98448ba2b2b09dd743c74e 5d19c4c0ff98448ba2b2b09dd743c74e--f3409bbffcbc4ef08adf52a0016179c1 8c86f35fa89441efb4e7250ddabf1d60 RX(b0) 5d19c4c0ff98448ba2b2b09dd743c74e--8c86f35fa89441efb4e7250ddabf1d60 8c86f35fa89441efb4e7250ddabf1d60--42192d76dfb24efba156a7d1b25552ed"},{"location":"tutorials/qml/qaoa/#train-the-qaoa-circuit-to-solve-maxcut","title":"Train the QAOA circuit to solve MaxCut","text":"<p>Given the QAOA circuit above, one can construct the associated Qadence <code>QuantumModel</code> and train it using standard gradient based optimization.</p> <p>The loss function to be minimized reads:</p> \\[\\mathcal{L} =-\\langle \\psi | H_c| \\psi \\rangle= -\\frac12 \\sum_{\\langle i,j\\rangle}  \\left(1 - \\langle \\psi | Z_i Z_j | \\psi \\rangle \\right)\\] <p>where \\(|\\psi\\rangle(\\beta, \\gamma)\\) is the wavefunction obtained by running the QAQA quantum circuit and the sum runs over the edges of the graph \\(\\langle i,j\\rangle\\).</p> <pre><code>import torch\nfrom qadence import QuantumModel\n\ntorch.manual_seed(seed)\n\n\ndef loss_function(model: QuantumModel):\n    # The loss corresponds to the expectation\n    # value of the cost Hamiltonian\n    return -1.0 * model.expectation().squeeze()\n\n\n# initialize the parameters to random values\nmodel = QuantumModel(circuit, observable=cost_ham)\nmodel.reset_vparams(torch.rand(model.num_vparams))\ninitial_loss = loss_function(model)\nprint(f\"Initial loss: {initial_loss}\")\n\n# train the model\nn_epochs = 100\nlr = 0.1\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\nfor i in range(n_epochs):\n    optimizer.zero_grad()\n    loss = loss_function(model)\n    loss.backward()\n    optimizer.step()\n    if (i + 1) % (n_epochs // 10) == 0:\n        print(f\"MaxCut cost at iteration {i+1}: {-loss.item()}\")\n</code></pre> <pre><code>Initial loss: -2.1782381363858794\nMaxCut cost at iteration 10: 3.7470706807026417\nMaxCut cost at iteration 20: 3.8378810288930216\nMaxCut cost at iteration 30: 3.9424197899236133\nMaxCut cost at iteration 40: 3.9981256255766002\nMaxCut cost at iteration 50: 3.996470528508214\nMaxCut cost at iteration 60: 3.9991374608876606\nMaxCut cost at iteration 70: 3.9994678542919555\nMaxCut cost at iteration 80: 3.999872558672829\nMaxCut cost at iteration 90: 3.9999475834121063\nMaxCut cost at iteration 100: 3.9999793311641003\n</code></pre> <p>Qadence offers some convenience functions to implement this training loop with advanced logging and metrics track features. You can refer to this tutorial for more details.</p>"},{"location":"tutorials/qml/qaoa/#results","title":"Results","text":"<p>Given the trained quantum model, one needs to sample the resulting quantum state to recover the bitstring with the highest probability which corresponds to the maximum cut of the graph.</p> <pre><code>samples = model.sample(n_shots=100)[0]\nmost_frequent = max(samples, key=samples.get)\n\nprint(f\"Most frequently sampled bitstring corresponding to the maximum cut: {most_frequent}\")\n\n# let's now draw the cut obtained with the QAOA procedure\ncolors = []\nlabels = {}\nfor node, b in zip(graph.nodes(), most_frequent):\n    colors.append(\"green\") if int(b) == 0 else colors.append(\"red\")\n    labels[node] = \"A\" if int(b) == 0 else \"B\"\n\nnx.draw_networkx(graph, node_color=colors, with_labels=True, labels=labels)\n</code></pre>   Most frequently sampled bitstring corresponding to the maximum cut: 1001  2025-01-08T18:02:44.494318 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/qaoa/#references","title":"References","text":"<ol> <li> <p>Farhi et al. - A Quantum Approximate Optimization Algorithm\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/qml/qcl/","title":"Quantum circuit learning","text":"<p>This tutorial shows how to apply <code>qadence</code> for solving a basic quantum machine learning application: fitting a simple function with the quantum circuit learning<sup>1</sup> (QCL) algorithm.</p> <p>QCL is a supervised quantum machine learning algorithm that uses a parametrized quantum neural network to learn the behavior of an arbitrary mathematical function using a set of function values as training data. This tutorial shows how to fit the \\(\\sin(x)\\) function in the \\([-1, 1]\\) domain.</p> <p>In the following, train and test data are defined.</p> <pre><code>import torch\nfrom torch.utils.data import random_split\n\n# make sure all tensors are kept on the same device\n# only available from PyTorch 2.0\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntorch.set_default_device(device)\n\ndef qcl_training_data(\n    domain: tuple = (0, 2*torch.pi), n_points: int = 200\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n\n    start, end = domain\n\n    x_rand, _ = torch.sort(torch.DoubleTensor(n_points).uniform_(start, end))\n    y_rand = torch.sin(x_rand)\n\n    return x_rand, y_rand\n\nx, y = qcl_training_data()\n\n# random train/test split of the dataset\ntrain_subset, test_subset = random_split(x, [0.75, 0.25])\ntrain_ind = sorted(train_subset.indices)\ntest_ind = sorted(test_subset.indices)\n\nx_train, y_train = x[train_ind], y[train_ind]\nx_test, y_test = x[test_ind], y[test_ind]\n</code></pre>"},{"location":"tutorials/qml/qcl/#train-the-qcl-model","title":"Train the QCL model","text":"<p>Qadence provides the <code>QNN</code> convenience constructor to build a quantum neural network. The <code>QNN</code> class needs a circuit and a list of observables; the number of feature parameters in the input circuit determines the number of input features (i.e. the dimensionality of the classical data given as input) whereas the number of observables determines the number of outputs of the quantum neural network.</p> <p>Total qubit magnetization is used as observable:</p> \\[ \\hat{O} = \\sum_i^N \\hat{\\sigma}_i^z \\] <p>In the following the observable, quantum circuit and corresponding QNN model are constructed.</p> <pre><code>import qadence as qd\n\nn_qubits = 4\n\n# create a simple feature map to encode the input data\nfeature_param = qd.FeatureParameter(\"phi\")\nfeature_map = qd.kron(qd.RX(i, feature_param) for i in range(n_qubits))\nfeature_map = qd.tag(feature_map, \"feature_map\")\n\n# create a digital-analog variational ansatz using Qadence convenience constructors\nansatz = qd.hea(n_qubits, depth=n_qubits)\nansatz = qd.tag(ansatz, \"ansatz\")\n\n# total qubit magnetization observable\nobservable = qd.hamiltonian_factory(n_qubits, detuning=qd.Z)\n\ncircuit = qd.QuantumCircuit(n_qubits, feature_map, ansatz)\nmodel = qd.QNN(circuit, [observable])\nexpval = model(values=torch.rand(10))\n</code></pre> <pre><code>tensor([[ 0.0092],\n        [-0.0207],\n        [-0.4616],\n        [-0.1845],\n        [-0.3761],\n        [-0.1463],\n        [-0.4563],\n        [-0.0016],\n        [-0.3436],\n        [-0.4252]], grad_fn=&lt;CatBackward0&gt;)\n</code></pre> <p>The QCL algorithm uses the output of the quantum neural network as a tunable universal function approximator. Standard PyTorch code is used for training the QNN using a mean-square error loss, Adam optimizer. Training is performend on the GPU if available:</p> <pre><code>n_epochs = 100\nlr = 0.25\n\ninput_values = {\"phi\": x_train}\nmse_loss = torch.nn.MSELoss()  # standard PyTorch loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)  # standard PyTorch Adam optimizer\n\nprint(f\"Initial loss: {mse_loss(model(values=x_train), y_train)}\")\ny_pred_initial = model(values=x_test)\n\nfor i in range(n_epochs):\n\n    optimizer.zero_grad()\n\n    # given a `n_batch` number of input points and a `n_observables`\n    # number of input observables to measure, the QNN returns\n    # an output of the following shape: [n_batch x n_observables]\n    # given that there is only one observable, a squeeze is applied to get\n    # a 1-dimensional tensor\n    loss = mse_loss(model(values=x_train).squeeze(), y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (i+1) % 20 == 0:\n        print(f\"Epoch {i+1} - Loss: {loss.item()}\")\n\nassert loss.item() &lt; 1e-3\n</code></pre> <pre><code>Initial loss: 0.6272721767455237\nEpoch 20 - Loss: 0.008173087377230498\nEpoch 40 - Loss: 0.0011247726222838813\nEpoch 60 - Loss: 0.0001415308609619855\nEpoch 80 - Loss: 2.3606578815826947e-05\nEpoch 100 - Loss: 2.503287372853267e-06\n</code></pre> <p>Qadence offers some convenience functions to implement this training loop with advanced logging and metrics track features. You can refer to this tutorial for more details.</p> <p>The quantum model is now trained on the training data points. To determine the quality of the results, one can check to see how well it fits the function on the test set.</p> <pre><code>import matplotlib.pyplot as plt\n\ny_pred = model({\"phi\": x_test})\n\n# convert all the results to numpy arrays for plotting\nx_train_np = x_train.cpu().detach().numpy().flatten()\ny_train_np = y_train.cpu().detach().numpy().flatten()\nx_test_np = x_test.cpu().detach().numpy().flatten()\ny_test_np = y_test.cpu().detach().numpy().flatten()\ny_pred_initial_np = y_pred_initial.cpu().detach().numpy().flatten()\ny_pred_np = y_pred.cpu().detach().numpy().flatten()\n\nfig, _ = plt.subplots()\nplt.scatter(x_test_np, y_test_np, label=\"Test points\", marker=\"o\", color=\"orange\")\nplt.plot(x_test_np, y_pred_initial_np, label=\"Initial prediction\", color=\"green\", alpha=0.5)\nplt.plot(x_test_np, y_pred_np, label=\"Final prediction\")\nplt.legend()\n</code></pre> 2025-01-08T18:02:48.648029 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/qcl/#references","title":"References","text":"<ol> <li> <p>Mitarai et al., Quantum Circuit Learning \u21a9</p> </li> </ol>"},{"location":"tutorials/qml/ml_tools/callbacks/","title":"Callbacks for Trainer","text":"<p>Qadence <code>ml_tools</code> provides a powerful callback system for customizing various stages of the training process. With callbacks, you can monitor, log, save, and alter your training workflow efficiently. A <code>CallbackManager</code> is used with <code>Trainer</code> to execute the training process with defined callbacks. Following default callbacks are already provided in the <code>Trainer</code>.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#default-callbacks","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks already implemented in the <code>CallbackManager</code> used with <code>Trainer</code>:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>This guide covers how to define and use callbacks in <code>TrainConfig</code>, integrate them with the <code>Trainer</code> class, and create custom callbacks using hooks.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#1-built-in-callbacks","title":"1. Built-in Callbacks","text":"<p>Qadence ml_tools offers several built-in callbacks for common tasks like saving checkpoints, logging metrics, and tracking models. Below is an overview of each.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#11-printmetrics","title":"1.1. <code>PrintMetrics</code>","text":"<p>Prints metrics at specified intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\nprint_metrics_callback = PrintMetrics(on=\"val_batch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[print_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#12-writemetrics","title":"1.2. <code>WriteMetrics</code>","text":"<p>Writes metrics to a specified logging destination.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import WriteMetrics\n\nwrite_metrics_callback = WriteMetrics(on=\"train_epoch_end\", called_every=50)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[write_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#13-plotmetrics","title":"1.3. <code>PlotMetrics</code>","text":"<p>Plots metrics based on user-defined plotting functions.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import PlotMetrics\n\nplot_metrics_callback = PlotMetrics(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=5000,\n    callbacks=[plot_metrics_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#14-loghyperparameters","title":"1.4. <code>LogHyperparameters</code>","text":"<p>Logs hyperparameters to keep track of training settings.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogHyperparameters\n\nlog_hyper_callback = LogHyperparameters(on=\"train_start\", called_every=1)\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_hyper_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#15-savecheckpoint","title":"1.5. <code>SaveCheckpoint</code>","text":"<p>Saves model checkpoints at specified intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint\n\nsave_checkpoint_callback = SaveCheckpoint(on=\"train_epoch_end\", called_every=100)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#16-savebestcheckpoint","title":"1.6. <code>SaveBestCheckpoint</code>","text":"<p>Saves the best model checkpoint based on a validation criterion.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveBestCheckpoint\n\nsave_best_checkpoint_callback = SaveBestCheckpoint(on=\"val_epoch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[save_best_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#17-loadcheckpoint","title":"1.7. <code>LoadCheckpoint</code>","text":"<p>Loads a saved model checkpoint at the start of training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LoadCheckpoint\n\nload_checkpoint_callback = LoadCheckpoint(on=\"train_start\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[load_checkpoint_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#18-logmodeltracker","title":"1.8. <code>LogModelTracker</code>","text":"<p>Logs the model structure and parameters.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LogModelTracker\n\nlog_model_callback = LogModelTracker(on=\"train_end\")\n\nconfig = TrainConfig(\n    max_iter=1000,\n    callbacks=[log_model_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#19-lrschedulerstepdecay","title":"1.9. <code>LRSchedulerStepDecay</code>","text":"<p>Reduces the learning rate by a factor at regular intervals.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerStepDecay\n\nlr_step_decay = LRSchedulerStepDecay(on=\"train_epoch_end\", called_every=100, gamma=0.5)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_step_decay]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#110-lrschedulercyclic","title":"1.10. <code>LRSchedulerCyclic</code>","text":"<p>Applies a cyclic learning rate schedule during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCyclic\n\nlr_cyclic = LRSchedulerCyclic(on=\"train_batch_end\", called_every=1, base_lr=0.001, max_lr=0.01, step_size=2000)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_cyclic]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#111-lrschedulercosineannealing","title":"1.11. <code>LRSchedulerCosineAnnealing</code>","text":"<p>Applies cosine annealing to the learning rate during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import LRSchedulerCosineAnnealing\n\nlr_cosine = LRSchedulerCosineAnnealing(on=\"train_batch_end\", called_every=1, t_max=5000, min_lr=1e-6)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[lr_cosine]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#112-earlystopping","title":"1.12. <code>EarlyStopping</code>","text":"<p>Stops training when a monitored metric has not improved for a specified number of epochs.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(on=\"val_epoch_end\", called_every=1, monitor=\"val_loss\", patience=5, mode=\"min\")\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[early_stopping]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#113-gradientmonitoring","title":"1.13. <code>GradientMonitoring</code>","text":"<p>Logs gradient statistics (e.g., mean, standard deviation, max) during training.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import GradientMonitoring\n\ngradient_monitoring = GradientMonitoring(on=\"train_batch_end\", called_every=10)\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[gradient_monitoring]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#2-custom-callbacks","title":"2. Custom Callbacks","text":"<p>The base <code>Callback</code> class in Qadence allows defining custom behavior that can be triggered at specified events (e.g., start of training, end of epoch). You can set parameters such as when the callback runs (<code>on</code>), frequency of execution (<code>called_every</code>), and optionally define a <code>callback_condition</code>.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#defining-callbacks","title":"Defining Callbacks","text":"<p>There are two main ways to define a callback: 1. Directly providing a function in the <code>Callback</code> instance. 2. Subclassing the <code>Callback</code> class and implementing custom logic.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-1-providing-a-callback-function-directly","title":"Example 1: Providing a Callback Function Directly","text":"<pre><code>from qadence.ml_tools.callbacks import Callback\n\n# Define a custom callback function\ndef custom_callback_function(trainer, config, writer):\n    print(\"Executing custom callback.\")\n\n# Create the callback instance\ncustom_callback = Callback(\n    on=\"train_end\",\n    callback=custom_callback_function\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-2-subclassing-the-callback","title":"Example 2: Subclassing the Callback","text":"<pre><code>from qadence.ml_tools.callbacks import Callback\n\nclass CustomCallback(Callback):\n    def run_callback(self, trainer, config, writer):\n        print(\"Custom behavior in run_callback method.\")\n\n# Create the subclassed callback instance\ncustom_callback = CustomCallback(on=\"train_batch_end\", called_every=10)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#3-adding-callbacks-to-trainconfig","title":"3. Adding Callbacks to <code>TrainConfig</code>","text":"<p>To use callbacks in <code>TrainConfig</code>, add them to the <code>callbacks</code> list when configuring the training process.</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.ml_tools.callbacks import SaveCheckpoint, PrintMetrics\n\nconfig = TrainConfig(\n    max_iter=10000,\n    callbacks=[\n        SaveCheckpoint(on=\"val_epoch_end\", called_every=50),\n        PrintMetrics(on=\"train_epoch_end\", called_every=100),\n    ]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/callbacks/#4-using-callbacks-with-trainer","title":"4. Using Callbacks with <code>Trainer</code>","text":"<p>The <code>Trainer</code> class in <code>qadence.ml_tools</code> provides built-in support for executing callbacks at various stages in the training process, managed through a callback manager. By default, several callbacks are added to specific hooks to automate common tasks, such as check-pointing, metric logging, and model tracking.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#default-callbacks_1","title":"Default Callbacks","text":"<p>Below is a list of the default callbacks and their assigned hooks:</p> <ul> <li><code>train_start</code>: <code>PlotMetrics</code>, <code>SaveCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_epoch_end</code>: <code>SaveCheckpoint</code>, <code>PrintMetrics</code>, <code>PlotMetrics</code>, <code>WriteMetrics</code></li> <li><code>val_epoch_end</code>: <code>SaveBestCheckpoint</code>, <code>WriteMetrics</code></li> <li><code>train_end</code>: <code>LogHyperparameters</code>, <code>LogModelTracker</code>, <code>WriteMetrics</code>, <code>SaveCheckpoint</code>, <code>PlotMetrics</code></li> </ul> <p>These defaults handle common needs, but you can also add custom callbacks to any hook.</p>"},{"location":"tutorials/qml/ml_tools/callbacks/#example-adding-a-custom-callback","title":"Example: Adding a Custom Callback","text":"<p>To create a custom <code>Trainer</code> that includes a <code>PrintMetrics</code> callback executed specifically at the end of each epoch, follow the steps below.</p> <pre><code>from qadence.ml_tools.trainer import Trainer\nfrom qadence.ml_tools.callbacks import PrintMetrics\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.print_metrics_callback = PrintMetrics(on=\"train_epoch_end\", called_every = 10)\n\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        self.print_metrics_callback.run_callback(self)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/data_and_config/","title":"Data and Configurations","text":""},{"location":"tutorials/qml/ml_tools/data_and_config/#1-dataloaders","title":"1. Dataloaders","text":"<p>When using Qadence, you can supply classical data to a quantum machine learning algorithm by using a standard PyTorch <code>DataLoader</code> instance. Qadence also provides the <code>DictDataLoader</code> convenience class which allows to build dictionaries of <code>DataLoader</code>s instances and easily iterate over them.</p> <pre><code>import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom qadence.ml_tools import DictDataLoader, to_dataloader\n\n\ndef dataloader(data_size: int = 25, batch_size: int = 5, infinite: bool = False) -&gt; DataLoader:\n    x = torch.linspace(0, 1, data_size).reshape(-1, 1)\n    y = torch.sin(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=infinite)\n\n\ndef dictdataloader(data_size: int = 25, batch_size: int = 5) -&gt; DictDataLoader:\n    dls = {}\n    for k in [\"y1\", \"y2\"]:\n        x = torch.rand(data_size, 1)\n        y = torch.sin(x)\n        dls[k] = to_dataloader(x, y, batch_size=batch_size, infinite=True)\n    return DictDataLoader(dls)\n\n\n# iterate over standard DataLoader\nfor (x,y) in dataloader(data_size=6, batch_size=2):\n    print(f\"Standard {x = }\")\n\n# construct an infinite dataset which will keep sampling indefinitely\nn_epochs = 5\ndl = iter(dataloader(data_size=6, batch_size=2, infinite=True))\nfor _ in range(n_epochs):\n    (x, y) = next(dl)\n    print(f\"Infinite {x = }\")\n\n# iterate over DictDataLoader\nddl = dictdataloader()\ndata = next(iter(ddl))\nprint(f\"{data = }\")\n</code></pre> <pre><code>Standard x = tensor([[0.0000],\n        [0.2000]])\nStandard x = tensor([[0.4000],\n        [0.6000]])\nStandard x = tensor([[0.8000],\n        [1.0000]])\nInfinite x = tensor([[0.2000],\n        [0.4000]])\nInfinite x = tensor([[1.0000],\n        [0.6000]])\nInfinite x = tensor([[0.0000],\n        [0.8000]])\nInfinite x = tensor([[0.2000],\n        [0.4000]])\nInfinite x = tensor([[1.0000],\n        [0.6000]])\ndata = {'y1': [tensor([[0.4409],\n        [0.0250],\n        [0.3490],\n        [0.3025],\n        [0.4626]]), tensor([[0.4267],\n        [0.0250],\n        [0.3420],\n        [0.2979],\n        [0.4463]])], 'y2': [tensor([[0.4027],\n        [0.4736],\n        [0.4973],\n        [0.2373],\n        [0.9213]]), tensor([[0.3919],\n        [0.4561],\n        [0.4770],\n        [0.2350],\n        [0.7964]])]}\n</code></pre> <p>Note:     In case of <code>infinite</code>=True, the dataloader iterator will provide a random sample from the dataset.</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#2-training-configuration","title":"2. Training Configuration","text":"<p>The <code>TrainConfig</code> class provides a comprehensive configuration setup for training quantam machine learning models in Qadence. This configuration includes settings for batch size, logging, check-pointing, validation, and additional custom callbacks that control the training process's granularity and flexibility.</p> <p>The <code>TrainConfig</code> tells <code>Trainer</code>  what batch_size should be used, how many epochs to train, in which intervals to print/log metrics and how often to store intermediate checkpoints. It is also possible to provide custom callback functions by instantiating a <code>Callback</code> with a function <code>callback</code>.</p> <p>For example of how to use the TrainConfig with <code>Trainer</code>, please see Examples in Trainer</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#21-explanation-of-trainconfig-attributes","title":"2.1 Explanation of <code>TrainConfig</code> Attributes","text":"Attribute Type Default Description <code>max_iter</code> <code>int</code> <code>10000</code> Total number of training epochs. <code>batch_size</code> <code>int</code> <code>1</code> Batch size for training. <code>print_every</code> <code>int</code> <code>0</code> Frequency of console output. Set to <code>0</code> to disable. <code>write_every</code> <code>int</code> <code>0</code> Frequency of logging metrics. Set to <code>0</code> to disable. <code>plot_every</code> <code>int</code> <code>0</code> Frequency of plotting metrics. Set to <code>0</code> to disable. <code>checkpoint_every</code> <code>int</code> <code>0</code> Frequency of saving checkpoints. Set to <code>0</code> to disable. <code>val_every</code> <code>int</code> <code>0</code> Frequency of validation checks. Set to <code>0</code> to disable. <code>val_epsilon</code> <code>float</code> <code>1e-5</code> Threshold for validation improvement. <code>validation_criterion</code> <code>Callable</code> <code>None</code> Function for validating metric improvement. <code>trainstop_criterion</code> <code>Callable</code> <code>None</code> Function to stop training early. <code>callbacks</code> <code>list[Callback]</code> <code>[]</code> List of custom callbacks. <code>root_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Root directory for saving logs and checkpoints. <code>log_folder</code> <code>Path</code> <code>\"./qml_logs\"</code> Logging directory for saving logs and checkpoints. <code>log_model</code> <code>bool</code> <code>False</code> Enables model logging. <code>verbose</code> <code>bool</code> <code>True</code> Enables detailed logging. <code>tracking_tool</code> <code>ExperimentTrackingTool</code> <code>TENSORBOARD</code> Tool for tracking training metrics. <code>plotting_functions</code> <code>tuple</code> <code>()</code> Functions for plotting metrics. <pre><code>from qadence.ml_tools import OptimizeResult, TrainConfig\nfrom qadence.ml_tools.callbacks import Callback\n\nbatch_size = 5\nn_epochs = 100\n\nprint_parameters = lambda opt_res: print(opt_res.model.parameters())\ncondition_print = lambda opt_res: opt_res.loss &lt; 1.0e-03\nmodify_extra_opt_res = {\"n_epochs\": n_epochs}\ncustom_callback = Callback(on=\"train_end\", callback = print_parameters, callback_condition=condition_print, modify_optimize_result=modify_extra_opt_res, called_every=10,)\n\nconfig = TrainConfig(\n    root_folder=\"some_path/\",\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n    callbacks = [custom_callback]\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/data_and_config/#22-key-configuration-options-in-trainconfig","title":"2.2 Key Configuration Options in <code>TrainConfig</code>","text":""},{"location":"tutorials/qml/ml_tools/data_and_config/#iterations-and-batch-size","title":"Iterations and Batch Size","text":"<ul> <li><code>max_iter</code> (int): Specifies the total number of training iterations (epochs). For an <code>InfiniteTensorDataset</code>, each epoch contains one batch; for a <code>TensorDataset</code>, it contains <code>len(dataloader)</code> batches.</li> <li><code>batch_size</code> (int): Defines the number of samples processed in each training iteration.</li> </ul> <p>Example: <pre><code>config = TrainConfig(max_iter=2000, batch_size=32)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#training-parameters","title":"Training Parameters","text":"<ul> <li><code>print_every</code> (int): Controls how often loss and metrics are printed to the console.</li> <li><code>write_every</code> (int): Determines how frequently metrics are written to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>checkpoint_every</code> (int): Sets the frequency for saving model checkpoints.</li> </ul> <p>Note: Set 0 to diable.</p> <p>Example: <pre><code>config = TrainConfig(print_every=100, write_every=50, checkpoint_every=50)\n</code></pre></p> <p>The user can provide either the <code>root_folder</code> or the <code>log_folder</code> for saving checkpoints and logging. When neither are provided, the default <code>root_folder</code> \"./qml_logs\" is used.</p> <ul> <li><code>root_folder</code> (Path): The root directory for saving checkpoints and logs. All training logs will be saved inside a subfolder in this root directory. (The path to these subfolders can be accessed using config._subfolders, and the current logging folder is config.log_folder)</li> <li><code>create_subfolder_per_run</code> (bool): Creates a unique subfolder for each training run within the specified folder.</li> <li><code>tracking_tool</code> (ExperimentTrackingTool): Specifies the tracking tool to log metrics, e.g., TensorBoard or MLflow.</li> <li><code>log_model</code> (bool): Enables logging of a serialized version of the model, which is useful for model versioning. Thi happens at the end of training.</li> </ul> <p>Note     - The user can also provide <code>log_folder</code> argument - which will only be used when <code>create_subfolder_per_run</code> = False.     -  <code>log_folder</code> (Path): The log folder used for saving checkpoints and logs.</p> <p>Example: <pre><code>config = TrainConfig(root_folder=\"path/to/checkpoints\", tracking_tool=ExperimentTrackingTool.MLFLOW, checkpoint_best_only=True)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#validation-parameters","title":"Validation Parameters","text":"<ul> <li><code>checkpoint_best_only</code> (bool): If set to <code>True</code>, saves checkpoints only when there is an improvement in the validation metric.</li> <li><code>val_every</code> (int): Frequency of validation checks. Setting this to <code>0</code> disables validation.</li> <li><code>val_epsilon</code> (float): A small threshold used to compare the current validation loss with previous best losses.</li> <li><code>validation_criterion</code> (Callable): A custom function to assess if the validation metric meets a specified condition.</li> </ul> <p>Example: <pre><code>config = TrainConfig(val_every=200, checkpoint_best_only = True, validation_criterion=lambda current, best: current &lt; best - 0.001)\n</code></pre></p> <p>If it is desired to only the save the \"best\" checkpoint, the following must be ensured:</p> <pre><code>(a) `checkpoint_best_only = True` is used while creating the configuration through `TrainConfig`,\n(b) `val_every` is set to a valid integer value (for example, `val_every = 10`) which controls the no. of iterations after which the validation data should be used to evaluate the model during training, which can also be set through `TrainConfig`,\n(c) a validation criterion is provided through the `validation_criterion`, set through `TrainConfig` to quantify the definition of \"best\", and\n(d) the validation dataloader passed to `Trainer` is of type `DataLoader`. In this case, it is expected that a validation dataloader is also provided along with the train dataloader since the validation data will be used to decide the \"best\" checkpoint.\n</code></pre> <p>The criterion used to decide the \"best\" checkpoint can be customized by <code>validation_criterion</code>, which should be a function that can take val_loss, best_loss, and val_epsilon arguments and return a boolean value (True or False) indicating whether some validation metric is satisfied or not. An example of a simple <code>validation_criterion</code> is: <pre><code>def validation_criterion(val_loss: float, best_val_loss: float, val_epsilon: float) -&gt; bool:\n    return val_loss &lt; (best_val_loss - val_epsilon)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#custom-callbacks","title":"Custom Callbacks","text":"<p><code>TrainConfig</code> supports custom callbacks that can be triggered at specific stages of training. The <code>callbacks</code> attribute accepts a list of callback instances, which allow for custom behaviors like early stopping or additional logging. See Callbacks for more details.</p> <ul> <li><code>callbacks</code> (list[Callback]): List of custom callbacks to execute during training.</li> </ul> <p>Example: <pre><code>from qadence.ml_tools.callbacks import Callback\n\ndef callback_fn(trainer, config, writer):\n    if trainer.opt_res.loss &lt; 0.001:\n        print(\"Custom Callback: Loss threshold reached!\")\n\ncustom_callback = Callback(on = \"train_epoch_end\", called_every = 10, callback_function = callback_fn )\n\nconfig = TrainConfig(callbacks=[custom_callback])\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#hyperparameters-and-plotting","title":"Hyperparameters and Plotting","text":"<ul> <li><code>hyperparams</code> (dict): A dictionary of hyperparameters (e.g., learning rate, regularization) to be tracked by the tracking tool.</li> <li><code>plot_every</code> (int): Determines how frequently plots are saved to the tracking tool, such as TensorBoard or MLflow.</li> <li><code>plotting_functions</code> (tuple[LoggablePlotFunction, ...]): Functions for in-training plotting of metrics or model state.</li> </ul> <p>Note: Please ensure that plotting_functions are provided when plot_every &gt; 0</p> <p>Example: <pre><code>config = TrainConfig(\n    plot_every=10,\n    hyperparams={\"learning_rate\": 0.001, \"batch_size\": 32},\n    plotting_functions=(plot_loss_function,)\n)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#3-experiment-tracking-with-mlflow","title":"3. Experiment tracking with mlflow","text":"<p>Qadence allows to track runs and log hyperparameters, models and plots with tensorboard and mlflow. In the following, we demonstrate the integration with mlflow.</p>"},{"location":"tutorials/qml/ml_tools/data_and_config/#mlflow-configuration","title":"mlflow configuration","text":"<p>We have control over our tracking configuration by setting environment variables. First, let's look at the tracking URI. For the purpose of this demo we will be working with a local database, in a similar fashion as described here, <pre><code>export MLFLOW_TRACKING_URI=sqlite:///mlruns.db\n</code></pre></p> <p>Qadence can also read the following two environment variables to define the mlflow experiment name and run name <pre><code>export MLFLOW_EXPERIMENT=test_experiment\nexport MLFLOW_RUN_NAME=run_0\n</code></pre></p> <p>If no tracking URI is provided, mlflow stores run information and artifacts in the local <code>./mlflow</code> directory and if no names are defined, the experiment and run will be named with random UUIDs.</p>"},{"location":"tutorials/qml/ml_tools/trainer/","title":"Qadence Trainer Guide","text":"<p>The <code>Trainer</code> class in <code>qadence.ml_tools</code> is a versatile tool designed to streamline the training of quantum machine learning models. It offers flexibility for both gradient-based and gradient-free optimization methods, supports custom loss functions, and integrates seamlessly with tracking tools like TensorBoard and MLflow. Additionally, it provides hooks for implementing custom behaviors during the training process.</p> <p>For training QML models, Qadence offers this out-of-the-box <code>Trainer</code> for optimizing differentiable models, e.g. <code>QNN</code>s and <code>QuantumModel</code>, containing either trainable and/or non-trainable parameters (see the parameters tutorial for detailed information about parameter types):</p>"},{"location":"tutorials/qml/ml_tools/trainer/#1-overview","title":"1. Overview","text":"<p>The <code>Trainer</code> class simplifies the training workflow by managing the training loop, handling data loading, and facilitating model evaluation. It is compatible with various optimization strategies and allows for extensive customization to meet specific training requirements.</p> <p>Example of initializing the <code>Trainer</code>:</p> <pre><code>from qadence.ml_tools import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\n# Initialize Trainer with model, optimizer, and configuration\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\n</code></pre> <p>Notes: <code>qadence</code> versions prior to 1.9.0 provided <code>train_with_grad</code> and <code>train_no_grad</code> functions, which are being replaced with <code>Trainer</code>. The user can transition as following. <pre><code>from qadence.ml_tools import train_with_grad\ntrain_with_grad(model=model, optimizer=optimizer, config=config, data = data)\n</code></pre> to <pre><code>from qadence.ml_tools import Trainer\ntrainer = Trainer(model=model, optimizer=optimizer, config=config)\ntrainer.fit(train_dataloader = data)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/trainer/#2-gradient-based-and-gradient-free-optimization","title":"2. Gradient-Based and Gradient-Free Optimization","text":"<p>The <code>Trainer</code> supports both gradient-based and gradient-free optimization methods. Default is gradient-based optimization.</p> <ul> <li>Gradient-Based Optimization: Utilizes optimizers from PyTorch's <code>torch.optim</code> module. This is the default behaviour of the <code>Trainer</code>, thus setting this is not necessary. However, it can be explicity mentioned as follows. Example of using gradient-based optimization:</li> </ul> <pre><code>from qadence.ml_tools import Trainer\n\n# set_use_grad(True) to enable gradient based training. This is the default behaviour of Trainer.\nTrainer.set_use_grad(True)\n</code></pre> <ul> <li>Gradient-Free Optimization: Employs optimization algorithms from the Nevergrad library.</li> </ul> <p>Example of using gradient-free optimization with Nevergrad:</p> <pre><code>from qadence.ml_tools import Trainer\n\n# set_use_grad(False) to disable gradient based training.\nTrainer.set_use_grad(False)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#using-context-managers-for-mixed-optimization","title":"Using Context Managers for Mixed Optimization","text":"<p>For cases requiring both optimization methods in a single training session, the <code>Trainer</code> class provides context managers to enable or disable gradients.</p> <pre><code># Temporarily switch to gradient-based optimization\nwith trainer.enable_grad_opt(optimizer):\n    print(\"Gradient Based Optimization\")\n    # trainer.fit(train_loader)\n\n# Switch to gradient-free optimization for specific steps\nwith trainer.disable_grad_opt(ng_optimizer):\n    print(\"Gradient Free Optimization\")\n    # trainer.fit(train_loader)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#3-custom-loss-functions","title":"3. Custom Loss Functions","text":"<p>Users can define custom loss functions tailored to their specific tasks. The <code>Trainer</code> accepts a <code>loss_fn</code> parameter, which should be a callable that takes the model and data as inputs and returns a tuple containing the loss tensor and a dictionary of metrics.</p> <p>Example of using a custom loss function:</p> <pre><code>import torch\nfrom itertools import count\ncnt = count()\ncriterion = torch.nn.MSELoss()\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n</code></pre> <p>This custom loss function can be used in the trainer <pre><code>from qadence.ml_tools import Trainer, TrainConfig\nfrom torch.optim import Adam\n\n# Initialize model and optimizer\nmodel = ...  # Define or load a quantum model here\noptimizer = Adam(model.parameters(), lr=0.01)\nconfig = TrainConfig(max_iter=100, print_every=10)\n\ntrainer = Trainer(model=model, optimizer=optimizer, config=config, loss_fn=loss_fn)\n</code></pre></p>"},{"location":"tutorials/qml/ml_tools/trainer/#4-hooks-for-custom-behavior","title":"4. Hooks for Custom Behavior","text":"<p>The <code>Trainer</code> class provides several hooks that enable users to inject custom behavior at different stages of the training process. These hooks are methods that can be overridden in a subclass to execute custom code. The available hooks include:</p> <ul> <li><code>on_train_start</code>: Called at the beginning of the training process.</li> <li><code>on_train_end</code>: Called at the end of the training process.</li> <li><code>on_train_epoch_start</code>: Called at the start of each training epoch.</li> <li><code>on_train_epoch_end</code>: Called at the end of each training epoch.</li> <li><code>on_train_batch_start</code>: Called at the start of each training batch.</li> <li><code>on_train_batch_end</code>: Called at the end of each training batch.</li> </ul> <p>Each \"start\" and \"end\" hook receives data and loss metrics as arguments. The specific values provided for these arguments depend on the training stage associated with the hook. The context of the training stage (e.g., training, validation, or testing) determines which metrics are relevant and how they are populated. For details of inputs on each hook, please review the documentation of <code>BaseTrainer</code>.</p> <pre><code>- Example of what inputs are provided to training hooks.\n\n    ```\n    def on_train_batch_start(self, batch: Tuple[torch.Tensor, ...] | None) -&gt; None:\n        \"\"\"\n        Called at the start of each training batch.\n\n        Args:\n            batch: A batch of data from the DataLoader. Typically a tuple containing\n                input tensors and corresponding target tensors.\n        \"\"\"\n        pass\n    ```\n    ```\n    def on_train_batch_end(self, train_batch_loss_metrics: Tuple[torch.Tensor, Any]) -&gt; None:\n        \"\"\"\n        Called at the end of each training batch.\n\n        Args:\n            train_batch_loss_metrics: Metrics for the training batch loss.\n                Tuple of (loss, metrics)\n        \"\"\"\n        pass\n    ```\n</code></pre> <p>Example of using a hook to log a message at the end of each epoch:</p> <pre><code>from qadence.ml_tools import Trainer\n\nclass CustomTrainer(Trainer):\n    def on_train_epoch_end(self, train_epoch_loss_metrics):\n        print(f\"End of epoch - Loss and Metrics: {train_epoch_loss_metrics}\")\n</code></pre> <p>Notes: Trainer offers inbuilt callbacks as well. Callbacks are mainly for logging/tracking purposes, but the above mentioned hooks are generic. The workflow for every train batch looks like: 1. perform on_train_batch_start callbacks, 2. call the on_train_batch_start hook, 3. do the batch training, 4. call the on_train_batch_end hook, and 5. perform on_train_batch_end callbacks.</p> <p>The use of <code>on_</code>{phase}<code>_start</code> and <code>on_</code>{phase}<code>_end</code> hooks is not specifically to add extra callbacks, but for any other generic pre/post processing. For example, reshaping input batch in case of RNNs/LSTMs, post processing loss and adding an extra metric. They could also be used to add more callbacks (which is not recommended - as we provide methods to add extra callbacks in the TrainCofig)</p>"},{"location":"tutorials/qml/ml_tools/trainer/#5-experiment-tracking-with-tensorboard-and-mlflow","title":"5. Experiment Tracking with TensorBoard and MLflow","text":"<p>The <code>Trainer</code> integrates with TensorBoard and MLflow for experiment tracking:</p> <ul> <li> <p>TensorBoard: Logs metrics and visualizations during training, allowing users to monitor the training process.</p> </li> <li> <p>MLflow: Tracks experiments, logs parameters, metrics, and artifacts, and provides a user-friendly interface for comparing different runs.</p> </li> </ul> <p>To utilize these tracking tools, the <code>Trainer</code> can be configured with appropriate writers that handle the logging of metrics and other relevant information during training.</p> <p>Example of using TensorBoard tracking:</p> <pre><code>from qadence.ml_tools import TrainConfig\nfrom qadence.types import ExperimentTrackingTool\n\n# Set up tracking with TensorBoard\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.TENSORBOARD)\n</code></pre> <p>Example of using MLflow tracking:</p> <pre><code>from qadence.types import ExperimentTrackingTool\n\n# Set up tracking with MLflow\nconfig = TrainConfig(max_iter=100, tracking_tool=ExperimentTrackingTool.MLFLOW)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#6-examples","title":"6. Examples","text":""},{"location":"tutorials/qml/ml_tools/trainer/#61-training-with-trainer-and-trainconfig","title":"6.1. Training with <code>Trainer</code> and <code>TrainConfig</code>","text":""},{"location":"tutorials/qml/ml_tools/trainer/#setup","title":"Setup","text":"<p>Let's do the necessary imports and declare a <code>DataLoader</code>. We can already define some hyperparameters here, including the seed for random number generators. mlflow can log hyperparameters with arbitrary types, for example the observable that we want to monitor (<code>Z</code> in this case, which has a <code>qadence.Operation</code> type).</p> <pre><code>import random\nfrom itertools import count\n\nimport numpy as np\nimport torch\nfrom matplotlib import pyplot as plt\nfrom matplotlib.figure import Figure\nfrom torch.nn import Module\nfrom torch.utils.data import DataLoader\n\nfrom qadence import hea, QuantumCircuit, Z\nfrom qadence.constructors import feature_map, hamiltonian_factory\nfrom qadence.ml_tools import Trainer, TrainConfig\nfrom qadence.ml_tools.data import to_dataloader\nfrom qadence.ml_tools.utils import rand_featureparameters\nfrom qadence.ml_tools.models import QNN, QuantumModel\nfrom qadence.types import ExperimentTrackingTool\n\nhyperparams = {\n    \"seed\": 42,\n    \"batch_size\": 10,\n    \"n_qubits\": 2,\n    \"ansatz_depth\": 1,\n    \"observable\": Z,\n}\n\nnp.random.seed(hyperparams[\"seed\"])\ntorch.manual_seed(hyperparams[\"seed\"])\nrandom.seed(hyperparams[\"seed\"])\n\n\ndef dataloader(batch_size: int = 25) -&gt; DataLoader:\n    x = torch.linspace(0, 1, batch_size).reshape(-1, 1)\n    y = torch.cos(x)\n    return to_dataloader(x, y, batch_size=batch_size, infinite=True)\n</code></pre> <p>We continue with the regular QNN definition, together with the loss function and optimizer.</p> <pre><code>obs = hamiltonian_factory(register=hyperparams[\"n_qubits\"], detuning=hyperparams[\"observable\"])\n\ndata = dataloader(hyperparams[\"batch_size\"])\nfm = feature_map(hyperparams[\"n_qubits\"], param=\"x\")\n\nmodel = QNN(\n    QuantumCircuit(\n        hyperparams[\"n_qubits\"], fm, hea(hyperparams[\"n_qubits\"], hyperparams[\"ansatz_depth\"])\n    ),\n    observable=obs,\n    inputs=[\"x\"],\n)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ninputs = rand_featureparameters(model, 1)\n\ndef loss_fn(model: QuantumModel, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    out = model.expectation(inputs)\n    loss = criterion(out, torch.rand(1))\n    return loss, {}\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#trainconfig-specifications","title":"<code>TrainConfig</code> specifications","text":"<p>Qadence offers different tracking options via <code>TrainConfig</code>. Here we use the <code>ExperimentTrackingTool</code> type to specify that we want to track the experiment with mlflow. Tracking with tensorboard is also possible. We can then indicate what and how often we want to track or log.</p> <p>For Training <code>write_every</code> controls the number of epochs after which the loss values is logged. Thanks to the <code>plotting_functions</code> and <code>plot_every</code>arguments, we are also able to plot model-related quantities throughout training. Notice that arbitrary plotting functions can be passed, as long as the signature is the same as <code>plot_fn</code> below. Finally, the trained model can be logged by setting <code>log_model=True</code>. Here is an example of plotting function and training configuration</p> <pre><code>def plot_fn(model: Module, iteration: int) -&gt; tuple[str, Figure]:\n    descr = f\"ufa_prediction_epoch_{iteration}.png\"\n    fig, ax = plt.subplots()\n    x = torch.linspace(0, 1, 100).reshape(-1, 1)\n    out = model.expectation(x)\n    ax.plot(x.detach().numpy(), out.detach().numpy())\n    return descr, fig\n\n\nconfig = TrainConfig(\n    root_folder=\"mlflow_demonstration\",\n    max_iter=10,\n    checkpoint_every=1,\n    plot_every=2,\n    write_every=1,\n    log_model=True,\n    tracking_tool=ExperimentTrackingTool.MLFLOW,\n    hyperparams=hyperparams,\n    plotting_functions=(plot_fn,),\n)\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#training-and-inspecting","title":"Training and inspecting","text":"<p>Model training happens as usual <pre><code>trainer = Trainer(model, optimizer, config, loss_fn)\ntrainer.fit(train_dataloader=data)\n</code></pre></p> <p>After training , we can inspect our experiment via the mlflow UI <pre><code>mlflow ui --port 8080 --backend-store-uri sqlite:///mlruns.db\n</code></pre> In this case, since we're running on a local server, we can access the mlflow UI by navigating to http://localhost:8080/.</p>"},{"location":"tutorials/qml/ml_tools/trainer/#62-fitting-a-function-with-a-qnn-using-ml_tools","title":"6.2. Fitting a function with a QNN using <code>ml_tools</code>","text":"<p>In Quantum Machine Learning, the general consensus is to use <code>complex128</code> precision for states and operators and <code>float64</code> precision for parameters. This is also the convention which is used in <code>qadence</code>. However, for specific usecases, lower precision can greatly speed up training and reduce memory consumption. When using the <code>pyqtorch</code> backend, <code>qadence</code> offers the option to move a <code>QuantumModel</code> instance to a specific precision using the torch <code>to</code> syntax.</p> <p>Let's look at a complete example of how to use <code>Trainer</code> now. Here we perform a validation check during training and use a validation criterion that checks whether the validation loss in the current iteration has decreased compared to the lowest validation loss from all previous iterations. For demonstration, the train and the validation data are kept the same here. However, it is beneficial and encouraged to keep them distinct in practice to understand model's generalization capabilities.</p> <pre><code>from pathlib import Path\nimport torch\nfrom functools import reduce\nfrom operator import add\nfrom itertools import count\nimport matplotlib.pyplot as plt\n\nfrom qadence import Parameter, QuantumCircuit, Z\nfrom qadence import hamiltonian_factory, hea, feature_map, chain\nfrom qadence import QNN\nfrom qadence.ml_tools import  TrainConfig, Trainer, to_dataloader\n\nTrainer.set_use_grad(True)\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDTYPE = torch.complex64\nn_qubits = 4\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 100\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncnt = count()\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\ndef loss_fn(model: torch.nn.Module, data: torch.Tensor) -&gt; tuple[torch.Tensor, dict]:\n    next(cnt)\n    x, y = data[0], data[1]\n    out = model(x)\n    loss = criterion(out, y)\n    return loss, {}\n\ndef validation_criterion(\n    current_validation_loss: float, current_best_validation_loss: float, val_epsilon: float\n) -&gt; bool:\n    return current_validation_loss &lt;= current_best_validation_loss - val_epsilon\n\nn_epochs = 300\n\nconfig = TrainConfig(\n    max_iter=n_epochs,\n    batch_size=batch_size,\n    checkpoint_best_only=True,\n    val_every=10,  # The model will be run on the validation data after every `val_every` epochs.\n    validation_criterion=validation_criterion\n)\n\nfn = lambda x, degree: .05 * reduce(add, (torch.cos(i*x) + torch.sin(i*x) for i in range(degree)), 0.)\nx = torch.linspace(0, 10, batch_size, dtype=torch.float32).reshape(-1, 1)\ny = fn(x, 5)\n\ntrain_dataloader = to_dataloader(x, y, batch_size=batch_size, infinite=True)\nval_dataloader =  to_dataloader(x, y, batch_size=batch_size, infinite=True)\n\ntrainer = Trainer(model, optimizer, config, loss_fn=loss_fn,\n                    train_dataloader = train_dataloader, val_dataloader = val_dataloader,\n                    device=DEVICE, dtype=DTYPE)\ntrainer.fit()\n\nplt.clf()\nplt.plot(x.numpy(), y.numpy(), label='truth')\nplt.plot(x.numpy(), model(x).detach().numpy(), \"--\", label=\"final\", linewidth=3)\nplt.legend()\n</code></pre> 2025-01-08T18:03:10.752304 image/svg+xml Matplotlib v3.10.0, https://matplotlib.org/"},{"location":"tutorials/qml/ml_tools/trainer/#63-fitting-a-function-low-level-api","title":"6.3. Fitting a function - Low-level API","text":"<p>For users who want to use the low-level API of <code>qadence</code>, here an example written without <code>Trainer</code>.</p> <pre><code>from pathlib import Path\nimport torch\nfrom itertools import count\nfrom qadence.constructors import hamiltonian_factory, hea, feature_map\nfrom qadence import chain, Parameter, QuantumCircuit, Z\nfrom qadence import QNN\nfrom qadence.ml_tools import TrainConfig\n\nn_qubits = 2\nfm = feature_map(n_qubits)\nansatz = hea(n_qubits=n_qubits, depth=3)\nobservable = hamiltonian_factory(n_qubits, detuning=Z)\ncircuit = QuantumCircuit(n_qubits, fm, ansatz)\n\nmodel = QNN(circuit, observable, backend=\"pyqtorch\", diff_mode=\"ad\")\nbatch_size = 1\ninput_values = {\"phi\": torch.rand(batch_size, requires_grad=True)}\npred = model(input_values)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nn_epochs=50\ncnt = count()\n\ntmp_path = Path(\"/tmp\")\n\nconfig = TrainConfig(\n    root_folder=tmp_path,\n    max_iter=n_epochs,\n    checkpoint_every=100,\n    write_every=100,\n    batch_size=batch_size,\n)\n\nx = torch.linspace(0, 1, batch_size).reshape(-1, 1)\ny = torch.sin(x)\n\nfor i in range(n_epochs):\n    out = model(x)\n    loss = criterion(out, y)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#64-custom-train-loop","title":"6.4. Custom <code>train</code> loop","text":"<p>If you need custom training functionality that goes beyond what is available in <code>qadence.ml_tools.Trainer</code> you can write your own training loop based on the building blocks that are available in Qadence.</p> <p>A simplified version of Qadence's train loop is defined below. Feel free to copy it and modify at will.</p> <p>For logging we can use the <code>get_writer</code> from the <code>Writer Registry</code>. This will set up the default writer based on the experiment tracking tool. All writers from the <code>Writer Registry</code> offer <code>open</code>, <code>close</code>, <code>print_metrics</code>, <code>write_metrics</code>, <code>plot_metrics</code>, etc methods.</p> <pre><code>from typing import Callable, Union\n\nfrom torch.nn import Module\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom qadence.ml_tools.config import TrainConfig\nfrom qadence.ml_tools.data import DictDataLoader, data_to_device\nfrom qadence.ml_tools.optimize_step import optimize_step\nfrom qadence.ml_tools.callbacks import get_writer\nfrom qadence.ml_tools.callbacks.saveload import load_checkpoint, write_checkpoint\n\n\ndef train(\n    model: Module,\n    data: DataLoader,\n    optimizer: Optimizer,\n    config: TrainConfig,\n    loss_fn: Callable,\n    device: str = \"cpu\",\n    optimize_step: Callable = optimize_step,\n    write_tensorboard: Callable = write_tensorboard,\n) -&gt; tuple[Module, Optimizer]:\n\n    # Move model to device before optimizer is loaded\n    model = model.to(device)\n\n    # load available checkpoint\n    init_iter = 0\n    if config.log_folder:\n        model, optimizer, init_iter = load_checkpoint(config.log_folder, model, optimizer)\n\n    # Initialize writer based on the tracking tool specified in the configuration\n    writer = get_writer(config.tracking_tool)  # Uses ExperimentTrackingTool to select writer\n    writer.open(config, iteration=init_iter)\n\n    dl_iter = iter(dataloader)\n\n    # outer epoch loop\n    for iteration in range(init_iter, init_iter + config.max_iter):\n        data = data_to_device(next(dl_iter), device)\n        loss, metrics = optimize_step(model, optimizer, loss_fn, data)\n\n        if iteration % config.print_every == 0 and config.verbose:\n            writer.print_metrics(OptimizeResult(iteration, model, optimizer, loss, metrics))\n\n        if iteration % config.write_every == 0:\n            writer.write(iteration, metrics)\n\n        if config.log_folder:\n            if iteration % config.checkpoint_every == 0:\n                write_checkpoint(config.log_folder, model, optimizer, iteration)\n\n    # Final writing and checkpointing\n    if config.log_folder:\n        write_checkpoint(config.log_folder, model, optimizer, iteration)\n    writer.write(iteration,metrics)\n    writer.close()\n\n    return model, optimizer\n</code></pre>"},{"location":"tutorials/qml/ml_tools/trainer/#65-gradient-free-optimization-using-trainer","title":"6.5. Gradient-free optimization using <code>Trainer</code>","text":"<p>We can achieve gradient free optimization with <code>Trainer.set_use_grad(False)</code> or <code>trainer.disable_grad_opt(ng_optimizer)</code>. An example solving a QUBO using gradient free optimization based on <code>Nevergrad</code> optimizers and <code>Trainer</code> is shown in the analog QUBO Tutorial.</p>"},{"location":"tutorials/realistic_sims/","title":"Realistic simulations","text":"<p>This section describes how to perform realistic simulations in Qadence.</p>"},{"location":"tutorials/realistic_sims/measurements/","title":"Measurement protocols","text":"<p>Sample-based measurement protocols are fundamental tools for the prediction and estimation of a quantum state as the result of NISQ programs executions. Their resource efficient implementation is a current and active research field. Qadence offers two main measurement protocols: quantum state tomography and classical shadows.</p>"},{"location":"tutorials/realistic_sims/measurements/#quantum-state-tomography","title":"Quantum state tomography","text":"<p>The fundamental task of quantum state tomography is to learn an approximate classical description of an output quantum state described by a density matrix \\(\\rho\\), from repeated measurements of copies on a chosen basis. To do so, \\(\\rho\\) is expanded in a basis of observables (the tomography step) and for a given observable \\(\\hat{\\mathcal{O}}\\), the expectation value is calculated with \\(\\langle \\hat{\\mathcal{O}} \\rangle=\\textrm{Tr}(\\hat{\\mathcal{O}}\\rho)\\). A number of measurement repetitions in a suitable basis is then required to estimate \\(\\langle \\hat{\\mathcal{O}} \\rangle\\).</p> <p>The main drawback is the scaling in measurements for the retrieval of the classical expression for a \\(n\\)-qubit quantum state as \\(2^n \\times 2^n\\), together with a large amount of classical post-processing.</p> <p>For an observable expressed as a Pauli string \\(\\hat{\\mathcal{P}}\\), the expectation value for a state \\(|\\psi \\rangle\\) can be derived as:</p> \\[ \\langle \\hat{\\mathcal{P}} \\rangle=\\langle \\psi | \\hat{\\mathcal{P}} |\\psi \\rangle=\\langle \\psi | \\hat{\\mathcal{R}}^\\dagger \\hat{\\mathcal{D}} \\hat{\\mathcal{R}} |\\psi \\rangle \\] <p>The operator \\(\\hat{\\mathcal{R}}\\) diagonalizes \\(\\hat{\\mathcal{P}}\\) and rotates the state into an eigenstate in the computational basis. Therefore, \\(\\hat{\\mathcal{R}}|\\psi \\rangle=\\sum\\limits_{z}a_z|z\\rangle\\) and the expectation value can finally be expressed as:</p> \\[ \\langle \\hat{\\mathcal{P}} \\rangle=\\sum_{z,z'}\\langle z |\\bar{a}_z\\hat{\\mathcal{D}}a_{z'}|z'\\rangle = \\sum_{z}|a_z|^2(-1)^{\\phi_z(\\hat{\\mathcal{P}})} \\] <p>In Qadence, running a tomographical experiment is made simple by defining a <code>Measurements</code> object that captures all options for execution:</p> <pre><code>from torch import tensor\nfrom qadence import hamiltonian_factory, BackendName, DiffMode\nfrom qadence import Parameter, chain, kron, RX, RY, Z, QuantumCircuit, QuantumModel\nfrom qadence.measurements import Measurements\n\n# Define parameters for a circuit.\ntheta1 = Parameter(\"theta1\", trainable=False)\ntheta2 = Parameter(\"theta2\", trainable=False)\ntheta3 = Parameter(\"theta3\", trainable=False)\ntheta4 = Parameter(\"theta4\", trainable=False)\n\nblocks = chain(\n    kron(RX(0, theta1), RY(1, theta2)),\n    kron(RX(0, theta3), RY(1, theta4)),\n)\n\nvalues = {\n    \"theta1\": tensor([0.5]),\n    \"theta2\": tensor([1.5]),\n    \"theta3\": tensor([2.0]),\n    \"theta4\": tensor([2.5]),\n}\n\n# Create a circuit and an observable.\ncircuit = QuantumCircuit(2, blocks)\nobservable = hamiltonian_factory(2, detuning=Z)\n\n# Create a model.\nmodel = QuantumModel(\n    circuit=circuit,\n    observable=observable,\n    backend=BackendName.PYQTORCH,\n    diff_mode=DiffMode.GPSR,\n)\n\n# Define a measurement protocol by passing the shot budget as an option.\ntomo_options = {\"n_shots\": 100000}\ntomo_measurement = Measurements(protocol=Measurements.TOMOGRAPHY, options=tomo_options)\n\n# Get the exact expectation value.\nexact_values = model.expectation(\n    values=values,\n)\n\n# Run the tomography experiment.\nestimated_values_tomo = model.expectation(\n    values=values,\n    measurement=tomo_measurement,\n)\n</code></pre> <pre><code>Exact expectation value = tensor([[-1.4548]])\nEstimated expectation value tomo = tensor([[-1.4566]])\n</code></pre>"},{"location":"tutorials/realistic_sims/measurements/#classical-shadows","title":"Classical shadows","text":"<p>Recently, a much less resource demanding protocol based on classical shadows has been proposed<sup>1</sup>. It combines ideas from shadow tomography<sup>2</sup> and randomized measurement protocols capable of learning a classical shadow of an unknown quantum state \\(\\rho\\). It relies on deliberately discarding the full classical characterization of the quantum state, and instead focuses on accurately predicting a restricted set of properties that provide efficient protocols for the study of the system.</p> <p>A random measurement consists of applying random unitary rotations before a fixed measurement on each copy of a state. Appropriately averaging over these measurements produces an efficient estimator for the expectation value of an observable. This protocol therefore creates a robust classical representation of the quantum state or classical shadow. The captured measurement information is then reuseable for multiple purposes, i.e. any observable expected value and available for noise mitigation postprocessing.</p> <p>A classical shadow is therefore an unbiased estimator of a quantum state \\(\\rho\\). Such an estimator is obtained with the following procedure<sup>1</sup>: first, apply a random unitary gate \\(U\\) to rotate the state: \\(\\rho \\rightarrow U \\rho U^\\dagger\\) and then perform a basis measurement to obtain a \\(n\\)-bit measurement \\(|\\hat{b}\\rangle \\in \\{0, 1\\}^n\\). Both unitary gates \\(U\\) and the measurement outcomes \\(|\\hat{b}\\rangle\\) are stored on a classical computer for postprocessing v \\(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U\\), a classical snapshot of the state \\(\\rho\\). The whole procedure can be seen as a quantum channel \\(\\mathcal{M}\\) that maps the initial unknown quantum state \\(\\rho\\) to the average result of the measurement protocol:</p> \\[ \\mathbb{E}[U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U] = \\mathcal{M}(\\rho) \\Rightarrow \\rho = \\mathbb{E}[\\mathcal{M}^{-1}(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U)] \\] <p>It is worth noting that the single classical snapshot \\(\\hat{\\rho}=\\mathcal{M}^{-1}(U^\\dagger |\\hat{b}\\rangle\\langle \\hat{b}|U)\\) equals \\(\\rho\\) in expectation: \\(\\mathbb{E}[\\hat{\\rho}]=\\rho\\) despite \\(\\mathcal{M}^{-1}\\) not being a completely positive map. Repeating this procedure \\(N\\) times results in an array of \\(N\\) independent, classical snapshots of \\(\\rho\\) called the classical shadow:</p> \\[ S(\\rho, N) = \\{ \\hat{\\rho}_1=\\mathcal{M}^{-1}(U_1^\\dagger |\\hat{b}_1\\rangle\\langle \\hat{b}_1|U_1),\\cdots,\\hat{\\rho}_N=\\mathcal{M}^{-1}(U_N^\\dagger |\\hat{b}_N\\rangle\\langle \\hat{b}_N|U_N)\\} \\] <p>Along the same lines as the example before, estimating the expectation value using classical shadows in Qadence only requires to pass the right set of parameters to the <code>Measurements</code> object:</p> <pre><code># Classical shadows are defined up to some accuracy and confidence.\nshadow_options = {\"accuracy\": 0.1, \"confidence\": 0.1}  # Shadow size N=54400.\nshadow_measurement = Measurements(protocol=Measurements.SHADOW, options=shadow_options)\n\n# Run the experiment with classical shadows.\nestimated_values_shadow = model.expectation(\n    values=values,\n    measurement=shadow_measurement,\n)\n</code></pre> <pre><code>Estimated expectation value shadow = tensor([[-1.4572]])\n</code></pre>"},{"location":"tutorials/realistic_sims/measurements/#references","title":"References","text":"<ol> <li> <p>Hsin-Yuan Huang, Richard Kueng and John Preskill, Predicting Many Properties of a Quantum System from Very Few Measurements (2020) \u21a9\u21a9</p> </li> <li> <p>S. Aaronson. Shadow tomography of quantum states. In Proceedings of the 50th Annual A ACM SIGACT Symposium on Theory of Computing, STOC 2018, pages 325\u2013338, New York, NY, USA, 2018. ACM\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/realistic_sims/mitigation/","title":"Error mitigation","text":"<p>Beyond running noisy simulations, Qadence offers a number of noise mitigation techniques to achieve better accuracy of simulation outputs. Currently, mitigation addresses readout errors and depolarizing and dephasing noise for analog blocks.</p>"},{"location":"tutorials/realistic_sims/mitigation/#readout-error-mitigation","title":"Readout error mitigation","text":"<p>The complete implementation of the mitigation technique is to measure \\(T\\) and classically apply \\(T^{\u22121}\\) to measured probability distributions. However there are several limitations of this approach:</p> <ul> <li>The complete implementation requires \\(2^n\\) characterization experiments (probability measurements), which is not scalable. The classical processing of the calibration data is also inefficient.</li> <li>The matrix \\(T\\) may become singular for large \\(n\\), preventing direct inversion.</li> <li>The inverse \\(T^{\u22121}\\) might not be a stochastic matrix, meaning that it can produce negative corrected probabilities.</li> <li>The correction is not rigorously justified, so we cannot be sure that we are only removing SPAM errors and not otherwise corrupting an estimated probability distribution.</li> </ul> <p>Qadence relies on the assumption of uncorrelated readout errors:</p> \\[ T=T_1\\otimes T_2\\otimes \\dots \\otimes T_n \\] <p>for which the inversion is straightforward:</p> \\[ T^{-1}=T_1^{-1}\\otimes T_2^{-1}\\otimes \\dots \\otimes T_n^{-1} \\] <p>However, even for a reduced \\(n\\) the third limitation holds. This can be avoided by reformulating into a minimization problem<sup>1</sup>:</p> \\[ \\lVert Tp_{\\textrm{corr}}-p_{\\textrm{raw}}\\rVert_{2}^{2} \\] <p>subjected to physicality constraints \\(0 \\leq p_{corr}(x) \\leq 1\\) and \\(\\lVert p_{corr} \\rVert = 1\\). At this point, two methods are implemented to solve this problem. The first one relies on solving using standard optimization tools, the second on Maximum-Likelihood Estimation<sup>2</sup>. In Qadence, this can be user defined using the mitigation protocol:</p> <pre><code>from qadence import QuantumModel, QuantumCircuit, kron, H, Z\nfrom qadence import hamiltonian_factory\nfrom qadence.noise import NoiseHandler\nfrom qadence.mitigations import Mitigations\nfrom qadence.types import ReadOutOptimization, NoiseProtocol\n\n# Simple circuit and observable construction.\nblock = kron(H(0), Z(1))\ncircuit = QuantumCircuit(2, block)\nobservable = hamiltonian_factory(circuit.n_qubits, detuning=Z)\n\n# Construct a quantum model.\nmodel = QuantumModel(circuit=circuit, observable=observable)\n\n# Define a noise model to use:\nnoise = NoiseHandler(NoiseProtocol.READOUT.INDEPENDENT)\n# Define the mitigation method solving the minimization problem:\noptions={\"optimization_type\": ReadOutOptimization.CONSTRAINED}  # ReadOutOptimization.MLE for the alternative method.\nmitigation = Mitigations(protocol=Mitigations.READOUT, options=options)\n\n# Run noiseless, noisy and mitigated simulations.\nn_shots = 100\nnoiseless_samples = model.sample(n_shots=n_shots)\nnoisy_samples = model.sample(noise=noise, n_shots=n_shots)\nmitigated_samples = model.sample(\n    noise=noise, mitigation=mitigation, n_shots=n_shots\n)\n\nprint(f\"noiseless {noiseless_samples}\")\nprint(f\"noisy {noisy_samples}\")\nprint(f\"mitigated {mitigated_samples}\")\n</code></pre> <pre><code>noiseless [OrderedCounter({'10': 53, '00': 47})]\nnoisy [OrderedCounter({'10': 60, '00': 39, '01': 1})]\nmitigated [Counter({'11': 61, '10': 31, '01': 8})]\n</code></pre>"},{"location":"tutorials/realistic_sims/mitigation/#wip-zero-noise-extrapolation-for-analog-blocks","title":"[WIP] Zero-noise extrapolation for analog blocks","text":"<p>Zero-noise extrapolation (ZNE) is an error mitigation technique in which an expectation value is computed at different noise levels and, as a second step, the ideal expectation value is inferred by extrapolating the measured results to the zero-noise limit. In digital computing, this is typically implemented by \"folding\" the circuit and its dagger to artificially increase the noise through sequences of identities<sup>3</sup>. In the analog ZNE variation, analog blocks are time stretched to again artificially increase noise<sup>3</sup>.</p>"},{"location":"tutorials/realistic_sims/mitigation/#references","title":"References","text":"<ol> <li> <p>Michael R. Geller and Mingyu Sun, Efficient correction of multiqubit measurement errors, (2020) \u21a9</p> </li> <li> <p>Smolin et al., Maximum Likelihood, Minimum Effort, (2011) \u21a9</p> </li> <li> <p>Mitiq: What's the theory behind ZNE? \u21a9\u21a9</p> </li> </ol>"},{"location":"tutorials/realistic_sims/noise/","title":"Simulated errors","text":"<p>Running programs on NISQ devices often leads to partially useful results due to the presence of noise. In order to perform realistic simulations, a number of noise models (for digital operations, analog operations and simulated readout errors) are supported in Qadence through their implementation in backends and corresponding error mitigation techniques whenever possible.</p>"},{"location":"tutorials/realistic_sims/noise/#noisehandler","title":"NoiseHandler","text":"<p>Noise models can be defined via the <code>NoiseHandler</code>. It is a container of several noise instances which require to specify a <code>protocols</code> and a dictionary of <code>options</code> (or lists). The <code>protocol</code> field is to be instantiated from <code>NoiseProtocol</code>.</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\nanalog_noise = NoiseHandler(protocol=NoiseProtocol.ANALOG.DEPOLARIZING, options={\"noise_probs\": 0.1})\ndigital_noise = NoiseHandler(protocol=NoiseProtocol.DIGITAL.DEPOLARIZING, options={\"error_probability\": 0.1})\nreadout_noise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options={\"error_probability\": 0.1, \"seed\": 0})\n</code></pre> <pre><code>\n</code></pre> <p>One can also define a <code>NoiseHandler</code> passing a list of protocols and a list of options (careful with the order):</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\nprotocols = [NoiseProtocol.DIGITAL.DEPOLARIZING, NoiseProtocol.READOUT]\noptions = [{\"error_probability\": 0.1}, {\"error_probability\": 0.1, \"seed\": 0}]\n\nnoise_combination = NoiseHandler(protocols, options)\nprint(noise_combination)\n</code></pre> <pre><code>Noise(Depolarizing, {'error_probability': 0.1})\nNoise(&lt;enum 'ReadoutNoise'&gt;, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>One can also append to a <code>NoiseHandler</code> other <code>NoiseHandler</code> instances:</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\n\ndepo_noise = NoiseHandler(protocol=NoiseProtocol.DIGITAL.DEPOLARIZING, options={\"error_probability\": 0.1})\nreadout_noise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options={\"error_probability\": 0.1, \"seed\": 0})\n\nnoise_combination = NoiseHandler(protocol=NoiseProtocol.DIGITAL.BITFLIP, options={\"error_probability\": 0.1})\nnoise_combination.append([depo_noise, readout_noise])\nprint(noise_combination)\n</code></pre> <pre><code>Noise(BitFlip, {'error_probability': 0.1})\nNoise(Depolarizing, {'error_probability': 0.1})\nNoise(Independent Readout, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>Finally, one can add directly a few pre-defined types using several <code>NoiseHandler</code> methods:</p> <pre><code>from qadence import NoiseHandler\nfrom qadence.types import NoiseProtocol\nnoise_combination = NoiseHandler(protocol=NoiseProtocol.DIGITAL.BITFLIP, options={\"error_probability\": 0.1})\nnoise_combination.digital_depolarizing({\"error_probability\": 0.1}).readout_independent({\"error_probability\": 0.1, \"seed\": 0})\nprint(noise_combination)\n</code></pre> <pre><code>Noise(BitFlip, {'error_probability': 0.1})\nNoise(Depolarizing, {'error_probability': 0.1})\nNoise(Independent Readout, {'error_probability': 0.1, 'seed': 0})\n</code></pre> <p>NoiseHandler scope</p> <p>Note it is not possible to define a <code>NoiseHandler</code> instances with both digital and analog noises, both readout and analog noises, several analog noises, several readout noises, or a readout noise that is not the last defined protocol within <code>NoiseHandler</code>.</p>"},{"location":"tutorials/realistic_sims/noise/#readout-errors","title":"Readout errors","text":"<p>State Preparation and Measurement (SPAM) in the hardware is a major source of noise in the execution of quantum programs. They are typically described using confusion matrices of the form:</p> \\[ T(x|x')=\\delta_{xx'} \\] <p>Two types of readout protocols are available: - <code>NoiseProtocol.READOUT.INDEPENDENT</code> where each bit can be corrupted independently of each other. - <code>NoiseProtocol.READOUT.CORRELATED</code> where we can define of confusion matrix of corruption between each possible bitstrings.</p> <p>Qadence offers to simulate readout errors with the <code>NoiseHandler</code> to corrupt the output samples of a simulation, through execution via a <code>QuantumModel</code>:</p> <pre><code>from qadence import QuantumModel, QuantumCircuit, kron, H, Z\nfrom qadence import hamiltonian_factory\n\n# Simple circuit and observable construction.\nblock = kron(H(0), Z(1))\ncircuit = QuantumCircuit(2, block)\nobservable = hamiltonian_factory(circuit.n_qubits, detuning=Z)\n\n# Construct a quantum model.\nmodel = QuantumModel(circuit=circuit, observable=observable)\n\n# Define a noise model to use.\nnoise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT)\n\n# Run noiseless and noisy simulations.\nnoiseless_samples = model.sample(n_shots=100)\nnoisy_samples = model.sample(noise=noise, n_shots=100)\n</code></pre> <pre><code>noiseless = [OrderedCounter({'10': 53, '00': 47})]\nnoisy = [OrderedCounter({'10': 52, '00': 40, '01': 5, '11': 3})]\n</code></pre> <p>It is possible to pass options to the noise model. In the previous example, a noise matrix is implicitly computed from a uniform distribution.</p> <p>For <code>NoiseProtocol.READOUT.INDEPENDENT</code>, the <code>option</code> dictionary argument accepts the following options:</p> <ul> <li><code>seed</code>: defaulted to <code>None</code>, for reproducibility purposes</li> <li><code>error_probability</code>: If float, the same probability is applied to every bit. By default, this is 0.1.     If a 1D tensor with the number of elements equal to the number of qubits, a different probability can be set for each qubit. If a tensor of shape (n_qubits, 2, 2) is passed, that is a confusion matrix obtained from experiments, we extract the error_probability.     and do not compute internally the confusion matrix as in the other cases.</li> <li><code>noise_distribution</code>: defaulted to <code>WhiteNoise.UNIFORM</code>, for non-uniform noise distributions</li> </ul> <p>For <code>NoiseProtocol.READOUT.CORRELATED</code>, the <code>option</code> dictionary argument accepts the following options: - <code>confusion_matrix</code>: The square matrix representing \\(T(x|x')\\) for each possible bitstring of length <code>n</code> qubits. Should be of size (2n, 2n). - <code>seed</code>: defaulted to <code>None</code>, for reproducibility purposes</p> <p>Noisy simulations go hand-in-hand with measurement protocols discussed in the previous section, to assess the impact of noise on expectation values. In this case, both measurement and noise protocols have to be defined appropriately. Please note that a noise protocol without a measurement protocol will be ignored for expectation values computations.</p> <pre><code>from qadence.measurements import Measurements\n\n# Define a noise model with options.\noptions = {\"error_probability\": 0.01}\nnoise = NoiseHandler(protocol=NoiseProtocol.READOUT.INDEPENDENT, options=options)\n\n# Define a tomographical measurement protocol with options.\noptions = {\"n_shots\": 10000}\nmeasurement = Measurements(protocol=Measurements.TOMOGRAPHY, options=options)\n\n# Run noiseless and noisy simulations.\nnoiseless_exp = model.expectation(measurement=measurement)\nnoisy_exp = model.expectation(measurement=measurement, noise=noise)\n</code></pre> <pre><code>noiseless = tensor([[0.9998]], grad_fn=&lt;TransposeBackward0&gt;)\nnoisy = tensor([[0.9972]], grad_fn=&lt;TransposeBackward0&gt;)\n</code></pre>"},{"location":"tutorials/realistic_sims/noise/#analog-noisy-simulation","title":"Analog noisy simulation","text":"<p>At the moment, analog noisy simulations are only compatable with the Pulser backend. <pre><code>from qadence import DiffMode, NoiseHandler, QuantumModel\nfrom qadence.blocks import chain, kron\nfrom qadence.circuit import QuantumCircuit\nfrom qadence.operations import AnalogRX, AnalogRZ, Z\nfrom qadence.types import PI, BackendName, NoiseProtocol\n\n\nanalog_block = chain(AnalogRX(PI / 2.0), AnalogRZ(PI))\nobservable = Z(0) + Z(1)\ncircuit = QuantumCircuit(2, analog_block)\n\noptions = {\"noise_probs\": 0.1}\nnoise = NoiseHandler(protocol=NoiseProtocol.ANALOG.DEPOLARIZING, options=options)\nmodel_noisy = QuantumModel(\n    circuit=circuit,\n    observable=observable,\n    backend=BackendName.PULSER,\n    diff_mode=DiffMode.GPSR,\n    noise=noise,\n)\nnoisy_expectation = model_noisy.expectation()\n</code></pre> <pre><code>noisy = tensor([[0.3597]])\n</code></pre> </p>"},{"location":"tutorials/realistic_sims/noise/#digital-noisy-simulation","title":"Digital noisy simulation","text":"<p>When dealing with programs involving only digital operations, several options are made available from PyQTorch via the <code>NoiseProtocol.DIGITAL</code>. One can define noisy digital operations as follows:</p> <pre><code>from qadence import NoiseProtocol, RX, run\nimport torch\n\nnoise = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, {\"error_probability\": 0.2})\nop = RX(0, torch.pi, noise = noise)\n\nprint(run(op))\n</code></pre> <pre><code>DensityMatrix([[[0.2000+0.0000e+00j, 0.0000+3.6739e-17j],\n                [0.0000-3.6739e-17j, 0.8000+0.0000e+00j]]])\n</code></pre> <p>It is also possible to set a noise configuration to gates within a composite block or circuit as follows:</p> <pre><code>from qadence import set_noise, chain\n\nn_qubits = 2\n\nblock = chain(RX(i, f\"theta_{i}\") for i in range(n_qubits))\n\nnoise = NoiseHandler(NoiseProtocol.DIGITAL.BITFLIP, {\"error_probability\": 0.1})\n\n# The function changes the block in place:\nset_noise(block, noise)\nprint(run(block))\n</code></pre> <pre><code>DensityMatrix([[[ 0.6710+0.0000j,  0.0000+0.1450j,  0.0000+0.2399j,\n                 -0.0518+0.0000j],\n                [ 0.0000-0.1450j,  0.1140+0.0000j,  0.0518+0.0000j,\n                  0.0000+0.0407j],\n                [ 0.0000-0.2399j,  0.0518+0.0000j,  0.1838+0.0000j,\n                  0.0000+0.0397j],\n                [-0.0518+0.0000j,  0.0000-0.0407j,  0.0000-0.0397j,\n                  0.0312+0.0000j]]])\n</code></pre> <p>There is an extra optional argument to specify the type of block we want to apply noise to. E.g., let's say we want to apply noise only to <code>X</code> gates, a <code>target_class</code> argument can be passed with the corresponding block:</p> <pre><code>from qadence import X\nblock = chain(RX(0, \"theta\"), X(0))\nset_noise(block, noise, target_class = X)\n\nfor block in block.blocks:\n    print(block.noise)\n</code></pre> <pre><code>None\nNoise(BitFlip, {'error_probability': 0.1})\n</code></pre>"}]}